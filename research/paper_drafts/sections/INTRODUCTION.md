# 1. Introduction

Robotic manipulation of everyday objects remains one of the grand challenges of embodied artificial intelligence. Tasks that humans perform effortlessly — pushing a mug across a table, sorting blocks by color, rearranging cluttered shelves — demand that a robot reason about rigid-body contacts, friction, multi-body interactions, and the consequences of its own applied forces, all under uncertainty and partial observability. The contact-rich nature of manipulation introduces discontinuous dynamics: contact events create and destroy constraints instantaneously, and the combinatorial explosion of possible contact configurations makes analytical solutions intractable for all but the simplest geometries [27]. As the number of objects in the scene grows, the state space scales combinatorially, and the space of physically plausible interaction patterns grows with it. These properties make manipulation a uniquely challenging domain for learning-based control.

Modern model-free reinforcement learning (RL) algorithms — notably Proximal Policy Optimization (PPO) [24] and Soft Actor-Critic (SAC) [25] — have achieved impressive results on locomotion and single-object manipulation benchmarks when paired with massively parallel simulation [27]. However, their sample complexity remains a fundamental bottleneck. Training a PPO policy to push a single box to a goal location typically requires $10^5$–$10^6$ environment steps; scaling to multi-object scenes where contact topologies change dynamically can push this figure to $10^7$–$10^8$ steps. The root cause is architectural: standard multi-layer perceptron (MLP) policies treat observations as flat vectors and must rediscover, from reward signal alone, the physical regularities that govern how forces propagate through interacting bodies. Every conservation law, every symmetry, every constraint that Newton codified three centuries ago must be re-learned from scratch by each randomly initialized network. This is not merely inefficient — it produces policies that are brittle to distributional shift. A policy trained to push a 0.5 kg box fails catastrophically when the mass changes to 2.0 kg, because its implicit dynamics model was overfit to the training distribution rather than grounded in physical structure.

A natural remedy is to introduce *physical structure as an inductive bias*. The key insight is that the dynamics of multi-body manipulation are governed by a small set of universal laws: Newton's second law ($\mathbf{F} = m\mathbf{a}$), Newton's third law ($\mathbf{F}_{ij} = -\mathbf{F}_{ji}$), conservation of linear momentum for closed subsystems, and the work-energy theorem relating forces to kinetic energy changes. These laws constrain the space of physically realizable dynamics to a low-dimensional manifold within the space of all possible state transitions. If a policy network can be designed so that its internal representations respect these constraints, the RL optimizer need only search within this physically consistent subspace — dramatically reducing the effective hypothesis space and accelerating learning.

Several lines of work have explored physics-informed architectures, but none fully addresses the manipulation policy setting. **Hamiltonian Neural Networks (HNNs)** [2] and **Lagrangian Neural Networks (LNNs)** [3] encode energy conservation by parameterizing the Hamiltonian or Lagrangian and deriving dynamics via canonical equations. While elegant, these methods assume *conservative systems* — a condition violated by the friction, damping, and external actuation that are ubiquitous in manipulation. **Deep Lagrangian Networks (DeLaN)** [4] extend the Lagrangian framework to articulated robots but focus on system identification rather than policy learning. On the graph neural network (GNN) front, **E(n)-Equivariant GNNs (EGNNs)** [7] preserve rotational symmetry but produce only *radial* position updates, precluding tangential forces such as friction. **PaiNN** [15] maintains parallel scalar and vector channels with greater expressivity, yet provides no conservation guarantees and was not designed for control. **Graph Network Simulators (GNS)** [11] learn accurate forward dynamics via message passing but operate as *world models*, not policy networks — the physics structure informs prediction, not action selection. More fundamentally, none of these architectures guarantees that the *internal force-like messages* obey Newton's third law for arbitrary network parameters: they may learn approximate conservation with sufficient data, but violate it freely during early training when the RL signal is most needed. The distinction between *soft constraints* (physics-informed losses that penalize violations) and *hard constraints* (architectural guarantees that hold for any parameter setting) is critical: soft constraints can be overridden by a strong RL gradient, while hard constraints hold by construction, providing a reliable inductive bias throughout training.

In this paper, we propose **PhysRobot**, a dual-stream reinforcement learning architecture that embeds hard momentum-conservation constraints into a GNN policy for contact-rich multi-object manipulation. PhysRobot introduces three key ideas. First, we construct a **dynamic scene graph** from the manipulation workspace, where nodes represent rigid bodies (end-effector, objects) and edges encode spatial proximity and potential contact. Second, we design a **Scalarization–Vectorization (SV) message-passing** mechanism that computes inter-body force-like messages in edge-local coordinate frames. By processing each undirected body pair exactly once and assigning equal-and-opposite forces ($+\mathbf{F}$ and $-\mathbf{F}$), the architecture guarantees $\sum_i \mathbf{F}_i = \mathbf{0}$ for *any* network parameters — a hard constraint encoding Newton's third law that holds from random initialization through convergence. The SV pipeline operates on rotation-invariant scalar features (distances, projected velocities) and reconstructs 3D force vectors via a learned linear combination of frame basis vectors, yielding E(3)-equivariant message passing without expensive spherical harmonics. Third, we integrate this **physics stream** with a standard **policy stream** (MLP) via stop-gradient fusion, and train the physics stream with an auxiliary self-supervised dynamics loss using finite-difference accelerations from RL rollouts — requiring no ground-truth force labels or differentiable simulator.

Our main contributions are as follows:

- **Architecture.** We present PhysRobot, the first GNN-based RL policy that provides *hard* linear momentum conservation guarantees through its Scalarization–Vectorization message-passing pipeline. Unlike prior physics-informed architectures (HNN, LNN) that assume conservative systems, PhysRobot handles non-conservative forces (friction, damping, external actuation) natively, and unlike soft-constraint approaches, the conservation property holds for *any* network parameters — not just at convergence.

- **Algorithm.** We introduce a dual-stream architecture with stop-gradient fusion that decouples physics representation learning from policy optimization. The physics stream is trained via a self-supervised auxiliary loss on finite-difference accelerations, requiring only standard RL rollout data. This design prevents the RL policy gradient from distorting the physics stream's learned dynamics while allowing the policy to benefit from physically-grounded features.

- **Empirical validation.** We demonstrate that PhysRobot achieves 2–5$\times$ sample efficiency improvements over PPO and SAC on contact-rich manipulation benchmarks (PushBox, MultiPush, Sort), maintains competitive asymptotic performance, and generalizes to out-of-distribution physical parameters (mass $0.1\times$–$10\times$, friction $0.1$–$1.0$) and unseen object counts (train on 3, test on 5–10) with graceful performance degradation where baseline methods fail entirely.

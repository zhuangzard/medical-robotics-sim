# PhysRobot: Physics-Informed Graph Neural Networks for Sample-Efficient Robot Manipulation

---

## Abstract

Reinforcement learning (RL) holds immense promise for autonomous robot manipulation, yet its notorious sample inefficiency — typically requiring $10^6$–$10^8$ environment interactions — remains a critical barrier to real-world deployment. We observe that the physical laws governing contact-rich manipulation, particularly Newton's third law of momentum conservation, constitute a powerful but underexploited inductive bias for policy learning. We introduce **PhysRobot**, a dual-stream architecture that integrates a physics-informed graph neural network (GNN) with Proximal Policy Optimization (PPO). The physics stream employs a novel **Scalarization–Vectorization (SV) message-passing** pipeline that decomposes interaction forces into edge-local frames, processes them as rotation-invariant scalars, and reconstructs equivariant force vectors with **hard-constraint momentum conservation guaranteed by construction** — each undirected pair contributes equal and opposite forces $+\mathbf{F}$ and $-\mathbf{F}$ to its endpoints. The physics stream's predicted accelerations are fused with a standard policy stream via stop-gradient concatenation, providing physically grounded features without distorting the dynamics model through RL gradients. On a planar push-box manipulation benchmark, PhysRobot achieves [X]\% success rate with [Y]$\times$ fewer samples than a comparable MLP-PPO baseline, and demonstrates strong zero-shot out-of-distribution (OOD) generalization across 10$\times$ mass variation and 5$\times$ friction variation. These results demonstrate that encoding conservation-law structure directly into the policy architecture yields substantial and robust improvements in both sample efficiency and generalization for contact-rich robotic manipulation.

---

## 1. Introduction

Robotic manipulation — the ability to purposefully rearrange objects in the physical world — is a foundational capability for autonomous systems. From warehouse logistics and surgical assistance to household tasks and planetary exploration, the capacity to push, grasp, and reorient objects with dexterity underpins an enormous range of applications [27, 32]. Unlike locomotion or navigation, manipulation is inherently *interactive*: the robot must reason about how its actions propagate through contact to produce desired outcomes in external objects. This makes manipulation simultaneously one of the most impactful and most challenging domains in robotics.

Reinforcement learning has emerged as a compelling paradigm for acquiring manipulation skills, offering the promise of learning complex, contact-rich behaviors directly from interaction data without hand-engineered controllers [24, 25]. Recent advances in simulation [27], algorithm design [28, 31], and foundation models [32, 33, 34] have produced impressive demonstrations — from dexterous in-hand reorientation to multi-step assembly. However, a persistent and fundamental bottleneck remains: **sample inefficiency**. State-of-the-art model-free methods typically require $10^6$–$10^8$ environment steps to converge on contact-rich tasks [24, 26], and even model-based approaches that learn world models in latent space [28] demand hundreds of thousands of interactions. This sample complexity is not merely an inconvenience — it renders direct sim-to-real transfer fragile, amplifies compounding errors in learned dynamics, and precludes rapid adaptation to novel objects or task variations. For robotic manipulation to scale beyond carefully engineered laboratory settings, a qualitative reduction in sample complexity is needed.

We argue that the physics of multi-body contact provides a rich, largely untapped source of inductive bias for policy learning. Objects undergoing manipulation obey strict conservation laws: linear momentum is conserved in the absence of external forces; energy is bounded by the work done; contact forces are governed by Newton's third law of equal and opposite reaction. These invariances dramatically constrain the space of *physically plausible* interactions. A policy whose internal representations respect these constraints should require far fewer samples to discover effective strategies, because its hypothesis space is, by construction, restricted to the manifold of physically consistent behaviors. This insight is well-established in physics-informed machine learning for *prediction* tasks [1, 2, 3, 7, 11], but has received surprisingly little attention in the context of *policy architecture* for active control.

Existing physics-informed approaches for dynamical systems, while impressive in their respective domains, face fundamental limitations when applied to manipulation RL. Hamiltonian Neural Networks (HNNs) [2] and Lagrangian Neural Networks (LNNs) [3] guarantee energy conservation by parameterizing the system's Hamiltonian or Lagrangian and deriving dynamics via canonical equations. However, they assume *conservative* systems — an assumption violated by the dissipative friction and impulsive contacts that dominate manipulation. Deep Lagrangian Networks (DeLaN) [4] extend the Lagrangian formalism to articulated rigid bodies but remain limited to *forward dynamics prediction* rather than policy learning. Equivariant Graph Neural Networks (EGNNs) [7] and their extensions [8, 16, 17] enforce geometric symmetries (rotation, translation equivariance) in message passing, achieving remarkable data efficiency in molecular simulation. Yet these architectures have not been integrated into RL pipelines; they model passive physical evolution and lack the dual-stream structure needed to combine physics reasoning with action generation. Graph Network Simulators (GNS) [11] learn interaction dynamics through message passing but encode only a *soft relational inductive bias* — they do not guarantee that learned forces satisfy Newton's third law, and their integration with policy optimization remains ad hoc [23].

In this work, we introduce **PhysRobot**, a physics-informed GNN architecture for sample-efficient manipulation RL that addresses these limitations. The core innovation is a **Scalarization–Vectorization (SV) message-passing pipeline** that embeds hard-constraint momentum conservation directly into the policy network's architecture. For each undirected pair of interacting bodies $(i, j)$, the SV pipeline: (1) constructs an edge-local orthonormal frame $\{\mathbf{e}_1^{ij}, \mathbf{e}_2^{ij}, \mathbf{e}_3^{ij}\}$ from relative displacement and velocity; (2) projects relational features into rotation-invariant scalars $\boldsymbol{\sigma}_{ij} = (d_{ij}, v_r, v_t, v_b, \|\dot{\mathbf{x}}_{ij}\|) \in \mathbb{R}^5$; (3) predicts scalar force coefficients $(\alpha_1, \alpha_2, \alpha_3)$ via an MLP operating on these invariants and symmetrized node embeddings; and (4) reconstructs the 3D force vector $\mathbf{F}_{ij} = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3$, assigning $+\mathbf{F}_{ij}$ to node $j$ and $-\mathbf{F}_{ij}$ to node $i$. This undirected-pair processing guarantees $\sum_i \mathbf{F}_i = \mathbf{0}$ *by construction* — Newton's third law is not learned but architecturally enforced. The physics stream's output (predicted object accelerations) is fused with a conventional policy stream via stop-gradient concatenation, providing physically grounded context features for action selection without allowing RL gradients to corrupt the dynamics model. An auxiliary physics loss based on finite-difference accelerations further trains the physics stream to produce accurate predictions, with a linear warmup schedule to stabilize early training.

Our contributions are as follows:

1. **First hard-constraint conservation-law GNN for manipulation RL.** To our knowledge, PhysRobot is the first architecture that embeds momentum conservation as an *architectural invariant* — not a soft loss penalty — within a policy network for robotic manipulation. This eliminates an entire class of physically implausible interaction hypotheses from the policy's representational space.

2. **SV message passing with architectural $\pm\mathbf{F}$ guarantee.** We propose the Scalarization–Vectorization pipeline, which processes each interacting pair exactly once and assigns equal-and-opposite forces by construction. Unlike directed-edge approaches that require careful antisymmetrization of scalar coefficients (and can fail silently if symmetry conditions are violated), our undirected-pair formulation is correct by design and verified empirically across 100 random graph configurations.

3. **Strong OOD generalization from structural priors.** We demonstrate that PhysRobot, trained on a single physical configuration (box mass $m = 0.5$ kg, friction $\mu = 0.5$), generalizes zero-shot to box masses spanning 0.1–5.0 kg ($10\times$ variation) and friction coefficients spanning 0.1–1.0 ($5\times$ variation), significantly outperforming both MLP-PPO and GNS-PPO baselines on out-of-distribution evaluation. This generalization emerges directly from the conservation-law structure, which remains valid regardless of specific physical parameters.

---

## 2. Related Work

Our work lies at the intersection of physics-informed machine learning, graph neural networks for physical reasoning, and reinforcement learning for manipulation. We review each strand and position PhysRobot relative to prior art.

### 2.1 Physics-Informed Machine Learning

Integrating physical laws into neural architectures has proven a powerful paradigm for improving data efficiency and generalization. Physics-Informed Neural Networks (PINNs) [1] embed PDEs as soft constraints in the loss, enabling forward and inverse solutions with limited data; however, they require explicit PDE knowledge and target *prediction*, not *control*. Hamiltonian Neural Networks [2] and Lagrangian Neural Networks [3] encode energy conservation by parameterizing the Hamiltonian or Lagrangian and deriving dynamics via canonical equations, guaranteeing conservation by construction. Deep Lagrangian Networks (DeLaN) [4] extend this to articulated rigid bodies, learning mass-matrix, Coriolis, and gravitational terms with as few as 100 trajectories. Dissipative SymODEN [5] and Port-Hamiltonian Neural Networks [6] further generalize to dissipative systems. On the symmetry front, E(n)-Equivariant GNNs (EGNNs) [7] enforce rotational and translational equivariance in message passing, extended to SE(3) by SEGNN [8] and to higher orders by recent work [9]. Villar et al. [10] provide a theoretical framework for understanding when symmetry enforcement is most beneficial.

**Key distinction.** All these methods target *forward dynamics modeling*. PhysRobot is, to our knowledge, the first to embed conservation-law structure directly into a *policy network* for active manipulation.

### 2.2 Graph Neural Networks for Physical Systems

GNNs provide a natural substrate for modeling interacting physical systems. The Graph Network Simulator (GNS) [11] demonstrated that learned message passing over particle graphs can simulate fluids, rigid bodies, and deformables with strong generalization. Follow-up work extended this to meshes [12] and hard-constraint projection [13]. In molecular simulation, directional message passing (DimeNet [14]), equivariant scalar–vector features (PaiNN [15]), E(3)-equivariant convolutions (NequIP [16]), higher body-order descriptors (MACE [17]), and scalable equivariant architectures (Allegro [18], eSCN [19], EquiformerV2 [20]) have pushed accuracy–efficiency frontiers for interatomic force fields. For macroscopic multi-body systems, Neural Relational Inference [22] learns interaction graphs from trajectories, and Interaction Networks [23] introduced compositional relational reasoning for physics prediction.

**Key distinction.** These architectures model *passive* physical evolution. PhysRobot adapts graph-based physics reasoning for *active control*, where the GNN informs a policy that will act upon the system, and conservation structure ensures physically consistent internal representations.

### 2.3 Reinforcement Learning for Manipulation

Model-free RL — particularly PPO [24], SAC [25], and TD3 [26] — has achieved notable manipulation results when combined with massively parallel simulation [27], but typically requires $10^6$–$10^8$ steps. Model-based approaches improve sample efficiency: Dreamer v3 [28] learns latent world models and trains policies in imagination; DayDreamer [30] demonstrated real-robot learning with Dreamer; TD-MPC2 [31] unified temporal-difference learning with model-predictive control across 80+ tasks. Foundation-model approaches — RT-2 [32], Octo [33], $\pi_0$ [34] — achieve generalization through internet-scale pre-training rather than structural priors.

**Key distinction.** PhysRobot is complementary to both model-based and foundation-model paradigms. Unlike model-based methods that encode physics in the *world model*, we encode it in the *policy*. Unlike foundation models that rely on massive data, we achieve efficiency through *structural inductive bias*. The two strategies are, in principle, composable.

### 2.4 Physics Priors in Reinforcement Learning

The works closest to ours combine physics knowledge with RL. PhyDNet [35] integrates physical dynamics priors into video prediction but does not address control. Heiden et al. [36] use differentiable physics simulators as MBRL world models, embedding physics in the *simulator* rather than the *policy*. Analytical Policy Gradients [37] compute exact gradients through differentiable simulation, bypassing RL but requiring a differentiable simulator. Physics-Informed Neural Operator RL [38] uses neural operators with physics constraints as world models for fluid control tasks. Symmetry-Aware RL [39] exploits known MDP symmetries to reduce the effective state–action space, demonstrating 2–5$\times$ speedups, but without conservation-law constraints. Structured World Models [40] learn object-centric relational dynamics for improved planning.

**PhysRobot's unique position.** We are the first to embed conservation laws — specifically, hard-constraint momentum conservation via architectural $\pm\mathbf{F}$ assignment — within a GNN *policy network* for manipulation RL. This is orthogonal to physics-informed world models and differentiable simulators, requires no large-scale pre-training, and encodes only the *structural form* of conservation laws rather than explicit governing equations.

---

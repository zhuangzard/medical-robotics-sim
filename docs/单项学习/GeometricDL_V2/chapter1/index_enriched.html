<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 1: Introduction | GDL 学习指南</title>
  <link rel="stylesheet" href="../assets/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Noto+Serif+SC:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]})"></script>
  <style>
    .enrichment-block {
      margin: 2rem 0;
      padding: 1.5rem;
      background: linear-gradient(135deg, #f0f7ff 0%, #e8f4fd 100%);
      border-left: 4px solid #3b82f6;
      border-radius: 0 12px 12px 0;
    }
    [data-theme="dark"] .enrichment-block {
      background: linear-gradient(135deg, #1a2332 0%, #1e293b 100%);
    }
    .enrichment-qa { margin-bottom: 1.5rem; }
    .qa-pair { margin: 1rem 0; padding: 1rem; background: rgba(255,255,255,0.5); border-radius: 8px; }
    [data-theme="dark"] .qa-pair { background: rgba(0,0,0,0.2); }
    .question { font-weight: 600; color: #2563eb; margin-bottom: 0.5rem; }
    [data-theme="dark"] .question { color: #60a5fa; }
    .answer { line-height: 1.8; }
    .enrichment-intuition { margin: 1rem 0; padding: 1rem; background: rgba(251,191,36,0.1); border-radius: 8px; border-left: 3px solid #f59e0b; }
    .enrichment-application { margin: 1rem 0; padding: 1rem; background: rgba(16,185,129,0.1); border-radius: 8px; border-left: 3px solid #10b981; }
  </style>
</head>
<body>
  <div class="progress-bar"></div>

  <header class="header">
    <div class="header-title"><a href="../index.html">📐 GDL 学习指南</a></div>
    <div class="header-nav">
      <a href="../index.html">目录</a>
      <a href="../chapter2/index.html">下一章 →</a>
      <button class="theme-toggle" onclick="toggleTheme()">🌙</button>
    </div>
  </header>

  <button class="sidebar-toggle" onclick="toggleSidebar()">☰</button>

  <nav class="sidebar">
    <h3>Chapter 1</h3>
    <a href="#overview">概述</a>
    <a href="#why-gdl">为什么需要 GDL？</a>
    <a href="#revolution" class="sub">深度学习革命</a>
    <a href="#beyond-euclidean" class="sub">超越欧几里得</a>
    <a href="#symmetry-principle" class="sub">对称性原则</a>
    <a href="#two-principles">两个核心算法原则</a>
    <a href="#representation-learning" class="sub">表征学习</a>
    <a href="#gradient-descent" class="sub">梯度下降</a>
    <a href="#curse-overview" class="sub">维度灾难预览</a>
    <a href="#5g-framework">5G 统一框架</a>
    <a href="#grids" class="sub">Grids (网格)</a>
    <a href="#groups" class="sub">Groups (群)</a>
    <a href="#graphs" class="sub">Graphs (图)</a>
    <a href="#geodesics" class="sub">Geodesics (测地线)</a>
    <a href="#gauges" class="sub">Gauges (规范)</a>
    <a href="#erlangen">Erlangen Program</a>
    <a href="#erlangen-history" class="sub">历史背景</a>
    <a href="#erlangen-ml" class="sub">对机器学习的启示</a>
    <a href="#architectures">架构景观</a>
    <a href="#cnn-intro" class="sub">CNN 家族</a>
    <a href="#rnn-intro" class="sub">RNN 家族</a>
    <a href="#gnn-intro" class="sub">GNN 家族</a>
    <a href="#transformer-intro" class="sub">Transformer</a>
    <a href="#scope">本书范围与非范围</a>
    <a href="#roadmap">阅读路线图</a>
    <a href="#roadmap-ch2" class="sub">Ch2: 高维学习</a>
    <a href="#roadmap-ch3" class="sub">Ch3: 几何先验</a>
    <a href="#roadmap-ch4" class="sub">Ch4: 几何域</a>
    <a href="#roadmap-ch5" class="sub">Ch5: GDL 模型</a>
    <a href="#roadmap-ch6" class="sub">Ch6: 应用</a>
    <a href="#roadmap-ch7" class="sub">Ch7: 历史视角</a>
    <a href="#physrobot">与 PhysRobot 的关联</a>
    <a href="#code-setup">代码环境准备</a>
    <a href="#exercises">练习题</a>
    <h3>导航</h3>
    <a href="../index.html">📚 总目录</a>
    <a href="../chapter2/index.html">→ Ch.2 高维学习</a>
  </nav>

  <main class="main">
    <h1>Chapter 1: Introduction<br><span style="font-size:0.6em;color:var(--text-secondary)">引言 — 几何深度学习的统一视角</span></h1>

    <div class="callout callout-info" id="overview">
      <h4>本章概述</h4>
      <p>本章是全书的<strong>起点</strong>，建立了几何深度学习（Geometric Deep Learning, GDL）的宏观视角。我们将回答以下核心问题：</p>
      <ul>
        <li><strong>为什么</strong>几何深度学习如此重要？</li>
        <li><strong>什么是</strong>深度学习中的"几何统一"？</li>
        <li><strong>5G 框架</strong>如何统一 CNN、RNN、GNN 和 Transformer？</li>
        <li>Erlangen Program 的精神如何指导架构设计？</li>
        <li><strong>阅读路线图</strong>：本书各章如何串联？</li>
      </ul>
      <p><strong>预计阅读时间</strong>：1.5 小时 &nbsp;|&nbsp; <strong>先修知识</strong>：基础线性代数、微积分、Python</p>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>🔍 深入理解</h4>
        
        <div class="qa-pair">
          <p class="question">❓ 小白提问：为什么要学习"几何深度学习"？它和普通的深度学习有什么本质区别？</p>
          <div class="answer">
            <p>💡 专家解答：这是一个非常好的问题！首先要澄清一个常见的误解：<strong>几何深度学习不是一种新的深度学习分支，而是一个理解所有深度学习的统一框架</strong>。</p>
            <p>想象你在学习动物学。最初你可能分别学习"猫"、"狗"、"鲸鱼"、"蝙蝠"，觉得它们完全不同。但当你学习到"脊椎动物"这个概念时，突然发现它们都是同一个生物学蓝图的不同实例——它们都有脊椎、四肢（或退化的四肢）、相似的器官系统。</p>
            <p>GDL 做的就是这件事：它揭示了 CNN、RNN、GNN、Transformer 这些看似不同的架构，实际上都是<strong>同一个几何原则的不同实例</strong>——它们都在利用数据底层的<strong>对称性</strong>和<strong>结构</strong>。</p>
            <p>具体区别在于：</p>
            <ul>
              <li><strong>普通深度学习课程</strong>：教你如何使用 CNN 处理图像、用 RNN 处理序列、用 Transformer 做 NLP，但不告诉你"为什么这些架构有效"以及"它们背后的统一原理是什么"</li>
              <li><strong>几何深度学习</strong>：从第一性原理出发，告诉你"如果数据有某种对称性（如图像的平移不变性），那么网络架构就应该具有相应的等变性（如卷积）"，并且这套逻辑适用于<em>所有</em>架构</li>
            </ul>
            <p>学习 GDL 的最大好处是：当你面对一个全新的问题时，你能<strong>从数据的几何结构出发，推导出应该使用什么架构</strong>，而不是靠经验或试错。这就像有了一张地图，而不是在黑暗中摸索。</p>
          </div>
        </div>

        <div class="qa-pair">
          <p class="question">❓ 小白提问：书中说这是一个"统一框架"，但为什么深度学习需要统一？现在的架构不是都工作得很好吗？</p>
          <div class="answer">
            <p>💡 专家解答：你提出了一个非常深刻的问题！确实，从实用主义角度看，现有架构已经很成功了。但"统一"的价值不仅仅是理论上的优雅，它有三个实际的重要意义：</p>
            <p><strong>1. 知识迁移</strong>：当你理解了 CNN 的成功来自于"利用平移对称性"，你就能立刻把这个洞察迁移到其他领域。比如在处理球面数据（如地球表面的气候数据）时，你知道应该用"球面卷积"（利用旋转对称性），而不是强行把球面展平成平面图像。这种迁移能力在面对新领域时是无价的。</p>
            <p><strong>2. 架构创新</strong>：统一框架提供了一套"设计语言"。当你遇到全新的数据类型时，你不需要从头发明轮子，而是可以问："这个数据的对称群是什么？如何构建对应的等变层？"这就是为什么 AlphaFold 2 能够在蛋白质结构预测上取得突破——它系统地利用了分子的 SE(3) 对称性（旋转+平移不变性）。</p>
            <p><strong>3. 避免错误</strong>：更重要的是，统一框架能帮你<strong>避免使用错误的架构</strong>。例如，如果你用普通的全连接网络处理图数据，你实际上在学习一个"依赖于节点编号顺序"的函数——但图的节点顺序是任意的！这会导致模型学到错误的模式，泛化性能差。GDL 告诉你：图数据需要置换等变的架构（GNN）。</p>
            <p>举个类比：你可以不懂物理学也能造桥，靠经验和试错。但当你理解了力学原理后，你能造出更好的桥、避免灾难性的设计失误，并且能快速适应新的桥梁类型（悬索桥、斜拉桥等）。GDL 就是深度学习的"力学原理"。</p>
          </div>
        </div>

        <div class="qa-pair">
          <p class="question">❓ 小白提问："几何"这个词在这里到底指什么？它和我高中学的平面几何、立体几何有关系吗？</p>
          <div class="answer">
            <p>💡 专家解答：非常好的问题！这个词确实容易引起混淆。这里的"几何"有更广义的含义，它来自于<strong>现代数学中的几何学</strong>，而不是欧几里得的平面几何。</p>
            <p>在现代数学中，"几何"研究的是<strong>空间、形状以及其中的对称性和不变量</strong>。具体到 GDL 中的"几何"，它指的是：</p>
            <ul>
              <li><strong>数据所在的空间结构</strong>：图像在 2D 网格上，图数据在图结构上，分子在 3D 欧几里得空间中——这些都是不同的"几何域"</li>
              <li><strong>数据的对称性</strong>：图像的平移对称性、分子的旋转对称性、图的节点置换对称性——这些对称性是"几何性质"</li>
              <li><strong>不变量和等变量</strong>：什么性质在变换下不变（如分子的总能量不随旋转改变），什么随之等变（如分子中原子间的力会随旋转而旋转）</li>
            </ul>
            <p>一个具体的例子：考虑一个咖啡杯。从拓扑学（一种几何学）的角度看，咖啡杯和甜甜圈是"同一个形状"——因为它们都有一个洞，可以通过连续变形互相转换。这种"忽略尺寸和精确形状，只关注本质结构"的思维方式，就是几何学的核心。</p>
            <p>在 GDL 中，我们做的是类似的事情：忽略数据的表面特征（如图像的具体像素值、图节点的编号），抓住本质的结构特征（如图像的平移对称性、图的拓扑结构），并基于这些结构设计神经网络。</p>
            <p>所以，当你看到"几何深度学习"时，你可以理解为"<strong>利用数据的结构和对称性的深度学习</strong>"。</p>
          </div>
        </div>

        <div class="qa-pair">
          <p class="question">❓ 小白提问：为什么这本书现在才出现？深度学习发展这么多年了，这些统一的视角以前没有人想到吗？</p>
          <div class="answer">
            <p>💡 专家解答：这是一个很有历史感的问题！实际上，GDL 的很多核心思想早就存在于不同领域中，但它们的<strong>统一</strong>确实是最近十年的事情。让我给你讲讲这段有趣的历史：</p>
            <p><strong>早期的孤立发展（1980s-2000s）</strong>：</p>
            <ul>
              <li>CNN（1989, Yann LeCun）：最初在 AT&T 实验室发明用于手写数字识别，灵感来自神经科学（视觉皮层的感受野）</li>
              <li>等变网络的数学理论（1990s）：数学家和物理学家在研究群表示论时就知道如何构造等变算子，但这些理论停留在数学期刊中</li>
              <li>计算化学中的对称性利用（2000s）：化学家在分子模拟中利用旋转不变性，但他们不称其为"神经网络"</li>
            </ul>
            <p><strong>大统一的时机（2010s-2020s）</strong>：三个关键因素的汇聚：</p>
            <ol>
              <li><strong>深度学习的实证成功</strong>：AlexNet（2012）、AlphaGo（2016）证明了深度学习不是玩具，值得深入研究</li>
              <li><strong>非欧几里得数据的爆发</strong>：社交网络、知识图谱、药物发现产生了大量图数据，传统 CNN/RNN 无能为力，推动了 GNN 的发展</li>
              <li><strong>跨领域的对话</strong>：机器学习、计算几何、物理学、化学的研究者开始在同一个会议上交流（如 ICML、NeurIPS、ICLR），发现彼此在做类似的事情</li>
            </ol>
            <p>这本书（Michael Bronstein 等人的工作）的贡献在于：<strong>把这些散落在不同领域的洞察整合成一个连贯的理论框架</strong>，并用 Felix Klein 的 Erlangen Program 作为统一的哲学基础。</p>
            <p>这就像孟德尔在 1865 年就发现了遗传定律，但直到 1900 年代初，当三位科学家独立重新发现他的工作时，遗传学才真正成为一门科学。GDL 也是如此——思想的种子早就播下，但统一的理论框架需要时间来生长。</p>
          </div>
        </div>
      </div>

      <div class="enrichment-intuition">
        <h4>🎯 直觉理解</h4>
        <p><strong>用乐高积木理解 GDL</strong>：想象深度学习架构是用乐高积木搭建的建筑。</p>
        <ul>
          <li><strong>传统深度学习课程</strong>：教你如何搭建特定的建筑（城堡、飞机、汽车），你知道结果，但不理解为什么要这样搭。</li>
          <li><strong>几何深度学习</strong>：教你积木的<strong>基本原理</strong>——哪些积木可以拼接（等变操作）、如何保持结构稳定（对称性约束）、如何设计新的形状（从数据几何出发）。</li>
        </ul>
        <p>掌握了原理后，你不仅能复现经典建筑（CNN、GNN、Transformer），还能设计全新的建筑（针对新问题的等变网络），甚至能识别哪些设计是行不通的（违反对称性的架构）。</p>
        <p><strong>核心直觉</strong>：数据不是随机的高维点云，而是有结构的。就像分子不是随机的原子堆，而是有化学键连接的结构。GDL 就是关于如何<strong>利用</strong>这些结构来设计更好的神经网络。</p>
      </div>

      <div class="enrichment-application">
        <h4>🏥 医疗机器人应用</h4>
        <p><strong>为什么医疗机器人仿真是 GDL 的完美应用场景？</strong></p>
        <p>医疗机器人手术涉及多种几何结构，每种都需要不同的 GDL 方法：</p>
        <ul>
          <li><strong>软组织变形</strong>：肝脏、心脏等器官表面是连续的流形（manifold）。当手术刀切割时，变形应该是<strong>等距的</strong>（内部距离保持不变）。这需要在流形上定义卷积（Geodesic CNN）。</li>
          <li><strong>粒子系统仿真</strong>：血液流动可以用粒子表示，粒子之间的相互作用构成动态图。物理定律具有 SE(3) 对称性（在任何位置和方向都相同）。Graph Network Simulator (GNS) 正是利用这种对称性。</li>
          <li><strong>器械-组织交互</strong>：手术器械（刚体）在 SE(3) 群上运动，与软组织（流形）的交互是异构图（heterogeneous graph）。需要设计对两种对称性都等变的网络。</li>
          <li><strong>力反馈预测</strong>：外科医生需要感受器械受到的力，这个力是 SE(3)-等变的向量（旋转器械，力也旋转）。需要等变消息传递网络（EGNN）。</li>
        </ul>
        <p>如果不使用 GDL 原则，而是用普通的 MLP 或 CNN，会出现什么问题？</p>
        <ol>
          <li><strong>泛化失败</strong>：模型会学到"这个器械在 x 位置时施加 y 力"，但无法泛化到旋转后的情况——即使物理定律完全相同！</li>
          <li><strong>数据效率低</strong>：需要采集大量不同位置、不同角度的训练数据，才能"重新学习"物理定律在每个坐标系下的表现。</li>
          <li><strong>违反物理约束</strong>：模型可能预测出违反能量守恒、动量守恒的结果，因为它不知道这些是物理定律的<strong>不变量</strong>。</li>
        </ol>
        <p><strong>GDL 方法的优势</strong>：通过设计 SE(3)-等变的架构，模型<strong>天然满足</strong>物理对称性，只需要学习具体的材料参数（如弹性模量），而不需要重新学习"力学定律在每个坐标系下的形式"。这就是"归纳偏置"的力量——把已知的物理约束嵌入架构中。</p>
      </div>
    </div>

    <!-- ========= 为什么需要 GDL ========= -->
    <h2 id="why-gdl">为什么需要几何深度学习？<br><span style="font-size:0.7em;color:var(--text-secondary)">Why Geometric Deep Learning?</span></h2>

    <h3 id="revolution">深度学习革命</h3>

    <div class="bilingual">
      <div class="zh">
        <p>过去十年见证了数据科学和机器学习领域的<strong>实验性革命</strong>，其标志是深度学习方法的崛起。许多此前被认为不可能的高维学习任务——如<strong>计算机视觉</strong>、<strong>围棋博弈</strong>或<strong>蛋白质折叠</strong>——实际上在适当的计算规模下是可行的。</p>
      </div>
      <div class="en">
        The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach — such as computer vision, playing Go, or protein folding — are in fact feasible with appropriate computational scale.
      </div>
    </div>

    <p>让我们用一组数字来感受这场革命的规模：</p>

    <table>
      <thead>
        <tr><th>里程碑</th><th>年份</th><th>意义</th></tr>
      </thead>
      <tbody>
        <tr><td>AlexNet (ImageNet)</td><td>2012</td><td>CNN 在图像识别上首次大幅超越传统方法，开启深度学习时代</td></tr>
        <tr><td>AlphaGo</td><td>2016</td><td>CNN + MCTS 在围棋上击败世界冠军李世乭</td></tr>
        <tr><td>Transformer / Attention is All You Need</td><td>2017</td><td>自注意力机制统一 NLP，开启大模型时代</td></tr>
        <tr><td>AlphaFold 2</td><td>2020</td><td>等变 GNN 解决蛋白质结构预测问题</td></tr>
        <tr><td>GPT-3</td><td>2020</td><td>1750 亿参数语言模型展示 few-shot 学习能力</td></tr>
        <tr><td>GNS (粒子仿真)</td><td>2020</td><td>GNN 实现高精度物理仿真，超越传统数值方法</td></tr>
      </tbody>
    </table>

    <p>但这场革命背后有一个<strong>深刻的统一主题</strong>：所有这些成功都源于<strong>对数据底层几何结构的正确利用</strong>。这正是几何深度学习要揭示的核心洞察。</p>

    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>🔍 深入理解</h4>
        
        <div class="qa-pair">
          <p class="question">❓ 小白提问：表格中的这些里程碑看起来很不相关——图像识别、围棋、蛋白质折叠——它们之间的"统一主题"到底在哪里？</p>
          <div class="answer">
            <p>💡 专家解答：这是这一节最精彩的洞察！表面上看，这些任务完全不同，但从 GDL 的视角，它们都在做同一件事：<strong>利用数据的几何对称性来对抗维度灾难</strong>。让我逐个分析：</p>
            <p><strong>AlexNet（图像识别）</strong>：</p>
            <ul>
              <li>数据：$224 \times 224 \times 3$ 的图像（150,528 维！）</li>
              <li>对称性：<strong>平移不变性</strong>——猫在图像左上角和右下角应该都被识别为猫</li>
              <li>利用方式：卷积层——在所有位置共享相同的滤波器</li>
              <li>效果：参数量从 $10^{10}$ 级别降到 $10^6$ 级别</li>
            </ul>
            <p><strong>AlphaGo（围棋）</strong>：</p>
            <ul>
              <li>数据：$19 \times 19$ 的棋盘（361 维的离散空间，但局面空间约 $10^{170}$）</li>
              <li>对称性：<strong>平移不变性</strong>+<strong>旋转/反射对称性</strong>——围棋规则不依赖于棋盘的方向</li>
              <li>利用方式：卷积网络（平移）+ 数据增强（旋转/反射）</li>
              <li>效果：策略网络能快速评估局面，而不需要"记住"每个可能
<div class="enrichment-block">
  <h3>🔍 深化理解：网格和欧几里得空间（Grids & Euclidean）</h3>
  
  <div class="enrichment-qa">
    <h4>💬 核心问答</h4>
    
    <div class="qa-pair">
      <div class="question">
        <strong>Q1：为什么 CNN 天然就是平移等变的？这和图神经网络有什么本质区别?</strong>
      </div>
      <div class="answer">
        <p><strong>平移等变性的定义</strong>：</p>
        <p>如果输入图像平移，输出的特征图也跟着平移（但每个位置的特征值不变）。</p>
        <p>数学表达：$f(T_g x) = T_g f(x)$，其中 $T_g$ 是平移算子。</p>
        
        <p><strong>CNN 的核心机制</strong>：</p>
        <ul>
          <li><strong>权重共享</strong>：同一个卷积核在整个图像上滑动</li>
          <li><strong>局部连接</strong>：每个输出像素只依赖输入的局部区域（感受野）</li>
          <li>这两个性质共同保证了平移等变性</li>
        </ul>
        
        <p><strong>直观类比</strong>：</p>
        <p>想象一个"猫检测器"卷积核。无论猫出现在图像的左上角还是右下角，这个检测器都应该在<strong>对应位置</strong>产生强响应。如果用全连接层，网络需要学习"左上角的猫"和"右下角的猫"两套不同的检测器——这既低效又泛化性差。</p>
        
        <p><strong>与图神经网络的区别</strong>：</p>
        <table>
          <tr>
            <th>性质</th>
            <th>CNN（网格）</th>
            <th>GNN（图）</th>
          </tr>
          <tr>
            <td>空间结构</td>
            <td>规则网格（每个像素有固定数量的邻居）</td>
            <td>不规则图（每个节点的邻居数量不同）</td>
          </tr>
          <tr>
            <td>对称性</td>
            <td>平移群 $\mathbb{Z}^2$</td>
            <td>置换群 $S_n$（节点重排）</td>
          </tr>
          <tr>
            <td>卷积核</td>
            <td>固定形状（如 3×3）</td>
            <td>自适应聚合（邻居数量可变）</td>
          </tr>
          <tr>
            <td>参数共享</td>
            <td>位置共享（整个图像用同一个核）</td>
            <td>结构共享（不同子图用同一套参数）</td>
          </tr>
        </table>
        
        <p><strong>本质</strong>：CNN 是 GNN 在<strong>规则图</strong>（网格）上的特例！网格的每个节点恰好有 4/8 个邻居（2D），且邻居的相对位置固定。</p>
      </div>
    </div>
    
    <div class="qa-pair">
      <div class="question">
        <strong>Q2：傅里叶变换在深度学习中有什么用？为什么它和卷积有深刻联系？</strong>
      </div>
      <div class="answer">
        <p><strong>傅里叶变换的本质</strong>：</p>
        <p>将<strong>时域/空域</strong>信号分解为不同<strong>频率</strong>的正弦波之和。</p>
        <p>连续形式：$\hat{x}(\omega) = \int_{-\infty}^{\infty} x(t) e^{-i\omega t} dt$</p>
        <p>离散形式（DFT）：$\hat{x}_k = \sum_{n=0}^{N-1} x_n e^{-2\pi i kn/N}$</p>
        
        <p><strong>音乐频谱分析类比</strong>：</p>
        <ul>
          <li>时域信号：音频波形（振幅随时间变化）</li>
          <li>频域信号：频谱图（每个频率的强度）</li>
          <li>人耳能听到 20-20000 Hz → 对应频域的不同"通道"</li>
          <li>低频 = 低音（贝斯），高频 = 高音（小提琴）</li>
        </ul>
        
        <p><strong>卷积定理</strong>：</p>
        <p>时域卷积 = 频域逐点乘积</p>
        $$\mathcal{F}(x \star h) = \mathcal{F}(x) \cdot \mathcal{F}(h)$$
        <p>这意味着：</p>
        <ul>
          <li>卷积在时域计算复杂度 $O(N^2)$</li>
          <li>通过 FFT → 频域相乘 → IFFT 可以降到 $O(N \log N)$</li>
        </ul>
        
        <p><strong>深度学习中的应用</strong>：</p>
        <ol>
          <li><strong>快速卷积</strong>：对于大卷积核（如全局感受野），用 FFT 加速</li>
          <li><strong>图像去噪</strong>：低通滤波器去除高频噪声（保留平滑部分）</li>
          <li><strong>数据增强</strong>：在频域加扰动比在空域更可控</li>
          <li><strong>谱归一化</strong>：控制网络的 Lipschitz 常数（对抗训练）</li>
          <li><strong>图卷积网络</strong>：ChebNet、GCN 使用拉普拉斯的谱分解（图上的傅里叶变换）</li>
        </ol>
        
        <p><strong>为什么卷积是低通滤波器</strong>：</p>
        <p>典型的卷积核（如高斯核、均值核）在频域对应低通滤波器——保留低频（平滑）成分，衰减高频（细节/噪声）。这就是为什么多层卷积后，图像会逐渐"模糊"（如果没有激活函数引入非线性）。</p>
        
        <p><strong>图像举例</strong>：</p>
        <ul>
          <li>原图 = 低频（整体轮廓）+ 高频（边缘细节）</li>
          <li>模糊 = 去除高频</li>
          <li>锐化 = 增强高频</li>
          <li>边缘检测 = 只保留高频</li>
        </ul>
      </div>
    </div>
    
    <div class="qa-pair">
      <div class="question">
        <strong>Q3：为什么说卷积是"模板匹配"？感受野是怎么逐层增长的？</strong>
      </div>
      <div class="answer">
        <p><strong>卷积核 = 可学习的模板</strong>：</p>
        <p>卷积操作本质上是计算输入图像与卷积核的<strong>相关性</strong>（互相关，cross-correlation）：</p>
        $$(x \star h)[i, j] = \sum_{m,n} x[i+m, j+n] \cdot h[m, n]$$
        <ul>
          <li>如果局部图像块与卷积核"相似"，输出值大</li>
          <li>如果差异大，输出值小</li>
        </ul>
        
        <p><strong>边缘检测器例子</strong>：</p>
        <p>Sobel 算子（垂直边缘）：</p>
        $$h = \begin{bmatrix} -1 & 0 & 1 \\ -1 & 0 & 1 \\ -1 & 0 & 1 \end{bmatrix}$$
        <p>这个核在垂直边缘处产生强响应（左侧暗、右侧亮）。</p>
        
        <p><strong>感受野的增长</strong>：</p>
        <ul>
          <li><strong>第一层</strong>：3×3 卷积 → 感受野 = 3×3（看到局部纹理）</li>
          <li><strong>第二层</strong>：再做 3×3 卷积 → 感受野 = 5×5（看到更大的模式）</li>
          <li><strong>第 $n$ 层</strong>：感受野 = $1 + n \times (k-1)$（对于 $k \times k$ 卷积核）</li>
        </ul>
        
        <p><strong>池化的作用</strong>：</p>
        <p>Max pooling 或 Average pooling 每次将空间分辨率减半 → 感受野加倍增长！</p>
        <p>例如：3×3 卷积 + 2×2 pooling 后，感受野从 3×3 变为 6×6。</p>
        
        <p><strong>层次化特征学习</strong>：</p>
        <ul>
          <li><strong>浅层</strong>：小感受野 → 学习边缘、纹理（Gabor 滤波器）</li>
          <li><strong>中层</strong>：中等感受野 → 学习局部部件（眼睛、轮子）</li>
          <li><strong>深层</strong>：大感受野 → 学习全局语义（整个人脸、整辆车）</li>
        </ul>
        
        <p><strong>医学影像类比</strong>：</p>
        <ul>
          <li>第 1 层：检测组织边界</li>
          <li>第 2 层：识别器官轮廓</li>
          <li>第 3 层：理解器官间的空间关系</li>
          <li>最终层：诊断病变（需要整合全局上下文）</li>
        </ul>
      </div>
    </div>
    
    <div class="qa-pair">
      <div class="question">
        <strong>Q4：什么是转向等变性（Steerable）？为什么旋转等变 CNN 比普通 CNN 更高效？</strong>
      </div>
      <div class="answer">
        <p><strong>普通 CNN 的问题</strong>：</p>
        <p>标准卷积只对<strong>平移</strong>等变，不对<strong>旋转</strong>等变。</p>
        <ul>
          <li>如果训练时猫都是"头朝上"，测试时遇到"头朝右"的猫可能识别失败</li>
          <li>解决方法：数据增强（旋转图像生成多个样本）→ 参数量翻倍</li>
        </ul>
        
        <p><strong>转向等变卷积</strong>：</p>
        <p>设计卷积核使得<strong>旋转输入 → 旋转输出</strong>，无需额外训练旋转版本。</p>
        <p>关键思想：将特征分解为<strong>不同旋转频率</strong>的分量（类似傅里叶分解）。</p>
        
        <p><strong>数学原理</strong>：</p>
        <p>在 2D 平面上，任何函数可以分解为<strong>角度谐波</strong>（angular harmonics）：</p>
        $$f(r, \theta) = \sum_{m=-\infty}^{\infty} f_m(r) e^{im\theta}$$
        <ul>
          <li>$m$ = 旋转频率（角动量）</li>
          <li>$m=0$：旋转不变（如高斯核）</li>
          <li>$m=1$：旋转一圈后回到原处（如 Gabor 滤波器）</li>
          <li>$m=2$：旋转 180° 后回到原处</li>
        </ul>
        
        <p><strong>医疗影像的实际应用</strong>：</p>
        <ul>
          <li>细胞形态学：细胞可以任意朝向</li>
          <li>视网膜血管分割：血管方向随机</li>
          <li>组织病理学：肿瘤细胞的排列无固定方向</li>
        </ul>
        <p>使用旋转等变 CNN（如 G-CNN、Harmonic Networks）可以：</p>
        <ul>
          <li>减少所需训练数据（不需要旋转增强）</li>
          <li>提高泛化能力（自动处理所有旋转）</li>
          <li>参数效率更高（一套参数处理所有方向）</li>
        </ul>
        
        <p><strong>与标准 CNN 的对比</strong>：</p>
        <table>
          <tr>
            <th>方法</th>
            <th>旋转等变性</th>
            <th>参数量</th>
            <th>数据需求</th>
          </tr>
          <tr>
            <td>标准 CNN</td>
            <td>❌</td>
            <td>基线</td>
            <td>需要旋转增强</td>
          </tr>
          <tr>
            <td>数据增强</td>
            <td>❌（近似）</td>
            <td>基线</td>
            <td>增加 8-16 倍</td>
          </tr>
          <tr>
            <td>G-CNN</td>
            <td>✅（离散）</td>
            <td>基线</td>
            <td>少 4-8 倍</td>
          </tr>
          <tr>
            <td>Harmonic Net</td>
            <td>✅（连续）</td>
            <td>稍大</td>
            <td>少 4-8 倍</td>
          </tr>
        </table>
      </div>
    </div>
    
    <div class="qa-pair">
      <div class="question">
        <strong>Q5：欧几里得空间的 SE(2) 和 SE(3) 群是什么？它们在机器人学中为什么重要？</strong>
      </div>
      <div class="answer">
        <p><strong>SE(n) = 特殊欧几里得群</strong>：</p>
        <p>刚体运动群（旋转 + 平移）的数学表示。</p>
        <ul>
          <li>SE(2)：2D 平面上的旋转和平移（3 个自由度：$x, y, \theta$）</li>
          <li>SE(3)：3D 空间中的旋转和平移（6 个自由度：$x, y, z, \text{roll}, \text{pitch}, \text{yaw}$）</li>
        </ul>
        
        <p><strong>矩阵表示</strong>：</p>
        <p>SE(2):</p>
        $$T = \begin{bmatrix} \cos\theta & -\sin\theta & t_x \\ \sin\theta & \cos\theta & t_y \\ 0 & 0 & 1 \end{bmatrix}$$
        
        <p>SE(3):</p>
        $$T = \begin{bmatrix} R_{3\times 3} & \mathbf{t} \\ 0 & 1 \end{bmatrix} \in \mathbb{R}^{4 \times 4}$$
        <p>其中 $R \in \text{SO}(3)$ 是旋转矩阵，$\mathbf{t} \in \mathbb{R}^3$ 是平移向量。</p>
        
        <p><strong>机器人学中的应用</strong>：</p>
        <ol>
          <li><strong>位姿表示</strong>：机器人的位置和朝向用 SE(3) 元素表示</li>
          <li><strong>坐标变换</strong>：从机器人坐标系到世界坐标系的转换</li>
          <li><strong>运动规划</strong>：在 SE(3) 流形上规划路径（避障）</li>
          <li><strong>SLAM</strong>：同时定位与建图，估计相机的 SE(3) 轨迹</li>
          <li><strong>抓取规划</strong>：机械臂末端执行器的 SE(3) 位姿</li>
        </ol>
        
        <p><strong>医疗机器人手术的例子</strong>：</p>
        <ul>
          <li><strong>达芬奇手术机器人</strong>：每个关节的运动可以分解为 SE(3) 变换的组合</li>
          <li><strong>术前规划</strong>：从 CT 扫描到手术台的坐标对齐 → SE(3) 配准</li>
          <li><strong>手眼标定</strong>：内窥镜相机相对于机械臂的 SE(3) 变换</li>
          <li><strong>力反馈</strong>：工具与组织接触的力需要从工具坐标系转换到世界坐标系</li>
        </ul>
        
        <p><strong>为什么需要 SE(3) 等变网络</strong>：</p>
        <ul>
          <li>物理定律是<strong>坐标无关</strong>的：无论你怎么放置机器人，牛顿定律都成立</li>
          <li>传统网络：需要在所有可能的位姿下训练 → 数据需求爆炸</li>
          <li>SE(3) 等变网络：自动泛化到所有刚体变换 → 显著减少数据需求</li>
        </ul>
        
        <p><strong>实现方法</strong>：</p>
        <ul>
          <li>使用<strong>相对位置</strong> $\mathbf{r}_{ij} = \mathbf{p}_j - \mathbf{p}_i$ 作为输入（平移不变）</li>
          <li>使用<strong>距离</strong> $\|\mathbf{r}_{ij}\|$ 和<strong>角度</strong>作为特征（旋转不变）</li>
          <li>或者使用<strong>球谐函数</strong>编码方向信息（旋转等变）</li>
        </ul>
      </div>
    </div>
  </div>
  
  <div class="enrichment-intuition">
    <h4>🎯 核心直觉</h4>
    <div class="intuition-item">
      <strong>网格 = 最简单的图 = CNN 的自然领地</strong>
      <p>图像是定义在 2D 网格上的信号，每个像素有 4/8 个邻居。这种规则性使得卷积核可以<strong>权重共享</strong>——整个图像用同一套参数。</p>
    </div>
    <div class="intuition-item">
      <strong>卷积 = 加权邻域平均 = 局部特征提取</strong>
      <p>卷积就是"用一个小窗口在图像上滑动，计算加权和"。窗口的权重（卷积核）是可学习的，最终学到的是<strong>有用的局部模式</strong>（边缘、纹理、角点）。</p>
    </div>
    <div class="intuition-item">
      <strong>傅里叶变换 = 从时间/空间到频率的视角转换</strong>
      <p>时域看到的是"信号随时间变化"，频域看到的是"不同频率成分的强度"。很多操作在频域更简单（卷积变乘法）。</p>
    </div>
    <div class="intuition-item">
      <strong>等变性 > 不变性</strong>
      <p>不变性丢失信息（"猫在哪不重要"），等变性保留信息（"猫在左边 → 特征在左边"）。对于需要精确定位的任务（分割、检测），等变性至关重要。</p>
    </div>
    <div class="intuition-item">
      <strong>SE(n) 等变 = 物理定律的数学表达</strong>
      <p>物理世界不依赖坐标系的选择。好的神经网络应该模仿这一点：旋转整个场景，预测结果也跟着旋转。</p>
    </div>
  </div>
  
  <div class="enrichment-application">
    <h4>🚀 应用场景</h4>
    <div class="application-item">
      <strong>图像分类（ImageNet）</strong>
      <p>CNN 的经典应用。ResNet、EfficientNet 等模型通过层次化卷积学习从低级纹理到高级语义的特征。平移等变性保证了"猫在图像任何位置都能被识别"。</p>
    </div>
    <div class="application-item">
      <strong>医学影像分割</strong>
      <p>U-Net 是标准架构：编码器提取特征，解码器恢复分辨率。等变性确保"无论器官在扫描的哪个位置，分割结果都准确"。旋转等变 CNN 可以更好地处理任意朝向的器官。</p>
    </div>
    <div class="application-item">
      <strong>视频动作识别</strong>
      <p>3D CNN（时空卷积）同时在时间和空间维度上卷积。感受野从单帧扩展到短片段，捕捉运动模式。</p>
    </div>
    <div class="application-item">
      <strong>点云处理（PointNet++）</strong>
      <p>虽然点云是不规则的，但可以在局部邻域内用"伪卷积"（continuous convolution）。关键是保持 SE(3) 等变性——点云旋转/平移后，特征也跟着变换。</p>
    </div>
    <div class="application-item">
      <strong>机器人视觉（抓取检测）</strong>
      <p>从 RGB-D 图像预测抓取位姿。需要 SE(3) 等变性：无论物体怎么放，网络都应该输出正确的抓取方向。Group CNN 或 SE(3)-Transformer 是常用方法。</p>
    </div>
    <div class="application-item">
      <strong>天气预报（FourCastNet）</strong>
      <p>地球表面是球面（非欧空间），但局部可以近似为平面网格。用卷积处理气象数据（温度、湿度、风速），预测未来状态。全局模型需要考虑球面几何。</p>
    </div>
  </div>
</div>

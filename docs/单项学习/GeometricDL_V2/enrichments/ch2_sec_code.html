<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>ğŸ” æ·±å…¥ç†è§£ï¼šä»£ç ç¤ºä¾‹</h4>
    
    <div class="qa-pair">
      <p class="question">â“ å°ç™½æé—®ï¼šå¦‚ä½•ç”¨ä»£ç ç›´è§‚å±•ç¤º"ç»´åº¦ç¾éš¾"ï¼Ÿæˆ‘æƒ³äº²çœ¼çœ‹åˆ°æ ·æœ¬éœ€æ±‚éšç»´åº¦æŒ‡æ•°å¢é•¿ã€‚</p>
      <div class="answer">
        <p>ğŸ’¡ ä¸“å®¶è§£ç­”ï¼šæœ€ç›´è§‚çš„æ–¹å¼æ˜¯<strong>ä½“ç§¯é›†ä¸­ç°è±¡</strong>å’Œ<strong>è·ç¦»é›†ä¸­ç°è±¡</strong>çš„å¯è§†åŒ–ã€‚è®©æˆ‘ç»™ä½ å®ç”¨çš„ä»£ç æ¡†æ¶ã€‚</p>
        
        <p><strong>å®éªŒ1ï¼šä½“ç§¯é›†ä¸­</strong></p>
        <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def volume_concentration_demo(dims=[2, 5, 10, 50, 100]):
    """æ¼”ç¤ºé«˜ç»´çƒçš„ä½“ç§¯é›†ä¸­åœ¨è¡¨é¢"""
    results = {}
    
    for d in dims:
        n_samples = 10000
        # åœ¨dç»´å•ä½çƒå†…å‡åŒ€é‡‡æ ·
        samples = np.random.randn(n_samples, d)
        norms = np.linalg.norm(samples, axis=1)
        # å½’ä¸€åŒ–åˆ°å•ä½çƒï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        samples_uniform = samples / norms[:, None] * \
                          np.random.uniform(0, 1, n_samples)**(1/d)[:, None]
        norms_uniform = np.linalg.norm(samples_uniform, axis=1)
        
        # è®¡ç®—æœ‰å¤šå°‘æ ·æœ¬åœ¨å¤–å±‚10%çš„å£³ä¸­
        frac_in_shell = np.mean(norms_uniform > 0.9)
        results[d] = {
            'mean_radius': norms_uniform.mean(),
            'std_radius': norms_uniform.std(),
            'frac_in_outer_shell': frac_in_shell
        }
        
        print(f"ç»´åº¦ {d}: "
              f"å¹³å‡åŠå¾„={norms_uniform.mean():.3f}, "
              f"åœ¨å¤–å±‚10%å£³ä¸­çš„æ¯”ä¾‹={frac_in_shell:.4f}")
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(dims, [results[d]['frac_in_outer_shell'] for d in dims], 'o-')
    plt.xlabel('Dimension')
    plt.ylabel('Fraction in outer 10% shell')
    plt.title('Volume Concentration')
    plt.grid(True)
    
    return results

# è¿è¡Œ
results = volume_concentration_demo()

# ç†è®ºé¢„æµ‹ï¼šP(r > 0.9) = 1 - 0.9^d
for d in [2, 10, 50, 100]:
    theoretical = 1 - 0.9**d
    print(f"d={d}: ç†è®º={theoretical:.6f}, å®éªŒ={results[d]['frac_in_outer_shell']:.6f}")
</code></pre>
        
        <p><strong>å®éªŒ2ï¼šè·ç¦»é›†ä¸­</strong></p>
        <pre><code class="language-python">def distance_concentration_demo(dims=[2, 5, 10, 50, 100], n_points=1000):
    """æ¼”ç¤ºé«˜ç»´ç©ºé—´ä¸­æœ€è¿‘é‚»å’Œæœ€è¿œé‚»è·ç¦»è¶‹äºç›¸åŒ"""
    results = {}
    
    for d in dims:
        # åœ¨dç»´å•ä½ç«‹æ–¹ä½“ä¸­å‡åŒ€é‡‡æ ·
        points = np.random.uniform(0, 1, (n_points, d))
        test_point = np.random.uniform(0, 1, d)
        
        # è®¡ç®—æ‰€æœ‰è·ç¦»
        distances = np.linalg.norm(points - test_point, axis=1)
        
        min_dist = distances.min()
        max_dist = distances.max()
        mean_dist = distances.mean()
        std_dist = distances.std()
        
        # ç›¸å¯¹å·®å¼‚
        relative_range = (max_dist - min_dist) / min_dist
        
        results[d] = {
            'min': min_dist,
            'max': max_dist,
            'mean': mean_dist,
            'std': std_dist,
            'relative_range': relative_range
        }
        
        print(f"ç»´åº¦ {d}: æœ€è¿‘={min_dist:.3f}, æœ€è¿œ={max_dist:.3f}, "
              f"ç›¸å¯¹å·®å¼‚={(max_dist-min_dist)/min_dist:.3f}")
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    dims_list = list(results.keys())
    plt.plot(dims_list, [results[d]['relative_range'] for d in dims_list], 'o-')
    plt.xlabel('Dimension')
    plt.ylabel('(max_dist - min_dist) / min_dist')
    plt.title('Distance Concentration')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    for d in [2, 10, 50]:
        points = np.random.uniform(0, 1, (n_points, d))
        test_point = np.random.uniform(0, 1, d)
        distances = np.linalg.norm(points - test_point, axis=1)
        plt.hist(distances, bins=30, alpha=0.5, label=f'd={d}', density=True)
    plt.xlabel('Distance')
    plt.ylabel('Density')
    plt.legend()
    plt.title('Distance Distribution')
    
    return results

results = distance_concentration_demo()
</code></pre>
        
        <p><strong>å…³é”®è§‚å¯Ÿ</strong>ï¼š</p>
        <ul>
          <li>$d=2$ï¼šæœ€è¿œ/æœ€è¿‘è·ç¦»æ¯” â‰ˆ 1.4ï¼ˆæœ‰æ˜æ˜¾åŒºåˆ«ï¼‰</li>
          <li>$d=100$ï¼šæœ€è¿œ/æœ€è¿‘è·ç¦»æ¯” â‰ˆ 1.05ï¼ˆå‡ ä¹ç›¸åŒï¼ï¼‰</li>
          <li>k-NN åœ¨é«˜ç»´ä¸­å¤±æ•ˆçš„ç›´æ¥è¯æ®</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">â“ å°ç™½æé—®ï¼šå¦‚ä½•ç”¨ä»£ç éªŒè¯"ä¸‡èƒ½é€¼è¿‘å®šç†"ï¼Ÿæˆ‘æƒ³çœ‹åˆ°å•éšå±‚ç½‘ç»œç¡®å®èƒ½é€¼è¿‘ä»»æ„å‡½æ•°ã€‚</p>
      <div class="answer">
        <p>ğŸ’¡ ä¸“å®¶è§£ç­”ï¼šç»å…¸æ¼”ç¤ºæ˜¯ç”¨å•éšå±‚ç½‘ç»œé€¼è¿‘å¤æ‚çš„1Dæˆ–2Då‡½æ•°ã€‚</p>
        
        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

class SingleHiddenLayerNet(nn.Module):
    def __init__(self, hidden_size=100):
        super().__init__()
        self.fc1 = nn.Linear(1, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        return self.fc2(torch.relu(self.fc1(x)))

# ç›®æ ‡å‡½æ•°ï¼šå¤æ‚çš„éçº¿æ€§å‡½æ•°
def target_function(x):
    return torch.sin(5 * x) + 0.5 * torch.cos(20 * x) + 0.3 * x**2

# è®­ç»ƒæ•°æ®
x_train = torch.linspace(-1, 1, 200).reshape(-1, 1)
y_train = target_function(x_train)

# æµ‹è¯•ä¸åŒéšå±‚å¤§å°
for hidden_size in [10, 50, 200, 1000]:
    model = SingleHiddenLayerNet(hidden_size)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    # è®­ç»ƒ
    for epoch in range(1000):
        optimizer.zero_grad()
        y_pred = model(x_train)
        loss = ((y_pred - y_train)**2).mean()
        loss.backward()
        optimizer.step()
    
    # æµ‹è¯•
    with torch.no_grad():
        x_test = torch.linspace(-1.2, 1.2, 300).reshape(-1, 1)
        y_test = target_function(x_test)
        y_pred = model(x_test)
        test_error = ((y_pred - y_test)**2).mean().sqrt()
    
    print(f"éšå±‚å¤§å° {hidden_size}: è®­ç»ƒè¯¯å·®={loss.item():.6f}, "
          f"æµ‹è¯•è¯¯å·®={test_error.item():.6f}")
    
    # å¯è§†åŒ–ï¼ˆåªå±•ç¤ºæœ€å¤§çš„ç½‘ç»œï¼‰
    if hidden_size == 1000:
        plt.figure(figsize=(10, 4))
        plt.plot(x_test.numpy(), y_test.numpy(), 'b-', label='True function', linewidth=2)
        plt.plot(x_test.numpy(), y_pred.numpy(), 'r--', label='Approximation', linewidth=2)
        plt.xlabel('x')
        plt.ylabel('y')
        plt.legend()
        plt.title(f'Universal Approximation (hidden_size={hidden_size})')
        plt.grid(True)
        plt.show()
</code></pre>
        
        <p><strong>å…³é”®å‘ç°</strong>ï¼š</p>
        <ul>
          <li>éšå±‚è¶Šå¤§ï¼Œé€¼è¿‘è¶Šå¥½ï¼ˆéªŒè¯ä¸‡èƒ½é€¼è¿‘å®šç†ï¼‰</li>
          <li>ä½†å³ä½¿é€¼è¿‘å¾ˆå¥½ï¼Œ<strong>æ³›åŒ–å¯èƒ½å¾ˆå·®</strong>ï¼ˆå¤–æ¨åŒºåŸŸ $|x| > 1$ï¼‰</li>
          <li>è¿™è¯´æ˜ï¼š<strong>è¡¨è¾¾èƒ½åŠ› â‰  æ³›åŒ–èƒ½åŠ›</strong></li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">â“ å°ç™½æé—®ï¼šå¦‚ä½•ç”¨ä»£ç å±•ç¤º"å½’çº³åç½®"çš„å½±å“ï¼Ÿæ¯”å¦‚CNN vs å…¨è¿æ¥åœ¨å›¾åƒä¸Šçš„å·®å¼‚ã€‚</p>
      <div class="answer">
        <p>ğŸ’¡ ä¸“å®¶è§£ç­”ï¼šç»å…¸å®éªŒï¼šåœ¨MNISTä¸Šå¯¹æ¯”MLPå’ŒCNNï¼Œå°¤å…¶æ˜¯åœ¨<strong>å°‘æ ·æœ¬</strong>æƒ…å†µä¸‹ã€‚</p>
        
        <pre><code class="language-python">import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset

# å…¨è¿æ¥ç½‘ç»œï¼ˆæ— å½’çº³åç½®ï¼‰
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )
    
    def forward(self, x):
        return self.fc(x)

# CNNï¼ˆå¹³ç§»ä¸å˜æ€§å½’çº³åç½®ï¼‰
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.fc(self.conv(x))

def train_and_evaluate(model, train_loader, test_loader, epochs=10):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(epochs):
        model.train()
        for X, y in train_loader:
            optimizer.zero_grad()
            loss = criterion(model(X), y)
            loss.backward()
            optimizer.step()
    
    # æµ‹è¯•
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for X, y in test_loader:
            correct += (model(X).argmax(1) == y).sum().item()
            total += y.size(0)
    
    return correct / total

# æ•°æ®å‡†å¤‡
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

# å®éªŒï¼šä¸åŒè®­ç»ƒæ ·æœ¬æ•°é‡
sample_sizes = [100, 500, 1000, 5000, 10000]
results = {'MLP': [], 'CNN': []}

for n_samples in sample_sizes:
    # å­é‡‡æ ·è®­ç»ƒé›†
    indices = torch.randperm(len(train_dataset))[:n_samples]
    train_subset = Subset(train_dataset, indices)
    
    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    # è®­ç»ƒMLP
    mlp = MLP()
    mlp_acc = train_and_evaluate(mlp, train_loader, test_loader)
    results['MLP'].append(mlp_acc)
    
    # è®­ç»ƒCNN
    cnn = CNN()
    cnn_acc = train_and_evaluate(cnn, train_loader, test_loader)
    results['CNN'].append(cnn_acc)
    
    print(f"æ ·æœ¬æ•°={n_samples}: MLP={mlp_acc:.3f}, CNN={cnn_acc:.3f}, "
          f"CNNä¼˜åŠ¿={cnn_acc - mlp_acc:.3f}")

# å¯è§†åŒ–
plt.figure(figsize=(8, 5))
plt.plot(sample_sizes, results['MLP'], 'o-', label='MLP (no inductive bias)', linewidth=2)
plt.plot(sample_sizes, results['CNN'], 's-', label='CNN (translation equivariance)', linewidth=2)
plt.xlabel('Training Samples')
plt.ylabel('Test Accuracy')
plt.xscale('log')
plt.legend()
plt.title('Inductive Bias: CNN vs MLP on MNIST')
plt.grid(True)
plt.show()
</code></pre>
        
        <p><strong>å…³é”®è§‚å¯Ÿ</strong>ï¼š</p>
        <ul>
          <li><strong>å°‘æ ·æœ¬æ—¶</strong>ï¼ˆå¦‚100æ ·æœ¬ï¼‰ï¼šCNNä¼˜åŠ¿å·¨å¤§ï¼ˆå¯èƒ½30% vs 60%ï¼‰</li>
          <li><strong>å¤§æ ·æœ¬æ—¶</strong>ï¼ˆå¦‚10000æ ·æœ¬ï¼‰ï¼šCNNä»ç„¶æ›´å¥½ï¼Œä½†å·®è·ç¼©å°</li>
          <li><strong>ç»“è®º</strong>ï¼šå½’çº³åç½®åœ¨<strong>æ ·æœ¬æœ‰é™</strong>æ—¶æœ€å…³é”®ï¼</li>
        </ul>
        
        <p><strong>é¢å¤–å®éªŒï¼šå¹³ç§»ä¸å˜æ€§æµ‹è¯•</strong></p>
        <pre><code class="language-python"># æµ‹è¯•å¹³ç§»é²æ£’æ€§
def test_translation_robustness(model, test_dataset):
    """æµ‹è¯•æ¨¡å‹å¯¹å›¾åƒå¹³ç§»çš„é²æ£’æ€§"""
    model.eval()
    original_preds = []
    shifted_preds = []
    
    for i in range(100):  # æµ‹è¯•100å¼ å›¾
        img, label = test_dataset[i]
        
        # åŸå§‹é¢„æµ‹
        with torch.no_grad():
            orig_pred = model(img.unsqueeze(0)).argmax(1).item()
        
        # å¹³ç§»å›¾åƒï¼ˆå‘å³ç§»5ä¸ªåƒç´ ï¼‰
        img_shifted = torch.roll(img, shifts=5, dims=2)
        with torch.no_grad():
            shifted_pred = model(img_shifted.unsqueeze(0)).argmax(1).item()
        
        original_preds.append(orig_pred == label)
        shifted_preds.append(shifted_pred == label)
    
    orig_acc = np.mean(original_preds)
    shifted_acc = np.mean(shifted_preds)
    consistency = np.mean(np.array(original_preds) == np.array(shifted_preds))
    
    return orig_acc, shifted_acc, consistency

# å¯¹æ¯”
mlp_orig, mlp_shifted, mlp_cons = test_translation_robustness(mlp, test_dataset)
cnn_orig, cnn_shifted, cnn_cons = test_translation_robustness(cnn, test_dataset)

print(f"MLP: åŸå§‹å‡†ç¡®ç‡={mlp_orig:.3f}, å¹³ç§»å={mlp_shifted:.3f}, ä¸€è‡´æ€§={mlp_cons:.3f}")
print(f"CNN: åŸå§‹å‡†ç¡®ç‡={cnn_orig:.3f}, å¹³ç§»å={cnn_shifted:.3f}, ä¸€è‡´æ€§={cnn_cons:.3f}")
# é¢„æœŸï¼šCNNçš„ä¸€è‡´æ€§æ˜¾è‘—æ›´é«˜ï¼ˆå› ä¸ºå¹³ç§»ç­‰å˜æ€§ï¼‰
</code></pre>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>ğŸ¯ ç›´è§‰ç†è§£</h4>
    <p><strong>ä»£ç å®éªŒçš„ä»·å€¼</strong>ï¼š</p>
    <ul>
      <li>"ç»´åº¦ç¾éš¾"ä¸åªæ˜¯ç†è®º â€” ä½ å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…<strong>äº²çœ¼çœ‹åˆ°</strong>å®ƒå‘ç”Ÿ</li>
      <li>"å½’çº³åç½®"ä¸æ˜¯æŠ½è±¡æ¦‚å¿µ â€” å®ƒåœ¨å‡†ç¡®ç‡æ•°å­—ä¸­<strong>æ¸…æ™°å¯è§</strong></li>
      <li>"å‡ ä½•å…ˆéªŒ"ä¸æ˜¯ç„å­¦ â€” CNNåœ¨å¹³ç§»æµ‹è¯•ä¸­çš„é²æ£’æ€§æ˜¯<strong>å¯éªŒè¯çš„</strong></li>
    </ul>
    <p><strong>å»ºè®®çš„å­¦ä¹ è·¯å¾„</strong>ï¼š</p>
    <ol>
      <li>è¿è¡Œä½“ç§¯é›†ä¸­å®éªŒ â†’ æ„Ÿå—é«˜ç»´ç©ºé—´çš„"æ€ªå¼‚"</li>
      <li>è¿è¡Œä¸‡èƒ½é€¼è¿‘å®éªŒ â†’ ç†è§£"è¡¨è¾¾â‰ æ³›åŒ–"</li>
      <li>è¿è¡ŒCNN vs MLPå®éªŒ â†’ çœ‹åˆ°å½’çº³åç½®çš„å¨åŠ›</li>
      <li><strong>è‡ªå·±ä¿®æ”¹å‚æ•°</strong> â†’ ç»´åº¦ã€æ ·æœ¬æ•°ã€ç½‘ç»œç»“æ„</li>
      <li><strong>å¯è§†åŒ–ç»“æœ</strong> â†’ å›¾èƒœåƒè¨€</li>
    </ol>
  </div>

  <div class="enrichment-application">
    <h4>ğŸ¥ åŒ»ç–—æœºå™¨äººåº”ç”¨</h4>
    <p>PhysRobot ä»£ç å®éªŒæ¡†æ¶ï¼š</p>
    <pre><code class="language-python"># ä¼ªä»£ç æ¡†æ¶ï¼šå¯¹æ¯”ä¸åŒæ¶æ„åœ¨è½¯ç»„ç»‡å½¢å˜é¢„æµ‹ä¸Šçš„è¡¨ç°

def compare_architectures_on_physrobot():
    """
    å¯¹æ¯”MLPã€GNNã€SE(3)-GNNåœ¨PhysRobotä»»åŠ¡ä¸Šçš„æ ·æœ¬æ•ˆç‡
    """
    # æ•°æ®ï¼šç»„ç»‡ç½‘æ ¼ + åŠ› â†’ å½¢å˜
    dataset = load_physrobot_dataset()  # (mesh, force) â†’ deformation
    
    architectures = {
        'MLP': FullyConnectedNet(input_dim=3000, hidden=[512, 256], output_dim=3000),
        'GNN': GraphNet(node_features=3, edge_features=1, hidden=128, num_layers=3),
        'SE3_GNN': SE3EquivariantGNN(node_features=3, hidden=128, num_layers=3),
        'SE3_GNN_Physics': SE3EquivariantGNN(..., physics_loss=True)
    }
    
    sample_sizes = [500, 1000, 2000, 5000, 10000, 50000]
    results = {name: [] for name in architectures}
    
    for n_samples in sample_sizes:
        train_data = dataset.sample(n_samples)
        
        for name, model in architectures.items():
            # è®­ç»ƒ
            model.train_on(train_data, epochs=100)
            
            # æµ‹è¯•è¯¯å·®ï¼ˆL2è·ç¦»ï¼‰
            test_error = model.evaluate(dataset.test)
            results[name].append(test_error)
            
            print(f"{name} with {n_samples} samples: test_error={test_error:.4f}")
    
    # å¯è§†åŒ–æ ·æœ¬æ•ˆç‡æ›²çº¿
    plt.figure(figsize=(10, 6))
    for name, errors in results.items():
        plt.plot(sample_sizes, errors, 'o-', label=name, linewidth=2)
    plt.xlabel('Training Samples')
    plt.ylabel('Test Error (mm)')
    plt.xscale('log')
    plt.yscale('log')
    plt.legend()
    plt.title('Sample Efficiency on PhysRobot')
    plt.grid(True)
    plt.show()
    
    # é¢„æœŸç»“æœï¼š
    # MLP: éœ€è¦>50kæ ·æœ¬æ‰èƒ½æ”¶æ•›
    # GNN: éœ€è¦~10kæ ·æœ¬
    # SE3_GNN: éœ€è¦~5kæ ·æœ¬
    # SE3_GNN_Physics: éœ€è¦~2kæ ·æœ¬ ï¼ˆæœ€å¼ºï¼ï¼‰
</code></pre>
    <p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼šä»£ç å®éªŒæ˜¯éªŒè¯ç†è®ºçš„<strong>è¯•é‡‘çŸ³</strong> â€” PhysRobotçš„æˆåŠŸä¸æ˜¯å¶ç„¶ï¼Œè€Œæ˜¯å‡ ä½•å…ˆéªŒçš„<strong>å¿…ç„¶ç»“æœ</strong>ã€‚</p>
  </div>
</div>

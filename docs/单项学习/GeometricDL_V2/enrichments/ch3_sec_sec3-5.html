<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>🔍 深入理解：GDL 蓝图 — 统一所有架构的 Master Recipe</h4>
    
    <div class="qa-pair">
      <p class="question">❓ 小白：什么是"GDL蓝图"？为什么说它能统一CNN、GNN、Transformer？</p>
      <div class="answer">
        <p>💡 专家：<strong>GDL蓝图 = 设计几何深度学习架构的四步菜谱</strong>。</p>
        <p><strong>核心思想</strong>：所有成功的深度学习架构都遵循同一个模式：</p>
        <ol style="background:var(--bg-code); padding:12px; border-radius:6px; margin:8px 0;">
          <li><strong>局部等变层</strong>：提取局部特征（尊重对称性）</li>
          <li><strong>非线性激活</strong>：引入表达能力</li>
          <li><strong>粗粒化/池化</strong>：降低分辨率（尺度分离）</li>
          <li><strong>不变聚合</strong>：最终输出（全局决策）</li>
        </ol>
        <p><strong>为什么能统一？</strong>因为它们只是选择了不同的：</p>
        <table style="width:100%; font-size:0.85em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th>组件</th><th>CNN</th><th>GNN</th><th>Transformer</th>
          </tr>
          <tr>
            <td><strong>域 $\Omega$</strong></td>
            <td>欧氏网格</td>
            <td>图</td>
            <td>集合（序列）</td>
          </tr>
          <tr>
            <td><strong>对称群 $\mathfrak{G}$</strong></td>
            <td>平移群</td>
            <td>置换群</td>
            <td>置换群</td>
          </tr>
          <tr>
            <td><strong>等变层</strong></td>
            <td>卷积</td>
            <td>消息传递</td>
            <td>自注意力</td>
          </tr>
          <tr>
            <td><strong>粗粒化</strong></td>
            <td>MaxPool</td>
            <td>图池化</td>
            <td>CLS token</td>
          </tr>
        </table>
        <p><strong>数学公式</strong>（通用形式）：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
h^{(l+1)}_v = σ( W^{(l)} · AGGREGATE({ h^{(l)}_u : u ∈ N(v) }) )
              ↑              ↑                    ↑
           非线性        等变线性层           邻域定义（依赖域）</pre>
        <p>只需替换"邻域定义"和"聚合方式"，就能得到不同架构！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：蓝图的"四大构建块"具体是什么？每块的作用是什么？</p>
      <div class="answer">
        <p>💡 专家：让我逐个拆解：</p>
        <p><strong>构建块 1：线性等变层（Linear $\mathfrak{G}$-equivariant layer）</strong></p>
        <ul>
          <li><strong>作用</strong>：提取局部特征，同时尊重对称性</li>
          <li><strong>数学</strong>：$f(ho(\mathfrak{g})x) = ho'(\mathfrak{g})f(x)$</li>
          <li><strong>实现</strong>：
            <ul>
              <li>CNN：卷积 $h \star \theta$（权重共享）</li>
              <li>GNN：$\sum_{u \in \mathcal{N}(v)} W \cdot h_u$（消息求和）</li>
              <li>Transformer：$\text{Attention}(Q, K, V)$（置换等变）</li>
            </ul>
          </li>
          <li><strong>为什么是线性？</strong>可微、可优化、叠加性原理</li>
        </ul>
        <p><strong>构建块 2：非线性激活（Nonlinearity）</strong></p>
        <ul>
          <li><strong>作用</strong>：打破线性限制，增加表达能力</li>
          <li><strong>常用</strong>：ReLU, GELU, Swish, LayerNorm</li>
          <li><strong>必须性</strong>：没有非线性 → 深度网络退化为单层线性映射</li>
          <li><strong>对称性</strong>：通常选择<strong>逐点</strong>激活（保持等变性）
            <ul>
              <li>✅ ReLU(h) 保持等变</li>
              <li>❌ BatchNorm 跨batch统计 → 破坏等变（需小心）</li>
            </ul>
          </li>
        </ul>
        <p><strong>构建块 3：局部池化/粗粒化（Local pooling / Coarsening）</strong></p>
        <ul>
          <li><strong>作用</strong>：降低分辨率、扩大感受野、实现尺度分离</li>
          <li><strong>实现</strong>：
            <ul>
              <li>CNN：MaxPool, AvgPool, Strided Conv</li>
              <li>GNN：TopK pooling, DiffPool, 图聚类</li>
              <li>Transformer：局部window（Swin）或直接全局</li>
            </ul>
          </li>
          <li><strong>近似等变</strong>：粗粒化会轻微破坏精确等变性，但换来效率</li>
        </ul>
        <p><strong>构建块 4：全局池化/不变聚合（Global pooling / Invariant layer）</strong></p>
        <ul>
          <li><strong>作用</strong>：从等变表示得到不变输出（用于分类/回归）</li>
          <li><strong>实现</strong>：
            <ul>
              <li>CNN：GlobalAvgPool, GlobalMaxPool</li>
              <li>GNN：$\sum_{v \in V} h_v$ 或 $\max_v h_v$</li>
              <li>Transformer：CLS token 或 mean pooling</li>
            </ul>
          </li>
          <li><strong>数学性质</strong>：置换/平移不变</li>
        </ul>
        <p><strong>组合方式</strong>：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
Input
  ↓
[Block 1+2] × N1 layers  ← 等变特征提取
  ↓
[Block 3]                 ← 粗粒化
  ↓
[Block 1+2] × N2 layers  ← 更高层等变处理
  ↓
[Block 3]                 ← 再次粗粒化
  ↓
...
  ↓
[Block 4]                 ← 全局聚合（不变）
  ↓
Output (classification/regression)</pre>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：蓝图公式里的"域 $\Omega$"、"对称群 $\mathfrak{G}$"、"信号空间 $\mathcal{X}$"这些抽象概念，对应到CNN具体是什么？</p>
      <div class="answer">
        <p>💡 专家：让我用CNN（最熟悉的架构）完整映射一遍：</p>
        <p><strong>1. 域 $\Omega = \mathbb{Z}^2$（整数网格）</strong></p>
        <ul>
          <li>含义：图像的像素位置集合</li>
          <li>例子：$224 \times 224$ 图像 → $\Omega = \{(i,j) : 0 \leq i,j < 224\}$</li>
        </ul>
        <p><strong>2. 对称群 $\mathfrak{G} = T(\mathbb{Z}^2)$（平移群）</strong></p>
        <ul>
          <li>群元素：$\mathfrak{g}_{(a,b)}$ = 平移 $(a, b)$ 个像素</li>
          <li>群作用：$\mathfrak{g}_{(a,b)}.(i,j) = (i+a, j+b)$</li>
          <li>例子：向右平移10像素 → $\mathfrak{g}_{(10,0)}$</li>
        </ul>
        <p><strong>3. 信号空间 $\mathcal{X}(\Omega) = \mathbb{R}^{H \times W \times C}$</strong></p>
        <ul>
          <li>含义：所有可能的图像</li>
          <li>信号：$x : \Omega \to \mathbb{R}^C$（每个位置有$C$个通道值）</li>
          <li>例子：RGB图像，$C=3$</li>
        </ul>
        <p><strong>4. 群表示 $ho(\mathfrak{g})$ 在信号上的作用</strong></p>
        <ul>
          <li>$(\rho(\mathfrak{g}_{(a,b)}) \cdot x)(i,j) = x(i-a, j-b)$</li>
          <li>含义：向右平移图像 = 每个像素取左边的值</li>
          <li>注意逆：$\mathfrak{g}^{-1}$ 保证群同态性</li>
        </ul>
        <p><strong>5. 等变线性层 = 卷积</strong></p>
        <ul>
          <li>$(h \star \theta)(i,j) = \sum_{(a,b)} \theta(a,b) \cdot h(i-a, j-b)$</li>
          <li>卷积核 $\theta$：$3 \times 3$ 权重矩阵</li>
          <li>权重共享：所有位置用<strong>同一个</strong> $\theta$</li>
          <li><strong>等变性</strong>：平移输入 → 输出也平移（见3.1节代码验证）</li>
        </ul>
        <p><strong>6. 非线性 = ReLU</strong></p>
        <ul>
          <li>$\sigma(h)(i,j) = \max(0, h(i,j))$</li>
          <li>逐像素应用 → 保持平移等变性</li>
        </ul>
        <p><strong>7. 粗粒化 = MaxPool(2×2, stride=2)</strong></p>
        <ul>
          <li>$224 \times 224 \to 112 \times 112$（降采样）</li>
          <li>新域：$\Omega' = \mathbb{Z}^2 / 2$（粗粒度网格）</li>
          <li>近似等变：对齐的平移保持等变，非对齐的有小误差</li>
        </ul>
        <p><strong>8. 全局池化 = GlobalAvgPool</strong></p>
        <ul>
          <li>$y = \frac{1}{HW} \sum_{i,j} h(i,j)$</li>
          <li>输出：$\mathbb{R}^C$（C个全局特征，无空间维度）</li>
          <li><strong>平移不变</strong>：$y$ 不随图像平移改变</li>
        </ul>
        <p><strong>完整流程</strong>：</p>
        <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
x ∈ ℝ^(224×224×3)     # 输入图像
  ↓ Conv(3→64) + ReLU  # 等变 + 非线性
h1 ∈ ℝ^(224×224×64)   # 等变特征
  ↓ MaxPool(2×2)       # 粗粒化
h2 ∈ ℝ^(112×112×64)   # 粗尺度等变特征
  ↓ Conv(64→128) × 3   # 多层等变处理
h3 ∈ ℝ^(56×56×128)    # 更高层次特征
  ↓ ...（重复）
h_final ∈ ℝ^(7×7×2048) # 最高层等变特征
  ↓ GlobalAvgPool      # 不变聚合
y ∈ ℝ^2048             # 不变全局表示
  ↓ FC(2048→1000)      # 分类头
logits ∈ ℝ^1000        # 类别分数</pre>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：GNN如何映射到GDL蓝图？和CNN的区别在哪？</p>
      <div class="answer">
        <p>💡 专家：核心区别在<strong>域和邻域定义</strong>！</p>
        <p><strong>1. 域 $\Omega = G = (V, E)$（图）</strong></p>
        <ul>
          <li>节点集 $V = \{v_1, ..., v_n\}$</li>
          <li>边集 $E \subseteq V \times V$</li>
          <li>例子：分子图（原子=节点，键=边）</li>
        </ul>
        <p><strong>2. 对称群 $\mathfrak{G} = S_n$（置换群）</strong></p>
        <ul>
          <li>群元素：$\sigma \in S_n$（节点重排）</li>
          <li>群作用：$\sigma.v_i = v_{\sigma(i)}$</li>
          <li>例子：[0,1,2] → [2,0,1] 是一个置换</li>
        </ul>
        <p><strong>3. 信号空间 $\mathcal{X}(G) = \mathbb{R}^{n \times d}$</strong></p>
        <ul>
          <li>每个节点一个 $d$ 维特征向量</li>
          <li>例子：分子中每个原子的化学特征（原子序号、电荷...）</li>
        </ul>
        <p><strong>4. 群表示 $\rho(\sigma)$ 的作用</strong></p>
        <ul>
          <li>$(\rho(\sigma) \cdot h)_i = h_{\sigma^{-1}(i)}$</li>
          <li>含义：重排节点 → 特征矩阵的行也重排</li>
        </ul>
        <p><strong>5. 等变线性层 = 消息传递</strong></p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
m_v = Σ_{u ∈ N(v)} W · h_u     # 聚合邻居消息
h'_v = UPDATE(h_v, m_v)         # 更新节点表示</pre>
        <ul>
          <li><strong>关键</strong>：求和 $\Sigma$ 对邻居顺序不敏感 → 置换等变！</li>
          <li>对比CNN：卷积 = 在规则网格上的消息传递</li>
        </ul>
        <p><strong>6. 邻域定义的差异</strong></p>
        <table style="width:100%; font-size:0.85em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th></th><th>CNN</th><th>GNN</th>
          </tr>
          <tr>
            <td>邻域</td>
            <td>固定几何（$3 \times 3$）</td>
            <td>图结构（边定义）</td>
          </tr>
          <tr>
            <td>邻居数</td>
            <td>固定（9个）</td>
            <td>可变（度数不同）</td>
          </tr>
          <tr>
            <td>权重</td>
            <td>卷积核参数化</td>
            <td>MLP或注意力</td>
          </tr>
        </table>
        <p><strong>7. GNN的粗粒化</strong></p>
        <ul>
          <li><strong>TopK池化</strong>：保留重要性最高的k个节点</li>
          <li><strong>图聚类</strong>：合并相似节点成"超节点"</li>
          <li><strong>层次化图</strong>：迭代粗化（类似多尺度CNN）</li>
        </ul>
        <p><strong>8. GNN的全局池化</strong></p>
        <ul>
          <li>$y = \sum_{v \in V} h_v$ 或 $y = \max_v h_v$</li>
          <li><strong>置换不变</strong>：节点顺序改变不影响和/最大值</li>
        </ul>
        <p><strong>数学公式对比</strong>：</p>
        <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
CNN:  h'(i,j) = σ( Σ_{a,b} θ(a,b) · h(i-a, j-b) )
                     ↑ 固定 3×3 邻域
GNN:  h'_v = σ( Σ_{u∈N(v)} W · h_u )
                  ↑ 图结构邻域（可变）</pre>
        <p>本质一样：都是"聚合邻居→非线性→更新"！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：Transformer也符合GDL蓝图吗？它看起来和CNN/GNN完全不同啊！</p>
      <div class="answer">
        <p>💡 专家：Transformer<strong>完全符合</strong>蓝图！它是<strong>全连接图上的GNN</strong>。</p>
        <p><strong>1. 域 $\Omega = $ 序列（集合+顺序）</strong></p>
        <ul>
          <li>可以看作<strong>完全图</strong>：每个token和所有token相连</li>
          <li>例子：句子 "I love AI" → 3个节点，两两相连</li>
        </ul>
        <p><strong>2. 对称群 $\mathfrak{G} = S_n$（置换群）</strong></p>
        <ul>
          <li>标准Transformer（无位置编码）对token顺序不敏感</li>
          <li>加位置编码 → <strong>破坏</strong>置换等变性（故意的！）</li>
        </ul>
        <p><strong>3. 等变层 = 自注意力</strong></p>
        <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
Q = XW_Q,  K = XW_K,  V = XW_V       # 线性投影
A = softmax(QK^T / √d)               # 注意力权重
H' = AV                              # 加权聚合</pre>
        <ul>
          <li>本质：<strong>可学习的消息传递权重</strong>！</li>
          <li>对比GNN：GNN权重固定（或基于边特征），Transformer权重由内容决定</li>
        </ul>
        <p><strong>4. 为什么是等变的？</strong></p>
        <ul>
          <li>置换输入 $X$ 的行 → $Q, K, V$ 的行也置换</li>
          <li>→ 注意力矩阵 $A$ 的行列同步置换</li>
          <li>→ 输出 $H' = AV$ 的行也置换 ✅</li>
        </ul>
        <p><strong>5. 与GNN的对比</strong>：</p>
        <table style="width:100%; font-size:0.85em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th></th><th>GNN</th><th>Transformer</th>
          </tr>
          <tr>
            <td>邻域</td>
            <td>稀疏（图结构）</td>
            <td>全连接（所有token）</td>
          </tr>
          <tr>
            <td>消息权重</td>
            <td>固定1或可学习MLP</td>
            <td>注意力分数（动态）</td>
          </tr>
          <tr>
            <td>复杂度</td>
            <td>$O(E)$（E=边数）</td>
            <td>$O(n^2)$（n=序列长度）</td>
          </tr>
          <tr>
            <td>归纳偏置</td>
            <td>强（稀疏性）</td>
            <td>弱（灵活性）</td>
          </tr>
        </table>
        <p><strong>6. Transformer的"粗粒化"</strong></p>
        <ul>
          <li><strong>分层Transformer</strong>：逐层减少token数（如Perceiver）</li>
          <li><strong>局部窗口</strong>：Swin Transformer（类似CNN的局部性）</li>
          <li><strong>Pooling token</strong>：BERT的[CLS]（特殊不变token）</li>
        </ul>
        <p><strong>7. Transformer的"全局池化"</strong></p>
        <ul>
          <li><strong>分类</strong>：取[CLS] token（BERT）</li>
          <li><strong>或</strong>：mean/max pooling所有token（某些变体）</li>
          <li>都是置换不变的聚合方式 ✅</li>
        </ul>
        <p><strong>关键洞察</strong>：</p>
        <p style="background:var(--example-bg); padding:10px; border-left:4px solid var(--example-border); border-radius:6px;">
          Transformer = GNN（全连接图） + 动态边权重（注意力）+ 位置编码（打破对称）
        </p>
        <p>这就是为什么Graph Transformer现在很火 — 它统一了两个框架！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：书中的"5G统一框架"是什么？为什么叫5G？</p>
      <div class="answer">
        <p>💡 专家：<strong>5G = 5个"G"开头的几何对象的层次</strong>，描述GDL架构的完整数学结构。</p>
        <p><strong>5个"G"</strong>（从底层到顶层）：</p>
        <ol style="background:var(--bg-code); padding:12px; border-radius:6px;">
          <li><strong>G₁ = Gauge（规范）</strong>：底层几何空间（域 $\Omega$）
            <ul>
              <li>例：欧氏空间 $\mathbb{R}^d$，流形，图</li>
            </ul>
          </li>
          <li><strong>G₂ = Group（群）</strong>：对称群 $\mathfrak{G}$
            <ul>
              <li>例：平移群、旋转群、置换群</li>
            </ul>
          </li>
          <li><strong>G₃ = Graph（图）</strong>：数据结构化表示
            <ul>
              <li>CNN中的像素网格可视为规则图</li>
              <li>GNN中的任意图</li>
            </ul>
          </li>
          <li><strong>G₄ = Geodesic（测地线）</strong>：域上的距离/度量
            <ul>
              <li>定义"局部性"（谁是邻居）</li>
              <li>例：欧氏距离、图最短路径、黎曼度量</li>
            </ul>
          </li>
          <li><strong>G₅ = Geometry（几何）</strong>：完整的几何先验集合
            <ul>
              <li>= 上述四者的组合 + 尺度分离</li>
            </ul>
          </li>
        </ol>
        <p><strong>为什么这个框架重要？</strong></p>
        <ul>
          <li>提供了一套<strong>统一语言</strong>描述所有几何DL架构</li>
          <li>指导<strong>设计新架构</strong>：明确需要定义哪些几何对象</li>
          <li>分析<strong>归纳偏置</strong>：每个"G"对应一种先验假设</li>
        </ul>
        <p><strong>实例：设计分子性质预测模型</strong></p>
        <ol>
          <li><strong>G₁ (Gauge)</strong>：3D欧氏空间 $\mathbb{R}^3$</li>
          <li><strong>G₂ (Group)</strong>：$\mathrm{SE}(3)$（旋转+平移）</li>
          <li><strong>G₃ (Graph)</strong>：分子图（原子=节点，键=边）</li>
          <li><strong>G₄ (Geodesic)</strong>：原子间距离（定义邻居）</li>
          <li><strong>G₅ (Geometry)</strong>：上述组合 → <strong>选择架构：E(n)-GNN</strong></li>
        </ol>
        <p><strong>不同任务的5G选择</strong>：</p>
        <table style="width:100%; font-size:0.85em;">
          <tr style="background:var(--bg-secondary);">
            <th>任务</th><th>G₁</th><th>G₂</th><th>G₃</th><th>推荐架构</th>
          </tr>
          <tr>
            <td>图像分类</td>
            <td>$\mathbb{Z}^2$</td>
            <td>$T(\mathbb{Z}^2)$</td>
            <td>规则网格</td>
            <td>CNN</td>
          </tr>
          <tr>
            <td>社交网络</td>
            <td>离散集合</td>
            <td>$S_n$</td>
            <td>任意图</td>
            <td>GNN</td>
          </tr>
          <tr>
            <td>蛋白质折叠</td>
            <td>$\mathbb{R}^3$</td>
            <td>$\mathrm{SE}(3)$</td>
            <td>残基图</td>
            <td>SE(3)-Transformer</td>
          </tr>
          <tr>
            <td>NLP</td>
            <td>序列</td>
            <td>$S_n$ (破坏)</td>
            <td>全连接</td>
            <td>Transformer</td>
          </tr>
        </table>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：如何用GDL蓝图设计一个全新的架构？有具体步骤吗？</p>
      <div class="answer">
        <p>💡 专家：有！<strong>五步设计法</strong>（从问题到架构）：</p>
        <p><strong>Step 1：识别域 $\Omega$</strong></p>
        <ul>
          <li>数据的"底层空间"是什么？</li>
          <li>例子：
            <ul>
              <li>图像 → $\mathbb{Z}^2$（网格）</li>
              <li>点云 → $\mathbb{R}^3$（连续空间）</li>
              <li>社交网络 → 图 $G=(V,E)$</li>
              <li>时间序列 → $\mathbb{R}$（时间轴）</li>
            </ul>
          </li>
        </ul>
        <p><strong>Step 2：确定对称群 $\mathfrak{G}$</strong></p>
        <ul>
          <li>什么变换不应改变任务结果？</li>
          <li>例子（分子性质预测）：
            <ul>
              <li>旋转分子 → 能量不变 → 需要 $\mathrm{SO}(3)$ 不变</li>
              <li>平移分子 → 能量不变 → 需要平移不变</li>
              <li>→ 对称群：$\mathrm{SE}(3)$ 或 $\mathrm{E}(3)$</li>
            </ul>
          </li>
        </ul>
        <p><strong>Step 3：设计等变线性层</strong></p>
        <ul>
          <li>满足 $f(\rho(\mathfrak{g})x) = \rho'(\mathfrak{g})f(x)$ 的线性映射</li>
          <li>常用模板：
            <ul>
              <li><strong>卷积</strong>（平移群）：$\sum \theta_k h_{u-k}$</li>
              <li><strong>消息传递</strong>（置换群）：$\sum_{u \in \mathcal{N}(v)} W h_u$</li>
              <li><strong>球谐函数</strong>（旋转群）：$\mathrm{SO}(3)$-等变卷积</li>
            </ul>
          </li>
          <li>如果没现成的 → 查文献（如 E(n)-GNN, TFN）</li>
        </ul>
        <p><strong>Step 4：选择粗粒化策略</strong></p>
        <ul>
          <li>如何降低分辨率？</li>
          <li>选择：
            <ul>
              <li>网格 → MaxPool / Strided Conv</li>
              <li>图 → 图聚类 / TopK pooling</li>
              <li>点云 → Farthest Point Sampling</li>
            </ul>
          </li>
        </ul>
        <p><strong>Step 5：选择全局聚合</strong></p>
        <ul>
          <li>任务需要不变输出吗？</li>
          <li>分类/回归 → 求和/平均/max（不变）</li>
          <li>分割/生成 → 跳过（保持等变）</li>
        </ul>
        <p><strong>实战例子：设计手术工具姿态估计网络</strong></p>
        <ol style="background:var(--bg-secondary); padding:12px; border-radius:6px; font-size:0.9em;">
          <li><strong>域</strong>：RGB-D点云 → $\mathbb{R}^3$</li>
          <li><strong>对称群</strong>：相机视角可变 → $\mathrm{SE}(3)$</li>
          <li><strong>等变层</strong>：SE(3)-等变卷积 / PointNet++</li>
          <li><strong>粗粒化</strong>：分层采样（4096→1024→256点）</li>
          <li><strong>输出</strong>：6D姿态（等变特征 + 回归头）</li>
        </ol>
        <p>→ 得到架构：<strong>SE(3)-PointNet</strong> ✅</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：蓝图说"先等变后不变"，为什么不能直接追求不变性？</p>
      <div class="answer">
        <p>💡 专家：<strong>直接不变 = 丢失所有中间信息</strong> — 太粗暴了！</p>
        <p><strong>错误做法</strong>（立即不变）：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
x ∈ ℝ^(224×224×3)
  ↓ 全局平均（直接聚合所有像素）
y ∈ ℝ^3  # 只剩3个数！
  ↓ MLP
output  # 准确率 < 30% ❌</pre>
        <p>问题：丢弃了所有空间结构！</p>
        <p><strong>正确做法</strong>（渐进式）：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
x ∈ ℝ^(224×224×3)
  ↓ Conv（等变）：提取边缘、纹理
h1 ∈ ℝ^(224×224×64)  # 保留空间信息
  ↓ Pool（粗化）：局部聚合
h2 ∈ ℝ^(112×112×64)  # 还有空间结构
  ↓ Conv（等变）：提取更高层特征
h3 ∈ ℝ^(56×56×128)   # 语义信息增加
  ↓ ...（重复）
h_final ∈ ℝ^(7×7×2048)  # 接近不变，但还有7×7位置
  ↓ GlobalPool（最终不变）
y ∈ ℝ^2048  # 准确率 > 95% ✅</pre>
        <p><strong>为什么渐进式更好？</strong></p>
        <ol>
          <li><strong>保留有用的空间信息</strong>：中间层知道"猫头在左上，尾巴在右下"</li>
          <li><strong>层次化抽象</strong>：从像素 → 纹理 → 部件 → 物体</li>
          <li><strong>可解释性</strong>：可以可视化中间层学到什么</li>
        </ol>
        <p><strong>类比：识别一个人</strong></p>
        <ul>
          <li><strong>直接不变</strong>：把所有像素求和 → 只知道"亮度总和"❌</li>
          <li><strong>渐进式</strong>：
            <ul>
              <li>Layer 1：眼睛、鼻子、嘴（等变特征）</li>
              <li>Layer 2：五官相对位置（等变，但粗化）</li>
              <li>Layer 3：整体脸型（接近不变）</li>
              <li>最后：身份（完全不变）✅</li>
            </ul>
          </li>
        </ul>
        <p><strong>数学角度</strong>：信息理论</p>
        <ul>
          <li>直接不变：信息瓶颈在输入端 → 丢弃所有细节</li>
          <li>渐进式：信息逐层压缩 → 保留任务相关的细节</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>🎯 直觉理解：GDL蓝图 = 建筑设计图纸</h4>
    <p><strong>想象建造一座房子</strong>：</p>
    <ul>
      <li><strong>域 $\Omega$</strong>：地基（地形、土壤类型）</li>
      <li><strong>对称群 $\mathfrak{G}$</strong>：抗震要求（结构必须对称以抗震）</li>
      <li><strong>等变层</strong>：墙壁、柱子（保持结构稳定）</li>
      <li><strong>粗粒化</strong>：楼层（从地基→1层→2层→屋顶，逐层抽象）</li>
      <li><strong>全局聚合</strong>：屋顶（最终覆盖整座房子）</li>
    </ul>
    <p><strong>不同"房子"（架构）</strong>：</p>
    <ul>
      <li><strong>CNN</strong>：规则网格地基上的公寓楼（统一布局）</li>
      <li><strong>GNN</strong>：不规则山地上的自适应建筑（根据地形调整）</li>
      <li><strong>Transformer</strong>：开放式广场（全连接，自由互动）</li>
    </ul>
    <p>虽然外观不同，但都遵循同一套<strong>建筑规范</strong>（GDL蓝图）！</p>
  </div>

  <div class="enrichment-application">
    <h4>🏥 医疗机器人应用：用GDL蓝图设计PhysRobot 2.0</h4>
    <p><strong>当前PhysRobot（GNS架构）</strong>：</p>
    <ul>
      <li>域：粒子图 $G=(V,E)$</li>
      <li>对称群：置换 $S_n$ + 平移 $T(\mathbb{R}^3)$</li>
      <li>等变层：EdgeFrame消息传递</li>
      <li>粗粒化：❌ 无（单尺度）</li>
      <li>全局聚合：❌ 无（逐粒子输出）</li>
    </ul>
    <p><strong>问题</strong>：大规模仿真（10K+粒子）效率低，泛化性有限</p>
    <p><strong>升级方案：PhysRobot 2.0（多尺度SE(3)-等变GNN）</strong></p>
    <ol style="background:var(--bg-secondary); padding:12px; border-radius:6px; font-size:0.9em;">
      <li><strong>增强对称性</strong>：
        <ul>
          <li>当前：平移不变（相对位置）</li>
          <li>升级：$\mathrm{SE}(3)$-等变（EGNN架构）</li>
          <li>优势：对旋转鲁棒，泛化到新姿态</li>
        </ul>
      </li>
      <li><strong>多尺度粗粒化</strong>：
        <ul>
          <li>Level 0：原始粒子（10K节点，细尺度力）</li>
          <li>Level 1：聚类成超粒子（1K节点，中尺度弹性）</li>
          <li>Level 2：再聚类（100节点，全局动量守恒）</li>
          <li>实现：DiffPool或可学习聚类</li>
        </ul>
      </li>
      <li><strong>跨尺度消息传递</strong>：
        <ul>
          <li>Bottom-up：细粒度聚合到粗粒度</li>
          <li>Top-down：粗粒度指导细粒度（U-Net风格）</li>
          <li>Skip connections：直接融合不同尺度</li>
        </ul>
      </li>
      <li><strong>全局约束</strong>：
        <ul>
          <li>全局池化：总动量、总能量（物理守恒律）</li>
          <li>作为软约束加入损失函数</li>
        </ul>
      </li>
    </ol>
    <p><strong>预期性能提升</strong>：</p>
    <table style="width:100%; font-size:0.85em;">
      <tr style="background:var(--bg-secondary);">
        <th>指标</th><th>PhysRobot 1.0</th><th>PhysRobot 2.0</th>
      </tr>
      <tr>
        <td>推理速度（10K粒子）</td>
        <td>120ms</td>
        <td>45ms ✅ (2.7x)</td>
      </tr>
      <tr>
        <td>长时rollout误差（500步）</td>
        <td>0.023m</td>
        <td>0.011m ✅ (2x)</td>
      </tr>
      <tr>
        <td>跨材料泛化</td>
        <td>75%准确率</td>
        <td>91%准确率 ✅</td>
      </tr>
      <tr>
        <td>参数量</td>
        <td>1.2M</td>
        <td>1.5M (略增)</td>
      </tr>
    </table>
    <p><strong>关键代码框架</strong>：</p>
    <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
class PhysRobot2(nn.Module):
    def __init__(self):
        self.encoder = EGNN(layers=3)  # SE(3)-等变
        self.pool1 = DiffPool(ratio=0.1)  # 10K → 1K
        self.processor1 = EGNN(layers=2)
        self.pool2 = DiffPool(ratio=0.1)  # 1K → 100
        self.processor2 = EGNN(layers=2)
        # U-Net 风格上采样
        self.unpool2 = GraphUpsample()
        self.unpool1 = GraphUpsample()
        self.decoder = EGNN(layers=2)
    
    def forward(self, pos, vel, edges):
        # 编码
        h0 = self.encoder(pos, vel, edges)
        
        # 下采样
        h1, idx1 = self.pool1(h0, edges)
        h1 = self.processor1(h1)
        h2, idx2 = self.pool2(h1)
        h2 = self.processor2(h2)
        
        # 上采样 + 跨层融合
        h1_up = self.unpool2(h2, idx2) + h1
        h0_up = self.unpool1(h1_up, idx1) + h0
        
        # 解码
        accel = self.decoder(h0_up)
        
        # 物理约束（软约束）
        momentum = global_sum(vel)  # 应接近0
        return accel, momentum</pre>
    <p><strong>结论</strong>：GDL蓝图不仅是理论工具，更是<strong>实用的架构设计指南</strong>！</p>
  </div>
</div>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 0: 数学前置知识 | GDL 学习指南</title>
  <link rel="stylesheet" href="../assets/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Noto+Serif+SC:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]})"></script>
  
  <style>
/* ========== Enrichment Blocks ========== */
.enrichment-block {
  margin: 2.5rem 0;
  padding: 2rem;
  background: linear-gradient(135deg, #f0f7ff 0%, #e8f4fd 100%);
  border-left: 4px solid #3b82f6;
  border-radius: 0 12px 12px 0;
  box-shadow: 0 2px 8px rgba(59, 130, 246, 0.1);
}
[data-theme="dark"] .enrichment-block {
  background: linear-gradient(135deg, #1a2332 0%, #1e293b 100%);
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
}

.enrichment-block h4 {
  margin-top: 0;
  font-size: 1.2rem;
  color: #1e40af;
}
[data-theme="dark"] .enrichment-block h4 {
  color: #93c5fd;
}

.enrichment-qa { margin-bottom: 1.5rem; }

.qa-pair {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(255,255,255,0.7);
  border-radius: 10px;
  transition: transform 0.2s;
}
.qa-pair:hover { transform: translateX(4px); }
[data-theme="dark"] .qa-pair {
  background: rgba(0,0,0,0.25);
}

.question {
  font-weight: 700;
  color: #2563eb;
  margin-bottom: 0.75rem;
  font-size: 1.05rem;
  line-height: 1.6;
}
[data-theme="dark"] .question { color: #60a5fa; }

.answer {
  line-height: 1.9;
  color: #374151;
  font-size: 1rem;
}
[data-theme="dark"] .answer { color: #d1d5db; }
.answer p { margin: 0.5rem 0; }

.enrichment-intuition {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(251,191,36,0.1);
  border-radius: 10px;
  border-left: 3px solid #f59e0b;
  line-height: 1.8;
}
[data-theme="dark"] .enrichment-intuition {
  background: rgba(251,191,36,0.05);
}

.enrichment-application {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(16,185,129,0.1);
  border-radius: 10px;
  border-left: 3px solid #10b981;
  line-height: 1.8;
}
[data-theme="dark"] .enrichment-application {
  background: rgba(16,185,129,0.05);
}

.enrichment-summary {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(139,92,246,0.1);
  border-radius: 10px;
  border-left: 3px solid #8b5cf6;
  line-height: 1.8;
}
[data-theme="dark"] .enrichment-summary {
  background: rgba(139,92,246,0.05);
}

.symbol-table {
  margin: 1.5rem 0;
  border-collapse: collapse;
  width: 100%;
  background: white;
  border-radius: 8px;
  overflow: hidden;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
[data-theme="dark"] .symbol-table {
  background: #1e293b;
}
.symbol-table th {
  background: #3b82f6;
  color: white;
  padding: 12px 16px;
  text-align: left;
  font-weight: 600;
}
.symbol-table td {
  padding: 12px 16px;
  border-bottom: 1px solid #e5e7eb;
}
[data-theme="dark"] .symbol-table td {
  border-bottom: 1px solid #374151;
}
.symbol-table tr:hover {
  background: #f0f7ff;
}
[data-theme="dark"] .symbol-table tr:hover {
  background: #1a2332;
}

.example-box {
  margin: 1.5rem 0;
  padding: 1.5rem;
  background: #fef3c7;
  border-left: 4px solid #f59e0b;
  border-radius: 0 8px 8px 0;
}
[data-theme="dark"] .example-box {
  background: #2d1f0a;
}

.counter-example-box {
  margin: 1.5rem 0;
  padding: 1.5rem;
  background: #fee2e2;
  border-left: 4px solid #ef4444;
  border-radius: 0 8px 8px 0;
}
[data-theme="dark"] .counter-example-box {
  background: #2d1010;
}

.formula-box {
  margin: 2rem 0;
  padding: 1.5rem;
  background: #f9fafb;
  border: 2px solid #3b82f6;
  border-radius: 12px;
}
[data-theme="dark"] .formula-box {
  background: #111827;
}
  </style>
</head>
<body>
  <div class="progress-bar"></div>

  <header class="header">
    <div class="header-title"><a href="../index.html">📐 GDL 学习指南</a></div>
    <div class="header-nav">
      <a href="../index.html">目录</a>
      <a href="../chapter1/index.html">下一章 →</a>
      <button class="theme-toggle" onclick="toggleTheme()">🌙</button>
    </div>
  </header>

  <button class="sidebar-toggle" onclick="toggleSidebar()">☰</button>

  <nav class="sidebar">
    <h3>Chapter 0</h3>
    <a href="#overview">为什么需要这章？</a>
    <a href="#notation">0.1 数学符号速查表</a>
    <a href="#group-theory">0.2 群论从零开始</a>
    <a href="#what-is-group" class="sub">0.2.1 什么是"群"？</a>
    <a href="#subgroups" class="sub">0.2.2 子群与商群</a>
    <a href="#group-action" class="sub">0.2.3 群作用</a>
    <a href="#representation" class="sub">0.2.4 群表示</a>
    <a href="#equivariance" class="sub">0.2.5 不变性与等变性</a>
    <a href="#tensors">0.3 张量入门</a>
    <a href="#manifolds">0.4 流形和拓扑直觉</a>
    <a href="#fourier">0.5 傅里叶变换直觉</a>
    <a href="#linear-algebra">0.6 线性代数复习</a>
    <a href="#key-formulas">0.7 书中关键公式速解</a>
    <a href="#learning-path">0.8 学习路径建议</a>
    <h3>导航</h3>
    <a href="../index.html">📚 总目录</a>
    <a href="../chapter1/index.html">→ Ch.1 引言</a>
  </nav>

  <main class="main">
    <h1>Chapter 0: 数学前置知识<br><span style="font-size:0.6em;color:var(--text-secondary)">Mathematical Prerequisites — 从零开始的几何深度学习数学之旅</span></h1>

    <div class="callout callout-warning" id="overview">
      <h4>🎯 为什么需要这一章？</h4>
      <p><strong>你的背景</strong>：你是一位工科研究生，学过<strong>高等数学（微积分）</strong>和<strong>线性代数</strong>，但从未接触过抽象代数、群论或微分几何。</p>
      <p><strong>这本书的挑战</strong>：《Geometric Deep Learning》大量使用群论、流形、张量等高级数学概念——这些对于理解为什么 CNN、GNN 和 Transformer 能工作至关重要。</p>
      <p><strong>这章能帮你什么</strong>：</p>
      <ul>
        <li>✅ <strong>符号不迷路</strong>：所有陌生符号都有解释</li>
        <li>✅ <strong>概念有直觉</strong>：用生活类比理解抽象概念</li>
        <li>✅ <strong>公式能看懂</strong>：关键公式逐个拆解</li>
        <li>✅ <strong>例子接地气</strong>：从旋转图片、排列物体等日常操作理解群论</li>
      </ul>
      <p><strong>阅读建议</strong>：先快速浏览一遍，标记不懂的部分。在阅读正文时遇到困难，再回来查阅相应章节。这是一本<strong>参考手册</strong>，不需要一次性全部掌握！</p>
      <p><strong>预计阅读时间</strong>：2-3 小时（首次浏览）+ 长期参考</p>
    </div>

    <!-- ========================================
         0.1 数学符号速查表
    ======================================== -->
    <h2 id="notation">0.1 数学符号速查表 📋<br><span style="font-size:0.7em;color:var(--text-secondary)">Notation Quick Reference</span></h2>

    <p>在几何深度学习中，你会遇到许多特殊符号。这里按类别整理，方便随时查阅。</p>

    <h3>集合与空间符号</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th style="width:15%">符号</th>
          <th style="width:25%">读作</th>
          <th>含义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$\mathbb{R}$</td>
          <td>实数集</td>
          <td>所有实数的集合，如 $\{-1.5, 0, \pi, 3.14, \ldots\}$</td>
        </tr>
        <tr>
          <td>$\mathbb{R}^n$</td>
          <td>n维实空间</td>
          <td>所有 $n$ 维实向量，如 $\mathbb{R}^3 = \{(x,y,z) \mid x,y,z \in \mathbb{R}\}$</td>
        </tr>
        <tr>
          <td>$\mathbb{Z}$</td>
          <td>整数集</td>
          <td>所有整数 $\{\ldots, -2, -1, 0, 1, 2, \ldots\}$</td>
        </tr>
        <tr>
          <td>$\mathbb{C}$</td>
          <td>复数集</td>
          <td>所有复数 $\{a + bi \mid a, b \in \mathbb{R}\}$</td>
        </tr>
        <tr>
          <td>$\mathbb{N}$</td>
          <td>自然数集</td>
          <td>非负整数 $\{0, 1, 2, 3, \ldots\}$（有些定义从 1 开始）</td>
        </tr>
      </tbody>
    </table>

    <h3>逻辑与关系符号</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th style="width:15%">符号</th>
          <th style="width:25%">读作</th>
          <th>含义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$\forall$</td>
          <td>对于所有</td>
          <td>全称量词，如 "$\forall x \in \mathbb{R}$" 表示"对于所有实数 $x$"</td>
        </tr>
        <tr>
          <td>$\exists$</td>
          <td>存在</td>
          <td>存在量词，如 "$\exists x \in \mathbb{R}$" 表示"存在实数 $x$"</td>
        </tr>
        <tr>
          <td>$\in$</td>
          <td>属于</td>
          <td>$x \in A$ 表示 $x$ 是集合 $A$ 的元素</td>
        </tr>
        <tr>
          <td>$\subset$</td>
          <td>真包含</td>
          <td>$A \subset B$ 表示 $A$ 是 $B$ 的真子集（$A \neq B$）</td>
        </tr>
        <tr>
          <td>$\subseteq$</td>
          <td>包含</td>
          <td>$A \subseteq B$ 表示 $A$ 是 $B$ 的子集（可以相等）</td>
        </tr>
        <tr>
          <td>$\cong$</td>
          <td>同构</td>
          <td>两个结构在某种意义下"相同"（保持所有重要性质的映射存在）</td>
        </tr>
        <tr>
          <td>$\simeq$</td>
          <td>同伦/等价</td>
          <td>拓扑空间同伦等价，或更一般的等价关系</td>
        </tr>
      </tbody>
    </table>

    <h3>函数与映射符号</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th style="width:15%">符号</th>
          <th style="width:25%">读作</th>
          <th>含义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$f: X \to Y$</td>
          <td>$f$ 从 $X$ 映射到 $Y$</td>
          <td>函数 $f$ 的定义域是 $X$，值域在 $Y$ 中</td>
        </tr>
        <tr>
          <td>$f \circ g$</td>
          <td>$f$ 复合 $g$</td>
          <td>函数复合，$(f \circ g)(x) = f(g(x))$，<strong>先执行 $g$ 再执行 $f$</strong></td>
        </tr>
        <tr>
          <td>$f^{-1}$</td>
          <td>$f$ 的逆映射</td>
          <td>如果 $f(x) = y$，则 $f^{-1}(y) = x$（需要 $f$ 可逆）</td>
        </tr>
        <tr>
          <td>$\mathrm{Id}$ 或 $\mathrm{id}$</td>
          <td>恒等映射</td>
          <td>$\mathrm{Id}(x) = x$，什么也不做的映射</td>
        </tr>
      </tbody>
    </table>

    <h3>线性代数符号</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th style="width:15%">符号</th>
          <th style="width:25%">读作</th>
          <th>含义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$\langle \mathbf{u}, \mathbf{v} \rangle$</td>
          <td>内积</td>
          <td>向量 $\mathbf{u}$ 和 $\mathbf{v}$ 的内积（点积），在 $\mathbb{R}^n$ 中等于 $\sum_i u_i v_i$</td>
        </tr>
        <tr>
          <td>$\|\mathbf{v}\|$</td>
          <td>范数</td>
          <td>向量的长度，常用欧几里得范数 $\|\mathbf{v}\| = \sqrt{\sum_i v_i^2}$</td>
        </tr>
        <tr>
          <td>$\ker(A)$</td>
          <td>核</td>
          <td>矩阵 $A$ 的零空间，$\ker(A) = \{\mathbf{x} \mid A\mathbf{x} = \mathbf{0}\}$</td>
        </tr>
        <tr>
          <td>$\mathrm{Im}(A)$</td>
          <td>像</td>
          <td>矩阵 $A$ 的列空间，$\mathrm{Im}(A) = \{A\mathbf{x} \mid \mathbf{x} \in \mathbb{R}^n\}$</td>
        </tr>
        <tr>
          <td>$\dim(V)$</td>
          <td>维度</td>
          <td>向量空间 $V$ 的维数</td>
        </tr>
        <tr>
          <td>$\det(A)$</td>
          <td>行列式</td>
          <td>方阵 $A$ 的行列式，衡量变换的"体积缩放因子"</td>
        </tr>
        <tr>
          <td>$\mathrm{tr}(A)$</td>
          <td>迹</td>
          <td>矩阵对角元素之和，$\mathrm{tr}(A) = \sum_i A_{ii}$</td>
        </tr>
      </tbody>
    </table>

    <h3>代数运算符号</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th style="width:15%">符号</th>
          <th style="width:25%">读作</th>
          <th>含义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$\otimes$</td>
          <td>张量积</td>
          <td>两个向量/矩阵的张量积，如 $\mathbf{u} \otimes \mathbf{v}$ 是外积矩阵</td>
        </tr>
        <tr>
          <td>$\oplus$</td>
          <td>直和</td>
          <td>向量空间的直和，或聚合操作（GNN 中常用求和、最大值等）</td>
        </tr>
        <tr>
          <td>$*$</td>
          <td>卷积</td>
          <td>函数卷积 $(f * g)(x) = \int f(y)g(x-y)dy$</td>
        </tr>
      </tbody>
    </table>

    <h3>微积分符号</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th style="width:15%">符号</th>
          <th style="width:25%">读作</th>
          <th>含义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$\partial$</td>
          <td>偏导数</td>
          <td>$\frac{\partial f}{\partial x}$ 表示 $f$ 对 $x$ 的偏导数</td>
        </tr>
        <tr>
          <td>$\nabla$</td>
          <td>梯度/nabla算子</td>
          <td>$\nabla f = \left(\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}\right)$</td>
        </tr>
        <tr>
          <td>$\Delta$</td>
          <td>拉普拉斯算子</td>
          <td>$\Delta f = \nabla^2 f = \sum_i \frac{\partial^2 f}{\partial x_i^2}$（在欧几里得空间）</td>
        </tr>
        <tr>
          <td>$\mathcal{L}$</td>
          <td>拉普拉斯矩阵/损失函数</td>
          <td>图拉普拉斯矩阵（$L = D - A$）或损失函数（Loss）</td>
        </tr>
      </tbody>
    </table>

    <h3>特殊字体的含义</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th style="width:15%">字体</th>
          <th style="width:25%">示例</th>
          <th>通常含义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>黑板粗体</td>
          <td>$\mathbb{R}, \mathbb{Z}, \mathbb{C}$</td>
          <td>数系（实数、整数、复数等）</td>
        </tr>
        <tr>
          <td>花体</td>
          <td>$\mathcal{F}, \mathcal{L}, \mathcal{H}$</td>
          <td>函数空间、损失函数、希尔伯特空间等抽象对象</td>
        </tr>
        <tr>
          <td>哥特体</td>
          <td>$\mathfrak{g}, \mathfrak{so}(3)$</td>
          <td>李代数（群的切空间）</td>
        </tr>
        <tr>
          <td>粗体</td>
          <td>$\mathbf{x}, \mathbf{A}$</td>
          <td>向量、矩阵</td>
        </tr>
        <tr>
          <td>斜体</td>
          <td>$x, f, G$</td>
          <td>标量、函数、群等</td>
        </tr>
      </tbody>
    </table>

    <div class="enrichment-block">
      <div class="enrichment-intuition">
        <h4>🎯 符号直觉</h4>
        <p><strong>为什么数学家喜欢用这么多特殊符号？</strong></p>
        <p>想象你在写代码。你会用 <code>int count = 0;</code> 而不是 <code>某个整数变量等于零</code>。数学符号就像编程语言的<strong>类型系统</strong>：</p>
        <ul>
          <li>$\mathbb{R}^n$ 告诉你"这是 n 维向量空间"（类型声明）</li>
          <li>$\forall$ 是循环 <code>for all</code></li>
          <li>$\exists$ 是条件 <code>if exists</code></li>
          <li>$\circ$ 是函数调用链 <code>f(g(x))</code></li>
        </ul>
        <p>一旦习惯，这些符号比文字更简洁、更精确！</p>
      </div>
    </div>

    <!-- ========================================
         0.2 群论从零开始
    ======================================== -->
    <h2 id="group-theory">0.2 群论从零开始 🔑<br><span style="font-size:0.7em;color:var(--text-secondary)">Group Theory for Beginners</span></h2>

    <div class="callout callout-info">
      <h4>为什么群论是 GDL 的核心？</h4>
      <p>几何深度学习的<strong>统一原理</strong>是：</p>
      <blockquote>
        <strong>好的神经网络应该尊重数据的对称性。</strong>
      </blockquote>
      <p>而<strong>对称性的数学语言就是群论</strong>。CNN 尊重平移对称性，球面 CNN 尊重旋转对称性，图神经网络尊重置换对称性——这些都可以用群的语言统一描述。</p>
      <p>如果你只能从这章学会一件事，那就是：<strong>群是一组"操作"，它们可以组合、可以撤销。</strong></p>
    </div>

    <h3 id="what-is-group">0.2.1 什么是"群"？</h3>

    <p>让我们从你熟悉的操作开始。</p>

    <div class="enrichment-intuition">
      <h4>🎯 生活中的群：转魔方</h4>
      <p>想象你在玩魔方。你可以做这些操作：</p>
      <ul>
        <li>转动顶层 90°（记为 $R$）</li>
        <li>转动底层 90°（记为 $L$）</li>
        <li>什么都不做（记为 $e$，恒等操作）</li>
        <li>……</li>
      </ul>
      <p>这些操作有四个关键性质：</p>
      <ol>
        <li><strong>组合还是操作</strong>：先转 $R$ 再转 $L$，结果还是一个魔方操作</li>
        <li><strong>组合有顺序但满足结合律</strong>：$(R \cdot L) \cdot R = R \cdot (L \cdot R)$（括号位置不影响结果）</li>
        <li><strong>有"什么都不做"</strong>：$e \cdot R = R \cdot e = R$</li>
        <li><strong>可以撤销</strong>：转 $R$ 四次回到原位，$R \cdot R \cdot R \cdot R = e$</li>
      </ol>
      <p>这四条性质就是<strong>群</strong>的定义！</p>
    </div>

    <div class="formula-box">
      <h4>群的正式定义</h4>
      <p>一个<strong>群</strong> $(G, \cdot)$ 是一个集合 $G$ 加上一个二元运算 $\cdot$，满足：</p>
      <ol>
        <li><strong>封闭性</strong>：$\forall g, h \in G$，有 $g \cdot h \in G$（组合后还在群里）</li>
        <li><strong>结合律</strong>：$\forall g, h, k \in G$，有 $(g \cdot h) \cdot k = g \cdot (h \cdot k)$</li>
        <li><strong>单位元</strong>：$\exists e \in G$，使得 $\forall g \in G$，有 $e \cdot g = g \cdot e = g$</li>
        <li><strong>逆元</strong>：$\forall g \in G$，$\exists g^{-1} \in G$，使得 $g \cdot g^{-1} = g^{-1} \cdot g = e$</li>
      </ol>
    </div>

    <h4>例子 1：整数加法群 $(\mathbb{Z}, +)$</h4>
    <div class="example-box">
      <p><strong>集合</strong>：所有整数 $\mathbb{Z} = \{\ldots, -2, -1, 0, 1, 2, \ldots\}$</p>
      <p><strong>运算</strong>：加法 $+$</p>
      <p><strong>验证</strong>：</p>
      <ul>
        <li>封闭性：整数相加还是整数 ✓</li>
        <li>结合律：$(a + b) + c = a + (b + c)$ ✓</li>
        <li>单位元：$0$（因为 $0 + a = a$）✓</li>
        <li>逆元：$a$ 的逆元是 $-a$（因为 $a + (-a) = 0$）✓</li>
      </ul>
      <p>所以整数加法是一个群！</p>
    </div>

    <h4>例子 2：旋转群</h4>
    <div class="example-box">
      <p>考虑正方形的旋转。你可以：</p>
      <ul>
        <li>$r_0$：不转（旋转 0°）</li>
        <li>$r_1$：转 90°</li>
        <li>$r_2$：转 180°</li>
        <li>$r_3$：转 270°</li>
      </ul>
      <p><strong>运算</strong>：连续旋转（先做一个再做另一个）</p>
      <p><strong>验证</strong>：</p>
      <ul>
        <li>封闭性：$r_1 \cdot r_1 = r_2$（转两次 90° = 转 180°）✓</li>
        <li>结合律：$(r_1 \cdot r_1) \cdot r_2 = r_1 \cdot (r_1 \cdot r_2)$ 都等于 $r_0$ ✓</li>
        <li>单位元：$r_0$（不转）✓</li>
        <li>逆元：$r_1$ 的逆是 $r_3$（转 90° 的撤销是转 270°）✓</li>
      </ul>
      <p>这个群有 4 个元素，称为<strong>循环群</strong> $C_4$。</p>
    </div>

    <h4>例子 3：对称群 $S_3$ — 3 个物体的排列</h4>
    <div class="example-box">
      <p>有 3 个球：🔴🔵🟢，你可以重新排列它们。</p>
      <p>所有可能的排列：</p>
      <ul>
        <li>$e$：不变（🔴🔵🟢）</li>
        <li>$(12)$：交换 1 和 2（🔵🔴🟢）</li>
        <li>$(13)$：交换 1 和 3（🟢🔵🔴）</li>
        <li>$(23)$：交换 2 和 3（🔴🟢🔵）</li>
        <li>$(123)$：循环（🔵🟢🔴）</li>
        <li>$(132)$：反向循环（🟢🔴🔵）</li>
      </ul>
      <p>共 $3! = 6$ 个元素。这是<strong>对称群</strong> $S_3$。</p>
      <p><strong>为什么重要</strong>：图神经网络的<strong>置换不变性</strong>就来自 $S_n$ 群！</p>
    </div>

    <h4>反例：什么<strong>不是</strong>群？</h4>
    <div class="counter-example-box">
      <p><strong>自然数加法 $(\mathbb{N}, +)$</strong>：</p>
      <ul>
        <li>封闭性：✓</li>
        <li>结合律：✓</li>
        <li>单位元：0 ✓</li>
        <li>逆元：<strong>✗</strong>（5 的逆是 -5，但 -5 不是自然数！）</li>
      </ul>
      <p><strong>结论</strong>：自然数加法<strong>不是</strong>群，因为没有逆元。它只是一个"幺半群"（monoid）。</p>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>🔍 深入理解</h4>
        <div class="qa-pair">
          <p class="question">❓ 小白：为什么要求"可以撤销"（逆元）？</p>
          <div class="answer">
            <p>💡 专家：想象你在调整图片：旋转、缩放、平移。如果不能撤销，你会很痛苦！群论研究的是<strong>可逆变换</strong>——这是几何学的基础。</p>
            <p>更深层的原因：可逆性保证了信息不丢失。在深度学习中，如果变换丢失信息，网络就很难学习到正确的表示。</p>
          </div>
        </div>
        <div class="qa-pair">
          <p class="question">❓ 小白：群的运算一定是"乘法"吗？</p>
          <div class="answer">
            <p>💡 专家：不！运算可以是加法（$\mathbb{Z}$ 的 $+$）、乘法（非零实数的 $\times$）、函数复合（$\circ$）、矩阵乘法，甚至是图的节点重新标号。</p>
            <p>我们写 $g \cdot h$ 只是习惯，实际含义取决于具体的群。关键是满足四条公理！</p>
          </div>
        </div>
      </div>
    </div>

    <h3 id="subgroups">0.2.2 子群、陪集、商群</h3>

    <h4>子群 (Subgroup)</h4>
    <p>群里面的"小群"。</p>

    <div class="formula-box">
      <p><strong>定义</strong>：设 $H \subseteq G$。如果 $H$ 本身也是群（用同样的运算），则称 $H$ 是 $G$ 的<strong>子群</strong>，记作 $H \leq G$。</p>
    </div>

    <div class="example-box">
      <p><strong>例子</strong>：$(\mathbb{Z}, +)$ 是 $(\mathbb{R}, +)$ 的子群。</p>
      <p><strong>例子</strong>：旋转群 $\{r_0, r_2\}$（只转 0° 和 180°）是 $C_4$ 的子群。</p>
    </div>

    <h4>正规子群与商群 (Normal Subgroup & Quotient Group)</h4>
    <p>这部分稍微抽象，但核心直觉是：</p>

    <div class="enrichment-intuition">
      <h4>🎯 商群直觉：分组</h4>
      <p>想象你有一群学生（群 $G$），你想按宿舍楼分组。每个宿舍楼是一个"陪集"，整个分组方案是"商群" $G/H$。</p>
      <p><strong>关键</strong>：商群把原来的群"粗粒化"了——不再关心具体是哪个学生,只关心在哪栋楼。</p>
      <p><strong>在 GDL 中的应用</strong>：当我们说 CNN 对平移"不变"，实际上是在用平移群的商群！</p>
    </div>

    <h3 id="group-action">0.2.3 群作用 — 群如何"控制"其他东西</h3>

    <p>群本身是抽象的操作集合。<strong>群作用</strong>让这些操作实际地作用在某个对象上。</p>

    <div class="formula-box">
      <h4>群作用的定义</h4>
      <p>设 $G$ 是群，$X$ 是集合。一个<strong>（左）群作用</strong>是映射：</p>
      <p style="text-align:center;">$\rho: G \times X \to X, \quad (g, x) \mapsto g \cdot x$</p>
      <p>满足：</p>
      <ol>
        <li>$e \cdot x = x$（单位元不改变对象）</li>
        <li>$(g \cdot h) \cdot x = g \cdot (h \cdot x)$（作用的组合等于组合后的作用）</li>
      </ol>
    </div>

    <div class="example-box">
      <h4>例子 1：旋转群作用在图片上</h4>
      <p>设 $G = C_4$（正方形旋转群），$X$ = 所有图片。</p>
      <p>$r_1 \cdot \text{🐱} = \text{🐱}^{\circlearrowleft}$（把图片顺时针转 90°）</p>
      <p>这就是为什么<strong>旋转等变卷积</strong>有意义：卷积核应该跟着图片一起转！</p>
    </div>

    <div class="example-box">
      <h4>例子 2：置换群作用在图的节点上</h4>
      <p>设 $G = S_n$（$n$ 个节点的置换群），$X$ = 图的节点特征矩阵。</p>
      <p>$\sigma \cdot X$：把节点重新标号（行重排）。</p>
      <p>图神经网络应该对 $\sigma$ 不敏感——这就是<strong>置换不变性</strong>！</p>
    </div>

    <h4>轨道 (Orbit) 和稳定子 (Stabilizer)</h4>
    <div class="formula-box">
      <p><strong>轨道</strong>：$\mathcal{O}_x = \{g \cdot x \mid g \in G\}$（从 $x$ 出发能到达的所有点）</p>
      <p><strong>稳定子</strong>：$\mathrm{Stab}(x) = \{g \in G \mid g \cdot x = x\}$（不改变 $x$ 的所有操作）</p>
    </div>

    <div class="enrichment-intuition">
      <h4>🎯 直觉</h4>
      <p><strong>轨道</strong>：把一个球放在光滑的碗里，它能滚到的所有位置。</p>
      <p><strong>稳定子</strong>：不管怎么转碗，球都不动的那些转法（比如绕球心旋转）。</p>
    </div>

    <h3 id="representation">0.2.4 群表示 — 把抽象的群变成矩阵</h3>

    <p>计算机不懂"旋转 90°"，但懂矩阵乘法。<strong>群表示</strong>把群变成矩阵！</p>

    <div class="formula-box">
      <h4>群表示的定义</h4>
      <p>群 $G$ 的一个<strong>表示</strong>是一个同态映射：</p>
      <p style="text-align:center;">$\rho: G \to \mathrm{GL}(V)$</p>
      <p>其中 $\mathrm{GL}(V)$ 是向量空间 $V$ 上的可逆线性变换群（矩阵群），满足：</p>
      <ul>
        <li>$\rho(e) = I$（单位元对应单位矩阵）</li>
        <li>$\rho(g \cdot h) = \rho(g) \rho(h)$（群运算对应矩阵乘法）</li>
      </ul>
    </div>

    <div class="example-box">
      <h4>例子：旋转群的矩阵表示</h4>
      <p>2D 平面旋转 $\theta$ 角度对应矩阵：</p>
      <p>$$\rho(r_\theta) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$$</p>
      <p>比如旋转 90° ($\theta = \pi/2$)：</p>
      <p>$$\rho(r_{90°}) = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$</p>
      <p>把向量 $(1, 0)$ 转成 $(0, 1)$ ✓</p>
    </div>

    <div class="example-box">
      <h4>例子：置换群的置换矩阵</h4>
      <p>置换 $(12)$（交换第 1 和第 2 个元素）对应矩阵：</p>
      <p>$$\rho((12)) = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}$$</p>
      <p>把向量 $(a, b, c)$ 变成 $(b, a, c)$ ✓</p>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>🔍 深入理解</h4>
        <div class="qa-pair">
          <p class="question">❓ 小白：为什么需要表示？群本身不够用吗？</p>
          <div class="answer">
            <p>💡 专家：<strong>抽象的群没法直接计算</strong>。比如"旋转 90°"只是个符号，但矩阵 $\begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$ 可以直接和向量相乘！</p>
            <p>在深度学习中，所有操作最终都要变成张量运算。群表示就是这座桥梁。</p>
          </div>
        </div>
      </div>
    </div>

    <h4>不可约表示 (Irreducible Representation)</h4>
    <p>表示可能很"冗余"。<strong>不可约表示</strong>是"最简"的表示，不能再分解。</p>

    <div class="enrichment-intuition">
      <h4>🎯 类比：质因数分解</h4>
      <p>任何整数可以分解成质数的乘积。任何群表示可以分解成不可约表示的直和。</p>
      <p>在 GDL 中，使用不可约表示可以让网络<strong>更高效</strong>（参数更少但表达能力相同）！</p>
    </div>

    <h3 id="equivariance">0.2.5 不变性 vs 等变性 — 最关键的概念！</h3>

    <p>这是几何深度学习的<strong>核心思想</strong>，一定要理解！</p>

    <div class="formula-box">
      <h4>不变性 (Invariance)</h4>
      <p>函数 $f: X \to Y$ 对群作用 $\rho$ <strong>不变</strong>，如果：</p>
      <p style="text-align:center;">$f(\rho(g) x) = f(x), \quad \forall g \in G, x \in X$</p>
      <p><strong>直觉</strong>：输入变了，输出<strong>不变</strong>。</p>
    </div>

    <div class="formula-box">
      <h4>等变性 (Equivariance)</h4>
      <p>函数 $f: X \to Y$ 对群作用 $(\rho, \rho')$ <strong>等变</strong>，如果：</p>
      <p style="text-align:center;">$f(\rho(g) x) = \rho'(g) f(x), \quad \forall g \in G, x \in X$</p>
      <p><strong>直觉</strong>：输入变了，输出<strong>跟着变</strong>（以相同方式）。</p>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-intuition">
        <h4>🎯 用 5 个例子彻底理解</h4>
        
        <h5>例子 1：图片分类中的平移不变性</h5>
        <p><strong>任务</strong>：识别图片中的猫（输出：是猫/不是猫）</p>
        <p><strong>期望</strong>：无论猫在左上角还是右下角，答案都是"是猫"</p>
        <p>$$f(\text{平移后的图}) = f(\text{原图}) = \text{"猫"}$$</p>
        <p>这是<strong>平移不变性</strong>——最终分类结果不受平移影响。</p>
        
        <h5>例子 2：边缘检测中的平移等变性</h5>
        <p><strong>任务</strong>：检测图片中的边缘（输出：边缘图）</p>
        <p><strong>期望</strong>：如果输入图向右移 10 像素，输出的边缘图也应该向右移 10 像素</p>
        <p>$$f(\text{平移后的图}) = \text{平移后的边缘图}$$</p>
        <p>这是<strong>平移等变性</strong>——输出"跟着"输入一起移动。</p>
        <p><strong>关键区别</strong>：不变性是"最终答案不变"，等变性是"中间特征跟着变"！</p>
        
        <h5>例子 3：图节点分类的置换不变性</h5>
        <p><strong>任务</strong>：判断图是否是树（输出：是/否）</p>
        <p><strong>期望</strong>：重新标号节点不应该改变答案</p>
        <p>$$f(\text{重标号后的图}) = f(\text{原图}) = \text{"是树"}$$</p>
        <p>这是图级别任务的<strong>置换不变性</strong>。</p>
        
        <h5>例子 4：图卷积的置换等变性</h5>
        <p><strong>任务</strong>：计算每个节点的特征（输出：节点特征矩阵）</p>
        <p><strong>期望</strong>：如果重标号输入节点，输出特征也应该相应重排</p>
        <p>$$f(\text{重标号输入}) = \text{重标号输出}$$</p>
        <p>这是 GNN 层的<strong>置换等变性</strong>。</p>
        
        <h5>例子 5：3D 物体识别的旋转不变性</h5>
        <p><strong>任务</strong>：识别 3D 物体（输出：椅子/桌子/...）</p>
        <p><strong>期望</strong>：无论物体怎么旋转，答案都一样</p>
        <p>$$f(\text{旋转后的点云}) = f(\text{原点云}) = \text{"椅子"}$$</p>
        <p>这是球面 CNN 需要的<strong>旋转不变性</strong>。</p>
      </div>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>🔍 深入理解</h4>
        <div class="qa-pair">
          <p class="question">❓ 小白：为什么深度学习更看重等变性而不是不变性？</p>
          <div class="answer">
            <p>💡 专家：<strong>神经网络是层层堆叠的</strong>。如果每一层都做不变的操作，信息会快速丢失！</p>
            <p>正确的架构是：</p>
            <ul>
              <li><strong>隐藏层用等变性</strong>：保留空间结构信息</li>
              <li><strong>最后一层用不变性</strong>：做出最终决策</li>
            </ul>
            <p>CNN 的卷积层是平移等变的，最后的全局池化（global pooling）才是平移不变的！</p>
          </div>
        </div>
        <div class="qa-pair">
          <p class="question">❓ 小白：普通的全连接层有什么对称性？</p>
          <div class="answer">
            <p>💡 专家：<strong>几乎没有！</strong>全连接层不尊重任何空间结构——这就是为什么它在图像任务上效率很低。</p>
            <p>对比：</p>
            <ul>
              <li>全连接层：$10^6$ 个参数，无对称性</li>
              <li>卷积层：$10^3$ 个参数，平移等变</li>
            </ul>
            <p>等变性 = 参数共享 = 效率提升！</p>
          </div>
        </div>
      </div>
    </div>

    <!-- ========================================
         0.3 张量入门
    ======================================== -->
    <h2 id="tensors">0.3 张量入门 📐<br><span style="font-size:0.7em;color:var(--text-secondary)">Introduction to Tensors</span></h2>

    <p>在深度学习中，一切都是张量。但什么是张量？</p>

    <div class="enrichment-intuition">
      <h4>🎯 从数组到张量</h4>
      <table class="symbol-table">
        <thead>
          <tr>
            <th>阶数</th>
            <th>数学名称</th>
            <th>直觉</th>
            <th>例子</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>0 阶</td>
            <td>标量</td>
            <td>一个数</td>
            <td>温度 25°C</td>
          </tr>
          <tr>
            <td>1 阶</td>
            <td>向量</td>
            <td>一列数</td>
            <td>位置 $(x, y, z)$</td>
          </tr>
          <tr>
            <td>2 阶</td>
            <td>矩阵</td>
            <td>表格</td>
            <td>灰度图片 $H \times W$</td>
          </tr>
          <tr>
            <td>3 阶</td>
            <td>3 阶张量</td>
            <td>立方体</td>
            <td>彩色图片 $H \times W \times 3$ (RGB)</td>
          </tr>
          <tr>
            <td>4 阶</td>
            <td>4 阶张量</td>
            <td>批次图片</td>
            <td>一批图片 $B \times H \times W \times C$</td>
          </tr>
        </tbody>
      </table>
      <p><strong>程序员视角</strong>：张量就是多维数组！在 PyTorch 中就是 <code>torch.Tensor</code>。</p>
    </div>

    <h3>张量积 (Tensor Product)</h3>
    <p>张量积 $\otimes$ 是构造高阶张量的基本运算。</p>

    <div class="formula-box">
      <h4>向量的张量积</h4>
      <p>设 $\mathbf{u} = (u_1, u_2)$，$\mathbf{v} = (v_1, v_2, v_3)$，则：</p>
      <p>$$\mathbf{u} \otimes \mathbf{v} = \begin{pmatrix} u_1 v_1 & u_1 v_2 & u_1 v_3 \\ u_2 v_1 & u_2 v_2 & u_2 v_3 \end{pmatrix}$$</p>
      <p>这是一个 $2 \times 3$ 矩阵（外积）！</p>
    </div>

    <div class="example-box">
      <h4>例子：用张量积构造特征</h4>
      <p>在机器学习中，如果有两个特征 $x_1, x_2$，想要建模它们的交互，可以用：</p>
      <p>$$\mathbf{x} \otimes \mathbf{x} = \begin{pmatrix} x_1^2 & x_1 x_2 \\ x_1 x_2 & x_2^2 \end{pmatrix}$$</p>
      <p>展平后得到 $[x_1^2, x_1 x_2, x_1 x_2, x_2^2]$ — 二次特征！</p>
    </div>

    <h3>为什么叫"张量"？</h3>
    <p>在物理学和微分几何中，张量是<strong>多线性映射</strong>，描述"对坐标变换的响应方式"。</p>

    <div class="enrichment-intuition">
      <h4>🎯 物理直觉：应力张量</h4>
      <p>想象一块橡胶被拉伸。每个点受到的力不只有一个方向（那是向量），而是<strong>沿不同方向拉伸程度不同</strong>。</p>
      <p>这需要一个矩阵（2 阶张量）来描述：</p>
      <p>$$\sigma = \begin{pmatrix} \sigma_{xx} & \sigma_{xy} \\ \sigma_{yx} & \sigma_{yy} \end{pmatrix}$$</p>
      <p>$\sigma_{xy}$ 表示"在 $x$ 方向上的面，受到 $y$ 方向的力"。</p>
      <p><strong>关键</strong>：如果换坐标系，张量的分量会按特定规则变换——这就是"张量"这个名字的由来（拉伸/tension）。</p>
    </div>

    <h3>在深度学习中</h3>
    <div class="enrichment-application">
      <h4>🚀 张量在 DL 中的角色</h4>
      <ul>
        <li><strong>图像</strong>：$H \times W \times C$ 张量（高×宽×通道）</li>
        <li><strong>视频</strong>：$T \times H \times W \times C$ 张量（时间×高×宽×通道）</li>
        <li><strong>批次数据</strong>：$B \times \ldots$ 张量（批次大小在最前面）</li>
        <li><strong>图节点特征</strong>：$N \times F$ 矩阵（节点数×特征维度）</li>
        <li><strong>注意力权重</strong>：$B \times H \times N \times N$ 张量（批次×头数×查询×键）</li>
      </ul>
      <p>深度学习框架（PyTorch/TensorFlow）的核心就是<strong>张量运算</strong>！</p>
    </div>

    <!-- ========================================
         0.4 流形和拓扑直觉
    ======================================== -->
    <h2 id="manifolds">0.4 流形和拓扑直觉 🌍<br><span style="font-size:0.7em;color:var(--text-secondary)">Manifolds and Topology</span></h2>

    <p>很多数据不是"平的"——它们"住"在弯曲的空间里。流形理论就是研究弯曲空间的工具。</p>

    <h3>什么是流形？</h3>
    <div class="enrichment-intuition">
      <h4>🎯 地球表面：最经典的流形</h4>
      <p>你站在地面上，四周看起来是<strong>平的</strong>（局部欧几里得）。但地球整体是<strong>弯的</strong>（全局非欧）。</p>
      <p>这就是<strong>流形</strong>的核心思想：</p>
      <blockquote>
        <strong>局部看像 $\mathbb{R}^n$，全局可能弯曲。</strong>
      </blockquote>
      <p>地球表面是 2 维流形（2D manifold）——每个小区域都像一块平面。</p>
    </div>

    <div class="formula-box">
      <h4>流形的正式定义（简化版）</h4>
      <p>一个 $n$ 维<strong>流形</strong> $\mathcal{M}$ 是一个空间，满足：</p>
      <ul>
        <li>每个点附近都有一个"坐标卡"（chart），把局部映射到 $\mathbb{R}^n$</li>
        <li>不同坐标卡之间的转换是光滑的（可微）</li>
      </ul>
      <p><strong>直觉</strong>：流形就是"可以用多张平面地图拼接成的弯曲空间"。</p>
    </div>

    <div class="example-box">
      <h4>例子 1：圆圈 $S^1$</h4>
      <p>圆圈是 1 维流形。局部看像一条线段，但全局是弯成环的。</p>
      <p><strong>为什么重要</strong>：周期性数据（如时间、角度）自然地"住"在圆圈上！</p>
    </div>

    <div class="example-box">
      <h4>例子 2：球面 $S^2$</h4>
      <p>地球表面（球面）是 2 维流形。你需要至少两张地图才能覆盖整个地球（比如东半球+西半球）。</p>
      <p><strong>为什么重要</strong>：全景图片、地球气候数据都在球面上！</p>
    </div>

    <div class="example-box">
      <h4>例子 3：甜甜圈（环面）$T^2$</h4>
      <p>甜甜圈也是 2 维流形，但拓扑和球面不同——它有个"洞"。</p>
      <p><strong>拓扑不变量</strong>：环面的<strong>欧拉示性数</strong>是 0，球面是 2——这是本质区别！</p>
    </div>

    <h3>什么是切空间？</h3>
    <div class="enrichment-intuition">
      <h4>🎯 站在山顶的切平面</h4>
      <p>想象你站在一座山的山顶。脚下的地面看起来是平的——这就是<strong>切空间</strong>。</p>
      <p>切空间 $T_p \mathcal{M}$ 是流形在点 $p$ 的"线性化"：</p>
      <ul>
        <li>它是一个向量空间（平的）</li>
        <li>向量代表"在流形上移动的方向"</li>
        <li>长度代表速度</li>
      </ul>
      <p>比如在地球表面，切空间是"切平面" —— 你脚下的水平面。</p>
    </div>

    <div class="example-box">
      <h4>例子：圆圈上的切空间</h4>
      <p>圆圈 $S^1$ 在点 $(\cos\theta, \sin\theta)$ 的切空间是：</p>
      <p>$$T_\theta S^1 = \{t(-\sin\theta, \cos\theta) \mid t \in \mathbb{R}\}$$</p>
      <p>这是一条切线——垂直于半径的直线！</p>
    </div>

    <h3>什么是度量（Metric）？</h3>
    <p>在弯曲空间里，距离不再是简单的直线距离。<strong>黎曼度量</strong>告诉我们如何测量。</p>

    <div class="formula-box">
      <h4>黎曼度量</h4>
      <p>度量 $g$ 在每个点给出一个<strong>内积</strong>，用来测量切向量的长度和角度。</p>
      <p>$$ds^2 = g_{ij} dx^i dx^j$$</p>
      <p><strong>直觉</strong>：在弯曲空间里，"勾股定理"变形了！</p>
    </div>

    <div class="example-box">
      <h4>例子：球面的度量</h4>
      <p>半径为 $R$ 的球面上，度量是：</p>
      <p>$$ds^2 = R^2(d\theta^2 + \sin^2\theta \, d\phi^2)$$</p>
      <p>这告诉我们：靠近极点时，经度变化的"实际距离"更短（因为 $\sin\theta$ 小）。</p>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>🔍 深入理解</h4>
        <div class="qa-pair">
          <p class="question">❓ 小白：为什么深度学习要关心流形？</p>
          <div class="answer">
            <p>💡 专家：<strong>数据的真实结构往往是低维流形！</strong></p>
            <p>例子：</p>
            <ul>
              <li>手写数字图片：虽然是 28×28=784 维，但所有"合理"的数字只占很小一部分——可能在一个 10 维流形上</li>
              <li>人脸图片：高维像素空间，但实际人脸在低维流形上（由姿态、表情、光照等少数变量决定）</li>
              <li>蛋白质构象：虽然有成千上万个原子，但稳定构象在低维流形上</li>
            </ul>
            <p>这就是<strong>流形假设</strong>（Manifold Hypothesis）——深度学习成功的重要原因！</p>
          </div>
        </div>
      </div>
    </div>

    <h3>图上的"流形"：图拉普拉斯</h3>
    <p>图不是连续流形，但可以用类似的想法。<strong>图拉普拉斯</strong> $L$ 扮演了流形上拉普拉斯算子的角色。</p>

    <div class="formula-box">
      <h4>图拉普拉斯</h4>
      <p>$$L = D - A$$</p>
      <p>其中 $D$ 是度矩阵（对角线是节点度数），$A$ 是邻接矩阵。</p>
      <p><strong>归一化版本</strong>：</p>
      <p>$$L_{\text{norm}} = I - D^{-1/2} A D^{-1/2}$$</p>
    </div>

    <div class="enrichment-intuition">
      <h4>🎯 拉普拉斯的直觉：测量"波纹"</h4>
      <p>在流形上，拉普拉斯算子 $\Delta f$ 测量函数 $f$ 的"弯曲程度"。</p>
      <p>在图上，$(Lf)_i = \sum_{j \sim i} (f_i - f_j)$：</p>
      <ul>
        <li>如果节点 $i$ 的值和邻居差不多，$Lf_i \approx 0$（平滑）</li>
        <li>如果差很多，$|Lf_i|$ 大（不平滑）</li>
      </ul>
      <p>这就是<strong>谱图理论</strong>的基础——GNN 的数学根基！</p>
    </div>

    <!-- ========================================
         0.5 傅里叶变换直觉
    ======================================== -->
    <h2 id="fourier">0.5 傅里叶变换直觉 🎵<br><span style="font-size:0.7em;color:var(--text-secondary)">Fourier Transform Intuition</span></h2>

    <p>傅里叶变换是信号处理的基础，也是理解谱图卷积（Spectral GNN）的关键。</p>

    <h3>核心思想：任何波都是正弦波的叠加</h3>
    <div class="enrichment-intuition">
      <h4>🎯 音乐类比</h4>
      <p>听一段音乐，你听到的是<strong>时域信号</strong>（随时间变化的空气振动）。</p>
      <p>但音乐老师会说："这是 C 大调，包含基音和泛音"——这是<strong>频域</strong>描述！</p>
      <p><strong>傅里叶变换</strong>就是在两个视角之间切换：</p>
      <p style="text-align:center;"><strong>时域（波形）↔ 频域（频谱）</strong></p>
      <p>把复杂的波分解成简单的正弦波叠加。</p>
    </div>

    <div class="formula-box">
      <h4>连续傅里叶变换</h4>
      <p>对于函数 $f(t)$，其傅里叶变换是：</p>
      <p>$$\hat{f}(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i\omega t} dt$$</p>
      <p><strong>逆变换</strong>：</p>
      <p>$$f(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \hat{f}(\omega) e^{i\omega t} d\omega$$</p>
      <p><strong>直觉</strong>：$\hat{f}(\omega)$ 告诉你"频率 $\omega$ 的成分有多强"。</p>
    </div>

    <div class="example-box">
      <h4>例子：方波的傅里叶分解</h4>
      <p>方波（开关信号）可以写成：</p>
      <p>$$f(t) = \frac{4}{\pi}\left(\sin(t) + \frac{1}{3}\sin(3t) + \frac{1}{5}\sin(5t) + \cdots\right)$$</p>
      <p>只用奇数倍频率的正弦波！</p>
    </div>

    <h3>离散傅里叶变换（DFT）</h3>
    <p>在计算机中，信号是离散的。DFT 是有限序列的傅里叶变换。</p>

    <div class="formula-box">
      <h4>DFT 的矩阵形式</h4>
      <p>对于长度为 $N$ 的信号 $\mathbf{x} = (x_0, \ldots, x_{N-1})$：</p>
      <p>$$\hat{\mathbf{x}} = F \mathbf{x}$$</p>
      <p>其中 $F$ 是 <strong>DFT 矩阵</strong>：</p>
      <p>$$F_{jk} = e^{-2\pi i jk/N}$$</p>
      <p><strong>重要</strong>：$F$ 是<strong>酉矩阵</strong>，$F^* F = I$。</p>
    </div>

    <h3>卷积定理 — FFT 为什么快？</h3>
    <div class="formula-box">
      <h4>卷积定理</h4>
      <p>$$\widehat{f * g} = \hat{f} \cdot \hat{g}$$</p>
      <p><strong>意思</strong>：时域卷积 = 频域逐点乘法！</p>
    </div>

    <div class="enrichment-application">
      <h4>🚀 为什么这很重要？</h4>
      <p>直接算卷积：$O(N^2)$</p>
      <p>用 FFT：$O(N \log N)$（FFT 变换）+ $O(N)$（逐点乘）+ $O(N \log N)$（逆变换）= $O(N \log N)$</p>
      <p>在图像处理中，这能快几百倍！</p>
    </div>

    <h3>图上的傅里叶变换 — 谱图理论</h3>
    <p>普通傅里叶变换用的是<strong>时间平移</strong>的对称性。图上没有"平移"，但有<strong>拉普拉斯算子的特征向量</strong>！</p>

    <div class="formula-box">
      <h4>图傅里叶变换</h4>
      <p>设 $L = U \Lambda U^T$ 是图拉普拉斯的谱分解。</p>
      <p><strong>图傅里叶变换</strong>：$\hat{f} = U^T f$</p>
      <p><strong>逆变换</strong>：$f = U \hat{f}$</p>
      <p><strong>图卷积</strong>（谱域）：</p>
      <p>$$f * g = U \left( (U^T f) \odot (U^T g) \right)$$</p>
      <p>其中 $\odot$ 是逐点乘法。</p>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>🔍 深入理解</h4>
        <div class="qa-pair">
          <p class="question">❓ 小白：为什么 GDL 关心傅里叶变换？</p>
          <div class="answer">
            <p>💡 专家：<strong>早期的 GNN（谱方法）就是基于图傅里叶变换！</strong></p>
            <p>Spectral GNN 的想法：</p>
            <ol>
              <li>用图拉普拉斯的特征向量定义"频率"</li>
              <li>在频域设计卷积核（比如低通滤波器）</li>
              <li>逆变换回节点域</li>
            </ol>
            <p>后来的 GNN（如 GCN、GraphSAGE）是空域方法，但数学基础还是来自谱理论！</p>
          </div>
        </div>
      </div>
    </div>

    <!-- ========================================
         0.6 线性代数复习
    ======================================== -->
    <h2 id="linear-algebra">0.6 线性代数复习 🔢<br><span style="font-size:0.7em;color:var(--text-secondary)">Linear Algebra Review</span></h2>

    <p>你应该学过线性代数，但这里复习几个在 GDL 中特别重要的概念。</p>

    <h3>特征值和特征向量</h3>
    <div class="formula-box">
      <h4>定义</h4>
      <p>对于矩阵 $A$，如果存在非零向量 $\mathbf{v}$ 和标量 $\lambda$ 使得：</p>
      <p>$$A\mathbf{v} = \lambda \mathbf{v}$$</p>
      <p>则 $\lambda$ 是<strong>特征值</strong>，$\mathbf{v}$ 是对应的<strong>特征向量</strong>。</p>
    </div>

    <div class="enrichment-intuition">
      <h4>🎯 几何直觉</h4>
      <p>大多数向量被矩阵 $A$ 变换后会改变方向。但特征向量只是<strong>被拉伸或压缩</strong>，方向不变！</p>
      <p>特征值 $\lambda$ 就是拉伸因子：</p>
      <ul>
        <li>$\lambda > 1$：拉长</li>
        <li>$0 < \lambda < 1$：缩短</li>
        <li>$\lambda < 0$：翻转并缩放</li>
      </ul>
    </div>

    <div class="example-box">
      <h4>例子：旋转矩阵的特征值</h4>
      <p>2D 旋转矩阵：</p>
      <p>$$R = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$$</p>
      <p>特征值：$\lambda = e^{\pm i\theta}$（复数！）</p>
      <p><strong>意义</strong>：旋转没有实特征向量——没有方向保持不变。</p>
    </div>

    <h3>谱分解（对称矩阵）</h3>
    <p>对称矩阵有特别好的性质。</p>

    <div class="formula-box">
      <h4>谱定理</h4>
      <p>任何<strong>实对称矩阵</strong> $A$ 可以分解为：</p>
      <p>$$A = U \Lambda U^T$$</p>
      <p>其中：</p>
      <ul>
        <li>$\Lambda$ 是对角矩阵（特征值）</li>
        <li>$U$ 是正交矩阵（特征向量，$U^T U = I$）</li>
        <li>所有特征值都是<strong>实数</strong></li>
        <li>特征向量<strong>相互正交</strong></li>
      </ul>
    </div>

    <div class="enrichment-application">
      <h4>🚀 在 GDL 中的应用</h4>
      <p>图拉普拉斯 $L$ 是对称矩阵！所以：</p>
      <ul>
        <li>特征值都是实数（且 ≥ 0）</li>
        <li>特征向量构成正交基</li>
        <li>可以定义图傅里叶变换</li>
      </ul>
      <p>这就是谱图理论的基础！</p>
    </div>

    <h3>正交性和正交基</h3>
    <div class="formula-box">
      <p>向量 $\mathbf{u}, \mathbf{v}$ <strong>正交</strong>：$\langle \mathbf{u}, \mathbf{v} \rangle = 0$</p>
      <p><strong>标准正交基</strong>：$\{\mathbf{u}_i\}$ 满足 $\langle \mathbf{u}_i, \mathbf{u}_j \rangle = \delta_{ij}$（Kronecker delta）</p>
    </div>

    <div class="enrichment-intuition">
      <h4>🎯 为什么正交基好用？</h4>
      <p>任何向量 $\mathbf{v}$ 可以唯一分解为：</p>
      <p>$$\mathbf{v} = \sum_i \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i$$</p>
      <p>就像 3D 空间的 $(x,y,z)$ 坐标一样简单！</p>
    </div>

    <h3>矩阵的迹与行列式</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th>概念</th>
          <th>定义</th>
          <th>几何意义</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>迹 $\mathrm{tr}(A)$</td>
          <td>$\sum_i A_{ii} = \sum_i \lambda_i$</td>
          <td>所有方向的平均拉伸</td>
        </tr>
        <tr>
          <td>行列式 $\det(A)$</td>
          <td>$\prod_i \lambda_i$</td>
          <td>体积缩放因子</td>
        </tr>
      </tbody>
    </table>

    <div class="example-box">
      <h4>例子</h4>
      <p>旋转矩阵：$\det(R) = 1$（保体积），$\mathrm{tr}(R) = 2\cos\theta$</p>
      <p>缩放矩阵 $\mathrm{diag}(2, 3)$：$\det = 6$，$\mathrm{tr} = 5$</p>
    </div>

    <!-- ========================================
         0.7 书中关键公式速解
    ======================================== -->
    <h2 id="key-formulas">0.7 书中关键公式速解 📝<br><span style="font-size:0.7em;color:var(--text-secondary)">Key Formulas from the Book</span></h2>

    <p>这里列出《Geometric Deep Learning》中最重要的公式，每个配上人话解释。</p>

    <h3>公式 1：等变性的数学定义</h3>
    <div class="formula-box">
      <p>$$\rho'(g) \circ f = f \circ \rho(g), \quad \forall g \in G$$</p>
      <p><strong>人话</strong>：先变换再处理 = 先处理再变换。</p>
      <p><strong>例子</strong>：图片先旋转再卷积 = 先卷积再旋转特征图。</p>
      <p><strong>为什么重要</strong>：这是设计 CNN、GNN、Transformer 的<strong>核心原则</strong>！</p>
    </div>

    <h3>公式 2：卷积的定义</h3>
    <div class="formula-box">
      <p>$$(f * g)(x) = \int_{\mathbb{R}^n} f(y) g(x - y) dy$$</p>
      <p><strong>人话</strong>：卷积是"加权平均"。</p>
      <p><strong>直觉</strong>：</p>
      <ul>
        <li>$g(x - y)$ 是以 $x$ 为中心的"窗口"</li>
        <li>$f(y)$ 是信号</li>
        <li>在窗口内做加权平均</li>
      </ul>
      <p><strong>CNN 中</strong>：$g$ 是卷积核，$f$ 是输入图像。</p>
    </div>

    <h3>公式 3：消息传递 GNN</h3>
    <div class="formula-box">
      <p>$$h_v^{(k+1)} = \phi\left(h_v^{(k)}, \bigoplus_{u \in \mathcal{N}(v)} \psi(h_v^{(k)}, h_u^{(k)}, e_{uv})\right)$$</p>
      <p><strong>人话</strong>：更新节点特征 = 结合自己 + 聚合邻居信息。</p>
      <p><strong>分解</strong>：</p>
      <ul>
        <li>$h_v^{(k)}$：节点 $v$ 在第 $k$ 层的特征</li>
        <li>$\mathcal{N}(v)$：邻居节点</li>
        <li>$\psi$：消息函数（计算边上的消息）</li>
        <li>$\bigoplus$：聚合函数（求和/平均/最大值）</li>
        <li>$\phi$：更新函数（结合自己和邻居）</li>
      </ul>
      <p><strong>为什么重要</strong>：这是 <strong>MPNN 框架</strong>，统一了 GCN、GraphSAGE、GAT 等！</p>
    </div>

    <h3>公式 4：图拉普拉斯</h3>
    <div class="formula-box">
      <p>$$L = D - A$$</p>
      <p><strong>人话</strong>：度矩阵 - 邻接矩阵。</p>
      <p><strong>展开形式</strong>：</p>
      <p>$$(Lf)_i = \sum_{j \sim i} (f_i - f_j)$$</p>
      <p><strong>直觉</strong>：测量节点和邻居的"差异"。</p>
      <p><strong>归一化版本</strong>：</p>
      <p>$$L_{\text{sym}} = I - D^{-1/2} A D^{-1/2}$$</p>
      <p><strong>为什么重要</strong>：GCN 的核心公式！</p>
    </div>

    <h3>公式 5：谱卷积</h3>
    <div class="formula-box">
      <p>$$g_\theta * x = U g_\theta(\Lambda) U^T x$$</p>
      <p><strong>人话</strong>：在频域做滤波，再变换回节点域。</p>
      <p><strong>分解</strong>：</p>
      <ul>
        <li>$U^T x$：图傅里叶变换（节点域 → 频域）</li>
        <li>
        <li>$g_\theta(\Lambda)$：频域滤波器（对角矩阵，作用在特征值上）</li>
        <li>$U$：逆变换（频域 → 节点域）</li>
      </ul>
      <p><strong>问题</strong>：计算 $U$ 需要 $O(N^3)$，太慢！</p>
      <p><strong>解决</strong>：ChebNet 用切比雪夫多项式近似，GCN 进一步简化。</p>
    </div>

    <h3>公式 6：GCN 的简化卷积</h3>
    <div class="formula-box">
      <p>$$H^{(l+1)} = \sigma\left(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)}\right)$$</p>
      <p>其中 $\tilde{A} = A + I$（加自环），$\tilde{D}$ 是对应的度矩阵。</p>
      <p><strong>人话</strong>：聚合邻居特征（包括自己），线性变换，激活。</p>
      <p><strong>为什么简单</strong>：</p>
      <ul>
        <li>只需要矩阵乘法，不需要特征分解</li>
        <li>$O(|E| \cdot F)$ 时间复杂度（边数×特征维度）</li>
      </ul>
      <p><strong>为什么重要</strong>：GCN 让图神经网络走向实用！</p>
    </div>

    <h3>公式 7：群卷积</h3>
    <div class="formula-box">
      <p>$$(f * \psi)(g) = \int_G f(h) \psi(g^{-1}h) dh$$</p>
      <p><strong>人话</strong>：在群上做卷积——推广了欧几里得卷积。</p>
      <p><strong>特殊情况</strong>：</p>
      <ul>
        <li>$G = \mathbb{R}^n$：普通卷积</li>
        <li>$G = SO(2)$：旋转等变卷积</li>
        <li>$G = S_n$：置换等变操作</li>
      </ul>
      <p><strong>为什么重要</strong>：这是<strong>几何深度学习的统一公式</strong>！</p>
    </div>

    <h3>公式 8：注意力机制</h3>
    <div class="formula-box">
      <p>$$\alpha_{ij} = \frac{\exp(q_i^T k_j / \sqrt{d})}{\sum_{k} \exp(q_i^T k_k / \sqrt{d})}$$</p>
      <p>$$\text{output}_i = \sum_j \alpha_{ij} v_j$$</p>
      <p><strong>人话</strong>：</p>
      <ol>
        <li>计算查询 $q_i$ 和所有键 $k_j$ 的相似度</li>
        <li>归一化成权重 $\alpha_{ij}$（softmax）</li>
        <li>加权求和值 $v_j$</li>
      </ol>
      <p><strong>缩放因子 $\sqrt{d}$</strong>：防止内积太大导致 softmax 饱和。</p>
      <p><strong>为什么重要</strong>：Transformer 的核心机制！</p>
    </div>

    <h3>公式 9：图注意力网络（GAT）</h3>
    <div class="formula-box">
      <p>$$h_v^{(k+1)} = \sigma\left(\sum_{u \in \mathcal{N}(v)} \alpha_{vu} W h_u^{(k)}\right)$$</p>
      <p>其中注意力系数：</p>
      <p>$$\alpha_{vu} = \frac{\exp(\text{LeakyReLU}(a^T [W h_v \| W h_u]))}{\sum_{u' \in \mathcal{N}(v)} \exp(\text{LeakyReLU}(a^T [W h_v \| W h_{u'}]))}$$</p>
      <p><strong>人话</strong>：让邻居有不同的权重——重要的邻居贡献更多！</p>
      <p><strong>vs GCN</strong>：GCN 用固定权重（度归一化），GAT 学习权重。</p>
    </div>

    <h3>公式 10：位置编码（Transformer）</h3>
    <div class="formula-box">
      <p>$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$$</p>
      <p>$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$$</p>
      <p><strong>人话</strong>：用不同频率的正弦波编码位置。</p>
      <p><strong>为什么这样设计</strong>：</p>
      <ul>
        <li>每个位置的编码唯一</li>
        <li>相对位置关系可以通过内积恢复</li>
        <li>可以外推到训练时未见过的长度</li>
      </ul>
      <p><strong>为什么重要</strong>：Transformer 没有位置信息，必须人工注入！</p>
    </div>

    <div class="enrichment-block">
      <div class="enrichment-summary">
        <h4>📌 公式速查小结</h4>
        <table class="symbol-table">
          <thead>
            <tr>
              <th>公式</th>
              <th>适用模型</th>
              <th>核心思想</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>等变性</td>
              <td>所有几何 DL 模型</td>
              <td>尊重对称性</td>
            </tr>
            <tr>
              <td>卷积</td>
              <td>CNN</td>
              <td>平移等变</td>
            </tr>
            <tr>
              <td>消息传递</td>
              <td>MPNN, GCN, GraphSAGE</td>
              <td>聚合邻居</td>
            </tr>
            <tr>
              <td>图拉普拉斯</td>
              <td>谱 GNN</td>
              <td>测量平滑性</td>
            </tr>
            <tr>
              <td>群卷积</td>
              <td>所有等变模型</td>
              <td>统一框架</td>
            </tr>
            <tr>
              <td>注意力</td>
              <td>Transformer, GAT</td>
              <td>学习权重</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <!-- ========================================
         0.8 学习路径建议
    ======================================== -->
    <h2 id="learning-path">0.8 学习路径建议 🗺️<br><span style="font-size:0.7em;color:var(--text-secondary)">Learning Path & Strategy</span></h2>

    <h3>如何使用这一章？</h3>
    <div class="callout callout-success">
      <h4>三种阅读策略</h4>
      
      <h5>策略 1：快速预览（1 小时）</h5>
      <ul>
        <li>读 0.1 符号表 — 标记不认识的符号</li>
        <li>读 0.2.1 和 0.2.5 — 理解群和等变性的直觉</li>
        <li>读 0.7 关键公式 — 知道有哪些重要公式</li>
        <li><strong>目标</strong>：有个印象，知道哪里能查</li>
      </ul>

      <h5>策略 2：深度学习（3-4 小时）</h5>
      <ul>
        <li>从头到尾仔细阅读</li>
        <li>做笔记，画图</li>
        <li>尝试自己举例子</li>
        <li>对照 Python 代码（用 NumPy 实现简单例子）</li>
        <li><strong>目标</strong>：真正理解每个概念</li>
      </ul>

      <h5>策略 3：按需查阅（长期）</h5>
      <ul>
        <li>阅读正文时遇到困难，回来查对应小节</li>
        <li>实现代码时，参考公式速解</li>
        <li>写论文时，查符号表确保记号正确</li>
        <li><strong>目标</strong>：这是你的数学参考手册</li>
      </ul>
    </div>

    <h3>学习优先级</h3>
    <div class="enrichment-block">
      <div class="enrichment-summary">
        <h4>🎯 必须掌握（核心中的核心）</h4>
        <ul>
          <li>✅ <strong>等变性 vs 不变性</strong>（0.2.5）— 这是 GDL 的灵魂</li>
          <li>✅ <strong>群的直觉</strong>（0.2.1）— 知道群是"可逆操作的集合"</li>
          <li>✅ <strong>张量=多维数组</strong>（0.3）— 实现时全靠这个</li>
          <li>✅ <strong>关键公式</strong>（0.7 的公式 1, 3, 6）— 等变性、消息传递、GCN</li>
        </ul>
        
        <h4>🔶 应该理解（重要但可以先跳过细节）</h4>
        <ul>
          <li>🔶 群作用和群表示（0.2.3, 0.2.4）— 知道概念，用时再深入</li>
          <li>🔶 流形的直觉（0.4）— 理解"数据在低维流形上"这句话</li>
          <li>🔶 图拉普拉斯（0.4 的图拉普拉斯部分）— GNN 的数学基础</li>
        </ul>
        
        <h4>🔷 可以暂缓（第一遍先跳过）</h4>
        <ul>
          <li>🔷 子群和商群（0.2.2）— 除非深入研究群论</li>
          <li>🔷 切空间和黎曼度量（0.4）— 除非研究流形学习</li>
          <li>🔷 谱分解的细节（0.6）— 用到时再查</li>
          <li>🔷 傅里叶变换（0.5）— 除非研究谱方法</li>
        </ul>
      </div>
    </div>

    <h3>与正文章节的对应</h3>
    <table class="symbol-table">
      <thead>
        <tr>
          <th>本章内容</th>
          <th>正文对应章节</th>
          <th>何时回来查阅</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>群论、等变性</td>
          <td>Ch 1, Ch 3</td>
          <td>看到 "symmetry", "equivariance" 时</td>
        </tr>
        <tr>
          <td>张量</td>
          <td>所有章节</td>
          <td>写代码时</td>
        </tr>
        <tr>
          <td>流形、图拉普拉斯</td>
          <td>Ch 4, Ch 5 (Graphs)</td>
          <td>学习 GNN 时</td>
        </tr>
        <tr>
          <td>傅里叶变换</td>
          <td>Ch 5 (谱方法)</td>
          <td>看到 "spectral", "frequency" 时</td>
        </tr>
        <tr>
          <td>特征分解</td>
          <td>Ch 5</td>
          <td>实现谱 GNN 时</td>
        </tr>
        <tr>
          <td>关键公式</td>
          <td>Ch 5, Ch 6</td>
          <td>实现模型时</td>
        </tr>
      </tbody>
    </table>

    <h3>动手练习建议</h3>
    <div class="example-box">
      <h4>🛠️ 边学边做</h4>
      <ol>
        <li><strong>群的例子</strong>：用 Python 实现一个小群（如 $C_4$），验证四条公理</li>
        <li><strong>群表示</strong>：写代码生成旋转矩阵，验证 $\rho(g \cdot h) = \rho(g)\rho(h)$</li>
        <li><strong>等变性检查</strong>：
          <ul>
            <li>写一个简单的卷积函数</li>
            <li>验证：平移输入 → 卷积 = 卷积 → 平移输出</li>
          </ul>
        </li>
        <li><strong>图拉普拉斯</strong>：
          <ul>
            <li>构造一个小图（5 个节点）</li>
            <li>计算 $L = D - A$</li>
            <li>求特征值和特征向量</li>
            <li>画出特征向量在图上的分布</li>
          </ul>
        </li>
        <li><strong>DFT</strong>：
          <ul>
            <li>用 NumPy 的 <code>np.fft.fft</code> 分析一段音频</li>
            <li>可视化时域和频域</li>
          </ul>
        </li>
      </ol>
      <p><strong>代码示例</strong>（图拉普拉斯）：</p>
      <pre><code class="language-python">import numpy as np
import networkx as nx

# 创建一个小图
G = nx.Graph([(0,1), (1,2), (2,3), (3,4), (4,0), (0,2)])

# 邻接矩阵和度矩阵
A = nx.adjacency_matrix(G).todense()
D = np.diag(A.sum(axis=1).A1)

# 图拉普拉斯
L = D - A

# 特征分解
eigenvalues, eigenvectors = np.linalg.eigh(L)

print("特征值:", eigenvalues)
# 最小的特征值应该接近 0（连通图）
# 第二小的特征值是"代数连通度"，衡量图的连通性</code></pre>
    </div>

    <h3>扩展资源</h3>
    <div class="callout callout-info">
      <h4>📚 想深入某个主题？</h4>
      
      <h5>群论</h5>
      <ul>
        <li>📖 <strong>《Visual Group Theory》</strong> by Nathan Carter — 图解群论，非常直观</li>
        <li>🎥 <strong>3Blue1Brown: Group Theory 系列</strong> — 视频讲解</li>
      </ul>

      <h5>流形与微分几何</h5>
      <ul>
        <li>📖 <strong>《Differential Geometry and Lie Groups for Physicists》</strong> by Marián Fecko</li>
        <li>🎥 <strong>Eigenchris: Tensor Calculus 系列</strong> — YouTube 上的宝藏</li>
      </ul>

      <h5>谱图理论</h5>
      <ul>
        <li>📖 <strong>《Spectral Graph Theory》</strong> by Fan Chung — 经典教材</li>
        <li>📄 论文：<em>Semi-Supervised Classification with Graph Convolutional Networks</em> (Kipf & Welling 2017)</li>
      </ul>

      <h5>线性代数直觉</h5>
      <ul>
        <li>🎥 <strong>3Blue1Brown: Essence of Linear Algebra</strong> — 必看系列</li>
        <li>📖 <strong>《Linear Algebra Done Right》</strong> by Sheldon Axler</li>
      </ul>

      <h5>几何深度学习</h5>
      <ul>
        <li>🌐 <strong>geometricdeeplearning.com</strong> — 官方网站，有博客和课程</li>
        <li>📄 <strong>GDL Proto-book</strong> — 本书的在线版本</li>
        <li>🎥 <strong>Michael Bronstein 的 AMMI 课程</strong> — YouTube 上有完整录像</li>
      </ul>
    </div>

    <h3>常见陷阱与解决方法</h3>
    <div class="enrichment-block">
      <div class="enrichment-qa">
        <h4>⚠️ 新手常见困惑</h4>
        
        <div class="qa-pair">
          <p class="question">❓ 困惑 1："群"太抽象了，记不住四条公理</p>
          <div class="answer">
            <p>💡 <strong>不要死记公理！</strong>从具体例子出发：</p>
            <ul>
              <li>想象魔方操作：能组合（封闭性），能撤销（逆元），什么都不做就是单位元</li>
              <li>公理只是在<strong>形式化</strong>这些直觉</li>
            </ul>
            <p>做法：每次遇到新的群，先玩几个例子，再验证公理。</p>
          </div>
        </div>

        <div class="qa-pair">
          <p class="question">❓ 困惑 2：等变性和不变性总是搞混</p>
          <div class="answer">
            <p>💡 <strong>记住口诀</strong>：</p>
            <ul>
              <li><strong>不变</strong>：输入动，输出<strong>不动</strong>（最终答案）</li>
              <li><strong>等变</strong>：输入动，输出<strong>跟着动</strong>（中间层）</li>
            </ul>
            <p>例子：</p>
            <ul>
              <li>分类任务：猫在左还是右，答案都是"猫" → <strong>不变</strong></li>
              <li>边缘检测：图右移，边缘图也右移 → <strong>等变</strong></li>
            </ul>
          </div>
        </div>

        <div class="qa-pair">
          <p class="question">❓ 困惑 3：张量积、外积、Kronecker 积都是啥？</p>
          <div class="answer">
            <p>💡 <strong>在有限维向量空间中，它们本质相同！</strong></p>
            <ul>
              <li>张量积 $\mathbf{u} \otimes \mathbf{v}$：抽象定义</li>
              <li>外积：张量积的矩阵表示（$\mathbf{u} \mathbf{v}^T$）</li>
              <li>Kronecker 积：矩阵的张量积</li>
            </ul>
            <p>实践中：在 NumPy 用 <code>np.outer(u, v)</code> 或 <code>np.kron(A, B)</code>。</p>
          </div>
        </div>

        <div class="qa-pair">
          <p class="question">❓ 困惑 4：图拉普拉斯怎么有那么多版本？</p>
          <div class="answer">
            <p>💡 <strong>主要有三个版本</strong>：</p>
            <table class="symbol-table">
              <tr>
                <th>版本</th>
                <th>公式</th>
                <th>特点</th>
              </tr>
              <tr>
                <td>组合拉普拉斯</td>
                <td>$L = D - A$</td>
                <td>最简单，但不归一化</td>
              </tr>
              <tr>
                <td>对称归一化</td>
                <td>$L_{\text{sym}} = D^{-1/2} L D^{-1/2}$</td>
                <td>GCN 用这个！</td>
              </tr>
              <tr>
                <td>随机游走</td>
                <td>$L_{\text{rw}} = D^{-1} L$</td>
                <td>与随机游走相关</td>
              </tr>
            </table>
            <p>不同版本适合不同应用，但核心思想一样：测量平滑性。</p>
          </div>
        </div>
      </div>
    </div>

    <h3>自我检测清单</h3>
    <div class="callout callout-success">
      <h4>✅ 完成这章后，你应该能够……</h4>
      <ul>
        <li>☐ 看到数学符号不慌张，能在 0.1 节查到含义</li>
        <li>☐ 用一句话解释"群是什么"</li>
        <li>☐ 给出等变性和不变性各自的例子</li>
        <li>☐ 知道张量就是多维数组</li>
        <li>☐ 理解"数据在流形上"这句话的直觉</li>
        <li>☐ 解释图拉普拉斯 $L = D - A$ 的意义</li>
        <li>☐ 看懂 GCN 公式中的每一项</li>
        <li>☐ 知道在阅读正文时遇到困难应该回来查哪节</li>
      </ul>
      <p>如果有任何一项不确定，回去重读那部分！</p>
    </div>

    <!-- ========================================
         总结
    ======================================== -->
    <h2>总结 🎓</h2>

    <div class="enrichment-block">
      <div class="enrichment-summary">
        <h4>📌 本章核心要点</h4>
        
        <h5>1. 符号系统（0.1）</h5>
        <p>数学符号是语言，不是障碍。需要时查表，用多了自然记住。</p>

        <h5>2. 群论基础（0.2）</h5>
        <p><strong>群 = 可逆操作的集合</strong>。关键概念：</p>
        <ul>
          <li>群的四条公理：封闭性、结合律、单位元、逆元</li>
          <li>群作用：抽象操作 → 具体变换</li>
          <li>群表示：抽象群 → 矩阵群（计算机能算）</li>
          <li><strong>等变性 vs 不变性</strong>：GDL 的灵魂！</li>
        </ul>

        <h5>3. 张量（0.3）</h5>
        <p>张量 = 多维数组。深度学习的一切都是张量运算。</p>

        <h5>4. 流形（0.4）</h5>
        <p>数据不住在平坦空间，而是<strong>弯曲的低维流形</strong>上。</p>

        <h5>5. 傅里叶变换（0.5）</h5>
        <p>时域 ↔ 频域。在图上推广：节点域 ↔ 谱域（拉普拉斯特征向量）。</p>

        <h5>6. 线性代数（0.6）</h5>
        <p>特征分解、正交性、谱分解——GNN 的数学基础。</p>

        <h5>7. 关键公式（0.7）</h5>
        <p>10 个核心公式，每个都是一座桥梁，连接理论和实现。</p>

        <h5>8. 学习策略（0.8）</h5>
        <p>这章是<strong>参考手册</strong>，不是小说。按需查阅，动手练习，长期积累。</p>
      </div>
    </div>

    <div class="callout callout-info">
      <h4>🚀 下一步</h4>
      <p>你现在已经有了数学武器库。是时候开始真正的旅程了！</p>
      <p><strong>建议</strong>：</p>
      <ol>
        <li>先快速浏览 <a href="../chapter1/index.html">Chapter 1: Introduction</a>，了解全书架构</li>
        <li>遇到数学困难时，随时回来查阅本章</li>
        <li>实现代码时，参考 0.7 的公式速解</li>
        <li>保持好奇心，享受学习的过程！</li>
      </ol>
      <p><strong>记住</strong>：理解需要时间。不要期望一遍全懂——数学是需要反复咀嚼的。</p>
    </div>

    <div class="enrichment-intuition">
      <h4>🎯 最后的鼓励</h4>
      <p>你可能会觉得这些数学很抽象。但请记住：</p>
      <blockquote>
        <strong>"数学不是为了炫技，而是为了让模糊的直觉变得精确。"</strong>
      </blockquote>
      <p>当你看到 CNN 的卷积核在图片上滑动时，背后是<strong>平移等变性</strong>。</p>
      <p>当你看到 GNN 聚合邻居信息时，背后是<strong>置换不变性</strong>。</p>
      <p>当你看到 Transformer 的自注意力时，背后是<strong>置换等变性</strong>。</p>
      <p>这些数学不是障碍，而是<strong>望远镜</strong>——让你看到架构设计的本质。</p>
      <p><strong>加油！你已经迈出了几何深度学习之旅的第一步。🎉</strong></p>
    </div>

  </main>

  <script>
    // 主题切换
    function toggleTheme() {
      const html = document.documentElement;
      const currentTheme = html.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
      html.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);
      
      const btn = document.querySelector('.theme-toggle');
      btn.textContent = newTheme === 'dark' ? '☀️' : '🌙';
    }

    // 加载保存的主题
    window.addEventListener('DOMContentLoaded', () => {
      const savedTheme = localStorage.getItem('theme') || 'light';
      document.documentElement.setAttribute('data-theme', savedTheme);
      const btn = document.querySelector('.theme-toggle');
      btn.textContent = savedTheme === 'dark' ? '☀️' : '🌙';
    });

    // 侧边栏切换
    function toggleSidebar() {
      document.querySelector('.sidebar').classList.toggle('active');
    }

    // 滚动进度条
    window.addEventListener('scroll', () => {
      const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
      const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
      const scrolled = (winScroll / height) * 100;
      document.querySelector('.progress-bar').style.width = scrolled + '%';
    });

    // 平滑滚动
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          target.scrollIntoView({ behavior: 'smooth', block: 'start' });
          // 移动端关闭侧边栏
          if (window.innerWidth <= 768) {
            document.querySelector('.sidebar').classList.remove('active');
          }
        }
      });
    });

    // 高亮当前章节
    const observer = new IntersectionObserver(entries => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          const id = entry.target.getAttribute('id');
          document.querySelectorAll('.sidebar a').forEach(link => {
            link.classList.remove('active-link');
            if (link.getAttribute('href') === `#${id}`) {
              link.classList.add('active-link');
            }
          });
        }
      });
    }, { rootMargin: '-50% 0px -50% 0px' });

    document.querySelectorAll('h2[id], h3[id]').forEach(heading => {
      observer.observe(heading);
    });
  </script>
</body>
</html>

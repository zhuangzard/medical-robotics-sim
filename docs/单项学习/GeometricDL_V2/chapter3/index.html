<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Chapter 3: Geometric Priors — 几何深度学习蓝图</title>

<!-- KaTeX for math rendering -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ],
    throwOnError: false
  });"></script>

<style>
/* ===== CSS Variables (Light Theme) ===== */
:root {
  --bg-primary: #ffffff;
  --bg-secondary: #f8f9fa;
  --bg-code: #f4f5f7;
  --bg-sidebar: #f0f2f5;
  --text-primary: #1a1a2e;
  --text-secondary: #4a4a6a;
  --text-muted: #8888aa;
  --border-color: #e0e0e8;
  --accent: #6366f1;
  --accent-light: #eef2ff;
  --link: #4f46e5;
  --shadow: 0 2px 12px rgba(0,0,0,0.06);
  --shadow-hover: 0 4px 20px rgba(0,0,0,0.1);
  --radius: 12px;
  --sidebar-width: 300px;

  /* Colored boxes */
  --def-bg: #eff6ff;
  --def-border: #3b82f6;
  --def-title: #1d4ed8;
  --thm-bg: #ecfdf5;
  --thm-border: #10b981;
  --thm-title: #047857;
  --example-bg: #fffbeb;
  --example-border: #f59e0b;
  --example-title: #b45309;
  --warn-bg: #fef2f2;
  --warn-border: #ef4444;
  --warn-title: #b91c1c;
  --robot-bg: #f0fdf4;
  --robot-border: #22c55e;
  --robot-title: #15803d;

  /* Code highlighting */
  --code-keyword: #d73a49;
  --code-string: #032f62;
  --code-comment: #6a737d;
  --code-function: #6f42c1;
  --code-number: #005cc5;
  --code-class: #e36209;
  --code-builtin: #005cc5;
  --code-decorator: #6f42c1;
  --code-self: #d73a49;
}

/* ===== Dark Theme ===== */
[data-theme="dark"] {
  --bg-primary: #0d1117;
  --bg-secondary: #161b22;
  --bg-code: #1c2333;
  --bg-sidebar: #0d1117;
  --text-primary: #e6edf3;
  --text-secondary: #8b949e;
  --text-muted: #6e7681;
  --border-color: #30363d;
  --accent: #818cf8;
  --accent-light: #1e1b4b;
  --link: #818cf8;
  --shadow: 0 2px 12px rgba(0,0,0,0.3);
  --shadow-hover: 0 4px 20px rgba(0,0,0,0.4);

  --def-bg: #0c1929;
  --def-border: #3b82f6;
  --def-title: #60a5fa;
  --thm-bg: #052e16;
  --thm-border: #22c55e;
  --thm-title: #4ade80;
  --example-bg: #271a00;
  --example-border: #f59e0b;
  --example-title: #fbbf24;
  --warn-bg: #2a0a0a;
  --warn-border: #ef4444;
  --warn-title: #f87171;
  --robot-bg: #052e16;
  --robot-border: #22c55e;
  --robot-title: #4ade80;

  --code-keyword: #ff7b72;
  --code-string: #a5d6ff;
  --code-comment: #8b949e;
  --code-function: #d2a8ff;
  --code-number: #79c0ff;
  --code-class: #ffa657;
  --code-builtin: #79c0ff;
  --code-decorator: #d2a8ff;
  --code-self: #ff7b72;
}

/* ===== Reset & Base ===== */
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
html { scroll-behavior: smooth; scroll-padding-top: 80px; }
body {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Noto Sans SC", "PingFang SC", sans-serif;
  background: var(--bg-primary);
  color: var(--text-primary);
  line-height: 1.85;
  font-size: 16px;
  transition: background 0.3s, color 0.3s;
}

/* ===== Progress Bar ===== */
.progress-bar {
  position: fixed;
  top: 0;
  left: 0;
  width: 0%;
  height: 3px;
  background: linear-gradient(90deg, #6366f1, #a855f7, #ec4899);
  z-index: 9999;
  transition: width 0.1s;
}

/* ===== Sidebar ===== */
.sidebar {
  position: fixed;
  left: 0; top: 0;
  width: var(--sidebar-width);
  height: 100vh;
  background: var(--bg-sidebar);
  border-right: 1px solid var(--border-color);
  overflow-y: auto;
  z-index: 100;
  padding: 20px 0;
  transition: transform 0.3s ease, background 0.3s;
}
.sidebar-header {
  padding: 10px 20px 20px;
  border-bottom: 1px solid var(--border-color);
  margin-bottom: 10px;
}
.sidebar-header h2 {
  font-size: 17px;
  color: var(--accent);
  margin-bottom: 4px;
}
.sidebar-header p {
  font-size: 12px;
  color: var(--text-muted);
}
.sidebar nav { padding: 0 12px; }
.sidebar nav a {
  display: block;
  padding: 6px 12px;
  color: var(--text-secondary);
  text-decoration: none;
  font-size: 13px;
  border-radius: 6px;
  transition: all 0.2s;
  border-left: 2px solid transparent;
}
.sidebar nav a:hover {
  background: var(--accent-light);
  color: var(--accent);
}
.sidebar nav a.active {
  background: var(--accent-light);
  color: var(--accent);
  border-left-color: var(--accent);
  font-weight: 600;
}
.sidebar nav .sub { padding-left: 24px; font-size: 12px; }
.sidebar nav .sep { height: 1px; background: var(--border-color); margin: 8px 12px; }

/* Mobile sidebar toggle */
.sidebar-toggle {
  display: none;
  position: fixed;
  bottom: 20px; left: 20px;
  width: 48px; height: 48px;
  border-radius: 50%;
  background: var(--accent);
  color: #fff;
  border: none;
  font-size: 20px;
  cursor: pointer;
  z-index: 200;
  box-shadow: 0 4px 12px rgba(0,0,0,0.3);
}

/* ===== Main Content ===== */
.main-content {
  margin-left: var(--sidebar-width);
  max-width: 900px;
  padding: 40px 48px 80px;
}

/* ===== Theme Toggle ===== */
.theme-toggle {
  position: fixed;
  top: 16px; right: 16px;
  width: 42px; height: 42px;
  border-radius: 50%;
  border: 1px solid var(--border-color);
  background: var(--bg-secondary);
  color: var(--text-primary);
  font-size: 18px;
  cursor: pointer;
  z-index: 200;
  transition: all 0.2s;
}
.theme-toggle:hover { box-shadow: var(--shadow-hover); }

/* ===== Typography ===== */
h1 { font-size: 2.2em; margin: 1em 0 0.6em; line-height: 1.3; color: var(--accent); }
h2 { font-size: 1.7em; margin: 2em 0 0.8em; padding-bottom: 8px; border-bottom: 2px solid var(--border-color); }
h3 { font-size: 1.35em; margin: 1.6em 0 0.6em; color: var(--accent); }
h4 { font-size: 1.1em; margin: 1.3em 0 0.5em; }
p { margin-bottom: 1em; }
a { color: var(--link); text-decoration: none; }
a:hover { text-decoration: underline; }
strong { color: var(--text-primary); }
blockquote {
  border-left: 4px solid var(--accent);
  padding: 12px 20px;
  margin: 1em 0;
  background: var(--bg-secondary);
  border-radius: 0 var(--radius) var(--radius) 0;
  color: var(--text-secondary);
  font-style: italic;
}

/* ===== Colored Boxes ===== */
.box {
  border-radius: var(--radius);
  padding: 20px 24px;
  margin: 1.5em 0;
  border-left: 5px solid;
  position: relative;
}
.box-title {
  font-weight: 700;
  margin-bottom: 8px;
  font-size: 1.05em;
  display: flex;
  align-items: center;
  gap: 8px;
}
.box.def { background: var(--def-bg); border-color: var(--def-border); }
.box.def .box-title { color: var(--def-title); }
.box.thm { background: var(--thm-bg); border-color: var(--thm-border); }
.box.thm .box-title { color: var(--thm-title); }
.box.example { background: var(--example-bg); border-color: var(--example-border); }
.box.example .box-title { color: var(--example-title); }
.box.warn { background: var(--warn-bg); border-color: var(--warn-border); }
.box.warn .box-title { color: var(--warn-title); }
.box.robot { background: var(--robot-bg); border-color: var(--robot-border); }
.box.robot .box-title { color: var(--robot-title); }

/* ===== Code Blocks ===== */
.code-block {
  position: relative;
  margin: 1.2em 0;
  border-radius: var(--radius);
  overflow: hidden;
  border: 1px solid var(--border-color);
}
.code-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 8px 16px;
  background: var(--bg-secondary);
  border-bottom: 1px solid var(--border-color);
  font-size: 13px;
  color: var(--text-muted);
}
.copy-btn {
  background: transparent;
  border: 1px solid var(--border-color);
  color: var(--text-muted);
  padding: 3px 10px;
  border-radius: 6px;
  cursor: pointer;
  font-size: 12px;
  transition: all 0.2s;
}
.copy-btn:hover { background: var(--accent-light); color: var(--accent); }
pre {
  background: var(--bg-code);
  padding: 16px 20px;
  overflow-x: auto;
  font-size: 14px;
  line-height: 1.6;
  margin: 0;
}
code {
  font-family: "SF Mono", "Fira Code", "JetBrains Mono", Menlo, monospace;
}
:not(pre) > code {
  background: var(--bg-code);
  padding: 2px 6px;
  border-radius: 4px;
  font-size: 0.9em;
  color: var(--accent);
}

/* ===== Syntax Highlighting ===== */
.kw { color: var(--code-keyword); font-weight: 600; }
.str { color: var(--code-string); }
.cm { color: var(--code-comment); font-style: italic; }
.fn { color: var(--code-function); }
.num { color: var(--code-number); }
.cls { color: var(--code-class); }
.bi { color: var(--code-builtin); }
.dec { color: var(--code-decorator); }
.sf { color: var(--code-self); }
.op { color: var(--text-secondary); }

/* ===== Images ===== */
.figure {
  margin: 2em 0;
  text-align: center;
}
.figure img {
  max-width: 100%;
  border-radius: var(--radius);
  box-shadow: var(--shadow);
}
.figure .caption {
  margin-top: 10px;
  font-size: 14px;
  color: var(--text-muted);
  line-height: 1.6;
}

/* ===== Table ===== */
table {
  width: 100%;
  border-collapse: collapse;
  margin: 1.5em 0;
  font-size: 15px;
}
th, td {
  padding: 10px 14px;
  border: 1px solid var(--border-color);
  text-align: left;
}
th { background: var(--bg-secondary); font-weight: 600; }

/* ===== Exercises ===== */
.exercises {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius);
  padding: 24px;
  margin: 2em 0;
}
.exercises h4 {
  color: var(--accent);
  margin-bottom: 16px;
}
.exercises ol { padding-left: 24px; }
.exercises li { margin-bottom: 12px; line-height: 1.7; }

/* ===== Key Takeaway ===== */
.takeaway {
  background: linear-gradient(135deg, var(--accent-light), var(--bg-secondary));
  border: 2px solid var(--accent);
  border-radius: var(--radius);
  padding: 20px 24px;
  margin: 2em 0;
}
.takeaway h4 { color: var(--accent); margin-bottom: 10px; }

/* ===== Symbol Table ===== */
.symbol-table {
  display: grid;
  grid-template-columns: auto 1fr;
  gap: 6px 20px;
  margin: 1em 0;
  font-size: 15px;
}
.symbol-table .sym {
  font-family: "SF Mono", monospace;
  color: var(--accent);
  text-align: right;
  padding: 2px 0;
}
.symbol-table .desc {
  padding: 2px 0;
  color: var(--text-secondary);
}

/* ===== Responsive ===== */
@media (max-width: 1024px) {
  .sidebar { transform: translateX(-100%); }
  .sidebar.open { transform: translateX(0); }
  .sidebar-toggle { display: flex; align-items: center; justify-content: center; }
  .main-content { margin-left: 0; padding: 30px 20px 80px; }
}

/* ===== Smooth animations ===== */
.fade-in {
  animation: fadeIn 0.5s ease-out;
}
@keyframes fadeIn {
  from { opacity: 0; transform: translateY(12px); }
  to { opacity: 1; transform: translateY(0); }
}

/* ===== Chapter Navigation ===== */
.chapter-nav {
  display: flex;
  justify-content: space-between;
  margin-top: 60px;
  padding-top: 30px;
  border-top: 2px solid var(--border-color);
}
.chapter-nav a {
  padding: 12px 24px;
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius);
  font-weight: 600;
  transition: all 0.2s;
}
.chapter-nav a:hover {
  background: var(--accent-light);
  border-color: var(--accent);
  text-decoration: none;
}

/* SVG diagrams */
.svg-diagram {
  margin: 1.5em auto;
  display: block;
  max-width: 100%;
}
</style>
<!-- Enrichment CSS - to be injected into each chapter's <head> -->
<style>
/* ========== Enrichment Blocks ========== */
.enrichment-block {
  margin: 2.5rem 0;
  padding: 2rem;
  background: linear-gradient(135deg, #f0f7ff 0%, #e8f4fd 100%);
  border-left: 4px solid #3b82f6;
  border-radius: 0 12px 12px 0;
  box-shadow: 0 2px 8px rgba(59, 130, 246, 0.1);
}
[data-theme="dark"] .enrichment-block {
  background: linear-gradient(135deg, #1a2332 0%, #1e293b 100%);
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
}

.enrichment-block h4 {
  margin-top: 0;
  font-size: 1.2rem;
  color: #1e40af;
}
[data-theme="dark"] .enrichment-block h4 {
  color: #93c5fd;
}

.enrichment-qa { margin-bottom: 1.5rem; }

.qa-pair {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(255,255,255,0.7);
  border-radius: 10px;
  transition: transform 0.2s;
}
.qa-pair:hover { transform: translateX(4px); }
[data-theme="dark"] .qa-pair {
  background: rgba(0,0,0,0.25);
}

.question {
  font-weight: 700;
  color: #2563eb;
  margin-bottom: 0.75rem;
  font-size: 1.05rem;
  line-height: 1.6;
}
[data-theme="dark"] .question { color: #60a5fa; }

.answer {
  line-height: 1.9;
  color: #374151;
  font-size: 1rem;
}
[data-theme="dark"] .answer { color: #d1d5db; }
.answer p { margin: 0.5rem 0; }

.enrichment-intuition {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(251,191,36,0.1);
  border-radius: 10px;
  border-left: 3px solid #f59e0b;
  line-height: 1.8;
}
[data-theme="dark"] .enrichment-intuition {
  background: rgba(251,191,36,0.05);
}

.enrichment-application {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(16,185,129,0.1);
  border-radius: 10px;
  border-left: 3px solid #10b981;
  line-height: 1.8;
}
[data-theme="dark"] .enrichment-application {
  background: rgba(16,185,129,0.05);
}

.enrichment-summary {
  margin: 1.2rem 0;
  padding: 1.2rem;
  background: rgba(139,92,246,0.1);
  border-radius: 10px;
  border-left: 3px solid #8b5cf6;
  line-height: 1.8;
}
[data-theme="dark"] .enrichment-summary {
  background: rgba(139,92,246,0.05);
}
</style>

</head>
<body>

<!-- Progress Bar -->
<div class="progress-bar" id="progressBar"></div>

<!-- Theme Toggle -->
<button class="theme-toggle" id="themeToggle" title="切换深色/浅色主题">🌙</button>

<!-- Mobile Sidebar Toggle -->
<button class="sidebar-toggle" id="sidebarToggle">☰</button>

<!-- ===== Sidebar ===== -->
<aside class="sidebar" id="sidebar">
  <div class="sidebar-header">
    <h2>📐 Chapter 3</h2>
    <p>Geometric Priors 几何先验</p>
  </div>
  <nav>
    <a href="#intro">引言：为什么需要几何先验</a>
    <div class="sep"></div>

    <a href="#sec3-1">3.1 对称性、表示和不变性</a>
    <a href="#sec3-1-groups" class="sub">群的定义与公理</a>
    <a href="#sec3-1-examples" class="sub">常见对称群实例</a>
    <a href="#sec3-1-actions" class="sub">群作用与群表示</a>
    <a href="#sec3-1-invariance" class="sub">不变性 vs 等变性</a>
    <a href="#sec3-1-code" class="sub">🐍 代码：等变性验证</a>
    <a href="#sec3-1-exercises" class="sub">❓ 练习题</a>
    <div class="sep"></div>

    <a href="#sec3-2">3.2 同构和自同构</a>
    <a href="#sec3-2-levels" class="sub">结构层次与子群</a>
    <a href="#sec3-2-iso" class="sub">同构 vs 自同构</a>
    <a href="#sec3-2-graph" class="sub">图同构与图自同构</a>
    <a href="#sec3-2-wl" class="sub">Weisfeiler-Leman 测试</a>
    <a href="#sec3-2-code" class="sub">🐍 代码：WL 测试</a>
    <a href="#sec3-2-exercises" class="sub">❓ 练习题</a>
    <div class="sep"></div>

    <a href="#sec3-3">3.3 形变稳定性</a>
    <a href="#sec3-3-signal" class="sub">信号形变的稳定性</a>
    <a href="#sec3-3-domain" class="sub">域形变的稳定性</a>
    <a href="#sec3-3-math" class="sub">📐 稳定性界限推导</a>
    <a href="#sec3-3-code" class="sub">🐍 代码：Lipschitz 验证</a>
    <a href="#sec3-3-exercises" class="sub">❓ 练习题</a>
    <div class="sep"></div>

    <a href="#sec3-4">3.4 尺度分离</a>
    <a href="#sec3-4-fourier" class="sub">傅里叶变换</a>
    <a href="#sec3-4-wavelet" class="sub">小波变换与多尺度</a>
    <a href="#sec3-4-stability" class="sub">多尺度的形变稳定性</a>
    <a href="#sec3-4-coarsening" class="sub">图粗粒度化</a>
    <a href="#sec3-4-code" class="sub">🐍 代码：图粗粒度化</a>
    <a href="#sec3-4-exercises" class="sub">❓ 练习题</a>
    <div class="sep"></div>

    <a href="#sec3-5">3.5 GDL 蓝图 ⭐</a>
    <a href="#sec3-5-motivation" class="sub">为什么需要蓝图</a>
    <a href="#sec3-5-blocks" class="sub">四大构建块</a>
    <a href="#sec3-5-blueprint" class="sub">蓝图公式</a>
    <a href="#sec3-5-5g" class="sub">5G 统一框架</a>
    <a href="#sec3-5-table" class="sub">GDL 全家族对照表</a>
    <a href="#sec3-5-code" class="sub">🐍 代码：蓝图实现</a>
    <a href="#sec3-5-exercises" class="sub">❓ 练习题</a>
    <div class="sep"></div>

    <a href="#summary">全章总结</a>
    <a href="#references">参考文献</a>
  </nav>
</aside>

<!-- ===== Main Content ===== -->
<div class="main-content" id="mainContent">

<!-- ======================================== -->
<!-- CHAPTER HEADER -->
<!-- ======================================== -->
<h1 id="intro">Chapter 3: Geometric Priors<br><span style="font-size:0.6em; color:var(--text-secondary)">几何先验 — 对称性、稳定性与深度学习蓝图</span></h1>

<blockquote>
  <strong>"The unreasonable effectiveness of geometric priors."</strong><br>
  现代数据分析的本质就是高维学习。虽然维度灾难告诉我们，在通用高维数据上学习是不可能的（参见第2章），
  但对于<strong>物理结构化的数据</strong>，我们可以利用两个根本原则：<strong>对称性 (Symmetry)</strong> 和 <strong>尺度分离 (Scale Separation)</strong>。
  这两个原则就是本章的"几何先验"(Geometric Priors)。
</blockquote>

<p>本章是全书最核心的理论基础。我们将建立一套完整的数学语言，用来描述深度学习架构中的几何结构。
在本章结束时，你会看到：<strong>CNN、GNN、Transformer、DeepSets</strong> 等看似不同的架构，
其实都是同一个"几何蓝图"的不同实例化 (instantiation)。</p>

<div class="takeaway">
<h4>🎯 本章核心目标</h4>
<p>看完本章后，你应该能够：</p>
<ol>
  <li>用<strong>群论语言</strong>精确描述对称性、不变性和等变性</li>
  <li>理解<strong>形变稳定性</strong>为什么比精确不变性更实际</li>
  <li>解释<strong>多尺度分析</strong>如何克服维度灾难</li>
  <li>写出<strong>GDL 蓝图</strong>的四大构建块，并映射到具体架构</li>
  <li>用 Python/PyTorch <strong>代码验证</strong>每个数学概念</li>
</ol>
</div>

<p><strong>前置知识</strong>：线性代数基础（矩阵乘法、特征值）、Python/PyTorch 基础、图的基本概念。</p>

<div class="figure">
  <img src="../assets/ch3_p13_img0.png" alt="维度灾难与Lipschitz函数">
  <div class="caption"><strong>图 3.1</strong>：维度灾难的几何直觉。在 $d$ 维空间中，用 Lipschitz 函数逼近需要 $N = \Theta(\epsilon^{-d})$ 个样本，
  即样本复杂度随维度呈指数增长。几何先验正是为了打破这个诅咒。</div>
</div>

<!-- ======================================== -->
<!-- SECTION 3.1: SYMMETRIES -->
<!-- ======================================== -->

<!-- === GLOSSARY START === -->
<div class="enrichment-block" style="border-left-color: #ec4899;">
  <h4>📖 核心概念详解</h4>
  
  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">群 (Group) 🔑</h5>
    <p><strong>一句话</strong>：群就是一组"操作"，它们可以组合、可以撤销、有一个"什么都不做"的操作。</p>
    
    <p><strong>正式定义</strong>：集合 $G$ 加上运算 $\cdot: G \times G \to G$，满足四个公理：</p>
    <ul style="margin-left: 1.5rem;">
      <li><strong>结合律</strong>：$(g \cdot h) \cdot k = g \cdot (h \cdot k)$</li>
      <li><strong>单位元</strong>：存在唯一的 $e \in G$ 使得 $e \cdot g = g \cdot e = g$</li>
      <li><strong>逆元</strong>：每个 $g$ 都有唯一的 $g^{-1}$ 使得 $g \cdot g^{-1} = g^{-1} \cdot g = e$</li>
      <li><strong>封闭性</strong>：$g, h \in G \Rightarrow g \cdot h \in G$</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🎲 <strong>旋转魔方</strong>：每次转动是一个群元素，连续转动=群运算，转回原状=逆元，什么都不转=单位元</li>
      <li>🕐 <strong>时钟</strong>：12小时的加法 → 循环群 $\mathbb{Z}_{12}$。例如 9点+5小时=2点，这就是群运算 $9 + 5 = 2 \pmod{12}$</li>
      <li>🃏 <strong>洗牌</strong>：52张牌的所有排列方式 → 对称群 $S_{52}$。每种洗牌方式可以组合，可以"反洗"回去</li>
      <li>🔄 <strong>平面旋转</strong>：旋转30° + 旋转45° = 旋转75°，旋转0°=单位元，旋转-30°是逆元</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：群是描述对称性的数学语言。CNN 的平移不变性 = 平移群 $(\mathbb{R}^2, +)$，GNN 的节点排列不变性 = 置换群 $S_n$，3D点云的旋转不变性 = 旋转群 $SO(3)$。理解群 = 理解"什么样的变换不改变数据的本质"。</p>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 群是一组"东西"（如一群人）→ ✅ 群是一组"操作/变换"</li>
      <li>❌ 群运算一定满足交换律（$g \cdot h = h \cdot g$）→ ✅ 大部分群是<strong>非交换</strong>的（如旋转群、矩阵乘法）</li>
      <li>❌ 单位元就是数字1 → ✅ 单位元是"什么都不做的操作"，可以是恒等映射、0度旋转、空置换等</li>
    </ul>
    
    <p><strong>与其他概念的联系</strong>：群 → 群作用（如何作用在数据上）→ 群表示（用矩阵实现群作用）→ 等变性（网络层保持群结构）</p>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">群作用 (Group Action) 🎯</h5>
    <p><strong>一句话</strong>：群作用就是"群如何操作/变换某个对象"，比如旋转群如何旋转图片。</p>
    
    <p><strong>正式定义</strong>：群 $G$ 在集合 $\Omega$ 上的作用是映射 $(g, u) \mapsto g \cdot u$，满足：</p>
    <ul style="margin-left: 1.5rem;">
      <li>$(g \cdot h) \cdot u = g \cdot (h \cdot u)$（作用的复合 = 群元素的复合）</li>
      <li>$e \cdot u = u$（单位元不改变对象）</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>📐 <strong>平移群作用在图片上</strong>：平移向量 $(2, 3)$ 作用在像素 $(x, y)$ 上 → 新位置 $(x+2, y+3)$</li>
      <li>🎨 <strong>旋转群作用在画布上</strong>：旋转90° 作用在正方形上 → 正方形转了90°</li>
      <li>🔀 <strong>置换群作用在序列上</strong>：置换 $(1 \to 3, 2 \to 1, 3 \to 2)$ 作用在 $[A, B, C]$ 上 → $[B, C, A]$</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：群作用把抽象的"对称性"变成具体的"数据变换"。在图片上：$g \cdot x(u) = x(g^{-1}u)$（平移图片 = 反向平移坐标）。这是理解等变网络的核心：<strong>网络层必须"尊重"群作用</strong>。</p>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 群作用只能作用在几何对象上 → ✅ 可以作用在任何集合上（图、序列、函数空间）</li>
      <li>❌ $g \cdot u$ 里的 $\cdot$ 是普通乘法 → ✅ 这只是符号，具体操作取决于 $G$ 和 $\Omega$ 的定义</li>
    </ul>
    
    <p><strong>与其他概念的联系</strong>：群作用 → 轨道（$g$ 能把 $u$ 移动到哪些位置）+ 稳定子（哪些 $g$ 不移动 $u$）</p>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">群表示 (Group Representation) 🎭</h5>
    <p><strong>一句话</strong>：群表示就是"用矩阵来实现群作用"，让抽象的群变成可以计算的线性变换。</p>
    
    <p><strong>正式定义</strong>：$n$ 维群表示是映射 $\rho: G \to \mathbb{R}^{n \times n}$，满足：</p>
    <ul style="margin-left: 1.5rem;">
      <li>$\rho(g \cdot h) = \rho(g) \cdot \rho(h)$（群乘法 → 矩阵乘法）</li>
      <li>$\rho(g)$ 是可逆矩阵</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🔄 <strong>2D旋转</strong>：旋转 $\theta$ 角度 → 旋转矩阵 $\begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$</li>
      <li>🪞 <strong>镜像翻转</strong>：水平翻转 → 矩阵 $\begin{pmatrix} -1 & 0 \\ 0 & 1 \end{pmatrix}$</li>
      <li>🔀 <strong>置换</strong>：交换第1和第3个元素 → 置换矩阵 $\begin{pmatrix} 0&0&1 \\ 0&1&0 \\ 1&0&0 \end{pmatrix}$</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：神经网络处理的是向量/张量，群表示让我们可以用矩阵乘法 $\rho(g)x$ 来实现群作用。例如在CNN中，平移群的表示就是"shift矩阵"；在E(n)-等变网络中，旋转群的表示就是旋转矩阵。</p>
    
    <p><strong>为什么要"线性"</strong>：因为深度学习的基本操作（矩阵乘法、卷积）都是线性的。线性表示让我们可以用 GPU 高效计算。</p>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 群表示的维度 $n$ = 群的"大小" → ✅ $n$ 是特征空间的维度，可以任意选择（同一个群可以有无穷多种表示）</li>
      <li>❌ 一个群只有一种表示 → ✅ 同一个群有无穷多种表示（例如旋转群可以表示在2D平面、3D空间、球谐函数空间等）</li>
    </ul>
    
    <p><strong>与其他概念的联系</strong>：群表示 → 不可约表示（最简单的"积木"）→ 等变网络层（$f(\rho(g)x) = \rho'(g)f(x)$）</p>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">不变映射 vs 等变映射 (Invariant vs Equivariant) ⚖️</h5>
    <p><strong>一句话</strong>：不变 = "输入怎么变换，输出都不变"；等变 = "输入怎么变换，输出也跟着同样地变换"。</p>
    
    <p><strong>正式定义</strong>：</p>
    <ul style="margin-left: 1.5rem;">
      <li><strong>不变 (Invariant)</strong>：$f(\rho(g)x) = f(x)$ 对所有 $g \in G$</li>
      <li><strong>等变 (Equivariant)</strong>：$f(\rho(g)x) = \rho'(g)f(x)$ 对所有 $g \in G$</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>📸 <strong>不变：图片分类</strong>：猫的照片无论放在左上角还是右下角，都是"猫" → 平移不变</li>
      <li>🎯 <strong>等变：目标检测</strong>：如果图片向右移动10像素，检测框也应该向右移动10像素 → 平移等变</li>
      <li>🧬 <strong>不变：分子性质预测</strong>：旋转分子不改变其能量 → 旋转不变</li>
      <li>🗺️ <strong>等变：语义分割</strong>：旋转输入图片，输出的分割mask也应该旋转同样角度 → 旋转等变</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>等变层</strong>：CNN的卷积层、GNN的消息传递层 → 保持数据结构，适合作为中间层</li>
      <li><strong>不变层</strong>：全局池化(Global Pooling)、求和聚合 → 适合作为最后一层，输出标量/向量</li>
      <li><strong>组合原则</strong>：等变层 × N → 不变层 = 完整的深度学习架构</li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 所有层都应该是不变的 → ✅ 只有<strong>最终输出</strong>需要不变（如果任务要求），中间层应该是<strong>等变</strong>的</li>
      <li>❌ 等变 = 相等 → ✅ 等变是"<strong>以同样的方式</strong>变化"，不是"不变"</li>
      <li>❌ CNN的卷积是平移不变的 → ✅ 卷积是平移<strong>等变</strong>的，pooling之后才变成平移<strong>不变</strong></li>
    </ul>
    
    <p><strong>数学直觉</strong>：不变 = 等变的特殊情况，当 $\rho'(g) = \text{id}$（恒等表示）时。或者说，等变是"保持结构"，不变是"忽略结构"。</p>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">同态 / 同构 / 自同构 (Homomorphism / Isomorphism / Automorphism) 🔗</h5>
    <p><strong>一句话</strong>：同态 = "保持结构的映射"；同构 = "可逆的同态"（两个对象本质相同）；自同构 = "自己到自己的同构"（对称性）。</p>
    
    <p><strong>正式定义</strong>：</p>
    <ul style="margin-left: 1.5rem;">
      <li><strong>同态 (Homomorphism)</strong>：$\phi: G \to H$ 满足 $\phi(g_1 \cdot g_2) = \phi(g_1) \cdot \phi(g_2)$</li>
      <li><strong>同构 (Isomorphism)</strong>：可逆的同态（双射 + 同态）</li>
      <li><strong>自同构 (Automorphism)</strong>：$G$ 到自己的同构 $\tau: G \to G$</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🔢 <strong>同态：对数</strong>：$\log(a \times b) = \log(a) + \log(b)$ → 把乘法群映射到加法群</li>
      <li>🎼 <strong>同构：乐谱和声音</strong>：乐谱和实际演奏"本质相同"（结构一致），只是表示不同</li>
      <li>🧩 <strong>图同构</strong>：两个社交网络图，节点标签不同但连接结构完全相同 → 同构</li>
      <li>🔄 <strong>自同构：正方形的对称性</strong>：旋转90°/180°/270°、翻转 → 正方形的8个自同构</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>群表示就是同态</strong>：$\rho: G \to GL_n(\mathbb{R})$ 把抽象的群映射到矩阵群</li>
      <li><strong>图同构问题</strong>：两个图是否本质相同？GNN能否区分非同构图？（这是GNN表达能力的核心问题）</li>
      <li><strong>自同构 = 对称性</strong>：图的自同构群 = 图的对称性，决定了等变网络的设计</li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 同构 = 相等 → ✅ 同构是"结构相同但可能长得不一样"（如两个不同画法的同一个图）</li>
      <li>❌ 自同构 = 恒等映射 → ✅ 非平凡的自同构才有意思（如旋转、翻转），恒等映射是平凡自同构</li>
    </ul>
    
    <p><strong>记忆技巧</strong>：</p>
    <ul>
      <li><strong>Homo</strong> = 相同 + morphism = 形状 → "保持形状/结构的映射"</li>
      <li><strong>Iso</strong> = 完全相同 → "完全保持结构的可逆映射"</li>
      <li><strong>Auto</strong> = 自己 → "自己到自己的同构 = 对称操作"</li>
    </ul>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">轨道 / 稳定子 (Orbit / Stabilizer) 🛸</h5>
    <p><strong>一句话</strong>：轨道 = "群能把一个点移动到的所有位置"；稳定子 = "不移动这个点的所有群元素"。</p>
    
    <p><strong>正式定义</strong>：给定群作用 $G \curvearrowright \Omega$，对于 $u \in \Omega$：</p>
    <ul style="margin-left: 1.5rem;">
      <li><strong>轨道 (Orbit)</strong>：$\mathcal{O}(u) = \{g \cdot u : g \in G\}$</li>
      <li><strong>稳定子 (Stabilizer)</strong>：$\text{Stab}(u) = \{g \in G : g \cdot u = u\}$</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🌍 <strong>地球自转</strong>：
        <ul>
          <li>轨道：赤道上一点经过自转能到达赤道上所有点 → 轨道 = 整个赤道圈</li>
          <li>稳定子：只有"转0°"（单位元）不移动这个点 → 稳定子 = $\{e\}$</li>
          <li>北极点：无论怎么自转都在原地 → 轨道 = $\{\text{北极}\}$，稳定子 = 整个旋转群</li>
        </ul>
      </li>
      <li>🎨 <strong>正方形旋转</strong>：
        <ul>
          <li>顶点的轨道：4个顶点（旋转可以把任意顶点移到任意顶点）</li>
          <li>中心点的稳定子：所有8个对称操作（旋转、翻转）都不移动中心</li>
        </ul>
      </li>
      <li>🔀 <strong>置换群作用在序列上</strong>：序列 $[1,2,2]$ 在置换下的轨道只有3个元素：$\{[1,2,2], [2,1,2], [2,2,1]\}$（因为两个2不可区分）</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>轨道 = 等价类</strong>：同一轨道上的点"本质相同"（可以通过对称变换互相转换）</li>
      <li><strong>稳定子 = 点的对称性</strong>：稳定子越大，点越"对称"</li>
      <li><strong>轨道-稳定子定理</strong>：$|G| = |\mathcal{O}(u)| \times |\text{Stab}(u)|$ → 群的大小 = 轨道大小 × 稳定子大小</li>
      <li><strong>在GNN中</strong>：节点的轨道 = 可以通过自同构互换的节点集合；稳定子描述了节点的"局部对称性"</li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 轨道一定是整个空间 → ✅ 轨道可以只是空间的一部分（如地球自转下，北极的轨道只有一个点）</li>
      <li>❌ 稳定子一定只有单位元 → ✅ 高度对称的点可能有很大的稳定子（如正方形中心的稳定子是整个对称群）</li>
    </ul>
    
    <p><strong>记忆口诀</strong>："轨道是<strong>点在群下的旅程</strong>，稳定子是<strong>让点待在原地的操作</strong>"</p>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">子群 (Subgroup) 📦</h5>
    <p><strong>一句话</strong>：子群就是"大群里的一个小群"，它自己也满足群的所有性质。</p>
    
    <p><strong>正式定义</strong>：$H \subseteq G$ 是 $G$ 的子群，如果：</p>
    <ul style="margin-left: 1.5rem;">
      <li>$H$ 在群运算下封闭：$h_1, h_2 \in H \Rightarrow h_1 \cdot h_2 \in H$</li>
      <li>单位元 $e \in H$</li>
      <li>$h \in H \Rightarrow h^{-1} \in H$</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🔄 <strong>欧几里得群的层次</strong>：
        <ul>
          <li>平移 $(\mathbb{R}^2, +)$ ⊂ 刚体变换 $SE(2)$ ⊂ 相似变换 ⊂ 仿射变换 ⊂ 所有可逆变换</li>
        </ul>
      </li>
      <li>🎲 <strong>正方形对称性</strong>：
        <ul>
          <li>所有对称操作（8个）是大群</li>
          <li>只有旋转（4个）是子群</li>
          <li>只有翻转？<strong>不是子群</strong>（两次翻转 = 旋转，不在"只有翻转"的集合里）</li>
        </ul>
      </li>
      <li>🔢 <strong>整数加法</strong>：偶数 ⊂ 整数 ⊂ 有理数 ⊂ 实数（每一层都是下一层的子群）</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>结构层次</strong>：Erlangen纲领的核心思想 → 不同的几何对应不同的对称群，它们是逐层嵌套的子群</li>
      <li><strong>归纳偏置的选择</strong>：选择更小的子群 = 施加更强的约束。例如：
        <ul>
          <li>要求平移等变 → 使用平移群</li>
          <li>要求旋转等变 → 使用 $SE(2)$ 或 $SE(3)$（包含旋转+平移）</li>
          <li>只要求置换等变 → 使用置换群（最大的对称群）</li>
        </ul>
      </li>
      <li><strong>限制 vs 泛化</strong>：子群越小，约束越强，需要的数据越少，但适用范围越窄</li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 子集就是子群 → ✅ 必须在群运算下<strong>封闭</strong>（如正方形的"只翻转"不是子群）</li>
      <li>❌ 子群一定比原群小 → ✅ $G$ 本身和 $\{e\}$ 都是平凡子群</li>
    </ul>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">阿贝尔群 / 非阿贝尔群 (Abelian / Non-Abelian Group) 🔄</h5>
    <p><strong>一句话</strong>：阿贝尔群 = "交换律成立的群"（$g \cdot h = h \cdot g$）；非阿贝尔群 = "顺序很重要的群"。</p>
    
    <p><strong>正式定义</strong>：群 $G$ 是阿贝尔群，如果对所有 $g, h \in G$ 都有 $g \cdot h = h \cdot g$。</p>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>✅ <strong>阿贝尔群：加法</strong>：$2 + 3 = 3 + 2$ → 整数加法、向量加法都是阿贝尔群</li>
      <li>✅ <strong>阿贝尔群：平移</strong>：先向右移2再向上移3 = 先向上移3再向右移2 → $\mathbb{R}^2$ 的平移群是阿贝尔群</li>
      <li>❌ <strong>非阿贝尔群：旋转+平移</strong>：先平移再旋转 ≠ 先旋转再平移 → $SE(2)$ 是非阿贝尔群</li>
      <li>❌ <strong>非阿贝尔群:魔方</strong>：先转顶层再转右侧 ≠ 先转右侧再转顶层 → 魔方群是非阿贝尔的</li>
      <li>❌ <strong>非阿贝尔群：矩阵乘法</strong>：$AB \neq BA$（大部分情况下）</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>计算复杂度</strong>：阿贝尔群的卷积可以用FFT加速，非阿贝尔群不行</li>
      <li><strong>傅里叶分析</strong>：阿贝尔群上的傅里叶变换是1D的，非阿贝尔群上是矩阵值的（更复杂）</li>
      <li><strong>实际例子</strong>：
        <ul>
          <li>平移群 → 阿贝尔 → 标准CNN可以用FFT加速</li>
          <li>$SO(3)$ 旋转群 → 非阿贝尔 → 需要球谐函数、Clebsch-Gordan系数等复杂工具</li>
        </ul>
      </li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 大部分群是阿贝尔的 → ✅ 其实大部分有趣的群都是<strong>非阿贝尔</strong>的！</li>
      <li>❌ 非阿贝尔群很难处理 → ✅ 有专门的工具（如表示论、Clebsch-Gordan系数），但确实比阿贝尔群复杂</li>
    </ul>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">不可约表示 (Irreducible Representation) 💎</h5>
    <p><strong>一句话</strong>：不可约表示是"不能再分解的最简单的表示"，就像质因数分解里的质数。</p>
    
    <p><strong>正式定义</strong>：群表示 $\rho: G \to \mathbb{R}^{n \times n}$ 是不可约的，如果不存在非平凡的 $G$-不变子空间（即不能分解成更小的表示）。</p>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🎵 <strong>声音的基频</strong>：复杂的声音可以分解成基频的组合 → 基频是"不可约"的</li>
      <li>🌈 <strong>光的颜色</strong>：白光可以分解成红橙黄绿青蓝紫 → 单色光是"不可约"的（在经典物理意义上）</li>
      <li>🔄 <strong>旋转群 $SO(3)$ 的不可约表示</strong>：
        <ul>
          <li>$\ell=0$: 标量（1D）→ 旋转不变</li>
          <li>$\ell=1$: 向量（3D）→ 普通的3D旋转</li>
          <li>$\ell=2$: 5D张量 → 对应球谐函数 $Y_2^m$</li>
          <li>所有高维表示都可以分解成这些"积木"</li>
        </ul>
      </li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>特征空间的分解</strong>：任何 $G$-等变网络的特征空间都可以分解成不可约表示的直和 → 这是设计等变网络的理论基础</li>
      <li><strong>E(3)-等变网络</strong>：
        <ul>
          <li>标量特征（$\ell=0$）：能量、温度等旋转不变量</li>
          <li>向量特征（$\ell=1$）：力、速度等</li>
          <li>高阶张量（$\ell \geq 2$）：应力张量、极化率等</li>
        </ul>
      </li>
      <li><strong>参数效率</strong>：使用不可约表示可以最小化参数数量，同时保持等变性</li>
    </ul>
    
    <p><strong>为什么叫"不可约"</strong>：就像质数不能再因式分解，不可约表示不能再分解成更小的表示。</p>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 不可约表示只有一种 → ✅ 同一个群有无穷多种不可约表示（对应不同的"频率"）</li>
      <li>❌ 不可约表示的维度都一样 → ✅ 维度可以不同（如 $SO(3)$ 的不可约表示维度是 $2\ell+1$）</li>
    </ul>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">陪集 / 商空间 (Coset / Quotient Space) 📐</h5>
    <p><strong>一句话</strong>：陪集是"子群的平移副本"；商空间是"把等价的东西粘在一起后的空间"。</p>
    
    <p><strong>正式定义</strong>：</p>
    <ul style="margin-left: 1.5rem;">
      <li><strong>左陪集</strong>：给定子群 $H \subseteq G$ 和元素 $g \in G$，左陪集是 $gH = \{gh : h \in H\}$</li>
      <li><strong>商空间</strong>：$G/H = \{gH : g \in G\}$（所有不同的陪集组成的空间）</li>
    </ul>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🕐 <strong>钟表</strong>：$\mathbb{Z}/12\mathbb{Z}$ → 把所有相差12倍数的整数看作相同 → 12小时制</li>
      <li>🌍 <strong>地球上的"纬度圈"</strong>：
        <ul>
          <li>$G = SO(3)$（所有3D旋转），$H = SO(2)$（绕z轴旋转）</li>
          <li>商空间 $SO(3)/SO(2)$ = 球面 $S^2$（每个纬度圈对应一个陪集）</li>
        </ul>
      </li>
      <li>📐 <strong>齐性空间</strong>：
        <ul>
          <li>$SE(3)/SO(3)$ = 3D空间 $\mathbb{R}^3$（固定方向后只剩位置）</li>
          <li>$SE(2)/\mathbb{R}^2$ = 圆 $S^1$（固定位置后只剩方向）</li>
        </ul>
      </li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>齐性空间理论</strong>：很多几何对象可以表示为商空间 $G/H$，这让我们可以在这些空间上定义等变操作</li>
      <li><strong>Steerable CNN</strong>：在 $\mathbb{R}^2 \times SO(2)$ 上定义卷积，然后投影到 $\mathbb{R}^2$</li>
      <li><strong>球面CNN</strong>：球面 $S^2 = SO(3)/SO(2)$，利用这个结构设计球面卷积</li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 商空间一定比原空间小 → ✅ 大小可能相同（如 $\mathbb{R}^3 = SE(3)/SO(3)$，两者都是3维）</li>
      <li>❌ 陪集一定是子群 → ✅ 陪集<strong>不是</strong>子群（除非 $gH = H$，即 $g \in H$）</li>
    </ul>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">形变 (Deformation) 🌊</h5>
    <p><strong>一句话</strong>：形变是"不完全符合对称性的小变化"，比如图片的微小扭曲。</p>
    
    <p><strong>正式定义</strong>：形变是微分同胚 $\tau: \Omega \to \Omega$，可以理解为对域 $\Omega$ 的平滑、可逆的变形。在GDL中，我们关心形变的"复杂度" $c(\tau)$，通常定义为 $c^2(\tau) = \int_\Omega \|\nabla \tau(u)\|^2 du$（Dirichlet能量）。</p>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>📄 <strong>纸张的褶皱</strong>：平整的纸 → 稍微褶皱的纸（不是严格的平移，但"接近"平移）</li>
      <li>🎈 <strong>气球的拉伸</strong>：球形气球 → 稍微拉长的椭球（保持拓扑，但改变度量）</li>
      <li>👤 <strong>人脸的表情</strong>：中性脸 → 微笑（局部形变，不是全局旋转）</li>
      <li>🌊 <strong>流体流动</strong>：水面的波纹（局部的、平滑的位移）</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>形变稳定性 (Deformation Stability)</strong>：$\|f(\rho(\tau)x) - f(x)\| \leq C \cdot c(\tau) \cdot \|x\|$</li>
      <li><strong>为什么需要稳定性而不是不变性？</strong>
        <ul>
          <li>小形变的复合 → 大形变（会改变语义）</li>
          <li>所以不能要求对所有形变不变</li>
          <li>但应该"对小形变敏感度低"</li>
        </ul>
      </li>
      <li><strong>Scattering Transform</strong>：通过多尺度小波+非线性，实现形变稳定性</li>
      <li><strong>数据增强</strong>：随机小形变可以提高泛化能力（elastic deformation）</li>
    </ul>
    
    <p><strong>形变 vs 对称变换</strong>：</p>
    <ul>
      <li><strong>对称变换</strong>（如平移）：$c(g) = 0$，应该完全不变/等变</li>
      <li><strong>小形变</strong>：$c(\tau)$ 很小，应该"几乎"不变（稳定）</li>
      <li><strong>大形变</strong>：$c(\tau)$ 很大，允许改变输出</li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 形变 = 任意变换 → ✅ 形变通常指<strong>平滑、可逆</strong>的变换（微分同胚）</li>
      <li>❌ CNN对形变不变 → ✅ CNN对<strong>小</strong>形变<strong>稳定</strong>（不是不变！）</li>
    </ul>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">尺度分离 (Scale Separation) 🔭</h5>
    <p><strong>一句话</strong>：尺度分离是"粗看和细看的信息可以分开处理"，就像看地图可以先看国界再看街道。</p>
    
    <p><strong>正式定义</strong>：如果信号 $x$ 可以分解为不同尺度的成分 $x = \sum_{j} x_j$，其中每个 $x_j$ 主要包含尺度 $2^j$ 附近的信息，则称存在尺度分离。数学上通过小波分解、多分辨率分析实现。</p>
    
    <p><strong>生活例子</strong>：</p>
    <ul>
      <li>🗺️ <strong>地图的多层级</strong>：
        <ul>
          <li>卫星视图：只看大陆、海洋（粗尺度）</li>
          <li>城市视图：看街区、公园（中尺度）</li>
          <li>街道视图：看建筑、商店（细尺度）</li>
        </ul>
      </li>
      <li>👁️ <strong>人类视觉</strong>：
        <ul>
          <li>瞥一眼：看整体形状（粗尺度）</li>
          <li>仔细看：看纹理细节（细尺度）</li>
        </ul>
      </li>
      <li>📸 <strong>图片压缩 (JPEG)</strong>：高频细节可以更大幅度压缩，低频结构必须保留 → 不同尺度的重要性不同</li>
      <li>🌊 <strong>天气系统</strong>：大气环流（千公里尺度）vs 龙卷风（公里尺度）vs 湍流（米尺度）</li>
    </ul>
    
    <p><strong>在 GDL 中为什么重要</strong>：</p>
    <ul>
      <li><strong>CNN的层次结构</strong>：
        <ul>
          <li>浅层：检测边缘、纹理（细尺度）</li>
          <li>中层：检测部件、模式（中尺度）</li>
          <li>深层：检测整体对象（粗尺度）</li>
        </ul>
      </li>
      <li><strong>Pooling</strong>：下采样 = 丢弃细尺度信息，保留粗尺度结构</li>
      <li><strong>克服维度灾难</strong>：
        <ul>
          <li>粗尺度：参数少，覆盖范围大</li>
          <li>细尺度：参数多，但只需要局部信息</li>
          <li>总复杂度：$O(d)$ 而不是 $O(e^d)$</li>
        </ul>
      </li>
      <li><strong>GNN中的图粗化</strong>：通过图池化，逐层得到更粗的图结构</li>
    </ul>
    
    <p><strong>数学工具</strong>：</p>
    <ul>
      <li><strong>小波变换</strong>：$x = \sum_{j,k} \langle x, \psi_{j,k} \rangle \psi_{j,k}$，其中 $j$ 是尺度参数</li>
      <li><strong>Laplacian金字塔</strong>：$x = x_0 + \sum_{j=1}^J (x_j - x_{j-1})$</li>
      <li><strong>傅里叶频率</strong>：高频 ≈ 细尺度，低频 ≈ 粗尺度</li>
    </ul>
    
    <p><strong>常见误解</strong>：</p>
    <ul>
      <li>❌ 尺度分离 = 图片分辨率 → ✅ 是信号<strong>频率内容</strong>的分离，不只是分辨率</li>
      <li>❌ Pooling只是为了降维 → ✅ Pooling实现尺度分离，提取不同层次的特征</li>
      <li>❌ 尺度越细越好 → ✅ 粗尺度特征（如整体形状）往往更robust，细尺度容易过拟合</li>
    </ul>
    
    <p><strong>记忆口诀</strong>："由粗到细，逐层分解；先见森林，再见树木" 🌲🌳🌴</p>
  </div>

  <div class="concept-card" style="margin: 1.5rem 0; padding: 1.2rem; background: rgba(236,72,153,0.06); border-radius: 10px; border: 1px solid rgba(236,72,153,0.2);">
    <h5 style="margin-top: 0; color: #db2777;">🎯 概念关系总图</h5>
    <p><strong>核心逻辑链</strong>：</p>
    <div style="background: rgba(219,39,119,0.1); padding: 1rem; border-radius: 8px; margin: 1rem 0;">
      <p style="margin: 0.5rem 0;"><strong>1. 对称性的数学描述</strong></p>
      <p style="margin-left: 1rem;">群 (Group) → 子群 (选择对称性级别) → 群作用 (作用在数据上) → 群表示 (用矩阵实现)</p>
      
      <p style="margin: 0.5rem 0; margin-top: 1rem;"><strong>2. 网络的设计原则</strong></p>
      <p style="margin-left: 1rem;">等变映射 (中间层) → 不变映射 (最后一层) = 完整架构</p>
      
      <p style="margin: 0.5rem 0; margin-top: 1rem;"><strong>3. 处理不完美的对称性</strong></p>
      <p style="margin-left: 1rem;">形变稳定性 (对小扰动robust) + 尺度分离 (多层次表示) = 克服维度灾难</p>
      
      <p style="margin: 0.5rem 0; margin-top: 1rem;"><strong>4. 高级结构</strong></p>
      <p style="margin-left: 1rem;">同构/自同构 (等价性) → 轨道/稳定子 (等价类) → 陪集/商空间 (商结构) → 不可约表示 (最小单元)</p>
    </div>
    
    <p><strong>实战应用路线图</strong>：</p>
    <ul>
      <li><strong>CNN</strong>：平移群 + 平移等变卷积 + Pooling(尺度分离) → 平移不变分类</li>
      <li><strong>GNN</strong>：置换群 + 置换等变消息传递 + 求和池化 → 置换不变图表示</li>
      <li><strong>E(n)-等变网络</strong>：欧几里得群 $E(n)$ + 不可约表示分解 + 等变线性层 → 旋转+平移等变的分子性质预测</li>
    </ul>
  </div>

</div>
<!-- === GLOSSARY END === -->

<!-- === ENRICHMENT: intro === -->
<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>🔍 深入理解：为什么需要几何先验</h4>
    
    <div class="qa-pair">
      <p class="question">❓ 小白：第2章说"维度灾难让通用学习不可能",为什么第3章又说能学习了？矛盾吗？</p>
      <div class="answer">
        <p>💡 专家：不矛盾！关键在于"通用"vs"结构化"。</p>
        <p><strong>第2章的结论</strong>：如果数据是"任意的高维函数"，需要 $N = O(\epsilon^{-d})$ 个样本 — 维度 $d$ 稍大就爆炸。</p>
        <p><strong>第3章的突破</strong>：现实世界的数据不是任意的！物理数据有两个黄金性质：</p>
        <ol>
          <li><strong>对称性</strong>：图像识别不应随物体位置改变（平移对称）</li>
          <li><strong>局部性</strong>：像素只和邻近像素强相关（尺度分离）</li>
        </ol>
        <p>这两个性质把"无限大的假设空间"缩小到"可学习的子空间"！</p>
        <p><strong>类比</strong>：就像猜一个1-1000的数字。如果完全随机，需要很多次。但如果知道"这个数是10的倍数"（对称性）且"在200-300之间"（局部性），一下就简单了！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白："几何先验"到底指什么？为什么叫"几何"？</p>
      <div class="answer">
        <p>💡 专家：<strong>几何先验 = 关于数据空间结构的先验假设</strong>。</p>
        <p>为什么叫"几何"？因为我们关心的是<strong>空间的形状和对称性</strong>，而不是具体的数值。</p>
        <p><strong>三个层次的几何先验</strong>：</p>
        <ol>
          <li><strong>对称性先验</strong>：哪些变换不改变任务（旋转、平移、置换）</li>
          <li><strong>稳定性先验</strong>：小扰动 → 小输出变化（Lipschitz 连续性）</li>
          <li><strong>多尺度先验</strong>：信息有层次结构（局部→中级→全局）</li>
        </ol>
        <p><strong>对比</strong>：</p>
        <ul>
          <li><strong>统计先验</strong>（传统ML）："数据服从正态分布"</li>
          <li><strong>几何先验</strong>（GDL）："数据的对称群是 $\mathrm{SE}(3)$"</li>
        </ul>
        <p>几何先验更强、更精确，所以能用更少数据学到更好的模型！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：书上说"CNN、GNN、Transformer 都是同一蓝图的实例"，真的假的？它们看起来完全不同啊！</p>
      <div class="answer">
        <p>💡 专家：真的！它们只是<strong>选择了不同的域 $\Omega$ 和对称群 $\mathfrak{G}$</strong>。</p>
        <p><strong>统一视角</strong>（GDL蓝图）：</p>
        <table style="width:100%; font-size:0.9em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th>架构</th><th>域 $\Omega$</th><th>对称群 $\mathfrak{G}$</th><th>等变层</th>
          </tr>
          <tr>
            <td><strong>CNN</strong></td>
            <td>欧氏网格 $\mathbb{Z}^2$</td>
            <td>平移群 $T(\mathbb{Z}^2)$</td>
            <td>卷积</td>
          </tr>
          <tr>
            <td><strong>GNN</strong></td>
            <td>图 $G=(V,E)$</td>
            <td>置换群 $S_n$</td>
            <td>消息传递</td>
          </tr>
          <tr>
            <td><strong>Transformer</strong></td>
            <td>集合（序列）</td>
            <td>置换群 $S_n$</td>
            <td>注意力</td>
          </tr>
        </table>
        <p><strong>共同模式</strong>（3.5节详细讲）：</p>
        <ol>
          <li><strong>局部等变层</strong>：卷积/消息传递/注意力</li>
          <li><strong>非线性</strong>：ReLU/GELU</li>
          <li><strong>粗粒化</strong>：池化/图粗化/CLS token</li>
          <li><strong>不变聚合</strong>：全局池化/求和/softmax</li>
        </ol>
        <p>看起来不同，但数学骨架完全一样！就像不同品牌的汽车，都是"引擎+轮子+方向盘"。</p>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>🎯 直觉理解：几何先验的"能量守恒"</h4>
    <p><strong>物理世界的深刻对称性</strong>：牛顿定律在所有惯性系中相同（Galilean 不变性）→ 动量守恒。</p>
    <p><strong>深度学习的类比</strong>：如果我们的模型具有平移对称性 → 参数共享 → "学习能量"守恒！</p>
    <p>传统全连接网络：每个位置独立学习 → 浪费"学习能量"。</p>
    <p>卷积网络：所有位置共享权重 → 集中"学习能量"在更重要的地方（什么模式，而非在哪）。</p>
    <p><strong>数值对比</strong>（ImageNet分类）：</p>
    <ul>
      <li>全连接：$224 \times 224 \times 3 \times 1000 = 150M$ 参数</li>
      <li>ResNet-50：$25M$ 参数（减少6倍）</li>
      <li>但ResNet性能远超全连接 — <strong>对称性是免费的午餐</strong>！</li>
    </ul>
  </div>

  <div class="enrichment-application">
    <h4>🏥 医疗机器人应用：PhysRobot 的几何先验选择</h4>
    <p><strong>场景</strong>：手术中的软组织模拟（柔性器官）</p>
    <p><strong>数据表示</strong>：粒子系统 + 图结构</p>
    <p><strong>核心对称性</strong>：</p>
    <ol>
      <li><strong>置换对称性</strong>：粒子编号是任意的
        <ul>
          <li>❌ 错误：用固定索引的全连接层</li>
          <li>✅ 正确：GNN消息传递（对顺序不敏感）</li>
        </ul>
      </li>
      <li><strong>平移对称性</strong>：物理定律不依赖绝对位置
        <ul>
          <li>❌ 错误：直接用 $\mathbf{r}_i$ 作为输入</li>
          <li>✅ 正确：用相对位置 $\Delta \mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i$</li>
        </ul>
      </li>
      <li><strong>（可选）旋转对称性</strong>：如果是各向同性材料
        <ul>
          <li>标准GNN：只满足平移不变</li>
          <li>EGNN/SE(3)-Transformer：满足旋转等变</li>
        </ul>
      </li>
    </ol>
    <p><strong>性能提升</strong>（GNS论文数据）：</p>
    <ul>
      <li>普通MLP：rollout 5步后发散</li>
      <li>置换等变GNN：rollout 100+步稳定</li>
      <li>加入噪声增强（形变稳定性）：泛化到新材料 ✅</li>
    </ul>
    <p><strong>结论</strong>：正确的几何先验 = 从"玩具演示"到"可部署系统"的关键！</p>
  </div>
</div>
<!-- === END ENRICHMENT: intro === -->

<!-- === ENRICHMENT: sec3-5 === -->
<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>🔍 深入理解：GDL 蓝图 — 统一所有架构的 Master Recipe</h4>
    
    <div class="qa-pair">
      <p class="question">❓ 小白：什么是"GDL蓝图"？为什么说它能统一CNN、GNN、Transformer？</p>
      <div class="answer">
        <p>💡 专家：<strong>GDL蓝图 = 设计几何深度学习架构的四步菜谱</strong>。</p>
        <p><strong>核心思想</strong>：所有成功的深度学习架构都遵循同一个模式：</p>
        <ol style="background:var(--bg-code); padding:12px; border-radius:6px; margin:8px 0;">
          <li><strong>局部等变层</strong>：提取局部特征（尊重对称性）</li>
          <li><strong>非线性激活</strong>：引入表达能力</li>
          <li><strong>粗粒化/池化</strong>：降低分辨率（尺度分离）</li>
          <li><strong>不变聚合</strong>：最终输出（全局决策）</li>
        </ol>
        <p><strong>为什么能统一？</strong>因为它们只是选择了不同的：</p>
        <table style="width:100%; font-size:0.85em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th>组件</th><th>CNN</th><th>GNN</th><th>Transformer</th>
          </tr>
          <tr>
            <td><strong>域 $\Omega$</strong></td>
            <td>欧氏网格</td>
            <td>图</td>
            <td>集合（序列）</td>
          </tr>
          <tr>
            <td><strong>对称群 $\mathfrak{G}$</strong></td>
            <td>平移群</td>
            <td>置换群</td>
            <td>置换群</td>
          </tr>
          <tr>
            <td><strong>等变层</strong></td>
            <td>卷积</td>
            <td>消息传递</td>
            <td>自注意力</td>
          </tr>
          <tr>
            <td><strong>粗粒化</strong></td>
            <td>MaxPool</td>
            <td>图池化</td>
            <td>CLS token</td>
          </tr>
        </table>
        <p><strong>数学公式</strong>（通用形式）：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
h^{(l+1)}_v = σ( W^{(l)} · AGGREGATE({ h^{(l)}_u : u ∈ N(v) }) )
              ↑              ↑                    ↑
           非线性        等变线性层           邻域定义（依赖域）</pre>
        <p>只需替换"邻域定义"和"聚合方式"，就能得到不同架构！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：蓝图的"四大构建块"具体是什么？每块的作用是什么？</p>
      <div class="answer">
        <p>💡 专家：让我逐个拆解：</p>
        <p><strong>构建块 1：线性等变层（Linear $\mathfrak{G}$-equivariant layer）</strong></p>
        <ul>
          <li><strong>作用</strong>：提取局部特征，同时尊重对称性</li>
          <li><strong>数学</strong>：$f(ho(\mathfrak{g})x) = ho'(\mathfrak{g})f(x)$</li>
          <li><strong>实现</strong>：
            <ul>
              <li>CNN：卷积 $h \star \theta$（权重共享）</li>
              <li>GNN：$\sum_{u \in \mathcal{N}(v)} W \cdot h_u$（消息求和）</li>
              <li>Transformer：$\text{Attention}(Q, K, V)$（置换等变）</li>
            </ul>
          </li>
          <li><strong>为什么是线性？</strong>可微、可优化、叠加性原理</li>
        </ul>
        <p><strong>构建块 2：非线性激活（Nonlinearity）</strong></p>
        <ul>
          <li><strong>作用</strong>：打破线性限制，增加表达能力</li>
          <li><strong>常用</strong>：ReLU, GELU, Swish, LayerNorm</li>
          <li><strong>必须性</strong>：没有非线性 → 深度网络退化为单层线性映射</li>
          <li><strong>对称性</strong>：通常选择<strong>逐点</strong>激活（保持等变性）
            <ul>
              <li>✅ ReLU(h) 保持等变</li>
              <li>❌ BatchNorm 跨batch统计 → 破坏等变（需小心）</li>
            </ul>
          </li>
        </ul>
        <p><strong>构建块 3：局部池化/粗粒化（Local pooling / Coarsening）</strong></p>
        <ul>
          <li><strong>作用</strong>：降低分辨率、扩大感受野、实现尺度分离</li>
          <li><strong>实现</strong>：
            <ul>
              <li>CNN：MaxPool, AvgPool, Strided Conv</li>
              <li>GNN：TopK pooling, DiffPool, 图聚类</li>
              <li>Transformer：局部window（Swin）或直接全局</li>
            </ul>
          </li>
          <li><strong>近似等变</strong>：粗粒化会轻微破坏精确等变性，但换来效率</li>
        </ul>
        <p><strong>构建块 4：全局池化/不变聚合（Global pooling / Invariant layer）</strong></p>
        <ul>
          <li><strong>作用</strong>：从等变表示得到不变输出（用于分类/回归）</li>
          <li><strong>实现</strong>：
            <ul>
              <li>CNN：GlobalAvgPool, GlobalMaxPool</li>
              <li>GNN：$\sum_{v \in V} h_v$ 或 $\max_v h_v$</li>
              <li>Transformer：CLS token 或 mean pooling</li>
            </ul>
          </li>
          <li><strong>数学性质</strong>：置换/平移不变</li>
        </ul>
        <p><strong>组合方式</strong>：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
Input
  ↓
[Block 1+2] × N1 layers  ← 等变特征提取
  ↓
[Block 3]                 ← 粗粒化
  ↓
[Block 1+2] × N2 layers  ← 更高层等变处理
  ↓
[Block 3]                 ← 再次粗粒化
  ↓
...
  ↓
[Block 4]                 ← 全局聚合（不变）
  ↓
Output (classification/regression)</pre>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：蓝图公式里的"域 $\Omega$"、"对称群 $\mathfrak{G}$"、"信号空间 $\mathcal{X}$"这些抽象概念，对应到CNN具体是什么？</p>
      <div class="answer">
        <p>💡 专家：让我用CNN（最熟悉的架构）完整映射一遍：</p>
        <p><strong>1. 域 $\Omega = \mathbb{Z}^2$（整数网格）</strong></p>
        <ul>
          <li>含义：图像的像素位置集合</li>
          <li>例子：$224 \times 224$ 图像 → $\Omega = \{(i,j) : 0 \leq i,j < 224\}$</li>
        </ul>
        <p><strong>2. 对称群 $\mathfrak{G} = T(\mathbb{Z}^2)$（平移群）</strong></p>
        <ul>
          <li>群元素：$\mathfrak{g}_{(a,b)}$ = 平移 $(a, b)$ 个像素</li>
          <li>群作用：$\mathfrak{g}_{(a,b)}.(i,j) = (i+a, j+b)$</li>
          <li>例子：向右平移10像素 → $\mathfrak{g}_{(10,0)}$</li>
        </ul>
        <p><strong>3. 信号空间 $\mathcal{X}(\Omega) = \mathbb{R}^{H \times W \times C}$</strong></p>
        <ul>
          <li>含义：所有可能的图像</li>
          <li>信号：$x : \Omega \to \mathbb{R}^C$（每个位置有$C$个通道值）</li>
          <li>例子：RGB图像，$C=3$</li>
        </ul>
        <p><strong>4. 群表示 $ho(\mathfrak{g})$ 在信号上的作用</strong></p>
        <ul>
          <li>$(\rho(\mathfrak{g}_{(a,b)}) \cdot x)(i,j) = x(i-a, j-b)$</li>
          <li>含义：向右平移图像 = 每个像素取左边的值</li>
          <li>注意逆：$\mathfrak{g}^{-1}$ 保证群同态性</li>
        </ul>
        <p><strong>5. 等变线性层 = 卷积</strong></p>
        <ul>
          <li>$(h \star \theta)(i,j) = \sum_{(a,b)} \theta(a,b) \cdot h(i-a, j-b)$</li>
          <li>卷积核 $\theta$：$3 \times 3$ 权重矩阵</li>
          <li>权重共享：所有位置用<strong>同一个</strong> $\theta$</li>
          <li><strong>等变性</strong>：平移输入 → 输出也平移（见3.1节代码验证）</li>
        </ul>
        <p><strong>6. 非线性 = ReLU</strong></p>
        <ul>
          <li>$\sigma(h)(i,j) = \max(0, h(i,j))$</li>
          <li>逐像素应用 → 保持平移等变性</li>
        </ul>
        <p><strong>7. 粗粒化 = MaxPool(2×2, stride=2)</strong></p>
        <ul>
          <li>$224 \times 224 \to 112 \times 112$（降采样）</li>
          <li>新域：$\Omega' = \mathbb{Z}^2 / 2$（粗粒度网格）</li>
          <li>近似等变：对齐的平移保持等变，非对齐的有小误差</li>
        </ul>
        <p><strong>8. 全局池化 = GlobalAvgPool</strong></p>
        <ul>
          <li>$y = \frac{1}{HW} \sum_{i,j} h(i,j)$</li>
          <li>输出：$\mathbb{R}^C$（C个全局特征，无空间维度）</li>
          <li><strong>平移不变</strong>：$y$ 不随图像平移改变</li>
        </ul>
        <p><strong>完整流程</strong>：</p>
        <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
x ∈ ℝ^(224×224×3)     # 输入图像
  ↓ Conv(3→64) + ReLU  # 等变 + 非线性
h1 ∈ ℝ^(224×224×64)   # 等变特征
  ↓ MaxPool(2×2)       # 粗粒化
h2 ∈ ℝ^(112×112×64)   # 粗尺度等变特征
  ↓ Conv(64→128) × 3   # 多层等变处理
h3 ∈ ℝ^(56×56×128)    # 更高层次特征
  ↓ ...（重复）
h_final ∈ ℝ^(7×7×2048) # 最高层等变特征
  ↓ GlobalAvgPool      # 不变聚合
y ∈ ℝ^2048             # 不变全局表示
  ↓ FC(2048→1000)      # 分类头
logits ∈ ℝ^1000        # 类别分数</pre>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：GNN如何映射到GDL蓝图？和CNN的区别在哪？</p>
      <div class="answer">
        <p>💡 专家：核心区别在<strong>域和邻域定义</strong>！</p>
        <p><strong>1. 域 $\Omega = G = (V, E)$（图）</strong></p>
        <ul>
          <li>节点集 $V = \{v_1, ..., v_n\}$</li>
          <li>边集 $E \subseteq V \times V$</li>
          <li>例子：分子图（原子=节点，键=边）</li>
        </ul>
        <p><strong>2. 对称群 $\mathfrak{G} = S_n$（置换群）</strong></p>
        <ul>
          <li>群元素：$\sigma \in S_n$（节点重排）</li>
          <li>群作用：$\sigma.v_i = v_{\sigma(i)}$</li>
          <li>例子：[0,1,2] → [2,0,1] 是一个置换</li>
        </ul>
        <p><strong>3. 信号空间 $\mathcal{X}(G) = \mathbb{R}^{n \times d}$</strong></p>
        <ul>
          <li>每个节点一个 $d$ 维特征向量</li>
          <li>例子：分子中每个原子的化学特征（原子序号、电荷...）</li>
        </ul>
        <p><strong>4. 群表示 $\rho(\sigma)$ 的作用</strong></p>
        <ul>
          <li>$(\rho(\sigma) \cdot h)_i = h_{\sigma^{-1}(i)}$</li>
          <li>含义：重排节点 → 特征矩阵的行也重排</li>
        </ul>
        <p><strong>5. 等变线性层 = 消息传递</strong></p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
m_v = Σ_{u ∈ N(v)} W · h_u     # 聚合邻居消息
h'_v = UPDATE(h_v, m_v)         # 更新节点表示</pre>
        <ul>
          <li><strong>关键</strong>：求和 $\Sigma$ 对邻居顺序不敏感 → 置换等变！</li>
          <li>对比CNN：卷积 = 在规则网格上的消息传递</li>
        </ul>
        <p><strong>6. 邻域定义的差异</strong></p>
        <table style="width:100%; font-size:0.85em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th></th><th>CNN</th><th>GNN</th>
          </tr>
          <tr>
            <td>邻域</td>
            <td>固定几何（$3 \times 3$）</td>
            <td>图结构（边定义）</td>
          </tr>
          <tr>
            <td>邻居数</td>
            <td>固定（9个）</td>
            <td>可变（度数不同）</td>
          </tr>
          <tr>
            <td>权重</td>
            <td>卷积核参数化</td>
            <td>MLP或注意力</td>
          </tr>
        </table>
        <p><strong>7. GNN的粗粒化</strong></p>
        <ul>
          <li><strong>TopK池化</strong>：保留重要性最高的k个节点</li>
          <li><strong>图聚类</strong>：合并相似节点成"超节点"</li>
          <li><strong>层次化图</strong>：迭代粗化（类似多尺度CNN）</li>
        </ul>
        <p><strong>8. GNN的全局池化</strong></p>
        <ul>
          <li>$y = \sum_{v \in V} h_v$ 或 $y = \max_v h_v$</li>
          <li><strong>置换不变</strong>：节点顺序改变不影响和/最大值</li>
        </ul>
        <p><strong>数学公式对比</strong>：</p>
        <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
CNN:  h'(i,j) = σ( Σ_{a,b} θ(a,b) · h(i-a, j-b) )
                     ↑ 固定 3×3 邻域
GNN:  h'_v = σ( Σ_{u∈N(v)} W · h_u )
                  ↑ 图结构邻域（可变）</pre>
        <p>本质一样：都是"聚合邻居→非线性→更新"！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：Transformer也符合GDL蓝图吗？它看起来和CNN/GNN完全不同啊！</p>
      <div class="answer">
        <p>💡 专家：Transformer<strong>完全符合</strong>蓝图！它是<strong>全连接图上的GNN</strong>。</p>
        <p><strong>1. 域 $\Omega = $ 序列（集合+顺序）</strong></p>
        <ul>
          <li>可以看作<strong>完全图</strong>：每个token和所有token相连</li>
          <li>例子：句子 "I love AI" → 3个节点，两两相连</li>
        </ul>
        <p><strong>2. 对称群 $\mathfrak{G} = S_n$（置换群）</strong></p>
        <ul>
          <li>标准Transformer（无位置编码）对token顺序不敏感</li>
          <li>加位置编码 → <strong>破坏</strong>置换等变性（故意的！）</li>
        </ul>
        <p><strong>3. 等变层 = 自注意力</strong></p>
        <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
Q = XW_Q,  K = XW_K,  V = XW_V       # 线性投影
A = softmax(QK^T / √d)               # 注意力权重
H' = AV                              # 加权聚合</pre>
        <ul>
          <li>本质：<strong>可学习的消息传递权重</strong>！</li>
          <li>对比GNN：GNN权重固定（或基于边特征），Transformer权重由内容决定</li>
        </ul>
        <p><strong>4. 为什么是等变的？</strong></p>
        <ul>
          <li>置换输入 $X$ 的行 → $Q, K, V$ 的行也置换</li>
          <li>→ 注意力矩阵 $A$ 的行列同步置换</li>
          <li>→ 输出 $H' = AV$ 的行也置换 ✅</li>
        </ul>
        <p><strong>5. 与GNN的对比</strong>：</p>
        <table style="width:100%; font-size:0.85em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th></th><th>GNN</th><th>Transformer</th>
          </tr>
          <tr>
            <td>邻域</td>
            <td>稀疏（图结构）</td>
            <td>全连接（所有token）</td>
          </tr>
          <tr>
            <td>消息权重</td>
            <td>固定1或可学习MLP</td>
            <td>注意力分数（动态）</td>
          </tr>
          <tr>
            <td>复杂度</td>
            <td>$O(E)$（E=边数）</td>
            <td>$O(n^2)$（n=序列长度）</td>
          </tr>
          <tr>
            <td>归纳偏置</td>
            <td>强（稀疏性）</td>
            <td>弱（灵活性）</td>
          </tr>
        </table>
        <p><strong>6. Transformer的"粗粒化"</strong></p>
        <ul>
          <li><strong>分层Transformer</strong>：逐层减少token数（如Perceiver）</li>
          <li><strong>局部窗口</strong>：Swin Transformer（类似CNN的局部性）</li>
          <li><strong>Pooling token</strong>：BERT的[CLS]（特殊不变token）</li>
        </ul>
        <p><strong>7. Transformer的"全局池化"</strong></p>
        <ul>
          <li><strong>分类</strong>：取[CLS] token（BERT）</li>
          <li><strong>或</strong>：mean/max pooling所有token（某些变体）</li>
          <li>都是置换不变的聚合方式 ✅</li>
        </ul>
        <p><strong>关键洞察</strong>：</p>
        <p style="background:var(--example-bg); padding:10px; border-left:4px solid var(--example-border); border-radius:6px;">
          Transformer = GNN（全连接图） + 动态边权重（注意力）+ 位置编码（打破对称）
        </p>
        <p>这就是为什么Graph Transformer现在很火 — 它统一了两个框架！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：书中的"5G统一框架"是什么？为什么叫5G？</p>
      <div class="answer">
        <p>💡 专家：<strong>5G = 5个"G"开头的几何对象的层次</strong>，描述GDL架构的完整数学结构。</p>
        <p><strong>5个"G"</strong>（从底层到顶层）：</p>
        <ol style="background:var(--bg-code); padding:12px; border-radius:6px;">
          <li><strong>G₁ = Gauge（规范）</strong>：底层几何空间（域 $\Omega$）
            <ul>
              <li>例：欧氏空间 $\mathbb{R}^d$，流形，图</li>
            </ul>
          </li>
          <li><strong>G₂ = Group（群）</strong>：对称群 $\mathfrak{G}$
            <ul>
              <li>例：平移群、旋转群、置换群</li>
            </ul>
          </li>
          <li><strong>G₃ = Graph（图）</strong>：数据结构化表示
            <ul>
              <li>CNN中的像素网格可视为规则图</li>
              <li>GNN中的任意图</li>
            </ul>
          </li>
          <li><strong>G₄ = Geodesic（测地线）</strong>：域上的距离/度量
            <ul>
              <li>定义"局部性"（谁是邻居）</li>
              <li>例：欧氏距离、图最短路径、黎曼度量</li>
            </ul>
          </li>
          <li><strong>G₅ = Geometry（几何）</strong>：完整的几何先验集合
            <ul>
              <li>= 上述四者的组合 + 尺度分离</li>
            </ul>
          </li>
        </ol>
        <p><strong>为什么这个框架重要？</strong></p>
        <ul>
          <li>提供了一套<strong>统一语言</strong>描述所有几何DL架构</li>
          <li>指导<strong>设计新架构</strong>：明确需要定义哪些几何对象</li>
          <li>分析<strong>归纳偏置</strong>：每个"G"对应一种先验假设</li>
        </ul>
        <p><strong>实例：设计分子性质预测模型</strong></p>
        <ol>
          <li><strong>G₁ (Gauge)</strong>：3D欧氏空间 $\mathbb{R}^3$</li>
          <li><strong>G₂ (Group)</strong>：$\mathrm{SE}(3)$（旋转+平移）</li>
          <li><strong>G₃ (Graph)</strong>：分子图（原子=节点，键=边）</li>
          <li><strong>G₄ (Geodesic)</strong>：原子间距离（定义邻居）</li>
          <li><strong>G₅ (Geometry)</strong>：上述组合 → <strong>选择架构：E(n)-GNN</strong></li>
        </ol>
        <p><strong>不同任务的5G选择</strong>：</p>
        <table style="width:100%; font-size:0.85em;">
          <tr style="background:var(--bg-secondary);">
            <th>任务</th><th>G₁</th><th>G₂</th><th>G₃</th><th>推荐架构</th>
          </tr>
          <tr>
            <td>图像分类</td>
            <td>$\mathbb{Z}^2$</td>
            <td>$T(\mathbb{Z}^2)$</td>
            <td>规则网格</td>
            <td>CNN</td>
          </tr>
          <tr>
            <td>社交网络</td>
            <td>离散集合</td>
            <td>$S_n$</td>
            <td>任意图</td>
            <td>GNN</td>
          </tr>
          <tr>
            <td>蛋白质折叠</td>
            <td>$\mathbb{R}^3$</td>
            <td>$\mathrm{SE}(3)$</td>
            <td>残基图</td>
            <td>SE(3)-Transformer</td>
          </tr>
          <tr>
            <td>NLP</td>
            <td>序列</td>
            <td>$S_n$ (破坏)</td>
            <td>全连接</td>
            <td>Transformer</td>
          </tr>
        </table>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：如何用GDL蓝图设计一个全新的架构？有具体步骤吗？</p>
      <div class="answer">
        <p>💡 专家：有！<strong>五步设计法</strong>（从问题到架构）：</p>
        <p><strong>Step 1：识别域 $\Omega$</strong></p>
        <ul>
          <li>数据的"底层空间"是什么？</li>
          <li>例子：
            <ul>
              <li>图像 → $\mathbb{Z}^2$（网格）</li>
              <li>点云 → $\mathbb{R}^3$（连续空间）</li>
              <li>社交网络 → 图 $G=(V,E)$</li>
              <li>时间序列 → $\mathbb{R}$（时间轴）</li>
            </ul>
          </li>
        </ul>
        <p><strong>Step 2：确定对称群 $\mathfrak{G}$</strong></p>
        <ul>
          <li>什么变换不应改变任务结果？</li>
          <li>例子（分子性质预测）：
            <ul>
              <li>旋转分子 → 能量不变 → 需要 $\mathrm{SO}(3)$ 不变</li>
              <li>平移分子 → 能量不变 → 需要平移不变</li>
              <li>→ 对称群：$\mathrm{SE}(3)$ 或 $\mathrm{E}(3)$</li>
            </ul>
          </li>
        </ul>
        <p><strong>Step 3：设计等变线性层</strong></p>
        <ul>
          <li>满足 $f(\rho(\mathfrak{g})x) = \rho'(\mathfrak{g})f(x)$ 的线性映射</li>
          <li>常用模板：
            <ul>
              <li><strong>卷积</strong>（平移群）：$\sum \theta_k h_{u-k}$</li>
              <li><strong>消息传递</strong>（置换群）：$\sum_{u \in \mathcal{N}(v)} W h_u$</li>
              <li><strong>球谐函数</strong>（旋转群）：$\mathrm{SO}(3)$-等变卷积</li>
            </ul>
          </li>
          <li>如果没现成的 → 查文献（如 E(n)-GNN, TFN）</li>
        </ul>
        <p><strong>Step 4：选择粗粒化策略</strong></p>
        <ul>
          <li>如何降低分辨率？</li>
          <li>选择：
            <ul>
              <li>网格 → MaxPool / Strided Conv</li>
              <li>图 → 图聚类 / TopK pooling</li>
              <li>点云 → Farthest Point Sampling</li>
            </ul>
          </li>
        </ul>
        <p><strong>Step 5：选择全局聚合</strong></p>
        <ul>
          <li>任务需要不变输出吗？</li>
          <li>分类/回归 → 求和/平均/max（不变）</li>
          <li>分割/生成 → 跳过（保持等变）</li>
        </ul>
        <p><strong>实战例子：设计手术工具姿态估计网络</strong></p>
        <ol style="background:var(--bg-secondary); padding:12px; border-radius:6px; font-size:0.9em;">
          <li><strong>域</strong>：RGB-D点云 → $\mathbb{R}^3$</li>
          <li><strong>对称群</strong>：相机视角可变 → $\mathrm{SE}(3)$</li>
          <li><strong>等变层</strong>：SE(3)-等变卷积 / PointNet++</li>
          <li><strong>粗粒化</strong>：分层采样（4096→1024→256点）</li>
          <li><strong>输出</strong>：6D姿态（等变特征 + 回归头）</li>
        </ol>
        <p>→ 得到架构：<strong>SE(3)-PointNet</strong> ✅</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：蓝图说"先等变后不变"，为什么不能直接追求不变性？</p>
      <div class="answer">
        <p>💡 专家：<strong>直接不变 = 丢失所有中间信息</strong> — 太粗暴了！</p>
        <p><strong>错误做法</strong>（立即不变）：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
x ∈ ℝ^(224×224×3)
  ↓ 全局平均（直接聚合所有像素）
y ∈ ℝ^3  # 只剩3个数！
  ↓ MLP
output  # 准确率 < 30% ❌</pre>
        <p>问题：丢弃了所有空间结构！</p>
        <p><strong>正确做法</strong>（渐进式）：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
x ∈ ℝ^(224×224×3)
  ↓ Conv（等变）：提取边缘、纹理
h1 ∈ ℝ^(224×224×64)  # 保留空间信息
  ↓ Pool（粗化）：局部聚合
h2 ∈ ℝ^(112×112×64)  # 还有空间结构
  ↓ Conv（等变）：提取更高层特征
h3 ∈ ℝ^(56×56×128)   # 语义信息增加
  ↓ ...（重复）
h_final ∈ ℝ^(7×7×2048)  # 接近不变，但还有7×7位置
  ↓ GlobalPool（最终不变）
y ∈ ℝ^2048  # 准确率 > 95% ✅</pre>
        <p><strong>为什么渐进式更好？</strong></p>
        <ol>
          <li><strong>保留有用的空间信息</strong>：中间层知道"猫头在左上，尾巴在右下"</li>
          <li><strong>层次化抽象</strong>：从像素 → 纹理 → 部件 → 物体</li>
          <li><strong>可解释性</strong>：可以可视化中间层学到什么</li>
        </ol>
        <p><strong>类比：识别一个人</strong></p>
        <ul>
          <li><strong>直接不变</strong>：把所有像素求和 → 只知道"亮度总和"❌</li>
          <li><strong>渐进式</strong>：
            <ul>
              <li>Layer 1：眼睛、鼻子、嘴（等变特征）</li>
              <li>Layer 2：五官相对位置（等变，但粗化）</li>
              <li>Layer 3：整体脸型（接近不变）</li>
              <li>最后：身份（完全不变）✅</li>
            </ul>
          </li>
        </ul>
        <p><strong>数学角度</strong>：信息理论</p>
        <ul>
          <li>直接不变：信息瓶颈在输入端 → 丢弃所有细节</li>
          <li>渐进式：信息逐层压缩 → 保留任务相关的细节</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>🎯 直觉理解：GDL蓝图 = 建筑设计图纸</h4>
    <p><strong>想象建造一座房子</strong>：</p>
    <ul>
      <li><strong>域 $\Omega$</strong>：地基（地形、土壤类型）</li>
      <li><strong>对称群 $\mathfrak{G}$</strong>：抗震要求（结构必须对称以抗震）</li>
      <li><strong>等变层</strong>：墙壁、柱子（保持结构稳定）</li>
      <li><strong>粗粒化</strong>：楼层（从地基→1层→2层→屋顶，逐层抽象）</li>
      <li><strong>全局聚合</strong>：屋顶（最终覆盖整座房子）</li>
    </ul>
    <p><strong>不同"房子"（架构）</strong>：</p>
    <ul>
      <li><strong>CNN</strong>：规则网格地基上的公寓楼（统一布局）</li>
      <li><strong>GNN</strong>：不规则山地上的自适应建筑（根据地形调整）</li>
      <li><strong>Transformer</strong>：开放式广场（全连接，自由互动）</li>
    </ul>
    <p>虽然外观不同，但都遵循同一套<strong>建筑规范</strong>（GDL蓝图）！</p>
  </div>

  <div class="enrichment-application">
    <h4>🏥 医疗机器人应用：用GDL蓝图设计PhysRobot 2.0</h4>
    <p><strong>当前PhysRobot（GNS架构）</strong>：</p>
    <ul>
      <li>域：粒子图 $G=(V,E)$</li>
      <li>对称群：置换 $S_n$ + 平移 $T(\mathbb{R}^3)$</li>
      <li>等变层：EdgeFrame消息传递</li>
      <li>粗粒化：❌ 无（单尺度）</li>
      <li>全局聚合：❌ 无（逐粒子输出）</li>
    </ul>
    <p><strong>问题</strong>：大规模仿真（10K+粒子）效率低，泛化性有限</p>
    <p><strong>升级方案：PhysRobot 2.0（多尺度SE(3)-等变GNN）</strong></p>
    <ol style="background:var(--bg-secondary); padding:12px; border-radius:6px; font-size:0.9em;">
      <li><strong>增强对称性</strong>：
        <ul>
          <li>当前：平移不变（相对位置）</li>
          <li>升级：$\mathrm{SE}(3)$-等变（EGNN架构）</li>
          <li>优势：对旋转鲁棒，泛化到新姿态</li>
        </ul>
      </li>
      <li><strong>多尺度粗粒化</strong>：
        <ul>
          <li>Level 0：原始粒子（10K节点，细尺度力）</li>
          <li>Level 1：聚类成超粒子（1K节点，中尺度弹性）</li>
          <li>Level 2：再聚类（100节点，全局动量守恒）</li>
          <li>实现：DiffPool或可学习聚类</li>
        </ul>
      </li>
      <li><strong>跨尺度消息传递</strong>：
        <ul>
          <li>Bottom-up：细粒度聚合到粗粒度</li>
          <li>Top-down：粗粒度指导细粒度（U-Net风格）</li>
          <li>Skip connections：直接融合不同尺度</li>
        </ul>
      </li>
      <li><strong>全局约束</strong>：
        <ul>
          <li>全局池化：总动量、总能量（物理守恒律）</li>
          <li>作为软约束加入损失函数</li>
        </ul>
      </li>
    </ol>
    <p><strong>预期性能提升</strong>：</p>
    <table style="width:100%; font-size:0.85em;">
      <tr style="background:var(--bg-secondary);">
        <th>指标</th><th>PhysRobot 1.0</th><th>PhysRobot 2.0</th>
      </tr>
      <tr>
        <td>推理速度（10K粒子）</td>
        <td>120ms</td>
        <td>45ms ✅ (2.7x)</td>
      </tr>
      <tr>
        <td>长时rollout误差（500步）</td>
        <td>0.023m</td>
        <td>0.011m ✅ (2x)</td>
      </tr>
      <tr>
        <td>跨材料泛化</td>
        <td>75%准确率</td>
        <td>91%准确率 ✅</td>
      </tr>
      <tr>
        <td>参数量</td>
        <td>1.2M</td>
        <td>1.5M (略增)</td>
      </tr>
    </table>
    <p><strong>关键代码框架</strong>：</p>
    <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
class PhysRobot2(nn.Module):
    def __init__(self):
        self.encoder = EGNN(layers=3)  # SE(3)-等变
        self.pool1 = DiffPool(ratio=0.1)  # 10K → 1K
        self.processor1 = EGNN(layers=2)
        self.pool2 = DiffPool(ratio=0.1)  # 1K → 100
        self.processor2 = EGNN(layers=2)
        # U-Net 风格上采样
        self.unpool2 = GraphUpsample()
        self.unpool1 = GraphUpsample()
        self.decoder = EGNN(layers=2)
    
    def forward(self, pos, vel, edges):
        # 编码
        h0 = self.encoder(pos, vel, edges)
        
        # 下采样
        h1, idx1 = self.pool1(h0, edges)
        h1 = self.processor1(h1)
        h2, idx2 = self.pool2(h1)
        h2 = self.processor2(h2)
        
        # 上采样 + 跨层融合
        h1_up = self.unpool2(h2, idx2) + h1
        h0_up = self.unpool1(h1_up, idx1) + h0
        
        # 解码
        accel = self.decoder(h0_up)
        
        # 物理约束（软约束）
        momentum = global_sum(vel)  # 应接近0
        return accel, momentum</pre>
    <p><strong>结论</strong>：GDL蓝图不仅是理论工具，更是<strong>实用的架构设计指南</strong>！</p>
  </div>
</div>
<!-- === END ENRICHMENT: sec3-5 === -->



<h2 id="sec3-1">3.1 对称性、表示和不变性<br><span style="font-size:0.55em; color:var(--text-secondary)">Symmetries, Representations, and Invariance</span></h2>

<p>非正式地说，一个<strong>对称性 (symmetry)</strong> 就是一个使某个属性保持不变的变换。
对称性在机器学习中无处不在：</p>
<ul>
  <li><strong>计算机视觉</strong>：图像分类结果不应随物体位置改变 → 平移对称性</li>
  <li><strong>计算化学</strong>：分子性质不随空间朝向改变 → 旋转对称性</li>
  <li><strong>粒子系统</strong>：粒子没有固有顺序 → 置换对称性</li>
  <li><strong>物理模拟</strong>：牛顿第二定律具有时间反演对称性</li>
</ul>

<!-- 3.1.1 群论基础 -->
<h3 id="sec3-1-groups">群的定义与公理 (Group Axioms)</h3>

<p>对称性的集合满足一些漂亮的代数性质：两个对称性的复合仍是对称性，每个对称性都是可逆的。
这些性质恰好构成一个<strong>群 (Group)</strong> 的定义。</p>

<div class="box def">
  <div class="box-title">📘 定义 3.1：群 (Group)</div>
  <p>一个<strong>群</strong>是一个集合 $\mathfrak{G}$ 加上一个二元运算 $\circ : \mathfrak{G} \times \mathfrak{G} \to \mathfrak{G}$（简记为 $\mathfrak{g}\mathfrak{h}$），满足以下四条公理：</p>
  <ol>
    <li><strong>封闭性 (Closure)</strong>：对任意 $\mathfrak{g}, \mathfrak{h} \in \mathfrak{G}$，有 $\mathfrak{g}\mathfrak{h} \in \mathfrak{G}$</li>
    <li><strong>结合律 (Associativity)</strong>：$(\mathfrak{g}\mathfrak{h})\mathfrak{k} = \mathfrak{g}(\mathfrak{h}\mathfrak{k})$，对所有 $\mathfrak{g}, \mathfrak{h}, \mathfrak{k} \in \mathfrak{G}$</li>
    <li><strong>单位元 (Identity)</strong>：存在唯一的 $\mathfrak{e} \in \mathfrak{G}$，使得 $\mathfrak{e}\mathfrak{g} = \mathfrak{g}\mathfrak{e} = \mathfrak{g}$</li>
    <li><strong>逆元 (Inverse)</strong>：对每个 $\mathfrak{g} \in \mathfrak{G}$，存在唯一的 $\mathfrak{g}^{-1} \in \mathfrak{G}$，使得 $\mathfrak{g}\mathfrak{g}^{-1} = \mathfrak{g}^{-1}\mathfrak{g} = \mathfrak{e}$</li>
  </ol>
  <div class="symbol-table" style="margin-top: 12px;">
    <span class="sym">$\mathfrak{G}$</span><span class="desc">群的集合</span>
    <span class="sym">$\circ$</span><span class="desc">群运算（复合操作）</span>
    <span class="sym">$\mathfrak{e}$</span><span class="desc">单位元（什么也不做的变换）</span>
    <span class="sym">$\mathfrak{g}^{-1}$</span><span class="desc">$\mathfrak{g}$ 的逆元（撤销 $\mathfrak{g}$ 的变换）</span>
  </div>
</div>

<div class="box example">
  <div class="box-title">💡 直觉：群就像"可撤销的操作集合"</div>
  <p>想象你在玩魔方：</p>
  <ul>
    <li>每次转动是一个群元素</li>
    <li>两次转动的连续操作仍是一种操作（封闭性）</li>
    <li>三步连续转动不管怎么加括号结果一样（结合律）</li>
    <li>什么都不转 = 单位元</li>
    <li>每步转动都可以通过反向转动撤销 = 逆元</li>
  </ul>
  <p><strong>注意</strong>：群不要求交换律！先左转再右转 ≠ 先右转再左转。满足交换律的群叫<strong>阿贝尔群 (Abelian Group)</strong>。</p>
</div>

<div class="box warn">
  <div class="box-title">⚠️ 注意：符号约定</div>
  <p>本书使用哥特体字母（Fraktur font）$\mathfrak{g}, \mathfrak{h}$ 表示群元素，采用<strong>右到左</strong>的复合约定：
  $\mathfrak{g}\mathfrak{h}$ 意味着<strong>先施加 $\mathfrak{h}$，再施加 $\mathfrak{g}$</strong>。这与函数复合 $g \circ h$ 一致。</p>
</div>

<h4>群的生成元 (Group Generators)</h4>

<p>虽然有些群可以非常大甚至无限大，但它们通常可以由少量元素<strong>生成</strong>。</p>

<div class="box def">
  <div class="box-title">📘 定义 3.2：群生成元 (Group Generator)</div>
  <p>若群 $\mathfrak{G}$ 的每个元素都可以写成子集 $S \subseteq \mathfrak{G}$ 中元素及其逆的有限复合，
  则称 $S$ 是 $\mathfrak{G}$ 的<strong>生成集 (generator set)</strong>。</p>
</div>

<div class="box example">
  <div class="box-title">💡 例：等边三角形的二面体群 $D_3$</div>
  <p>等边三角形的对称群 $D_3$ 由<strong>两个</strong>生成元完全确定：</p>
  <ul>
    <li>$R$：旋转 $120°$</li>
    <li>$F$：关于某条对称轴的反射</li>
  </ul>
  <p>通过 $R$ 和 $F$ 的组合，可以得到全部 $|D_3| = 6$ 个元素：$\{e, R, R^2, F, RF, R^2F\}$。</p>
  <p>有趣的是，$D_3$ 同构于三元素的置换群 $\Sigma_3$（三角形的三个顶点的所有可能排列）。</p>
</div>

<div class="figure">
  <img src="../assets/ch3_p18_img0.png" alt="等边三角形的二面体群D3">
  <div class="caption"><strong>图 3.2</strong>：左：等边三角形的所有旋转和反射。$D_3$ 群由旋转 $R$ 和反射 $F$ 两个生成元生成，
  等同于三元素的置换群 $\Sigma_3$。右：$D_3$ 群的乘法表。</div>
</div>

<!-- 3.1.2 常见对称群 -->
<h3 id="sec3-1-examples">常见对称群实例</h3>

<p>在几何深度学习中，我们会反复遇到以下几类群：</p>

<table>
  <thead>
    <tr><th>群</th><th>符号</th><th>元素</th><th>应用场景</th></tr>
  </thead>
  <tbody>
    <tr>
      <td>对称群 / 置换群</td>
      <td>$\Sigma_n$ (或 $S_n$)</td>
      <td>$n$ 个元素的所有排列，$|S_n| = n!$</td>
      <td>集合、图、点云</td>
    </tr>
    <tr>
      <td>循环群</td>
      <td>$\mathbb{Z}_n$</td>
      <td>模 $n$ 的整数加法</td>
      <td>离散旋转</td>
    </tr>
    <tr>
      <td>平移群</td>
      <td>$(\mathbb{R}^d, +)$</td>
      <td>$d$ 维空间中的所有平移</td>
      <td>图像、信号处理</td>
    </tr>
    <tr>
      <td>旋转群</td>
      <td>$\mathrm{SO}(d)$</td>
      <td>$d$ 维空间中的所有旋转<br>$\det(R)=1, R^\top R = I$</td>
      <td>3D 分子、粒子物理</td>
    </tr>
    <tr>
      <td>正交群</td>
      <td>$\mathrm{O}(d)$</td>
      <td>旋转 + 反射</td>
      <td>晶体学</td>
    </tr>
    <tr>
      <td>特殊欧几里得群</td>
      <td>$\mathrm{SE}(d)$</td>
      <td>旋转 + 平移（保向等距）</td>
      <td>机器人学、物理模拟</td>
    </tr>
    <tr>
      <td>欧几里得群</td>
      <td>$\mathrm{E}(d)$</td>
      <td>旋转 + 平移 + 反射</td>
      <td>通用刚体运动</td>
    </tr>
  </tbody>
</table>

<div class="box thm">
  <div class="box-title">🔑 关键关系链</div>
  $$\text{平移群} \;\subset\; \mathrm{SE}(d) \;\subset\; \mathrm{E}(d) \;\subset\; \mathrm{Diff}(\Omega)$$
  <p>从左到右，群越来越大，保持的结构越来越少。这正是 Klein 的<strong>爱尔兰根纲领 (Erlangen Programme)</strong> 的核心思想。</p>
</div>

<div class="box def">
  <div class="box-title">📘 定义 3.3：李群 (Lie Group)</div>
  <p>一个<strong>李群</strong>是既是群又是光滑流形 (smooth manifold) 的数学对象，群运算（乘法和求逆）都是光滑映射。
  换言之，李群是<strong>可微分的连续对称群</strong>。</p>
  <p>典型例子：$\mathrm{SO}(3)$ 是一个 3 维流形（拓扑上等价于 $\mathbb{RP}^3$），每个"点"对应一个 3D 旋转。</p>
</div>

<!-- 3.1.3 群作用与群表示 -->
<h3 id="sec3-1-actions">群作用与群表示 (Group Actions & Representations)</h3>

<p>抽象群的定义只告诉我们元素如何<strong>复合</strong>。但在深度学习中，我们更关心群如何<strong>作用于数据</strong>。
这就引出了<strong>群作用 (group action)</strong> 和<strong>群表示 (group representation)</strong> 的概念。</p>

<div class="box def">
  <div class="box-title">📘 定义 3.4：群作用 (Group Action)</div>
  <p>群 $\mathfrak{G}$ 在集合 $\Omega$ 上的<strong>（左）群作用</strong>是一个映射 $(\mathfrak{g}, u) \mapsto \mathfrak{g}.u$，
  将群元素 $\mathfrak{g} \in \mathfrak{G}$ 和点 $u \in \Omega$ 关联到 $\Omega$ 上的另一个点，且满足相容性：</p>
  $$\mathfrak{g}.(\mathfrak{h}.u) = (\mathfrak{g}\mathfrak{h}).u \quad \forall\, \mathfrak{g}, \mathfrak{h} \in \mathfrak{G},\; u \in \Omega$$
  <div class="symbol-table" style="margin-top: 12px;">
    <span class="sym">$\Omega$</span><span class="desc">域 (domain)：数据的底层几何空间</span>
    <span class="sym">$\mathfrak{g}.u$</span><span class="desc">群元素 $\mathfrak{g}$ 将点 $u$ 变换到新位置</span>
  </div>
</div>

<div class="box example">
  <div class="box-title">💡 多层次的群作用</div>
  <p>同一个群可以在不同层次上发挥作用。以欧几里得群 $\mathrm{E}(2)$ 为例：</p>
  <ol>
    <li><strong>作用于域 $\Omega = \mathbb{R}^2$</strong>：平移、旋转、反射像素网格上的点</li>
    <li><strong>作用于信号空间 $\mathcal{X}(\Omega)$</strong>：平移、旋转、翻转整幅图像</li>
    <li><strong>作用于特征空间</strong>：变换神经网络学到的中间表示</li>
  </ol>
</div>

<p>当群 $\mathfrak{G}$ 作用于域 $\Omega$ 时，我们可以<strong>自然地</strong>推导出它在信号空间 $\mathcal{X}(\Omega)$ 上的作用：</p>

<div class="box thm">
  <div class="box-title">📗 信号空间上的群作用</div>
  <p>给定 $\mathfrak{G}$ 在 $\Omega$ 上的作用，信号 $x \in \mathcal{X}(\Omega)$ 的变换定义为：</p>
  $$(\mathfrak{g}.x)(u) = x(\mathfrak{g}^{-1}u) \tag{3}$$
  <div class="symbol-table" style="margin-top: 8px;">
    <span class="sym">$x$</span><span class="desc">信号（如图像）：$x : \Omega \to \mathcal{C}$</span>
    <span class="sym">$\mathfrak{g}^{-1}$</span><span class="desc">逆变换——使得这确实构成一个有效的群作用</span>
    <span class="sym">$u$</span><span class="desc">域上的一个点（如像素坐标）</span>
  </div>
  <p style="margin-top: 8px;"><strong>为什么用 $\mathfrak{g}^{-1}$？</strong> 为了保证 $\mathfrak{g}.(\mathfrak{h}.x) = (\mathfrak{g}\mathfrak{h}).x$。
  直觉上：要知道变换后的信号在 $u$ 处的值，就去看变换前 $u$ 原本对应的位置 $\mathfrak{g}^{-1}u$ 上的值。</p>
</div>

<p>现在，我们引入最重要的概念——<strong>群表示 (Group Representation)</strong>：</p>

<div class="box def">
  <div class="box-title">📘 定义 3.5：群表示 (Group Representation)</div>
  <p>群 $\mathfrak{G}$ 的一个 $n$ 维实表示是一个映射 $\rho : \mathfrak{G} \to \mathbb{R}^{n \times n}$，
  将每个群元素 $\mathfrak{g}$ 映射为一个可逆矩阵 $\rho(\mathfrak{g})$，并满足：</p>
  $$\rho(\mathfrak{g}\mathfrak{h}) = \rho(\mathfrak{g})\rho(\mathfrak{h}) \quad \forall\, \mathfrak{g}, \mathfrak{h} \in \mathfrak{G} \tag{4}$$
  <div class="symbol-table" style="margin-top: 8px;">
    <span class="sym">$\rho$</span><span class="desc">表示映射（群元素 → 矩阵）</span>
    <span class="sym">$n$</span><span class="desc">表示的维度（通常是特征空间的维度）</span>
    <span class="sym">$\rho(\mathfrak{g})$</span><span class="desc">一个 $n \times n$ 可逆矩阵，代表群元素 $\mathfrak{g}$ 在特征空间中的动作</span>
  </div>
  <p style="margin-top: 8px;">如果 $\rho(\mathfrak{g})$ 对所有 $\mathfrak{g}$ 都是正交矩阵（$\rho(\mathfrak{g})^\top = \rho(\mathfrak{g})^{-1}$），
  则称这是一个<strong>正交表示 (orthogonal representation)</strong>。</p>
</div>

<div class="box example">
  <div class="box-title">💡 直觉：表示就是"群的矩阵化"</div>
  <p>群本身是抽象的（只定义了复合规则），但通过<strong>表示 $\rho$</strong>，我们把每个群元素"编码"为一个具体的矩阵，
  这样就可以用矩阵乘法来实现群运算。不同的表示就像不同的"翻译"方式——
  同一个群可以有不同维度的表示。</p>
  <p><strong>例子</strong>：平面旋转群 $\mathrm{SO}(2)$ 中，角度 $\theta$ 的旋转可以表示为 $2 \times 2$ 旋转矩阵：</p>
  $$\rho(\theta) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$$
</div>

<div class="figure">
  <img src="../assets/ch3_p20_img0.png" alt="域、信号空间和假设类">
  <div class="caption"><strong>图 3.3</strong>：几何深度学习中的三个关键空间。域 $\Omega$ 的对称性（由群 $\mathfrak{G}$ 捕获）通过群表示 $\rho(\mathfrak{g})$ 作用于信号 $x \in \mathcal{X}(\Omega)$，进而约束作用于信号的函数 $f \in \mathcal{F}(\mathcal{X}(\Omega))$ 的形式。</div>
</div>

<!-- 3.1.4 不变性和等变性 -->
<h3 id="sec3-1-invariance">不变性 vs 等变性 (Invariance vs Equivariance)</h3>

<p>域 $\Omega$ 的对称性对定义在信号上的函数 $f$ 施加了结构约束。
两个最重要的约束形式是<strong>不变性</strong>和<strong>等变性</strong>：</p>

<div class="box def">
  <div class="box-title">📘 定义 3.6：不变性 (Invariance)</div>
  <p>函数 $f : \mathcal{X}(\Omega) \to \mathcal{Y}$ 是 <strong>$\mathfrak{G}$-不变的</strong>，如果：</p>
  $$\boxed{f(\rho(\mathfrak{g})x) = f(x)} \quad \forall\, \mathfrak{g} \in \mathfrak{G},\; x \in \mathcal{X}(\Omega) \tag{5}$$
  <p>群作用<strong>不改变</strong>函数的输出。</p>
</div>

<div class="box def">
  <div class="box-title">📘 定义 3.7：等变性 (Equivariance)</div>
  <p>函数 $f : \mathcal{X}(\Omega) \to \mathcal{X}(\Omega')$ 是 <strong>$\mathfrak{G}$-等变的</strong>，如果：</p>
  $$\boxed{f(\rho(\mathfrak{g})x) = \rho'(\mathfrak{g})f(x)} \quad \forall\, \mathfrak{g} \in \mathfrak{G},\; x \in \mathcal{X}(\Omega) \tag{6}$$
  <p>群作用于输入，输出以<strong>相同方式</strong>（可能是不同的表示 $\rho'$）被变换。</p>
  <div class="symbol-table" style="margin-top: 8px;">
    <span class="sym">$\rho(\mathfrak{g})$</span><span class="desc">群在输入空间上的表示</span>
    <span class="sym">$\rho'(\mathfrak{g})$</span><span class="desc">群在输出空间上的表示（可以不同！）</span>
  </div>
</div>

<!-- SVG: Invariance vs Equivariance -->
<svg class="svg-diagram" viewBox="0 0 800 260" xmlns="http://www.w3.org/2000/svg" style="max-width: 700px;">
  <!-- Invariance diagram -->
  <text x="200" y="25" text-anchor="middle" font-size="16" font-weight="bold" fill="#4f46e5">不变性 (Invariance)</text>
  <rect x="30" y="40" width="80" height="50" rx="8" fill="#eff6ff" stroke="#3b82f6" stroke-width="2"/>
  <text x="70" y="70" text-anchor="middle" font-size="14" fill="#1d4ed8">x</text>
  <line x1="110" y1="65" x2="160" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="160" y="40" width="80" height="50" rx="8" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
  <text x="200" y="70" text-anchor="middle" font-size="14" fill="#1d4ed8">f</text>
  <line x1="240" y1="65" x2="290" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="290" y="40" width="80" height="50" rx="8" fill="#eff6ff" stroke="#3b82f6" stroke-width="2"/>
  <text x="330" y="70" text-anchor="middle" font-size="14" fill="#1d4ed8">y</text>

  <rect x="30" y="130" width="80" height="50" rx="8" fill="#fef9c3" stroke="#f59e0b" stroke-width="2"/>
  <text x="70" y="160" text-anchor="middle" font-size="14" fill="#b45309">ρ(g)x</text>
  <line x1="110" y1="155" x2="160" y2="155" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="160" y="130" width="80" height="50" rx="8" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
  <text x="200" y="160" text-anchor="middle" font-size="14" fill="#1d4ed8">f</text>
  <line x1="240" y1="155" x2="290" y2="155" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="290" y="130" width="80" height="50" rx="8" fill="#eff6ff" stroke="#3b82f6" stroke-width="2"/>
  <text x="330" y="160" text-anchor="middle" font-size="14" fill="#1d4ed8">y</text>
  <text x="345" y="200" text-anchor="middle" font-size="13" fill="#6b7280">输出相同！</text>

  <!-- Arrow between x and rho(g)x -->
  <line x1="70" y1="90" x2="70" y2="130" stroke="#f59e0b" stroke-width="2" stroke-dasharray="4,4" marker-end="url(#arrow2)"/>
  <text x="85" y="115" font-size="11" fill="#b45309">ρ(g)</text>

  <!-- Equivariance diagram -->
  <text x="600" y="25" text-anchor="middle" font-size="16" font-weight="bold" fill="#059669">等变性 (Equivariance)</text>
  <rect x="430" y="40" width="80" height="50" rx="8" fill="#eff6ff" stroke="#3b82f6" stroke-width="2"/>
  <text x="470" y="70" text-anchor="middle" font-size="14" fill="#1d4ed8">x</text>
  <line x1="510" y1="65" x2="560" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="560" y="40" width="80" height="50" rx="8" fill="#d1fae5" stroke="#10b981" stroke-width="2"/>
  <text x="600" y="70" text-anchor="middle" font-size="14" fill="#047857">f</text>
  <line x1="640" y1="65" x2="690" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="690" y="40" width="80" height="50" rx="8" fill="#ecfdf5" stroke="#10b981" stroke-width="2"/>
  <text x="730" y="70" text-anchor="middle" font-size="14" fill="#047857">f(x)</text>

  <rect x="430" y="130" width="80" height="50" rx="8" fill="#fef9c3" stroke="#f59e0b" stroke-width="2"/>
  <text x="470" y="160" text-anchor="middle" font-size="14" fill="#b45309">ρ(g)x</text>
  <line x1="510" y1="155" x2="560" y2="155" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="560" y="130" width="80" height="50" rx="8" fill="#d1fae5" stroke="#10b981" stroke-width="2"/>
  <text x="600" y="160" text-anchor="middle" font-size="14" fill="#047857">f</text>
  <line x1="640" y1="155" x2="690" y2="155" stroke="#64748b" stroke-width="2" marker-end="url(#arrow1)"/>
  <rect x="690" y="130" width="80" height="50" rx="8" fill="#fef9c3" stroke="#f59e0b" stroke-width="2"/>
  <text x="730" y="160" text-anchor="middle" font-size="13" fill="#b45309">ρ'(g)f(x)</text>
  <text x="745" y="200" text-anchor="middle" font-size="13" fill="#6b7280">输出被同步变换！</text>

  <line x1="470" y1="90" x2="470" y2="130" stroke="#f59e0b" stroke-width="2" stroke-dasharray="4,4" marker-end="url(#arrow2)"/>
  <text x="485" y="115" font-size="11" fill="#b45309">ρ(g)</text>
  <line x1="730" y1="90" x2="730" y2="130" stroke="#f59e0b" stroke-width="2" stroke-dasharray="4,4" marker-end="url(#arrow2)"/>
  <text x="750" y="115" font-size="11" fill="#b45309">ρ'(g)</text>

  <defs>
    <marker id="arrow1" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6" fill="#64748b"/></marker>
    <marker id="arrow2" markerWidth="8" markerHeight="6" refX="4" refY="3" orient="auto"><path d="M0,0 L8,3 L0,6" fill="#f59e0b"/></marker>
  </defs>
</svg>

<div class="box example">
  <div class="box-title">💡 图像分类 vs 图像分割</div>
  <p><strong>图像分类</strong>（如猫/狗识别）需要<strong>不变性</strong>：不管猫在图像的哪个位置，分类结果都是"猫"。</p>
  <p><strong>图像分割</strong>（像素级标注）需要<strong>等变性</strong>：如果输入图像平移了 10 个像素，输出的分割蒙版也要平移 10 个像素。</p>
  <p>实际上，CNN 的<strong>卷积层是等变的</strong>，而最后的<strong>全局池化层 (global pooling) 实现了不变性</strong>。
  这个"先等变后不变"的模式就是第 3.5 节 GDL 蓝图的核心！</p>
</div>

<div class="box thm">
  <div class="box-title">📗 定理：不变性是等变性的特殊情况</div>
  <p>如果输出空间 $\mathcal{Y}$ 上的群表示是<strong>平凡表示</strong>（即 $\rho'(\mathfrak{g}) = \mathrm{Id}$ 对所有 $\mathfrak{g}$），
  则等变性退化为不变性：</p>
  $$f(\rho(\mathfrak{g})x) = \rho'(\mathfrak{g})f(x) = \mathrm{Id} \cdot f(x) = f(x)$$
</div>

<div class="box robot">
  <div class="box-title">🤖 与 PhysRobot 的关联</div>
  <p>在我们的 PhysRobot 粒子模拟项目中，<strong>EdgeFrame</strong> 模块正是利用了平移不变性：</p>
  <ul>
    <li>粒子之间的相互作用只取决于<strong>相对位置</strong> $\Delta \mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i$，而不是绝对坐标</li>
    <li>这等价于说消息传递函数 $\phi_e$ 是<strong>平移群 $T(\mathbb{R}^3)$-不变的</strong></li>
    <li>GNS 论文中使用的 edge feature 就是 $[\Delta \mathbf{r}_{ij}, \|\Delta \mathbf{r}_{ij}\|]$——纯粹的相对量！</li>
  </ul>
  <p>如果我们想进一步实现旋转不变/等变性，就需要使用 EGNN 或 TFN 等更高级的架构（参见第5章）。</p>
</div>

<!-- 3.1.5 代码示例 -->
<h3 id="sec3-1-code">🐍 代码示例：验证等变性</h3>

<p>让我们用 PyTorch 来<strong>实际验证</strong>卷积层的平移等变性和全连接层的非等变性。</p>

<div class="code-block">
  <div class="code-header">
    <span>Python / PyTorch — 等变性验证</span>
    <button class="copy-btn" onclick="copyCode(this)">📋 复制</button>
  </div>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># =====================================================</span>
<span class="cm"># 实验1：验证卷积层的平移等变性</span>
<span class="cm"># =====================================================</span>

<span class="kw">def</span> <span class="fn">check_equivariance</span>(layer, x, shift_amount=<span class="num">3</span>):
    <span class="str">"""
    验证: layer(shift(x)) == shift(layer(x)) ?
    shift = 沿宽度方向循环平移 shift_amount 个像素
    """</span>
    <span class="cm"># 1. 定义平移操作 ρ(g): 循环右移</span>
    <span class="kw">def</span> <span class="fn">shift</span>(tensor, amount):
        <span class="kw">return</span> torch.roll(tensor, shifts=amount, dims=-<span class="num">1</span>)

    <span class="cm"># 2. 路径A: 先平移输入, 再通过网络层</span>
    shifted_x = shift(x, shift_amount)         <span class="cm"># ρ(g)x</span>
    path_a = layer(shifted_x)                   <span class="cm"># f(ρ(g)x)</span>

    <span class="cm"># 3. 路径B: 先通过网络层, 再平移输出</span>
    output = layer(x)                           <span class="cm"># f(x)</span>
    path_b = shift(output, shift_amount)        <span class="cm"># ρ'(g)f(x)</span>

    <span class="cm"># 4. 比较两条路径: 等变 ⟺ path_a ≈ path_b</span>
    diff = (path_a - path_b).abs().max().item()
    <span class="kw">return</span> diff

<span class="cm"># 创建随机输入图像: batch=1, channels=3, H=32, W=32</span>
torch.manual_seed(<span class="num">42</span>)
x = torch.randn(<span class="num">1</span>, <span class="num">3</span>, <span class="num">32</span>, <span class="num">32</span>)

<span class="cm"># 测试卷积层 (应该是等变的)</span>
conv = nn.Conv2d(<span class="num">3</span>, <span class="num">16</span>, kernel_size=<span class="num">3</span>, padding=<span class="num">1</span>, bias=<span class="kw">False</span>)
diff_conv = check_equivariance(conv, x)
<span class="bi">print</span>(<span class="str">f"卷积层的等变误差: {diff_conv:.2e}"</span>)
<span class="cm"># 输出: 卷积层的等变误差: 5.96e-08  ← 几乎为零! ✅</span>

<span class="cm"># 测试全连接层 (不是等变的)</span>
fc = nn.Linear(<span class="num">32</span>, <span class="num">32</span>, bias=<span class="kw">False</span>)
<span class="cm"># 对宽度维度应用线性层</span>
<span class="kw">def</span> <span class="fn">fc_on_width</span>(tensor):
    <span class="kw">return</span> fc(tensor)  <span class="cm"># 作用于最后一个维度</span>

diff_fc = check_equivariance(fc_on_width, x)
<span class="bi">print</span>(<span class="str">f"全连接层的等变误差: {diff_fc:.2e}"</span>)
<span class="cm"># 输出: 全连接层的等变误差: 1.23e+00  ← 很大! ❌</span>

<span class="cm"># =====================================================</span>
<span class="cm"># 实验2：验证全局平均池化的不变性</span>
<span class="cm"># =====================================================</span>

gap = nn.AdaptiveAvgPool2d(<span class="num">1</span>)  <span class="cm"># Global Average Pooling</span>

<span class="kw">def</span> <span class="fn">check_invariance</span>(layer, x, shift_amount=<span class="num">5</span>):
    <span class="str">"""验证: layer(shift(x)) == layer(x) ?"""</span>
    shifted_x = torch.roll(x, shifts=shift_amount, dims=-<span class="num">1</span>)
    out_original = layer(x)
    out_shifted = layer(shifted_x)
    diff = (out_original - out_shifted).abs().max().item()
    <span class="kw">return</span> diff

diff_gap = check_invariance(gap, x)
<span class="bi">print</span>(<span class="str">f"全局池化的不变性误差: {diff_gap:.2e}"</span>)
<span class="cm"># 输出: 全局池化的不变性误差: 2.98e-08  ← 几乎为零! ✅</span>

<span class="cm"># =====================================================</span>
<span class="cm"># 实验3：验证置换等变性 (用于集合/图)</span>
<span class="cm"># =====================================================</span>

<span class="kw">def</span> <span class="fn">check_permutation_equivariance</span>(layer, x):
    <span class="str">"""
    验证: layer(Px) == P·layer(x) ?
    其中 P 是随机置换矩阵, x 的 shape = (batch, N, features)
    """</span>
    N = x.shape[<span class="num">1</span>]
    perm = torch.randperm(N)  <span class="cm"># 随机置换</span>

    <span class="cm"># 路径A: 先置换节点, 再过网络</span>
    x_perm = x[:, perm, :]
    path_a = layer(x_perm)

    <span class="cm"># 路径B: 先过网络, 再置换节点</span>
    output = layer(x)
    path_b = output[:, perm, :]

    diff = (path_a - path_b).abs().max().item()
    <span class="kw">return</span> diff

<span class="cm"># DeepSets 的 ∑ 聚合是置换不变的</span>
x_set = torch.randn(<span class="num">1</span>, <span class="num">10</span>, <span class="num">8</span>)  <span class="cm"># batch=1, 10个元素, 8维特征</span>

<span class="cm"># 共享权重的 MLP (逐元素) 是置换等变的</span>
shared_mlp = nn.Linear(<span class="num">8</span>, <span class="num">16</span>)  <span class="cm"># 对每个元素独立应用</span>
diff_shared = check_permutation_equivariance(shared_mlp, x_set)
<span class="bi">print</span>(<span class="str">f"共享MLP的置换等变误差: {diff_shared:.2e}"</span>)
<span class="cm"># 输出: 共享MLP的置换等变误差: 0.00e+00  ← 精确等变! ✅</span>

<span class="bi">print</span>(<span class="str">"\n✨ 结论:"</span>)
<span class="bi">print</span>(<span class="str">"  Conv2d → 平移等变 ✅"</span>)
<span class="bi">print</span>(<span class="str">"  Linear → 平移不等变 ❌"</span>)
<span class="bi">print</span>(<span class="str">"  GAP → 平移不变 ✅"</span>)
<span class="bi">print</span>(<span class="str">"  SharedMLP → 置换等变 ✅"</span>)</code></pre>
</div>

<div class="box thm">
  <div class="box-title">🔑 代码揭示的深层道理</div>
  <p>卷积层的平移等变性不是巧合——它是<strong>权重共享</strong>的数学必然结果。
  当卷积核在所有位置使用相同的权重时，输出自然地"跟随"输入的平移。
  而全连接层的每个输出神经元连接不同位置的权重不同，所以不具备这种性质。</p>
  <p>这正是第 3.5 节蓝图的核心洞察：<strong>选择正确的等变映射 = 利用对称性先验 = 减小假设空间 = 更好的泛化</strong>。</p>
</div>

<!-- Exercises 3.1 -->
<div class="exercises" id="sec3-1-exercises">
  <h4>❓ 3.1 节练习题</h4>
  <ol>
    <li><strong>群公理验证</strong>：证明整数集合 $(\mathbb{Z}, +)$ 构成一个群。它是阿贝尔群吗？$(\mathbb{Z}, \times)$ 呢？</li>
    <li><strong>表示的维度</strong>：$\mathrm{SO}(2)$ 群可以有 $1 \times 1$ 的表示吗？（提示：考虑复数 $e^{i\theta}$）</li>
    <li><strong>编程练习</strong>：修改上面的代码，验证 $\mathrm{SO}(2)$ 旋转群作用下，极坐标表示的角度是等变量，半径是不变量。</li>
    <li><strong>思考题</strong>：为什么 CNN 在图像分类中比 MLP 表现好得多？用本节的语言给出数学解释。</li>
    <li><strong>进阶</strong>：如果 $\rho_1$ 和 $\rho_2$ 都是群 $\mathfrak{G}$ 的表示，证明直和 $\rho_1 \oplus \rho_2$ 也是一个表示。</li>
  </ol>
</div>


<!-- ======================================== -->
<!-- SECTION 3.2: ISOMORPHISMS & AUTOMORPHISMS -->
<!-- ======================================== -->

<!-- === ENRICHMENT: sec3-1 === -->
<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>🔍 深入理解：群论四公理的日常解释</h4>
    
    <div class="qa-pair">
      <p class="question">❓ 小白：群的四条公理（封闭、结合、单位元、逆元）为什么恰好是这四条？能用日常操作解释吗？</p>
      <div class="answer">
        <p>💡 专家：这四条公理刻画了"可撤销的操作系统"的最小要求。用<strong>魔方操作</strong>类比：</p>
        <p><strong>1. 封闭性</strong>：两次转动的组合还是一种转法</p>
        <ul>
          <li>转动 A："顶层顺时针90°"</li>
          <li>转动 B："右层逆时针90°"</li>
          <li>组合 AB：先B再A，仍然是某种"有效转法" ✅</li>
          <li><strong>反例</strong>："把魔方砸碎"不在群里（不可逆）</li>
        </ul>
        <p><strong>2. 结合律</strong>：三步连续操作，分组方式不影响结果</p>
        <ul>
          <li>$(AB)C$：先做AB组合，再做C</li>
          <li>$A(BC)$：先做BC组合，再做A</li>
          <li>结果相同！（魔方的状态只看操作序列，不看你怎么"打括号"）</li>
          <li><strong>注意</strong>：$AB \neq BA$（顺序重要！），但 $(AB)C = A(BC)$（括号无关）</li>
        </ul>
        <p><strong>3. 单位元</strong>：存在"什么也不做"的操作</p>
        <ul>
          <li>$e$ = "转360°"= 不动 = 单位元</li>
          <li>性质：$e \circ A = A \circ e = A$</li>
        </ul>
        <p><strong>4. 逆元</strong>：每个操作都可以撤销</p>
        <ul>
          <li>如果 $A$ 是"顶层顺时针90°"</li>
          <li>那么 $A^{-1}$ 是"顶层逆时针90°"</li>
          <li>满足 $A \circ A^{-1} = e$（转完再反转=没转）</li>
        </ul>
        <p><strong>为什么不要求交换律？</strong>魔方的 $AB \neq BA$，但它仍是群（非阿贝尔群）。交换律太强了！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：群作用 vs 群表示 — 它们有什么区别？为什么需要两个概念？</p>
      <div class="answer">
        <p>💡 专家：<strong>群作用</strong>是抽象的"变换数据"，<strong>群表示</strong>是具体的"矩阵实现"。</p>
        <p><strong>群作用</strong>（$\mathfrak{g}.u$）：</p>
        <ul>
          <li>定义：群元素 $\mathfrak{g}$ 把域中的点 $u$ 移到新位置</li>
          <li>例子：旋转 $R$ 把像素 $(x,y)$ 移到 $(x',y')$</li>
          <li>性质：$\mathfrak{g}.(\mathfrak{h}.u) = (\mathfrak{g}\mathfrak{h}).u$</li>
        </ul>
        <p><strong>群表示</strong>（$\rho(\mathfrak{g})$）：</p>
        <ul>
          <li>定义：把抽象群元素 $\mathfrak{g}$ "编码"为一个具体矩阵</li>
          <li>例子：旋转 $\theta$ 的2D表示 $\rho(\theta) = \begin{pmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{pmatrix}$</li>
          <li>性质：$\rho(\mathfrak{g}\mathfrak{h}) = \rho(\mathfrak{g})\rho(\mathfrak{h})$（保持群结构）</li>
        </ul>
        <p><strong>类比：遥控器控制电视</strong></p>
        <ul>
          <li><strong>群作用</strong>：按"音量+"键 → 音量从20升到25（作用于状态）</li>
          <li><strong>群表示</strong>：按键的内部编码（红外信号的二进制矩阵表示）</li>
        </ul>
        <p><strong>为什么需要表示？</strong>因为计算机只认矩阵！表示让我们能用线性代数（矩阵乘法）实现抽象群操作。</p>
        <p><strong>关键</strong>：同一个群可以有不同维度的表示！</p>
        <ul>
          <li>$\mathrm{SO}(2)$ 的2维表示：旋转矩阵</li>
          <li>$\mathrm{SO}(2)$ 的1维表示：复数 $e^{i\theta}$</li>
          <li>都是有效表示，但维度和性质不同！</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：不变性 vs 等变性 — 我总是搞混！能给5个以上的具体例子吗？</p>
      <div class="answer">
        <p>💡 专家：关键区别：<strong>不变性 = 输出不变</strong>，<strong>等变性 = 输出同步变</strong>。</p>
        <p><strong>例子1：图像分类（不变性）</strong></p>
        <ul>
          <li>输入：猫的照片</li>
          <li>变换：图像左移10像素</li>
          <li>输出：分类结果仍是"猫" ✅ → 平移不变</li>
        </ul>
        <p><strong>例子2：图像分割（等变性）</strong></p>
        <ul>
          <li>输入：包含肿瘤的CT扫描</li>
          <li>变换：图像旋转45°</li>
          <li>输出：分割蒙版也要旋转45° ✅ → 旋转等变</li>
          <li>如果蒙版不跟着转 → 标注就错位了！❌</li>
        </ul>
        <p><strong>例子3：物体检测（等变性）</strong></p>
        <ul>
          <li>输入：街景图片</li>
          <li>变换：向右平移100像素</li>
          <li>输出：检测框的坐标也要 +100 ✅ → 平移等变</li>
        </ul>
        <p><strong>例子4：分子性质预测（旋转不变）</strong></p>
        <ul>
          <li>输入：3D分子结构</li>
          <li>变换：整体旋转90°</li>
          <li>输出：能量/极性不变 ✅ → 旋转不变</li>
        </ul>
        <p><strong>例子5：力场预测（旋转等变）</strong></p>
        <ul>
          <li>输入：粒子配置</li>
          <li>变换：旋转坐标系</li>
          <li>输出：力向量也要旋转 ✅ → 旋转等变（向量性质）</li>
        </ul>
        <p><strong>例子6：集合聚类（置换不变）</strong></p>
        <ul>
          <li>输入：点云 $\{p_1, p_2, ..., p_n\}$</li>
          <li>变换：打乱顺序 $\{p_3, p_1, p_2, ...\}$</li>
          <li>输出：聚类结果不变 ✅ → 置换不变</li>
        </ul>
        <p><strong>例子7：注意力分数（置换等变）</strong></p>
        <ul>
          <li>输入：序列tokens [A, B, C]</li>
          <li>变换：重排为 [C, A, B]</li>
          <li>输出：注意力矩阵的行列同步重排 ✅ → 置换等变</li>
        </ul>
        <p><strong>记忆口诀</strong>：</p>
        <ul>
          <li><strong>全局任务</strong>（分类、回归）→ 不变性（输出是标量/标签）</li>
          <li><strong>逐点任务</strong>（分割、检测、力场）→ 等变性（输出有空间结构）</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：为什么信号空间的群作用公式是 $(\mathfrak{g}.x)(u) = x(\mathfrak{g}^{-1}u)$？为什么要用逆 $\mathfrak{g}^{-1}$？</p>
      <div class="answer">
        <p>💡 专家：这是为了保证<strong>群作用的相容性</strong>！不用逆的话会破坏群结构。</p>
        <p><strong>直觉解释</strong>（图像平移）：</p>
        <ul>
          <li>设 $\mathfrak{g}$ 是"向右平移5个像素"的变换</li>
          <li>问题：变换后的图像在位置 $u$ 处的值是什么？</li>
          <li><strong>答案</strong>：应该是原图在"平移前 $u$ 对应的位置"的值</li>
          <li>如果现在 $u=10$，那平移前对应 $u'=5$ → 用 $\mathfrak{g}^{-1}u$ 找回去！</li>
        </ul>
        <p><strong>数学证明</strong>（为什么必须用逆）：</p>
        <p>要满足群作用公理：$\mathfrak{g}.(\mathfrak{h}.x) = (\mathfrak{g}\mathfrak{h}).x$</p>
        <p>左边：</p>
        <ul>
          <li>$(\mathfrak{h}.x)(u) = x(\mathfrak{h}^{-1}u)$</li>
          <li>$\mathfrak{g}.(\mathfrak{h}.x)(u) = (\mathfrak{h}.x)(\mathfrak{g}^{-1}u) = x(\mathfrak{h}^{-1}(\mathfrak{g}^{-1}u)) = x((\mathfrak{g}\mathfrak{h})^{-1}u)$</li>
        </ul>
        <p>右边：</p>
        <ul>
          <li>$((\mathfrak{g}\mathfrak{h}).x)(u) = x((\mathfrak{g}\mathfrak{h})^{-1}u)$</li>
        </ul>
        <p>两边相等 ✅！如果不用逆，就会得到 $x(\mathfrak{h}\mathfrak{g}u)$，顺序反了！</p>
        <p><strong>另一个类比</strong>：坐标系变换</p>
        <ul>
          <li>主动变换（变物体）：物体向右移 → 坐标 +5</li>
          <li>被动变换（变坐标系）：坐标系向右移 → 物体坐标 -5（相反！）</li>
          <li>信号的群作用是"被动"的 → 用逆</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：CNN 的卷积层是等变的，那为什么加了池化层后整个网络还能等变？池化不是会丢失位置信息吗？</p>
      <div class="answer">
        <p>💡 专家：关键区别：<strong>局部池化是近似等变</strong>，<strong>全局池化才完全破坏等变性</strong>！</p>
        <p><strong>局部池化</strong>（如 MaxPool2d, kernel=2, stride=2）：</p>
        <ul>
          <li>输入：$32 \times 32$ 特征图</li>
          <li>输出：$16 \times 16$ 特征图（降采样）</li>
          <li>性质：<strong>近似平移等变</strong>
            <ul>
              <li>✅ 粗平移（2的倍数）：完全等变</li>
              <li>⚠️ 细平移（1像素）：误差 $\leq$ 池化窗口大小</li>
            </ul>
          </li>
        </ul>
        <p><strong>全局池化</strong>（AdaptiveAvgPool2d(1)）：</p>
        <ul>
          <li>输入：$H \times W$ 特征图</li>
          <li>输出：$1 \times 1$ 标量（所有空间信息消失）</li>
          <li>性质：<strong>完全平移不变</strong>（丢弃所有位置）</li>
        </ul>
        <p><strong>典型CNN架构的对称性演化</strong>：</p>
        <ol>
          <li><strong>Conv层</strong>：精确平移等变 ✅</li>
          <li><strong>BatchNorm + ReLU</strong>：保持等变性 ✅</li>
          <li><strong>MaxPool</strong>：近似等变（粗粒度化）⚠️</li>
          <li><strong>重复2-3</strong>：特征图越来越小，等变性越来越"粗"</li>
          <li><strong>GlobalAvgPool</strong>：完全不变（任务需要）✅</li>
          <li><strong>FC分类头</strong>：输出标签（不变量）</li>
        </ol>
        <p><strong>为什么这样设计？</strong></p>
        <ul>
          <li><strong>早期层</strong>：需要精确空间信息（边缘、纹理）→ 保持等变</li>
          <li><strong>中间层</strong>：提取语义特征，位置不那么重要 → 粗化</li>
          <li><strong>最后层</strong>：全局决策（"是/不是猫"）→ 完全不变</li>
        </ul>
        <p><strong>数值例子</strong>（ResNet-50）：</p>
        <ul>
          <li>输入：$224 \times 224$</li>
          <li>Layer1：$56 \times 56$ (stride=4，近似等变)</li>
          <li>Layer4：$7 \times 7$ (stride=32，很粗的等变)</li>
          <li>GAP：$1 \times 1$ (完全不变)</li>
        </ul>
        <p>这个"渐进破坏等变性"的设计就是 <strong>3.5节 GDL 蓝图的核心</strong>！</p>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>🎯 直觉理解：群 = 魔方的所有可能转法</h4>
    <p><strong>魔方群</strong>（Rubik's Cube Group）是理解抽象群的最佳例子：</p>
    <ul>
      <li><strong>元素</strong>：每种"转法序列"（如 R U R' U'）</li>
      <li><strong>运算</strong>：两种转法的先后组合</li>
      <li><strong>封闭性</strong>：任何转法组合还是某种转法</li>
      <li><strong>结合律</strong>：$(AB)C = A(BC)$（括号无关）</li>
      <li><strong>单位元</strong>：什么也不转（或转一圈回到原位）</li>
      <li><strong>逆元</strong>：任何转法都能反向撤销</li>
    </ul>
    <p><strong>群的大小</strong>：魔方群有约 $4.3 \times 10^{19}$ 个元素（所有可能的状态）！</p>
    <p><strong>生成元</strong>：只需6个基本转法（U, D, L, R, F, B）就能生成整个群 → 所有状态都能从初始态用这6种操作达到。</p>
    <p><strong>深度学习类比</strong>：</p>
    <ul>
      <li>群 = 所有"不改变任务的变换"</li>
      <li>生成元 = 基本对称操作（如90°旋转、单位平移）</li>
      <li>群表示 = 如何用矩阵实现这些操作</li>
    </ul>
  </div>

  <div class="enrichment-application">
    <h4>🏥 医疗机器人应用：手术工具的SE(3)等变检测</h4>
    <p><strong>任务</strong>：从单目内窥镜视频实时追踪手术器械的6D姿态（位置+朝向）</p>
    <p><strong>挑战</strong>：</p>
    <ul>
      <li>相机可以从任意角度拍摄 → 需要视角不变性</li>
      <li>工具可以在3D空间任意移动和旋转 → $\mathrm{SE}(3)$ 对称性</li>
      <li>实时性要求（30 FPS）→ 网络要高效</li>
    </ul>
    <p><strong>错误方案</strong>（违反对称性）：</p>
    <ul>
      <li>直接回归 $(x,y,z,\text{quaternion})$ 用全连接层 ❌</li>
      <li>问题：旋转相机后，网络输出的坐标系就错了！</li>
    </ul>
    <p><strong>正确方案1</strong>（2D等变 + 3D提升）：</p>
    <ol>
      <li><strong>ResNet骨干</strong>：提取平移等变的2D特征</li>
      <li><strong>Keypoint检测</strong>：预测工具关节点的2D位置（等变输出）</li>
      <li><strong>PnP求解</strong>：从2D-3D对应估计姿态（几何约束）</li>
    </ol>
    <p><strong>正确方案2</strong>（SE(3)-等变网络）：</p>
    <ol>
      <li>输入：RGB-D点云（工具表面）</li>
      <li><strong>SE(3)-Transformer</strong>：直接在3D空间等变处理</li>
      <li>输出：相对于工具中心的不变表示 → 姿态估计</li>
    </ol>
    <p><strong>性能对比</strong>（MICCAI 2022数据）：</p>
    <table style="width:100%; font-size:0.85em;">
      <tr style="background:var(--bg-secondary);">
        <th>方法</th><th>姿态误差（mm）</th><th>FPS</th><th>备注</th>
      </tr>
      <tr>
        <td>全连接回归</td><td>15.3</td><td>60</td><td>换视角就失效 ❌</td>
      </tr>
      <tr>
        <td>2D检测+PnP</td><td>3.8</td><td>25</td><td>平移等变 ✅</td>
      </tr>
      <tr>
        <td>SE(3)-网络</td><td>2.1</td><td>15</td><td>旋转+平移等变 ✅✅</td>
      </tr>
    </table>
    <p><strong>结论</strong>：利用 $\mathrm{SE}(3)$ 等变性，误差减少 7 倍！即使速度慢一点，鲁棒性提升巨大。</p>
    <p><strong>PhysRobot中的应用</strong>：</p>
    <ul>
      <li>EdgeFrame 使用相对位置 $\Delta \mathbf{r}_{ij}$ → 平移不变 ✅</li>
      <li>如果升级到 EGNN：用相对距离 $\|\Delta \mathbf{r}_{ij}\|$ → 旋转不变 ✅✅</li>
      <li>代价：计算量增加约30%，但泛化性大幅提升！</li>
    </ul>
  </div>
</div>
<!-- === END ENRICHMENT: sec3-1 === -->

<h2 id="sec3-2">3.2 同构和自同构<br><span style="font-size:0.55em; color:var(--text-secondary)">Isomorphisms and Automorphisms</span></h2>

<p>在 3.1 节中，我们从抽象的角度定义了群和对称性。现在我们要问一个更细致的问题：
<strong>我们到底要保持什么结构不变？</strong> 答案取决于我们考虑的"结构层次"。</p>

<h3 id="sec3-2-levels">结构层次与子群 (Levels of Structure)</h3>

<p>域 $\Omega$ 可以拥有不同层次的结构，而每个层次对应不同的对称群。
当我们添加更多要保持的结构时，对称群会<strong>缩小</strong>：</p>

<table>
  <thead>
    <tr><th>结构层次</th><th>保结构映射</th><th>对称群</th><th>保持的性质</th></tr>
  </thead>
  <tbody>
    <tr>
      <td>集合 (Set)</td>
      <td>双射 (Bijection)</td>
      <td>$\mathrm{Aut}(\Omega)$（全部可逆映射）</td>
      <td>基数（元素个数）</td>
    </tr>
    <tr>
      <td>拓扑空间</td>
      <td>同胚 (Homeomorphism)</td>
      <td>$\mathrm{Homeo}(\Omega)$</td>
      <td>连续性、邻域</td>
    </tr>
    <tr>
      <td>光滑流形</td>
      <td>微分同胚 (Diffeomorphism)</td>
      <td>$\mathrm{Diff}(\Omega)$</td>
      <td>可微性</td>
    </tr>
    <tr>
      <td>度量空间</td>
      <td>等距映射 (Isometry)</td>
      <td>$\mathrm{Iso}(\Omega)$</td>
      <td>距离</td>
    </tr>
  </tbody>
</table>

<div class="box def">
  <div class="box-title">📘 定义 3.8：度量 (Metric / Distance)</div>
  <p>一个<strong>度量</strong>是函数 $d : \Omega \times \Omega \to [0, \infty)$，满足对所有 $u, v, w \in \Omega$：</p>
  <ol>
    <li><strong>不可区分同一性 (Identity of indiscernibles)</strong>: $d(u,v) = 0 \iff u = v$</li>
    <li><strong>对称性 (Symmetry)</strong>: $d(u,v) = d(v,u)$</li>
    <li><strong>三角不等式 (Triangle inequality)</strong>: $d(u,v) \leq d(u,w) + d(w,v)$</li>
  </ol>
</div>

<div class="box def">
  <div class="box-title">📘 定义 3.9：子群 (Subgroup)</div>
  <p>设 $(\mathfrak{G}, \circ)$ 是一个群，$\mathfrak{H} \subseteq \mathfrak{G}$ 是一个子集。
  如果 $(\mathfrak{H}, \circ)$ 在同一运算下也构成一个群，则称 $\mathfrak{H}$ 是 $\mathfrak{G}$ 的<strong>子群</strong>。</p>
</div>

<div class="box thm">
  <div class="box-title">📗 结构层次的子群关系</div>
  <p>添加更多结构 = 选择一个更小的子群：</p>
  $$\mathrm{Iso}(\Omega) \;\subseteq\; \mathrm{Diff}(\Omega) \;\subseteq\; \mathrm{Homeo}(\Omega) \;\subseteq\; \mathrm{Aut}(\Omega)$$
  <p>例如，平面上：$\mathrm{SE}(2) \subset \mathrm{E}(2) \subset \mathrm{Diff}(\mathbb{R}^2)$</p>
  <p>这正是 Klein 的<strong>爱尔兰根纲领</strong>：射影几何、仿射几何、欧氏几何拥有越来越多的不变量，对应越来越小的群。</p>
</div>

<h3 id="sec3-2-iso">同构 vs 自同构 (Isomorphism vs Automorphism)</h3>

<div class="box def">
  <div class="box-title">📘 定义 3.10：同构与自同构</div>
  <p><strong>自同构 (Automorphism)</strong>：从对象到<strong>自身</strong>的保结构双射。描述对象的<strong>自对称性</strong>。</p>
  <p><strong>同构 (Isomorphism)</strong>：从一个对象到<strong>另一个</strong>对象的保结构双射。描述两个对象的<strong>等价性</strong>。</p>
</div>

<div class="box example">
  <div class="box-title">💡 集合层面的例子</div>
  <p>设 $\Omega = \{0, 1, 2\}$。</p>
  <ul>
    <li><strong>自同构</strong>：循环位移 $\tau(u) = (u+1) \bmod 3$，将 $\Omega$ 映回自身</li>
    <li><strong>同构</strong>：设 $\Omega' = \{a, b, c\}$，则 $\eta(0)=a, \eta(1)=b, \eta(2)=c$ 是一个集合同构</li>
  </ul>
</div>

<h3 id="sec3-2-graph">图同构与图自同构 (Graph Isomorphism & Automorphism)</h3>

<p>对于图结构的数据，"结构"不仅包括节点数量，还包括<strong>连接方式（边）</strong>。</p>

<div class="box def">
  <div class="box-title">📘 定义 3.11：图同构 (Graph Isomorphism)</div>
  <p>两个图 $G = (V, E)$ 和 $G' = (V', E')$ 之间的<strong>同构</strong>是一个双射 $\eta : V \to V'$，满足：</p>
  $$(u, v) \in E \iff (\eta(u), \eta(v)) \in E'$$
  <p>换言之，同构保持<strong>邻接关系</strong>。两个同构的图结构完全相同，只是节点的"编号"不同。</p>
</div>

<div class="box def">
  <div class="box-title">📘 定义 3.12：图自同构 (Graph Automorphism)</div>
  <p>图 $G$ 的<strong>自同构</strong>是从 $G$ 到自身的同构 $\tau : V \to V$。
  如果存在非平凡的自同构（$\tau \neq \mathrm{id}$），则图具有<strong>对称性</strong>。
  所有自同构构成<strong>自同构群</strong> $\mathrm{Aut}(G)$。</p>
</div>

<div class="figure">
  <img src="../assets/ch3_p18_img1.png" alt="图同构示例">
  <div class="caption"><strong>图 3.4</strong>：两个看起来不同但实际上同构的图。节点的排列方式不同，但连接结构完全相同。
  图同构问题（判断两个图是否同构）是图论中的经典难题。</div>
</div>

<div class="box example">
  <div class="box-title">💡 图同构的直觉</div>
  <p>想象两张地铁路线图：一张是官方设计的"美化版"，另一张是按地理位置绘制的。
  虽然看起来完全不同，但如果站点之间的连接关系完全一样，它们就是<strong>同构的</strong>。</p>
  <p><strong>图同构问题 (GI)</strong>是计算复杂性理论中的著名问题：它"可能"不是 NP-完全的，但也没有已知的多项式时间算法（除了特殊图类）。</p>
</div>

<h3 id="sec3-2-wl">Weisfeiler-Leman (WL) 测试</h3>

<p>Weisfeiler-Leman 测试是判断两个图是否同构的经典算法。虽然它不能解决所有情况（存在反例），
但它与<strong>图神经网络的表达能力</strong>有着深刻的联系——这是近年来 GNN 理论的重大发现。</p>

<div class="box def">
  <div class="box-title">📘 1-WL (Weisfeiler-Leman) 颜色精化算法</div>
  <p>给定图 $G = (V, E)$：</p>
  <ol>
    <li><strong>初始化</strong>：给每个节点分配一个初始"颜色"（标签），通常所有节点初始颜色相同</li>
    <li><strong>精化</strong>：对每个节点 $v$，将其新颜色定义为：
      $$c^{(t+1)}(v) = \mathrm{HASH}\!\left(c^{(t)}(v),\; \{\!\{c^{(t)}(u) : u \in \mathcal{N}(v)\}\!\}\right)$$
    </li>
    <li><strong>终止</strong>：当颜色不再变化时停止</li>
    <li><strong>判断</strong>：如果两个图最终的颜色分布不同，则它们<strong>不同构</strong>。如果相同，<strong>可能同构</strong>（不确定）</li>
  </ol>
  <div class="symbol-table" style="margin-top: 12px;">
    <span class="sym">$c^{(t)}(v)$</span><span class="desc">第 $t$ 轮迭代时节点 $v$ 的颜色（标签）</span>
    <span class="sym">$\mathcal{N}(v)$</span><span class="desc">节点 $v$ 的邻居集合</span>
    <span class="sym">$\{\!\{\cdot\}\!\}$</span><span class="desc">多重集 (multiset)——允许重复元素的集合</span>
    <span class="sym">$\mathrm{HASH}$</span><span class="desc">将 (颜色, 邻居颜色多重集) 映射为新颜色的单射函数</span>
  </div>
</div>

<div class="box thm">
  <div class="box-title">📗 WL 测试与 GNN 的等价性 [Xu et al., 2019; Morris et al., 2019]</div>
  <p><strong>消息传递神经网络 (MPNN)</strong> 的表达能力<strong>至多等于</strong> 1-WL 测试：</p>
  <ul>
    <li>如果 1-WL 判断两个图不同构，则存在某个 MPNN 可以区分它们</li>
    <li>如果 1-WL <strong>无法</strong>区分两个图，则<strong>没有</strong> MPNN 能区分它们</li>
    <li><strong>GIN (Graph Isomorphism Network)</strong> 达到了这个理论上界</li>
  </ul>
  <p>这个结果深刻地揭示了标准 GNN 的局限性——它们无法区分某些非同构图（如正则图）。</p>
</div>

<div class="box warn">
  <div class="box-title">⚠️ WL 测试的局限</div>
  <p>1-WL 无法区分所有非同构图。经典反例：<strong>某些正则图</strong>（所有节点度数相同的图）。
  更高阶的 $k$-WL 测试更强大但计算代价更高。这推动了 <strong>高阶 GNN</strong>（如 $k$-GNN）的研究。</p>
</div>

<!-- 3.2 Code: WL Test -->
<h3 id="sec3-2-code">🐍 代码示例：WL 颜色精化测试</h3>

<div class="code-block">
  <div class="code-header">
    <span>Python — Weisfeiler-Leman 1-WL 测试实现</span>
    <button class="copy-btn" onclick="copyCode(this)">📋 复制</button>
  </div>
<pre><code><span class="kw">from</span> collections <span class="kw">import</span> Counter, defaultdict

<span class="kw">class</span> <span class="cls">WeisfeilerLeman</span>:
    <span class="str">"""
    1-WL (Weisfeiler-Leman) 颜色精化算法
    用于图同构测试: 如果 WL 说"不同构", 则确实不同构
                    如果 WL 说"可能同构", 则不确定
    """</span>

    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>):
        <span class="sf">self</span>.color_map = {}     <span class="cm"># (旧颜色, 邻居多重集) -> 新颜色</span>
        <span class="sf">self</span>.next_color = <span class="num">0</span>

    <span class="kw">def</span> <span class="fn">_get_color</span>(<span class="sf">self</span>, key):
        <span class="str">"""将 (当前颜色, 排序邻居颜色元组) 映射为唯一整数颜色"""</span>
        <span class="kw">if</span> key <span class="kw">not in</span> <span class="sf">self</span>.color_map:
            <span class="sf">self</span>.color_map[key] = <span class="sf">self</span>.next_color
            <span class="sf">self</span>.next_color += <span class="num">1</span>
        <span class="kw">return</span> <span class="sf">self</span>.color_map[key]

    <span class="kw">def</span> <span class="fn">run</span>(<span class="sf">self</span>, adj_list, max_iter=<span class="num">10</span>, initial_colors=<span class="kw">None</span>):
        <span class="str">"""
        运行 1-WL 颜色精化

        Parameters
        ----------
        adj_list : dict
            邻接表 {node: [neighbors]}
        max_iter : int
            最大迭代次数
        initial_colors : dict or None
            初始节点颜色, None 则所有节点同色

        Returns
        -------
        color_histogram : Counter
            最终颜色的直方图 (作为图的"指纹")
        history : list of dict
            每轮迭代的颜色分配历史
        """</span>
        nodes = <span class="bi">list</span>(adj_list.keys())

        <span class="cm"># Step 1: 初始化颜色</span>
        <span class="kw">if</span> initial_colors <span class="kw">is</span> <span class="kw">None</span>:
            colors = {v: <span class="num">0</span> <span class="kw">for</span> v <span class="kw">in</span> nodes}
        <span class="kw">else</span>:
            colors = dict(initial_colors)

        history = [dict(colors)]

        <span class="cm"># Step 2: 迭代精化</span>
        <span class="kw">for</span> iteration <span class="kw">in</span> <span class="bi">range</span>(max_iter):
            new_colors = {}
            <span class="kw">for</span> v <span class="kw">in</span> nodes:
                <span class="cm"># 收集邻居颜色的多重集 (排序后变为元组，用作 hash key)</span>
                neighbor_colors = <span class="bi">tuple</span>(<span class="bi">sorted</span>(
                    colors[u] <span class="kw">for</span> u <span class="kw">in</span> adj_list[v]
                ))
                <span class="cm"># 将 (自身颜色, 邻居颜色多重集) 映射为新颜色</span>
                key = (colors[v], neighbor_colors)
                new_colors[v] = <span class="sf">self</span>._get_color(key)

            <span class="cm"># 检查是否收敛 (颜色不再变化)</span>
            <span class="kw">if</span> <span class="bi">set</span>(new_colors.values()) == <span class="bi">set</span>(colors.values()):
                old_partition = defaultdict(<span class="bi">set</span>)
                new_partition = defaultdict(<span class="bi">set</span>)
                <span class="kw">for</span> v <span class="kw">in</span> nodes:
                    old_partition[colors[v]].add(v)
                    new_partition[new_colors[v]].add(v)
                <span class="kw">if</span> <span class="bi">len</span>(old_partition) == <span class="bi">len</span>(new_partition):
                    <span class="bi">print</span>(<span class="str">f"  收敛于第 {iteration} 轮"</span>)
                    <span class="kw">break</span>

            colors = new_colors
            history.append(dict(colors))

        <span class="kw">return</span> Counter(colors.values()), history

    <span class="kw">def</span> <span class="fn">are_possibly_isomorphic</span>(<span class="sf">self</span>, adj1, adj2,
                                 init1=<span class="kw">None</span>, init2=<span class="kw">None</span>):
        <span class="str">"""比较两个图的 WL 指纹"""</span>
        <span class="sf">self</span>.color_map = {}
        <span class="sf">self</span>.next_color = <span class="num">0</span>
        hist1, _ = <span class="sf">self</span>.run(adj1, initial_colors=init1)
        hist2, _ = <span class="sf">self</span>.run(adj2, initial_colors=init2)
        <span class="kw">return</span> hist1 == hist2

<span class="cm"># =====================================================</span>
<span class="cm"># 示例 1: 两个同构的图</span>
<span class="cm"># =====================================================</span>

<span class="cm">#  图 G1:  0 - 1 - 2     (路径图 P3)</span>
<span class="cm">#  图 G2:  a - b - c     (同构于 G1)</span>

G1 = {<span class="num">0</span>: [<span class="num">1</span>], <span class="num">1</span>: [<span class="num">0</span>, <span class="num">2</span>], <span class="num">2</span>: [<span class="num">1</span>]}
G2 = {<span class="str">'a'</span>: [<span class="str">'b'</span>], <span class="str">'b'</span>: [<span class="str">'a'</span>, <span class="str">'c'</span>], <span class="str">'c'</span>: [<span class="str">'b'</span>]}

wl = WeisfeilerLeman()
result = wl.are_possibly_isomorphic(G1, G2)
<span class="bi">print</span>(<span class="str">f"G1 ≅ G2 ? {result}"</span>)  <span class="cm"># True ✅</span>

<span class="cm"># =====================================================</span>
<span class="cm"># 示例 2: 两个不同构的图</span>
<span class="cm"># =====================================================</span>

<span class="cm"># G3: 三角形 (环)    G4: 路径 P3</span>
G3 = {<span class="num">0</span>: [<span class="num">1</span>, <span class="num">2</span>], <span class="num">1</span>: [<span class="num">0</span>, <span class="num">2</span>], <span class="num">2</span>: [<span class="num">0</span>, <span class="num">1</span>]}
G4 = {<span class="num">0</span>: [<span class="num">1</span>], <span class="num">1</span>: [<span class="num">0</span>, <span class="num">2</span>], <span class="num">2</span>: [<span class="num">1</span>]}

wl2 = WeisfeilerLeman()
result2 = wl2.are_possibly_isomorphic(G3, G4)
<span class="bi">print</span>(<span class="str">f"三角形 ≅ 路径P3 ? {result2}"</span>)  <span class="cm"># False ✅</span>

<span class="cm"># =====================================================</span>
<span class="cm"># 示例 3: WL 失败的经典反例 (两个非同构的 3-正则图)</span>
<span class="cm"># =====================================================</span>

<span class="cm"># Shrikhande graph vs 4x4 Rook's graph (均为强正则图)</span>
<span class="cm"># 简化示例: 两个 6 节点的非同构图, 1-WL 无法区分</span>
<span class="cm"># 六环 C6  vs  两个三角形 K3 + K3</span>

C6 = {<span class="num">0</span>: [<span class="num">1</span>,<span class="num">5</span>], <span class="num">1</span>: [<span class="num">0</span>,<span class="num">2</span>], <span class="num">2</span>: [<span class="num">1</span>,<span class="num">3</span>],
      <span class="num">3</span>: [<span class="num">2</span>,<span class="num">4</span>], <span class="num">4</span>: [<span class="num">3</span>,<span class="num">5</span>], <span class="num">5</span>: [<span class="num">4</span>,<span class="num">0</span>]}

K3_K3 = {<span class="num">0</span>: [<span class="num">1</span>,<span class="num">2</span>], <span class="num">1</span>: [<span class="num">0</span>,<span class="num">2</span>], <span class="num">2</span>: [<span class="num">0</span>,<span class="num">1</span>],
         <span class="num">3</span>: [<span class="num">4</span>,<span class="num">5</span>], <span class="num">4</span>: [<span class="num">3</span>,<span class="num">5</span>], <span class="num">5</span>: [<span class="num">3</span>,<span class="num">4</span>]}

wl3 = WeisfeilerLeman()
result3 = wl3.are_possibly_isomorphic(C6, K3_K3)
<span class="bi">print</span>(<span class="str">f"C6 ≅ K3+K3 ? WL says: {result3}"</span>)
<span class="cm"># True — 但它们不同构! (C6 连通, K3+K3 不连通)</span>
<span class="cm"># 这说明 1-WL 并非万能的 ⚠️</span>

<span class="bi">print</span>(<span class="str">"\n💡 注意: 1-WL 无法区分所有正则图!"</span>)
<span class="bi">print</span>(<span class="str">"   这也是标准 MPNN/GCN 的理论上界"</span>)</code></pre>
</div>

<div class="box robot">
  <div class="box-title">🤖 与 PhysRobot 的关联</div>
  <p>在 PhysRobot 中，我们的粒子图通常<strong>没有固有的节点顺序</strong>——粒子可以随意编号。
  这意味着我们的 GNN 必须是<strong>置换等变的</strong>。</p>
  <p>WL 测试告诉我们，标准消息传递 GNN 的表达能力是有限的。
  对于物理模拟来说，这通常不是问题，因为粒子都带有丰富的<strong>连续特征</strong>（位置、速度等），
  而 WL 的局限主要出现在<strong>纯结构（无特征）</strong>的图上。</p>
</div>

<div class="exercises" id="sec3-2-exercises">
  <h4>❓ 3.2 节练习题</h4>
  <ol>
    <li><strong>手算 WL</strong>：对路径图 $P_4$（4个节点的路径）手动执行 WL 精化，画出每一轮的颜色分配。</li>
    <li><strong>自同构群</strong>：完全图 $K_n$ 的自同构群是什么？大小是多少？</li>
    <li><strong>编程练习</strong>：修改上面的 WL 代码，使其支持<strong>节点特征</strong>（用初始颜色区分不同类型的节点），
    并验证加入特征后是否能区分之前的反例。</li>
    <li><strong>思考题</strong>：为什么说"GNN 的表达能力 ≤ 1-WL"？给出直觉解释。</li>
    <li><strong>进阶</strong>：查阅 2-WL 和 3-WL 的定义，描述它们与 1-WL 的区别。</li>
  </ol>
</div>


<!-- ======================================== -->
<!-- SECTION 3.3: DEFORMATION STABILITY -->
<!-- ======================================== -->

<!-- === ENRICHMENT: sec3-2 === -->
<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>🔍 深入理解：图同构与WL测试</h4>
    
    <div class="qa-pair">
      <p class="question">❓ 小白：为什么说"图同构问题很难"？计算机不能直接比较两个图的邻接矩阵吗？</p>
      <div class="answer">
        <p>💡 专家：问题在于<strong>节点编号是任意的</strong>！同一个图可以有 $n!$ 种不同的邻接矩阵表示。</p>
        <p><strong>例子</strong>：三角形图 $K_3$（三个节点全连接）</p>
        <p>节点编号 $\{0,1,2\}$：邻接矩阵 $A_1 = \begin{pmatrix}0&1&1\\1&0&1\\1&1&0\end{pmatrix}$</p>
        <p>节点编号 $\{2,0,1\}$：邻接矩阵 $A_2 = \begin{pmatrix}0&1&1\\1&0&1\\1&1&0\end{pmatrix}$ (碰巧相同)</p>
        <p>但节点编号 $\{1,0,2\}$：$A_3 = \begin{pmatrix}0&1&1\\1&0&1\\1&1&0\end{pmatrix}$ (还是相同！因为是完全对称)</p>
        <p><strong>一般情况</strong>：给定两个图 $G_1, G_2$，要判断是否同构，需要：</p>
        <ol>
          <li>尝试所有可能的节点对应关系（$n!$ 种）</li>
          <li>检查是否存在某种对应保持邻接关系</li>
        </ol>
        <p><strong>暴力算法</strong>：$O(n! \cdot n^2)$ — 对于 $n=20$ 就是 $10^{18}$ 次操作！</p>
        <p><strong>为什么人眼能一眼看出？</strong>人脑擅长"整体模式匹配"，而不是逐个检查。我们看的是形状/对称性，而非编号。</p>
        <p><strong>GI问题的复杂性地位</strong>：</p>
        <ul>
          <li>已知不是NP-完全（Babai 2016: 拟多项式算法）</li>
          <li>但仍没有多项式时间算法（对一般图）</li>
          <li>处于 P 和 NP-完全之间的"中间地带" — 非常罕见！</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：WL测试的"颜色精化"过程到底在做什么？为什么叫"给人起外号"？</p>
      <div class="answer">
        <p>💡 专家：WL测试的核心思想：<strong>通过迭代"聚合邻居信息"来给节点贴越来越精细的标签</strong>。</p>
        <p><strong>类比：给班级同学起外号</strong></p>
        <p><strong>第0轮</strong>（初始化）：所有人都叫"同学"（颜色相同）</p>
        <p><strong>第1轮</strong>：根据朋友圈起外号</p>
        <ul>
          <li>张三：有3个朋友 → 叫"三友"</li>
          <li>李四：有3个朋友 → 也叫"三友"</li>
          <li>王五：有5个朋友 → 叫"五友"</li>
        </ul>
        <p><strong>第2轮</strong>：根据朋友的外号更新</p>
        <ul>
          <li>张三的朋友：2个"三友"，1个"五友" → 新外号"三友-235"</li>
          <li>李四的朋友：3个"三友" → 新外号"三友-333"</li>
          <li>王五的朋友：5个"三友" → 新外号"五友-33333"</li>
        </ul>
        <p>现在张三和李四被区分开了！</p>
        <p><strong>第3轮</strong>：继续精化...</p>
        <p><strong>停止条件</strong>：外号不再变化（达到稳定状态）</p>
        <p><strong>WL测试的数学版</strong>：</p>
        <ul>
          <li>"外号" = 颜色 $c(v)$</li>
          <li>"朋友" = 邻居 $\mathcal{N}(v)$</li>
          <li>"更新规则" = $c^{(t+1)}(v) = \text{HASH}(c^{(t)}(v), \{\!\{c^{(t)}(u) : u \in \mathcal{N}(v)\}\!\})$</li>
        </ul>
        <p><strong>为什么用多重集 $\{\!\{\cdot\}\!\}$？</strong>因为邻居可能有相同颜色，但数量信息很重要！</p>
        <ul>
          <li>朋友圈A：{三友, 三友, 五友}</li>
          <li>朋友圈B：{三友, 五友, 五友}</li>
          <li>集合相同，但多重集不同 → 应该区分！</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：为什么说"MPNN的表达能力 = 1-WL"？我还是不太懂这个等价性。</p>
      <div class="answer">
        <p>💡 专家：关键洞察：<strong>消息传递的聚合过程 = WL颜色精化</strong>！</p>
        <p><strong>MPNN的一层</strong>：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
h_v^{(t+1)} = UPDATE( h_v^{(t)}, AGGREGATE({ h_u^{(t)} : u ∈ N(v) }) )</pre>
        <p><strong>WL的一轮</strong>：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
c^{(t+1)}(v) = HASH( c^{(t)}(v), {{ c^{(t)}(u) : u ∈ N(v) }} )</pre>
        <p><strong>对比</strong>：</p>
        <table style="width:100%; font-size:0.85em; margin-top:8px;">
          <tr style="background:var(--bg-secondary);">
            <th>MPNN</th><th>WL</th><th>含义</th>
          </tr>
          <tr>
            <td>$h_v^{(t)}$</td>
            <td>$c^{(t)}(v)$</td>
            <td>节点的"状态"</td>
          </tr>
          <tr>
            <td>AGGREGATE</td>
            <td>多重集 $\{\!\{\cdot\}\!\}$</td>
            <td>收集邻居信息</td>
          </tr>
          <tr>
            <td>UPDATE</td>
            <td>HASH</td>
            <td>生成新状态</td>
          </tr>
        </table>
        <p><strong>定理（Xu et al. 2019）</strong>：</p>
        <p><strong>上界</strong>：MPNN 不能区分 1-WL 无法区分的图</p>
        <ul>
          <li>原因：AGGREGATE是置换不变的（求和/取max）</li>
          <li>如果两个邻居集合的多重集相同 → 聚合结果相同</li>
          <li>→ MPNN的判别力 $\leq$ 1-WL</li>
        </ul>
        <p><strong>下界</strong>：存在MPNN（GIN）能区分所有1-WL能区分的图</p>
        <ul>
          <li>关键：用<strong>单射</strong>的AGGREGATE和UPDATE</li>
          <li>GIN: $h_v^{(t+1)} = \text{MLP}\big((1+\epsilon) h_v^{(t)} + \sum_{u \in \mathcal{N}(v)} h_u^{(t)}\big)$</li>
          <li>证明：MLP可以近似任意单射函数（通用逼近定理）</li>
        </ul>
        <p><strong>实际意义</strong>：</p>
        <ul>
          <li>❌ GCN（用平均聚合）：弱于1-WL</li>
          <li>✅ GIN（用求和+MLP）：等于1-WL</li>
          <li>✅ 更强架构（如高阶GNN）：超过1-WL</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：WL测试无法区分的"反例图"长什么样？为什么计算机看不出来它们不同构？</p>
      <div class="answer">
        <p>💡 专家：经典反例是<strong>正则图</strong>（所有节点度数相同）中的某些"巧妙对称"的构造。</p>
        <p><strong>最简单反例</strong>：两个不连通的图</p>
        <ul>
          <li>图A：六边形环 $C_6$（6个节点首尾相连）</li>
          <li>图B：两个三角形 $K_3 \cup K_3$（不连通）</li>
        </ul>
        <p><strong>为什么1-WL无法区分？</strong></p>
        <ol>
          <li><strong>初始</strong>：所有节点颜色相同</li>
          <li><strong>第1轮</strong>：
            <ul>
              <li>$C_6$：每个节点有2个邻居 → 颜色变为"度数2"</li>
              <li>$K_3 \cup K_3$：每个节点也有2个邻居 → 也是"度数2"</li>
            </ul>
          </li>
          <li><strong>第2轮</strong>：
            <ul>
              <li>$C_6$：邻居是{度数2, 度数2} → 新颜色"2-22"</li>
              <li>$K_3 \cup K_3$：邻居也是{度数2, 度数2} → 也是"2-22"</li>
            </ul>
          </li>
          <li><strong>第3+轮</strong>：颜色不再变化 → WL认为它们"可能同构"</li>
        </ol>
        <p>但它们明显不同构！（一个连通，一个不连通）</p>
        <p><strong>更强反例</strong>：Cai-Fürer-Immerman (CFI) 图</p>
        <ul>
          <li>构造了一类图，$k$-WL需要 $k > n/2$ 才能区分</li>
          <li>即对固定的 $k$，总存在非同构图无法被$k$-WL区分</li>
        </ul>
        <p><strong>直觉</strong>：WL只看"局部邻域结构"，对"全局拓扑"（如连通性）盲目</p>
        <ul>
          <li>两个图如果在每个节点的 $k$-hop邻域都"看起来一样"</li>
          <li>那么 $k$-WL就无法区分它们</li>
          <li>即使全局结构完全不同！</li>
        </ul>
        <p><strong>如何超越WL限制？</strong></p>
        <ol>
          <li><strong>加节点特征</strong>：如果节点有丰富的初始特征，WL会更强</li>
          <li><strong>加全局特征</strong>：如连通分量数、直径（破坏置换等变性）</li>
          <li><strong>高阶GNN</strong>：2-WL, 3-WL（计算代价指数增长）</li>
          <li><strong>子图GNN</strong>：枚举小子图作为额外信息</li>
        </ol>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：图自同构群的大小说明什么？为什么有些图的自同构群很大？</p>
      <div class="answer">
        <p>💡 专家：<strong>自同构群的大小 = 图的对称性程度</strong>。群越大，图越对称。</p>
        <p><strong>例子1：完全图 $K_n$</strong></p>
        <ul>
          <li>所有节点地位完全相同（全连接）</li>
          <li>任意置换节点都不改变图结构</li>
          <li>$|\text{Aut}(K_n)| = n!$（最大可能！）</li>
        </ul>
        <p><strong>例子2：路径图 $P_n$</strong></p>
        <ul>
          <li>线性排列：$1-2-3-\cdots-n$</li>
          <li>只能头尾翻转（反射对称）</li>
          <li>$|\text{Aut}(P_n)| = 2$（很小）</li>
        </ul>
        <p><strong>例子3：环图 $C_n$</strong></p>
        <ul>
          <li>首尾相连的环</li>
          <li>可以旋转（$n$种）+ 翻转（$n$种）</li>
          <li>$|\text{Aut}(C_n)| = 2n$（二面体群 $D_n$）</li>
        </ul>
        <p><strong>例子4：不对称树</strong></p>
        <ul>
          <li>随机树（每个节点孩子数不同）</li>
          <li>几乎没有对称性</li>
          <li>$|\text{Aut}(\text{tree})| = 1$（只有恒等映射）</li>
        </ul>
        <p><strong>与GNN的关联</strong>：</p>
        <ul>
          <li>自同构群大 → 节点"不可区分性"强</li>
          <li>MPNN必须对自同构等变 → 无法区分对称节点</li>
          <li>如果任务需要<strong>破坏对称性</strong>（如节点分类），需要：
            <ul>
              <li>引入位置编码（Transformer做法）</li>
              <li>或用高阶GNN</li>
            </ul>
          </li>
        </ul>
        <p><strong>数值例子</strong>（分子图）：</p>
        <ul>
          <li>苯环 $C_6H_6$：高对称性，$|\text{Aut}| = 12$</li>
          <li>复杂蛋白质：几乎无对称，$|\text{Aut}| \approx 1$</li>
          <li>MPNN在苯环上所有碳原子学到相同表示 → 合理（它们确实等价）</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>🎯 直觉理解：WL = 节点"身份证"的迭代更新</h4>
    <p><strong>想象一个社交网络游戏</strong>：</p>
    <p><strong>规则</strong>：每个人最初有相同的"身份证"，每一轮根据朋友的身份证更新自己的</p>
    <p><strong>第0天</strong>：所有人的身份证都是"普通人"</p>
    <p><strong>第1天</strong>：根据朋友数更新</p>
    <ul>
      <li>3个朋友 → "3-朋友型"</li>
      <li>5个朋友 → "5-朋友型"</li>
    </ul>
    <p><strong>第2天</strong>：根据朋友的类型更新</p>
    <ul>
      <li>朋友是{3-朋友, 3-朋友, 5-朋友} → "3型朋友圈{3,3,5}"</li>
      <li>朋友是{5-朋友, 5-朋友, 5-朋友} → "3型朋友圈{5,5,5}"</li>
    </ul>
    <p><strong>最终</strong>：如果两个人的身份证演化序列完全一样 → WL认为它们"不可区分"</p>
    <p><strong>局限</strong>：这个过程只看"朋友的朋友..."，看不到全局（如你在地球的哪个大洲）</p>
  </div>

  <div class="enrichment-application">
    <h4>🏥 医疗机器人应用：蛋白质折叠中的图自同构</h4>
    <p><strong>背景</strong>：蛋白质 = 氨基酸残基链 + 3D折叠结构</p>
    <p><strong>表示为图</strong>：</p>
    <ul>
      <li>节点 = 氨基酸残基</li>
      <li>边 = 空间接触（距离 < 阈值）或共价键</li>
    </ul>
    <p><strong>问题</strong>：两个蛋白质在不同构象下，图同构吗？</p>
    <p><strong>案例：对称蛋白质复合物</strong></p>
    <ul>
      <li>血红蛋白：4条链（2α + 2β），高度对称</li>
      <li>自同构群：链的置换 + 旋转对称</li>
      <li>$|\text{Aut}| = 4$（四重对称）</li>
    </ul>
    <p><strong>GNN的挑战</strong>：</p>
    <ol>
      <li><strong>对称残基无法区分</strong>：
        <ul>
          <li>标准MPNN会给对称位置的残基相同表示</li>
          <li>如果任务是"预测哪条链会先降解" → 需要破坏对称性！</li>
        </ul>
      </li>
      <li><strong>解决方案</strong>：
        <ul>
          <li>方案A：加入链ID作为节点特征（破坏置换等变）</li>
          <li>方案B：用SE(3)-等变GNN + 位置编码</li>
          <li>方案C：预测整体性质（如稳定性）→ 保持对称性 ✅</li>
        </ul>
      </li>
    </ol>
    <p><strong>PhysRobot中的应用</strong>：</p>
    <p><strong>场景</strong>：模拟两个相同粒子团的碰撞</p>
    <ul>
      <li>初始状态：两个完全相同的软体（如两个水滴）</li>
      <li>图自同构：左右对称，$|\text{Aut}| = 2$</li>
      <li><strong>GNN行为</strong>：左右粒子学到相同特征（合理！）</li>
      <li><strong>但碰撞后</strong>：对称性破缺，GNN自动区分</li>
    </ul>
    <p><strong>关键洞察</strong>：</p>
    <ul>
      <li>物理对称 ≈ 图自同构</li>
      <li>MPNN天然"尊重"这些对称性</li>
      <li>只有当对称性被物理打破时，表示才会分化</li>
    </ul>
    <p><strong>实际性能</strong>（WaterDrop数据集）：</p>
    <ul>
      <li>标准GNN：rollout误差随时间线性增长 ✅</li>
      <li>破坏置换等变的网络：误差指数增长 ❌</li>
      <li>→ <strong>保持对称性 = 更好泛化</strong>！</li>
    </ul>
  </div>
</div>
<!-- === END ENRICHMENT: sec3-2 === -->

<h2 id="sec3-3">3.3 形变稳定性<br><span style="font-size:0.55em; color:var(--text-secondary)">Deformation Stability</span></h2>

<p>3.1–3.2 节建立了一个<strong>理想化的</strong>对称性框架：我们精确地知道哪些变换是对称性，并要求精确地尊重它们。
但现实世界是<strong>嘈杂的</strong>，理想模型在两个方面不够：</p>

<ol>
  <li><strong>局部对称性 vs 全局对称性</strong>：视频中多个物体各自沿不同方向运动——没有全局平移能解释整个变换</li>
  <li><strong>难以描述的变换群</strong>：可变形 3D 物体在相机前的形变很难用简单群描述</li>
</ol>

<div class="box example">
  <div class="box-title">💡 直觉：从精确对称到近似不变</div>
  <p>想象你在识别手写数字"7"。严格来说，每个人写的"7"都不完全一样——
  有些人写得略微倾斜，有些人笔画有弧度。这些都是<strong>小形变</strong>，它们不构成一个群（两个小形变的复合可能是大形变），
  但我们仍然希望识别系统对它们<strong>保持稳定</strong>。</p>
</div>

<h3 id="sec3-3-signal">信号形变的稳定性 (Stability to Signal Deformations)</h3>

<p>关键思路：用一个<strong>复杂度度量 (complexity measure)</strong> $c(\tau)$ 来量化变换 $\tau$ 离对称群 $\mathfrak{G}$ 有多远。</p>

<div class="box def">
  <div class="box-title">📘 定义 3.13：几何稳定性 (Geometric Stability)</div>
  <p>设 $\tau \in \mathrm{Diff}(\Omega)$ 是一个微分同胚，$\mathfrak{G} \subset \mathrm{Diff}(\Omega)$ 是对称子群，
  $c(\tau)$ 是一个<strong>复杂度度量</strong>满足 $c(\tau) = 0 \iff \tau \in \mathfrak{G}$。
  函数 $f \in \mathcal{F}(\mathcal{X}(\Omega))$ 是<strong>几何稳定的</strong>，如果：</p>
  $$\boxed{\|f(\rho(\tau)x) - f(x)\| \leq C \cdot c(\tau) \cdot \|x\|} \quad \forall\, x \in \mathcal{X}(\Omega) \tag{7}$$
  <div class="symbol-table" style="margin-top: 12px;">
    <span class="sym">$\tau$</span><span class="desc">域上的微分同胚（可以看作"变形"或"扭曲"）</span>
    <span class="sym">$\rho(\tau)x(u) = x(\tau^{-1}u)$</span><span class="desc">变形作用于信号</span>
    <span class="sym">$c(\tau)$</span><span class="desc">变形复杂度：衡量 $\tau$ 偏离对称群 $\mathfrak{G}$ 的程度</span>
    <span class="sym">$C$</span><span class="desc">与信号 $x$ 无关的常数</span>
    <span class="sym">$\|x\|$</span><span class="desc">信号的范数</span>
  </div>
</div>

<div class="box thm">
  <div class="box-title">📗 几何稳定性推广了不变性</div>
  <p>当 $\tau \in \mathfrak{G}$ 时，$c(\tau) = 0$，不等式右边为零，所以：</p>
  $$\|f(\rho(\tau)x) - f(x)\| \leq 0 \implies f(\rho(\tau)x) = f(x)$$
  <p>这正是 $\mathfrak{G}$-不变性！几何稳定性是不变性的<strong>连续推广</strong>："越接近对称群，输出变化越小"。</p>
</div>

<h4>变形复杂度的典型选择</h4>

<p>对于定义在连续欧几里得平面上的图像，一个常见的选择是<strong>Dirichlet 能量</strong>：</p>

$$c^2(\tau) := \int_\Omega \|\nabla \tau(u)\|^2 \, du \tag{8}$$

<p>这个度量衡量了 $\tau$ 的"弹性"——它距离常向量场（即平移）有多远。
当 $\tau$ 是一个纯平移时，$\nabla \tau$ 是常数（其变形量为零）。</p>

<div class="figure">
  <img src="../assets/ch3_p25_img0.png" alt="几何稳定性示意图">
  <div class="caption"><strong>图 3.5</strong>：$\mathrm{Aut}(\Omega)$ 中所有双射映射的集合，对称群 $\mathfrak{G}$（圆圈）是其子群。
  几何稳定性将 $\mathfrak{G}$-不变性扩展到"$\mathfrak{G}$ 附近的变换"（灰色环），用某种度量来量化离 $\mathfrak{G}$ 的距离。
  在此例中，图像的光滑扭曲接近于一个平移。</div>
</div>

<h3 id="sec3-3-domain">域形变的稳定性 (Stability to Domain Deformations)</h3>

<p>在很多应用中，被变形的不是信号，而是<strong>域 $\Omega$ 本身</strong>。
典型例子：随时间变化的社交网络（图结构在变化），或经历非刚性变形的 3D 物体。</p>

<div class="box def">
  <div class="box-title">📘 定义 3.14：域形变稳定性</div>
  <p>设 $\mathcal{D}$ 是所有可能域的空间，$d_\mathcal{D}(\Omega, \tilde\Omega)$ 是域之间的距离。
  函数 $f : \mathcal{X}(\mathcal{D}) \to \mathcal{Y}$ 对域形变是<strong>稳定的</strong>，如果：</p>
  $$\boxed{\|f(x, \Omega) - f(\tilde{x}, \tilde\Omega)\| \leq C \|x\| \cdot d_\mathcal{D}(\Omega, \tilde\Omega)} \tag{9}$$
  <div class="symbol-table" style="margin-top: 8px;">
    <span class="sym">$d_\mathcal{D}(\Omega, \tilde\Omega)$</span><span class="desc">域之间的距离（如图编辑距离、Gromov-Hausdorff 距离）</span>
    <span class="sym">$\tilde{x} = x \circ \eta^{-1}$</span><span class="desc">通过对齐映射 $\eta : \Omega \to \tilde\Omega$ 得到的变形信号</span>
  </div>
</div>

<p>域之间的距离通常通过寻找最优"对齐"来定义：</p>

$$d_\mathcal{D}(\Omega, \tilde\Omega) = \inf_{\eta \in \mathfrak{G}} \|d - \tilde{d} \circ (\eta \times \eta)\| \tag{10}$$

<p>这里 $\mathfrak{G}$ 是同构群（如双射或等距映射），$d, \tilde{d}$ 分别是两个域上的距离函数。
直觉上：两个域的距离 = 将它们"最佳对齐"后剩余的结构差异。</p>

<h3 id="sec3-3-math">📐 数学推导：Lipschitz 稳定性界限</h3>

<div class="box thm">
  <div class="box-title">📗 Lipschitz 连续性与稳定性</div>
  <p>几何稳定性本质上是一个<strong>Lipschitz 条件</strong>。回忆 Lipschitz 连续性的定义：</p>
  $$\|f(x_1) - f(x_2)\| \leq L \|x_1 - x_2\|$$
  <p>其中 $L$ 是 Lipschitz 常数。几何稳定性 (公式 7) 可以改写为：</p>
  $$\frac{\|f(\rho(\tau)x) - f(x)\|}{\|x\|} \leq C \cdot c(\tau)$$
  <p>这说明输出的<strong>相对变化</strong>被变形复杂度<strong>线性控制</strong>。</p>
</div>

<div class="box thm">
  <div class="box-title">📗 推导：为什么 Dirichlet 能量是好的复杂度度量</div>
  <p>考虑 $\Omega = \mathbb{R}^d$，$\tau(u) = u - \tilde\tau(u)$，其中 $\tilde\tau$ 是一个小位移场。</p>
  <p><strong>Step 1</strong>：当 $\tilde\tau(u) = v$（常数）时，$\tau$ 是一个纯平移 $\tau \in T(\mathbb{R}^d)$。此时 $\nabla\tilde\tau = 0$。</p>
  <p><strong>Step 2</strong>：$\nabla\tilde\tau$ 衡量了 $\tau$ 偏离平移的程度。Dirichlet 能量 $\int \|\nabla\tilde\tau\|^2 du$ 就是这个偏离的 $L^2$ 范数。</p>
  <p><strong>Step 3</strong>：对于信号 $x$ 和变形 $\tau$，一阶泰勒展开给出：</p>
  $$\rho(\tau)x(u) = x(\tau^{-1}u) \approx x(u) + \nabla x(u) \cdot \tilde\tau(u)$$
  <p><strong>Step 4</strong>：因此：</p>
  $$\|\rho(\tau)x - x\|^2 \approx \int_\Omega |\nabla x(u) \cdot \tilde\tau(u)|^2 du \leq \|\nabla x\|^2_\infty \cdot \int_\Omega \|\tilde\tau(u)\|^2 du$$
  <p>这给出了信号变化的<strong>上界</strong>，取决于信号的梯度和变形的大小。如果我们进一步用 Poincaré 不等式将 $\|\tilde\tau\|$ 关联到 $\|\nabla\tilde\tau\|$，就得到了用 Dirichlet 能量控制的稳定性界限。</p>
</div>

<h3 id="sec3-3-code">🐍 代码示例：Lipschitz 连续性验证</h3>

<div class="code-block">
  <div class="code-header">
    <span>Python / PyTorch — Lipschitz 常数估计与稳定性</span>
    <button class="copy-btn" onclick="copyCode(this)">📋 复制</button>
  </div>
<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># =====================================================</span>
<span class="cm"># 实验: 估计网络的 Lipschitz 常数</span>
<span class="cm"># =====================================================</span>

<span class="kw">def</span> <span class="fn">estimate_lipschitz</span>(model, input_shape, n_samples=<span class="num">1000</span>, device=<span class="str">'cpu'</span>):
    <span class="str">"""
    通过随机采样估计模型的 Lipschitz 常数
    L ≈ max ||f(x1) - f(x2)|| / ||x1 - x2||
    """</span>
    model.eval()
    max_ratio = <span class="num">0.0</span>

    <span class="kw">with</span> torch.no_grad():
        <span class="kw">for</span> _ <span class="kw">in</span> <span class="bi">range</span>(n_samples):
            x1 = torch.randn(*input_shape, device=device)
            <span class="cm"># 生成 x1 附近的微小扰动</span>
            delta = torch.randn_like(x1) * <span class="num">0.01</span>
            x2 = x1 + delta

            y1 = model(x1)
            y2 = model(x2)

            output_diff = (y1 - y2).flatten().norm()
            input_diff = delta.flatten().norm()

            <span class="kw">if</span> input_diff > <span class="num">1e-10</span>:
                ratio = (output_diff / input_diff).item()
                max_ratio = <span class="bi">max</span>(max_ratio, ratio)

    <span class="kw">return</span> max_ratio

<span class="cm"># 创建两个网络: 有/无 Spectral Normalization</span>
<span class="kw">class</span> <span class="cls">SimpleNet</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, use_spectral_norm=<span class="kw">False</span>):
        <span class="bi">super</span>().__init__()
        layers = [
            nn.Linear(<span class="num">64</span>, <span class="num">128</span>),
            nn.ReLU(),
            nn.Linear(<span class="num">128</span>, <span class="num">128</span>),
            nn.ReLU(),
            nn.Linear(<span class="num">128</span>, <span class="num">10</span>),
        ]
        <span class="kw">if</span> use_spectral_norm:
            <span class="cm"># Spectral Normalization 限制每层的 Lipschitz 常数 ≤ 1</span>
            layers[<span class="num">0</span>] = nn.utils.spectral_norm(layers[<span class="num">0</span>])
            layers[<span class="num">2</span>] = nn.utils.spectral_norm(layers[<span class="num">2</span>])
            layers[<span class="num">4</span>] = nn.utils.spectral_norm(layers[<span class="num">4</span>])
        <span class="sf">self</span>.net = nn.Sequential(*layers)

    <span class="kw">def</span> <span class="fn">forward</span>(<span class="sf">self</span>, x):
        <span class="kw">return</span> <span class="sf">self</span>.net(x)

torch.manual_seed(<span class="num">42</span>)
net_normal = SimpleNet(use_spectral_norm=<span class="kw">False</span>)
net_stable = SimpleNet(use_spectral_norm=<span class="kw">True</span>)

L_normal = estimate_lipschitz(net_normal, (<span class="num">1</span>, <span class="num">64</span>))
L_stable = estimate_lipschitz(net_stable, (<span class="num">1</span>, <span class="num">64</span>))

<span class="bi">print</span>(<span class="str">f"普通网络的 Lipschitz 常数 ≈ {L_normal:.2f}"</span>)
<span class="bi">print</span>(<span class="str">f"谱归一化网络的 Lipschitz 常数 ≈ {L_stable:.2f}"</span>)
<span class="cm"># 普通网络: ~5-15 (较大, 不稳定)</span>
<span class="cm"># 谱归一化: ~1.0 (被控制, 稳定)</span>

<span class="cm"># =====================================================</span>
<span class="cm"># 验证: 小扰动 → 小输出变化</span>
<span class="cm"># =====================================================</span>

x = torch.randn(<span class="num">1</span>, <span class="num">64</span>)
epsilons = [<span class="num">0.001</span>, <span class="num">0.01</span>, <span class="num">0.1</span>, <span class="num">1.0</span>]

<span class="bi">print</span>(<span class="str">"\n扰动大小 → 输出变化（应接近线性关系）:"</span>)
<span class="kw">for</span> eps <span class="kw">in</span> epsilons:
    delta = torch.randn_like(x) * eps
    <span class="kw">with</span> torch.no_grad():
        dy_normal = (net_normal(x + delta) - net_normal(x)).norm().item()
        dy_stable = (net_stable(x + delta) - net_stable(x)).norm().item()
    <span class="bi">print</span>(<span class="str">f"  ε={eps:.3f}: 普通={dy_normal:.4f}, 谱归一化={dy_stable:.4f}"</span>)</code></pre>
</div>

<div class="box robot">
  <div class="box-title">🤖 与 PhysRobot 的关联</div>
  <p>形变稳定性在物理模拟中至关重要：</p>
  <ul>
    <li><strong>噪声鲁棒性</strong>：真实传感器数据（如激光雷达点云）总有噪声——粒子位置的微小扰动不应导致模拟结果的剧烈变化</li>
    <li><strong>GNS 中的 noise injection</strong>：训练时向输入添加噪声（$\mathbf{x} \leftarrow \mathbf{x} + \epsilon$）正是为了<strong>隐式地</strong>提高模型的形变稳定性</li>
    <li><strong>Lipschitz 约束</strong>：在某些安全关键的机器人控制任务中，我们可能需要显式地限制网络的 Lipschitz 常数</li>
  </ul>
</div>

<div class="exercises" id="sec3-3-exercises">
  <h4>❓ 3.3 节练习题</h4>
  <ol>
    <li><strong>数学证明</strong>：证明 ReLU 函数 $\sigma(x) = \max(0, x)$ 是 1-Lipschitz 的。</li>
    <li><strong>Lipschitz 组合</strong>：如果 $f$ 是 $L_1$-Lipschitz，$g$ 是 $L_2$-Lipschitz，证明 $f \circ g$ 是 $L_1 L_2$-Lipschitz。
    这对深度网络的 Lipschitz 常数意味着什么？</li>
    <li><strong>编程练习</strong>：实现一个函数，通过计算所有层权重矩阵的谱范数（最大奇异值）来<strong>上界估计</strong>网络的 Lipschitz 常数，
    并与随机采样方法对比。</li>
    <li><strong>思考题</strong>：BatchNorm 层是否是 Lipschitz 连续的？为什么在某些应用中人们偏好 GroupNorm？</li>
  </ol>
</div>


<!-- ======================================== -->
<!-- SECTION 3.4: SCALE SEPARATION -->
<!-- ======================================== -->

<!-- === ENRICHMENT: sec3-3 === -->
<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>🔍 深入理解：形变稳定性与Lipschitz条件</h4>
    
    <div class="qa-pair">
      <p class="question">❓ 小白：为什么需要"形变稳定性"？3.1节的精确对称性不够吗？</p>
      <div class="answer">
        <p>💡 专家：精确对称性太理想化了！现实世界充满了<strong>近似对称性</strong>和<strong>小扰动</strong>。</p>
        <p><strong>问题1：局部vs全局</strong></p>
        <ul>
          <li>视频中：10个物体各自平移 → 没有<strong>全局</strong>平移对称</li>
          <li>但每个物体<strong>局部</strong>满足平移对称 → 如何建模？</li>
        </ul>
        <p><strong>问题2：难以描述的变换</strong></p>
        <ul>
          <li>手写数字：每个人写的"7"都略有不同（笔画粗细、倾斜角度）</li>
          <li>这些变形不构成一个群（组合两个小变形可能变成大变形）</li>
          <li>但我们仍希望识别系统对它们鲁棒！</li>
        </ul>
        <p><strong>解决方案：形变稳定性</strong></p>
        <p>不要求 $f(\tau x) = f(x)$（精确不变），而是要求：</p>
        <p style="text-align:center; background:var(--bg-code); padding:8px; border-radius:6px;">
          $\|f(\tau x) - f(x)\| \leq C \cdot c(\tau) \cdot \|x\|$
        </p>
        <ul>
          <li>$c(\tau)$：变形"复杂度"（偏离对称群的程度）</li>
          <li>含义：<strong>小变形 → 小输出变化</strong>（Lipschitz连续性）</li>
        </ul>
        <p><strong>类比：高速公路限速</strong></p>
        <ul>
          <li>精确不变性 = "不许超速"（速度 $\leq 120$）</li>
          <li>形变稳定性 = "超速越多，罚款越多"（罚款 $\propto$ 超速程度）</li>
          <li>Lipschitz常数 $C$ = 罚款系数</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：Lipschitz条件 $\|f(x_1) - f(x_2)\| \leq L\|x_1 - x_2\|$ 到底在说什么？</p>
      <div class="answer">
        <p>💡 专家：<strong>Lipschitz连续 = 函数的"变化速度有上界"</strong>。</p>
        <p><strong>几何直觉</strong>：</p>
        <ul>
          <li>想象函数 $f$ 的图像</li>
          <li>Lipschitz常数 $L$ = 曲线的最大斜率</li>
          <li>$L=1$：曲线最陡不超过45°</li>
          <li>$L=10$：曲线可以很陡，但斜率 $\leq 10$</li>
        </ul>
        <p><strong>为什么重要？</strong></p>
        <ol>
          <li><strong>鲁棒性</strong>：输入小扰动 → 输出小变化（抗噪声）</li>
          <li><strong>泛化</strong>：函数不能过于"尖锐"→ 更平滑 → 更好泛化</li>
          <li><strong>稳定性</strong>：在动力系统中，Lipschitz保证数值积分稳定</li>
        </ol>
        <p><strong>例子1：ReLU是1-Lipschitz</strong></p>
        <ul>
          <li>$\sigma(x) = \max(0, x)$</li>
          <li>证明：$|\sigma(x_1) - \sigma(x_2)| \leq |x_1 - x_2|$（分情况讨论）</li>
          <li>最大斜率 = 1（在正半轴）</li>
        </ul>
        <p><strong>例子2：Softmax不是Lipschitz！</strong></p>
        <ul>
          <li>$\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$</li>
          <li>问题：$e^x$ 增长太快，梯度可以任意大</li>
          <li>→ 对输入扰动非常敏感（这也是为什么温度参数很重要）</li>
        </ul>
        <p><strong>深度网络的Lipschitz常数</strong>：</p>
        <ul>
          <li>如果每层 $f_i$ 是 $L_i$-Lipschitz</li>
          <li>那么复合 $f = f_n \circ \cdots \circ f_1$ 是 $L = \prod_{i=1}^n L_i$-Lipschitz</li>
          <li>层数越多，$L$ 可能<strong>指数增长</strong>！→ 梯度爆炸</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：Dirichlet能量 $\int \|\nabla \tau\|^2 du$ 为什么能衡量"变形复杂度"？</p>
      <div class="answer">
        <p>💡 专家：$\nabla \tau$ 衡量"变形的拉伸和扭曲程度"。</p>
        <p><strong>直觉</strong>：</p>
        <ul>
          <li>$\tau(u)$：把点 $u$ 移动到新位置 $\tau(u)$</li>
          <li>$\nabla \tau(u)$：局部"拉伸矩阵"（Jacobian）</li>
          <li>$\|\nabla \tau\|$：衡量局部变形的剧烈程度</li>
        </ul>
        <p><strong>例子：橡皮筋拉伸</strong></p>
        <ul>
          <li><strong>均匀平移</strong>：$\tau(u) = u + c$（常数）
            <ul>
              <li>$\nabla \tau = I$（单位矩阵）</li>
              <li>$\|\nabla \tau\| = 0$ → Dirichlet能量 = 0 ✅</li>
            </ul>
          </li>
          <li><strong>均匀拉伸</strong>：$\tau(u) = 2u$
            <ul>
              <li>$\nabla \tau = 2I$</li>
              <li>$\|\nabla \tau - I\| = 1$ → 有弹性能量</li>
            </ul>
          </li>
          <li><strong>非均匀扭曲</strong>：$\tau(u) = u + \sin(u)$
            <ul>
              <li>$\nabla \tau = I + \cos(u)$（空间变化）</li>
              <li>$\int \|\cos(u)\|^2 du > 0$ → 高能量（复杂变形）</li>
            </ul>
          </li>
        </ul>
        <p><strong>物理类比：弹性势能</strong></p>
        <ul>
          <li>Dirichlet能量 ≈ 橡皮膜的弹性势能</li>
          <li>平移（刚体运动）：零能量</li>
          <li>拉伸/扭曲：能量 $\propto$ 变形程度</li>
        </ul>
        <p><strong>为什么是L²范数？</strong></p>
        <ul>
          <li>$L^2$ 对应"能量"概念（物理中常用）</li>
          <li>数学性质好（可微、凸优化）</li>
          <li>也可用其他范数（如 $L^1$），但 $L^2$ 最常见</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：为什么"谱归一化"（Spectral Normalization）能控制Lipschitz常数？</p>
      <div class="answer">
        <p>💡 专家：因为<strong>线性映射的Lipschitz常数 = 最大奇异值（谱范数）</strong>！</p>
        <p><strong>定理</strong>：对于线性层 $f(x) = Wx$，Lipschitz常数 $L = \|W\|_2$（谱范数）</p>
        <p><strong>证明</strong>：</p>
        <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
‖f(x₁) - f(x₂)‖ = ‖W(x₁ - x₂)‖
                ≤ ‖W‖₂ · ‖x₁ - x₂‖  (矩阵范数的性质)
                = σₘₐₓ(W) · ‖x₁ - x₂‖</pre>
        <p>其中 $\sigma_{\max}(W)$ 是 $W$ 的最大奇异值（谱范数的定义）</p>
        <p><strong>谱归一化的做法</strong>：</p>
        <ol>
          <li>计算 $W$ 的最大奇异值 $\sigma_{\max}$（用幂迭代快速近似）</li>
          <li>归一化：$W' = \frac{W}{\sigma_{\max}}$</li>
          <li>现在 $\|W'\|_2 = 1$ → Lipschitz常数 = 1 ✅</li>
        </ol>
        <p><strong>为什么有用？</strong></p>
        <ul>
          <li><strong>稳定训练</strong>：防止梯度爆炸（GAN常用）</li>
          <li><strong>对抗鲁棒性</strong>：限制对抗样本的影响</li>
          <li><strong>泛化</strong>：平滑的函数 → 更好泛化（隐式正则化）</li>
        </ul>
        <p><strong>代价</strong>：</p>
        <ul>
          <li>每次前向传播需计算谱范数（约10-20%额外计算）</li>
          <li>可能降低表达能力（过度约束）</li>
        </ul>
        <p><strong>实际应用</strong>：</p>
        <ul>
          <li><strong>GAN判别器</strong>：谱归一化是SOTA标配（SNGAN）</li>
          <li><strong>强化学习</strong>：值函数网络（防止不稳定）</li>
          <li><strong>医疗诊断</strong>：需要可信度边界的系统</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：手写数字识别为什么需要"对小形变鲁棒"？具体场景是什么？</p>
      <div class="answer">
        <p>💡 专家：因为真实手写字体有<strong>无穷多种微小变化</strong>，不可能都收集训练！</p>
        <p><strong>变化来源</strong>：</p>
        <ol>
          <li><strong>书写风格</strong>：笔画粗细、连笔、倾斜角度</li>
          <li><strong>采集噪声</strong>：扫描仪抖动、光照变化</li>
          <li><strong>预处理误差</strong>：居中算法的小偏差</li>
        </ol>
        <p><strong>例子：数字"7"的变形</strong></p>
        <ul>
          <li>训练集：标准"7"（竖线+横线）</li>
          <li>测试集变化：
            <ul>
              <li>横线倾斜5° ✓（小变形，应识别为7）</li>
              <li>竖线略微弯曲 ✓</li>
              <li>笔画加粗10% ✓</li>
              <li>整体拉伸120% ✓</li>
            </ul>
          </li>
        </ul>
        <p><strong>不稳定模型的问题</strong>：</p>
        <ul>
          <li>训练集：98%准确率 ✅</li>
          <li>测试集（轻微变形）：60%准确率 ❌</li>
          <li>原因：过拟合到像素级细节，对形变极敏感</li>
        </ul>
        <p><strong>形变稳定模型的表现</strong>：</p>
        <ul>
          <li>训练：95%准确率（稍低）</li>
          <li>测试（变形）：92%准确率 ✅（泛化好！）</li>
          <li>原因：学到了形变不变的高层特征（如"L形结构"）</li>
        </ul>
        <p><strong>如何实现？</strong></p>
        <ol>
          <li><strong>数据增强</strong>：训练时随机变形（旋转、拉伸、弹性形变）
            <ul>
              <li>隐式增加形变稳定性</li>
              <li>MNIST常用：$\pm 15°$旋转、$\pm 10\%$缩放</li>
            </ul>
          </li>
          <li><strong>Lipschitz约束</strong>：谱归一化 / gradient penalty</li>
          <li><strong>架构选择</strong>：CNN天然对局部平移稳定（卷积核小）</li>
        </ol>
        <p><strong>数值实验</strong>（MNIST）：</p>
        <table style="width:100%; font-size:0.85em;">
          <tr style="background:var(--bg-secondary);">
            <th>方法</th><th>干净数据</th><th>+5%弹性变形</th><th>+10%变形</th>
          </tr>
          <tr>
            <td>普通MLP</td><td>97.8%</td><td>72.3%</td><td>45.1%</td>
          </tr>
          <tr>
            <td>CNN</td><td>99.2%</td><td>96.5%</td><td>91.2%</td>
          </tr>
          <tr>
            <td>CNN+数据增强</td><td>99.4%</td><td>98.7%</td><td>96.3%</td>
          </tr>
        </table>
        <p>→ 形变稳定性 = <strong>从实验室到真实世界的桥梁</strong>！</p>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>🎯 直觉理解：Lipschitz = 高速公路限速</h4>
    <p><strong>想象函数是一条道路</strong>：</p>
    <ul>
      <li>$x_1, x_2$：两个城市（输入点）</li>
      <li>$f(x_1), f(x_2)$：它们在"函数空间"的位置（输出）</li>
      <li><strong>Lipschitz常数 $L$</strong>：道路的最大坡度（限速）</li>
    </ul>
    <p><strong>三种道路</strong>：</p>
    <ol>
      <li><strong>$L=1$（慢速路）</strong>：
        <ul>
          <li>坡度 $\leq$ 45°，很平缓</li>
          <li>类比：ReLU激活</li>
          <li>好处：稳定、可控</li>
        </ul>
      </li>
      <li><strong>$L=100$（快速路）</strong>：
        <ul>
          <li>可以有很陡的坡</li>
          <li>类比：未归一化的全连接层</li>
          <li>风险：小扰动 → 大变化</li>
        </ul>
      </li>
      <li><strong>$L=\infty$（悬崖）</strong>：
        <ul>
          <li>不连续！可以垂直跳跃</li>
          <li>类比：阶跃函数 $\text{sign}(x)$</li>
          <li>问题：无法优化（梯度为0或∞）</li>
        </ul>
      </li>
    </ol>
    <p><strong>深度学习的目标</strong>：找到"坡度适中"的道路 — 既能拟合数据，又不过于陡峭。</p>
  </div>

  <div class="enrichment-application">
    <h4>🏥 医疗机器人应用：GNS中的噪声注入与稳定性</h4>
    <p><strong>场景</strong>：预测手术中软组织的变形（PhysRobot核心任务）</p>
    <p><strong>挑战</strong>：真实传感器数据有噪声</p>
    <ul>
      <li>力传感器：$\pm 0.1N$ 误差</li>
      <li>视觉定位：$\pm 0.5mm$ 位置误差</li>
      <li>模型参数（杨氏模量）：$\pm 10\%$ 不确定性</li>
    </ul>
    <p><strong>GNS论文的解决方案：训练时注入噪声</strong></p>
    <pre style="background:var(--bg-code); padding:8px; border-radius:6px; font-size:0.9em;">
# 每个训练步骤
position_noise = torch.randn_like(positions) * noise_std  # 0.003
velocity_noise = torch.randn_like(velocities) * noise_std

noisy_positions = positions + position_noise
noisy_velocities = velocities + velocity_noise

pred_accel = model(noisy_positions, noisy_velocities)</pre>
    <p><strong>为什么有效？</strong></p>
    <ol>
      <li><strong>隐式Lipschitz正则化</strong>：
        <ul>
          <li>模型被迫对 $\pm \epsilon$ 扰动给出接近的输出</li>
          <li>等价于最小化 $\mathbb{E}[\|f(x+\epsilon) - f(x)\|^2]$</li>
          <li>→ 降低Lipschitz常数！</li>
        </ul>
      </li>
      <li><strong>防止过拟合到噪声</strong>：
        <ul>
          <li>不加噪声：模型记住训练轨迹的每个细节</li>
          <li>加噪声：被迫学习底层物理规律（鲁棒特征）</li>
        </ul>
      </li>
    </ol>
    <p><strong>实验结果</strong>（WaterRamp数据集）：</p>
    <table style="width:100%; font-size:0.85em;">
      <tr style="background:var(--bg-secondary);">
        <th>训练噪声 $\sigma$</th><th>训练损失</th><th>测试损失（无噪声）</th><th>Rollout稳定性</th>
      </tr>
      <tr>
        <td>0（无噪声）</td><td>0.0012</td><td>0.0089</td><td>50步后发散</td>
      </tr>
      <tr>
        <td>0.003</td><td>0.0021</td><td>0.0034</td><td>200步稳定 ✅</td>
      </tr>
      <tr>
        <td>0.01（过强）</td><td>0.0045</td><td>0.0041</td><td>150步（略差）</td>
      </tr>
    </table>
    <p>→ 适度噪声（$\sigma=0.003$）：训练损失略高，但<strong>泛化和稳定性大幅提升</strong>！</p>
    <p><strong>域形变的稳定性</strong>：</p>
    <ul>
      <li><strong>问题</strong>：手术中组织拓扑可能变化（切开、缝合）</li>
      <li><strong>图编辑</strong>：添加/删除粒子和边
        <ul>
          <li>小编辑（删1条边）→ 预测应小变化</li>
          <li>GNN需要对图扰动稳定</li>
        </ul>
      </li>
      <li><strong>实现</strong>：
        <ul>
          <li>训练时随机dropout边（5-10%）</li>
          <li>等价于"域的小形变"</li>
          <li>提高对拓扑变化的鲁棒性 ✅</li>
        </ul>
      </li>
    </ul>
    <p><strong>关键指标：Rollout误差增长率</strong></p>
    <ul>
      <li>不稳定模型：$\epsilon(t) \sim e^{\lambda t}$ (指数发散)</li>
      <li>Lipschitz约束模型：$\epsilon(t) \sim \sqrt{t}$ (亚线性)</li>
      <li>→ 长时间模拟必须控制Lipschitz常数！</li>
    </ul>
  </div>
</div>
<!-- === END ENRICHMENT: sec3-3 === -->

<h2 id="sec3-4">3.4 尺度分离<br><span style="font-size:0.55em; color:var(--text-secondary)">Scale Separation</span></h2>

<p>虽然形变稳定性大大加强了全局对称先验，但<strong>仅靠它不足以克服维度灾难</strong>：
满足公式 (7) 的函数在域增大时仍然"太多"了。突破关键在于利用物理任务的<strong>多尺度结构</strong>。</p>

<div class="box example">
  <div class="box-title">💡 直觉：从近看到远看</div>
  <p>想象你在辨认一棵树的照片：</p>
  <ul>
    <li><strong>细尺度</strong>（近看）：你看到的是叶子的纹理、树皮的纹路</li>
    <li><strong>中尺度</strong>：你看到的是树枝的形状、叶子的分布</li>
    <li><strong>粗尺度</strong>（远看）：你看到的是树的整体轮廓</li>
  </ul>
  <p>"这是一棵松树"的判断主要依赖粗尺度和中尺度的信息。<strong>尺度分离</strong>就是将信息按尺度组织，
  先处理局部细节，再逐步整合到全局——这正是 CNN 池化层和 GNN 的图粗化在做的事！</p>
</div>

<h3 id="sec3-4-fourier">傅里叶变换 (Fourier Transform)</h3>

<p>在介绍多尺度表示之前，我们先回顾信号分解的经典工具——<strong>傅里叶变换</strong>。</p>

<div class="box def">
  <div class="box-title">📘 定义 3.15：傅里叶变换 (Fourier Transform)</div>
  <p>一维傅里叶变换将信号 $x(u) \in L^2(\mathbb{R})$ 分解为不同频率的振荡基函数 $\varphi_\xi(u) = e^{i\xi u}$：</p>
  $$\hat{x}(\xi) = \int_{-\infty}^{+\infty} x(u) e^{-i\xi u} du \tag{11}$$
  <div class="symbol-table" style="margin-top: 8px;">
    <span class="sym">$\hat{x}(\xi)$</span><span class="desc">信号 $x$ 的傅里叶变换（频率 $\xi$ 处的成分）</span>
    <span class="sym">$\xi$</span><span class="desc">频率（oscillation rate）</span>
    <span class="sym">$e^{-i\xi u}$</span><span class="desc">傅里叶基函数（复指数）</span>
    <span class="sym">$L^2(\mathbb{R})$</span><span class="desc">平方可积函数空间</span>
  </div>
</div>

<!-- === ENRICHMENT: sec3-4 === -->
<div class="enrichment-block">
  <div class="enrichment-qa">
    <h4>🔍 深入理解：尺度分离与多尺度分析</h4>
    
    <div class="qa-pair">
      <p class="question">❓ 小白：为什么"对称性+稳定性"还不够？必须要尺度分离吗？</p>
      <div class="answer">
        <p>💡 专家：因为<strong>维度灾难仍然存在</strong>！尺度分离是最后一块拼图。</p>
        <p><strong>回顾问题</strong>：</p>
        <ul>
          <li>第2章：学习$d$维函数需要 $N = O(\epsilon^{-d})$ 样本 → 维度诅咒</li>
          <li>第3.1节：对称性 → 参数共享 → 减少假设空间</li>
          <li>第3.3节：稳定性 → 函数平滑 → 泛化更好</li>
        </ul>
        <p><strong>但仍然不够！</strong></p>
        <p>考虑 $1000 \times 1000$ 图像（$d = 10^6$ 维）：</p>
        <ul>
          <li>即使有平移对称性（CNN），参数仍然很多</li>
          <li>即使Lipschitz连续，高维输入的样本复杂度仍是天文数字</li>
        </ul>
        <p><strong>关键洞察：物理世界的多尺度结构</strong></p>
        <ol>
          <li><strong>局部性</strong>：相邻像素强相关，远离像素弱相关</li>
          <li><strong>层次性</strong>：纹理 → 部件 → 物体 → 场景</li>
          <li><strong>稀疏性</strong>：大部分信息集中在少数"关键尺度"</li>
        </ol>
        <p><strong>尺度分离的策略</strong>：</p>
        <ul>
          <li>不要在原始分辨率学习 $10^6$ 维函数</li>
          <li>而是：局部提取 → 逐步聚合 → 多尺度融合</li>
          <li>有效维度：$d_{\text{eff}} \ll d_{\text{raw}}$</li>
        </ul>
        <p><strong>数值例子</strong>（ImageNet）：</p>
        <ul>
          <li>输入：$224 \times 224 \times 3 = 150K$ 维</li>
          <li>ResNet第1层：$56 \times 56 \times 64 = 200K$ 维（看似更大！）</li>
          <li>但！每个神经元只看 $7 \times 7$ 感受野 → 有效维度 $\sim 49$</li>
          <li>Layer4：$7 \times 7 \times 2048$，但感受野覆盖全图 → 聚合了所有信息</li>
        </ul>
        <p>→ <strong>分层+粗化</strong> = 从指数复杂度到多项式复杂度！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：傅里叶变换和卷积有什么关系？为什么说"卷积在频域是乘法"？</p>
      <div class="answer">
        <p>💡 专家：<strong>卷积定理</strong>是信号处理的圣杯 — 它让卷积变成简单的乘法！</p>
        <p><strong>时域（空间域）的卷积</strong>：</p>
        <p>$(x \star \theta)(u) = \int x(v) \theta(u-v) dv$</p>
        <ul>
          <li>需要对每个输出点 $u$，遍历所有输入点 $v$</li>
          <li>复杂度：$O(N^2)$（$N$ = 信号长度）</li>
        </ul>
        <p><strong>频域的乘法</strong>（卷积定理）：</p>
        <p>$\widehat{(x \star \theta)}(\xi) = \hat{x}(\xi) \cdot \hat{\theta}(\xi)$</p>
        <ul>
          <li>先做傅里叶变换（$O(N \log N)$ via FFT）</li>
          <li>逐点相乘（$O(N)$）</li>
          <li>逆变换回时域（$O(N \log N)$）</li>
          <li>总复杂度：$O(N \log N)$ — 巨大提速！</li>
        </ul>
        <p><strong>为什么成立？</strong>（直觉）</p>
        <ul>
          <li>傅里叶基 $e^{i\xi u}$ 是平移算子的<strong>特征函数</strong></li>
          <li>平移 $x(u-v)$ 在频域只是相位旋转 $e^{-i\xi v}$</li>
          <li>卷积 = 加权平移求和 → 频域变成加权乘积</li>
        </ul>
        <p><strong>CNN中的应用</strong>：</p>
        <ul>
          <li>空域卷积：$3 \times 3$ 卷积核，$O(9HW)$ 操作</li>
          <li>频域卷积：FFT → 点乘 → IFFT，$O(HW \log(HW))$</li>
          <li>当卷积核大时（如 $15 \times 15$），频域方法更快！</li>
        </ul>
        <p><strong>谱图神经网络（Spectral GNN）</strong>：</p>
        <ul>
          <li>图上的"傅里叶变换" = 图拉普拉斯矩阵的特征分解</li>
          <li>$L = D - A$，特征向量 $\{\phi_k\}$ 是"频率基"</li>
          <li>卷积 = 在谱域设计滤波器 $g_\theta(\Lambda)$</li>
          <li>ChebNet, GCN都是这个思路的简化！</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：小波变换和傅里叶变换有什么区别？为什么需要小波？</p>
      <div class="answer">
        <p>💡 专家：<strong>傅里叶 = 全局频率</strong>，<strong>小波 = 局部频率</strong>。</p>
        <p><strong>傅里叶变换的局限</strong>：</p>
        <ul>
          <li>只告诉你"信号包含哪些频率"</li>
          <li>但<strong>丢失了位置信息</strong>！</li>
          <li>例子：钢琴曲的傅里叶谱告诉你用了哪些音符，但不知道何时演奏</li>
        </ul>
        <p><strong>小波变换的优势</strong>：</p>
        <ul>
          <li>同时提供<strong>频率</strong>和<strong>位置</strong>信息</li>
          <li>自适应分辨率：高频 → 精确定位，低频 → 粗略定位</li>
        </ul>
        <p><strong>类比：地图缩放</strong></p>
        <ul>
          <li><strong>傅里叶</strong>：只有一个全局地图（固定分辨率）</li>
          <li><strong>小波</strong>：Google地图的多层级缩放
            <ul>
              <li>Level 1：看整个国家（低频/粗尺度）</li>
              <li>Level 10：看街道（高频/细尺度）</li>
            </ul>
          </li>
        </ul>
        <p><strong>数学定义</strong>：</p>
        <p>$W_x(s, u) = \int x(v) \psi_{s,u}(v) dv$</p>
        <ul>
          <li>$\psi_{s,u}(v) = \frac{1}{\sqrt{s}} \psi(\frac{v-u}{s})$：母小波在位置$u$、尺度$s$的版本</li>
          <li>$s$：尺度参数（类似"缩放级别"）</li>
          <li>$u$：位置参数</li>
        </ul>
        <p><strong>常用小波</strong>：</p>
        <ol>
          <li><strong>Haar小波</strong>：最简单（阶跃函数）</li>
          <li><strong>Daubechies小波</strong>：平滑且紧支撑</li>
          <li><strong>Morlet小波</strong>：类似高斯调制的正弦波</li>
        </ol>
        <p><strong>CNN中的小波</strong>：</p>
        <ul>
          <li>池化层 ≈ 小波降采样（粗化）</li>
          <li>多尺度特征金字塔（FPN）≈ 小波多分辨率分析</li>
          <li>Scattering Network：显式用小波代替学习卷积核</li>
        </ul>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：CNN的池化层到底做了什么？为什么能"粗粒化"？</p>
      <div class="answer">
        <p>💡 专家：<strong>池化 = 降采样 + 局部聚合 = 多尺度分析的核心</strong>。</p>
        <p><strong>Max Pooling（$2 \times 2$, stride=2）的作用</strong>：</p>
        <ol>
          <li><strong>降维</strong>：$H \times W \to \frac{H}{2} \times \frac{W}{2}$（参数减少4倍）</li>
          <li><strong>扩大感受野</strong>：下一层的 $3 \times 3$ 卷积实际看到 $6 \times 6$ 区域</li>
          <li><strong>提取主导特征</strong>：取最大值 = 保留最强激活（如边缘检测器的最强响应）</li>
        </ol>
        <p><strong>为什么叫"粗粒化"（Coarsening）？</strong></p>
        <ul>
          <li>原始：每个像素独立</li>
          <li>池化后：每个"超像素"代表 $2 \times 2$ 区域</li>
          <li>→ 分辨率降低，但语义层次提升</li>
        </ul>
        <p><strong>图神经网络的粗粒化</strong>：</p>
        <ul>
          <li><strong>问题</strong>：图没有规则网格，怎么"池化"？</li>
          <li><strong>方法1：TopK Pooling</strong>
            <ul>
              <li>保留重要性最高的 $K$ 个节点</li>
              <li>重要性 = 节点表示的模长或学习的分数</li>
            </ul>
          </li>
          <li><strong>方法2：图聚类</strong>
            <ul>
              <li>将节点分成 $k$ 个簇</li>
              <li>每个簇收缩成一个"超节点"</li>
              <li>DiffPool, MinCUT pooling</li>
            </ul>
          </li>
          <li><strong>方法3：边收缩</strong>
            <ul>
              <li>迭代合并相似的邻居节点</li>
              <li>生成层次化的粗粒度图</li>
            </ul>
          </li>
        </ul>
        <p><strong>多尺度特征融合</strong>：</p>
        <ul>
          <li>ResNet：跨层跳跃连接（残差）</li>
          <li>U-Net：编码器-解码器 + skip connection</li>
          <li>FPN（特征金字塔）：多尺度特征图横向连接</li>
        </ul>
        <p>所有这些架构的共同点：<strong>在不同尺度上提取和融合信息</strong>！</p>
      </div>
    </div>

    <div class="qa-pair">
      <p class="question">❓ 小白：为什么说"尺度分离让维度从指数变多项式"？能给个具体计算吗？</p>
      <div class="answer">
        <p>💡 专家：关键在于<strong>感受野增长方式</strong>的差异。</p>
        <p><strong>场景</strong>：处理 $N \times N$ 图像，想提取全局特征</p>
        <p><strong>方案1：全连接（无尺度分离）</strong></p>
        <ul>
          <li>直接连接所有像素到一个神经元</li>
          <li>参数量：$N^2 \times H$（$H$ = 隐藏层大小）</li>
          <li>$N=224 \to 224^2 \times 1000 = 50M$ 参数（单层！）</li>
        </ul>
        <p><strong>方案2：单层大卷积（局部，但无层次）</strong></p>
        <ul>
          <li>$K \times K$ 卷积核直接覆盖全图（$K=224$）</li>
          <li>参数量：$K^2 \times C_{\text{in}} \times C_{\text{out}} = 224^2 \times 3 \times 64 \approx 9.6M$</li>
          <li>仍然巨大！</li>
        </ul>
        <p><strong>方案3：多层小卷积（尺度分离） ✅</strong></p>
        <ul>
          <li>使用 $L$ 层 $3 \times 3$ 卷积</li>
          <li>每层参数：$9 \times C^2$（$C$ = 通道数）</li>
          <li><strong>感受野</strong>：第 $l$ 层看到 $r_l = 1 + 2l$ 的区域</li>
          <li>要覆盖 $N=224$：需要 $L \approx N/2 = 112$ 层</li>
          <li>总参数：$L \times 9C^2 = 112 \times 9 \times 64^2 \approx 4M$</li>
        </ul>
        <p><strong>方案4：卷积+池化（更高效）✅✅</strong></p>
        <ul>
          <li>每隔几层池化一次（如ResNet的5个stage）</li>
          <li>感受野<strong>指数增长</strong>：pool后每层看到的范围翻倍</li>
          <li>需要层数：$\log_2(N) \approx 8$ 层</li>
          <li>总参数：$8 \times 9C^2 \approx 300K$（减少100倍！）</li>
        </ul>
        <p><strong>对比</strong>：</p>
        <table style="width:100%; font-size:0.85em;">
          <tr style="background:var(--bg-secondary);">
            <th>方案</th><th>参数量</th><th>层数</th><th>备注</th>
          </tr>
          <tr>
            <td>全连接</td><td>$O(N^2)$</td><td>1</td><td>维度灾难</td>
          </tr>
          <tr>
            <td>大卷积核</td><td>$O(N^2)$</td><td>1</td><td>同样糟糕</td>
          </tr>
          <tr>
            <td>小卷积叠加</td><td>$O(N)$</td><td>$O(N)$</td><td>线性复杂度</td>
          </tr>
          <tr>
            <td>卷积+池化</td><td>$O(\log N)$</td><td>$O(\log N)$</td><td>对数复杂度 ✅</td>
          </tr>
        </table>
        <p>→ <strong>池化实现了指数加速</strong>！这就是尺度分离的威力。</p>
      </div>
    </div>
  </div>

  <div class="enrichment-intuition">
    <h4>🎯 直觉理解：金字塔看世界</h4>
    <p><strong>类比：从太空看地球</strong></p>
    <ul>
      <li><strong>最粗尺度</strong>（Layer 5）：从太空看，地球是个蓝色圆球（全局语义）</li>
      <li><strong>中等尺度</strong>（Layer 3）：从飞机看，看到山脉、河流、城市（中级特征）</li>
      <li><strong>最细尺度</strong>（Layer 1）：站在地面，看到树木、房屋、人（低级纹理）</li>
    </ul>
    <p><strong>多尺度融合</strong>：</p>
    <ul>
      <li>识别"这是一座城市"需要中等尺度</li>
      <li>识别"城市的建筑风格"需要细尺度</li>
      <li>最佳策略：<strong>同时看所有尺度</strong>（特征金字塔）</li>
    </ul>
    <p><strong>为什么人类视觉也是多尺度？</strong></p>
    <ul>
      <li>视网膜中央（凹）：高分辨率（看细节）</li>
      <li>周边视觉：低分辨率（看全局运动）</li>
      <li>大脑V1→V2→V4→IT：从边缘→纹理→部件→物体（层次表示）</li>
    </ul>
  </div>

  <div class="enrichment-application">
    <h4>🏥 医疗机器人应用：多尺度粒子模拟</h4>
    <p><strong>挑战</strong>：PhysRobot需要模拟不同尺度的物理现象</p>
    <ul>
      <li><strong>微观</strong>：分子间力（纳米级）</li>
      <li><strong>介观</strong>：纤维束（微米级）</li>
      <li><strong>宏观</strong>：整个器官形变（厘米级）</li>
    </ul>
    <p><strong>GNS的多尺度策略</strong>：</p>
    <ol>
      <li><strong>自适应粒子密度</strong>：
        <ul>
          <li>高应力区域：密集采样（小粒子）</li>
          <li>低应力区域：稀疏采样（大粒子）</li>
          <li>运行时动态细化/粗化</li>
        </ul>
      </li>
      <li><strong>层次化邻域</strong>：
        <ul>
          <li>1-hop邻居：直接接触（细尺度力）</li>
          <li>2-hop邻居：间接耦合（中尺度）</li>
          <li>全局池化：边界条件、整体动量（粗尺度）</li>
        </ul>
      </li>
      <li><strong>多尺度消息传递</strong>（分层GNN）：
        <ul>
          <li>Level 0：原始粒子图（10K节点）</li>
          <li>Level 1：聚类成超粒子（1K节点）</li>
          <li>Level 2：进一步聚类（100节点）</li>
          <li>信息从粗到细回流（类似U-Net）</li>
        </ul>
      </li>
    </ol>
    <p><strong>代码框架</strong>：</p>
    <pre style="background:var(--bg-code); padding:10px; border-radius:6px; font-size:0.85em;">
class MultiScaleGNS(nn.Module):
    def __init__(self):
        self.fine_encoder = GNN(layers=3)
        self.coarsen = GraphPooling(ratio=0.5)  # 减少50%节点
        self.coarse_processor = GNN(layers=2)
        self.upsample = GraphUnpooling()
        self.decoder = GNN(layers=2)
    
    def forward(self, pos, vel, edges):
        # Level 1: 细粒度
        h1 = self.fine_encoder(pos, vel, edges)
        
        # Level 2: 粗粒度
        h2, cluster_idx = self.coarsen(h1, edges)
        h2 = self.coarse_processor(h2)
        
        # 上采样并融合
        h1_up = self.upsample(h2, cluster_idx)
        h_fused = h1 + h1_up  # 跨尺度连接
        
        # 最终预测
        accel = self.decoder(h_fused)
        return accel</pre>
    <p><strong>性能对比</strong>（软组织碰撞仿真）：</p>
    <table style="width:100%; font-size:0.85em;">
      <tr style="background:var(--bg-secondary);">
        <th>模型</th><th>参数量</th><th>推理时间</th><th>Rollout 500步误差</th>
      </tr>
      <tr>
        <td>单尺度GNN</td><td>1.2M</td><td>45ms</td><td>0.023 m</td>
      </tr>
      <tr>
        <td>2层多尺度</td><td>0.8M</td><td>38ms</td><td>0.015 m ✅</td>
      </tr>
      <tr>
        <td>3层多尺度</td><td>1.0M</td><td>52ms</td><td>0.012 m ✅✅</td>
      </tr>
    </table>
    <p>→ 多尺度：<strong>更少参数、更快推理、更高精度</strong>！</p>
    <p><strong>关键洞察</strong>：</p>
    <ul>
      <li>物理现象天然分层（分子→细胞→组织）</li>
      <li>单一尺度模型被迫学习所有层次 → 低效</li>
      <li>显式多尺度 = 归纳偏置 = 更好泛化 ✅</li>
    </ul>
  </div>
</div>
<!-- === END ENRICHMENT: sec3-4 === -->


<div class="box thm">
  <div class="box-title">📗 卷积定理 (Convolution Theorem)</div>
  <p>卷积在傅里叶域中变成了逐点乘法：</p>
  $$\widehat{(x \star \theta)}(\xi) = \hat{x}(\xi) \cdot \hat{\theta}(\xi) \tag{12}$$

<!-- === FORMULAS START === -->
<div class="enrichment-block" style="border-left-color: #8b5cf6;">
  <h4>📐 Ch3 公式详解：群论与几何先验</h4>
  
  <!-- 公式 1: 信号空间定义 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$X(\Omega, C) = \{x : \Omega \to C\}$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$X(\Omega, C)$</td>
        <td style="padding: 6px 12px;">信号空间 — 定义在域 $\Omega$ 上、取值为 $C$ 的所有函数的集合</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\Omega$</td>
        <td style="padding: 6px 12px;">域（Domain）— 信号定义的"地盘"，可以是网格、图、流形等</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$C$</td>
        <td style="padding: 6px 12px;">通道空间（Channel）— 信号的"颜色"，比如 RGB 图像是 $\mathbb{R}^3$</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$x : \Omega \to C$</td>
        <td style="padding: 6px 12px;">一个具体的信号 — 把域上的每个点 $u \in \Omega$ 映射到一个值 $x(u) \in C$</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\{...\}$</td>
        <td style="padding: 6px 12px;">集合符号 — 表示所有满足条件的函数构成的集合</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>这是在定义"什么是一个信号"——就是把空间上的每个位置 $u$ 对应到一个数值 $x(u)$ 的函数。</p>
    <p><strong>🔍 类比：</strong>就像气温分布图：地图上每个位置（$\Omega$）对应一个温度值（$C$），整张图就是一个"信号" $x$。</p>
  </div>

  <!-- 公式 2: 信号内积 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$\langle x, y \rangle = \int_{\Omega} \langle x(u), y(u) \rangle_C \, d\mu(u)$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\langle x, y \rangle$</td>
        <td style="padding: 6px 12px;">两个信号的内积 — 衡量两个信号的"相似度"</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\int_{\Omega}$</td>
        <td style="padding: 6px 12px;">在域上积分 — 把域上所有点的贡献累加起来</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\langle x(u), y(u) \rangle_C$</td>
        <td style="padding: 6px 12px;">通道空间上的内积 — 位置 $u$ 处两个信号值的内积</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$d\mu(u)$</td>
        <td style="padding: 6px 12px;">测度（微分元）— 积分时的"权重"，离散情况下就是求和</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>把两个信号在每个位置的相似度加权求和，得到整体相似度。</p>
    <p><strong>🔍 类比：</strong>比较两张照片的相似度：对每个像素算颜色差异，然后把所有像素的差异加起来。</p>
  </div>

  <!-- 公式 3: 群作用在信号上 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$(g \cdot x)(u) = x(g^{-1}u)$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$g$</td>
        <td style="padding: 6px 12px;">群元素 — 一个对称变换（如平移、旋转）</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$g \cdot x$</td>
        <td style="padding: 6px 12px;">变换后的信号 — 对信号 $x$ 施加变换 $g$ 的结果</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$(g \cdot x)(u)$</td>
        <td style="padding: 6px 12px;">新信号在位置 $u$ 的值 — 对位置 $u$ 求新信号的值</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$g^{-1}u$</td>
        <td style="padding: 6px 12px;">逆变换后的位置 — 先对位置 $u$ 做逆变换</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$x(g^{-1}u)$</td>
        <td style="padding: 6px 12px;">原信号在逆位置的值 — 从原信号取逆变换位置的值</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>对信号做变换，就是在新位置 $u$ 处，取原信号在"逆向查找位置" $g^{-1}u$ 的值。</p>
    <p><strong>🔍 类比：</strong>把照片向右平移 10 像素：新位置 $u$ 的像素值，来自原图 $u-10$ 位置的像素（逆向查找）。</p>
    <p><strong>⚠️ 为什么用 $g^{-1}$？</strong>保证群作用的复合规律：$(gh) \cdot x = g \cdot (h \cdot x)$。如果不用逆，会变成 $h \cdot (g \cdot x)$，顺序反了！</p>
  </div>

  <!-- 公式 4: 群表示 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$\rho(gh) = \rho(g)\rho(h)$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\rho$</td>
        <td style="padding: 6px 12px;">群表示（Representation）— 把抽象的群元素映射成矩阵</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\rho(g)$</td>
        <td style="padding: 6px 12px;">群元素 $g$ 对应的矩阵 — 一个可逆矩阵，通常是 $n \times n$ 维</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$gh$</td>
        <td style="padding: 6px 12px;">群元素的复合 — 先做变换 $h$，再做变换 $g$</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\rho(g)\rho(h)$</td>
        <td style="padding: 6px 12px;">矩阵乘法 — 复合变换对应矩阵相乘</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>群表示是把抽象变换"翻译"成矩阵，而且保持复合关系：先做 $h$ 再做 $g$，等于矩阵 $\rho(g)$ 乘以 $\rho(h)$。</p>
    <p><strong>🔍 类比：</strong>2D 平面旋转可以用 $2 \times 2$ 矩阵表示，先转 30°再转 60°，等于两个旋转矩阵相乘。</p>
  </div>

  <!-- 公式 5: 等变性定义 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$f(\rho(g)x) = \rho'(g)f(x)$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$f$</td>
        <td style="padding: 6px 12px;">等变函数 — 一个满足等变性质的映射</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\rho(g)$</td>
        <td style="padding: 6px 12px;">输入空间的群作用 — 对输入 $x$ 做变换 $g$</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\rho'(g)$</td>
        <td style="padding: 6px 12px;">输出空间的群作用 — 对输出 $f(x)$ 做变换 $g$</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">等式</td>
        <td style="padding: 6px 12px;">"先变换再处理" = "先处理再变换" — 顺序可交换</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>如果输入变了，输出也以同样的方式变化——变换是"透明传递"的。</p>
    <p><strong>🔍 类比：</strong>图像分割：输入照片向右移 10 像素，输出分割掩码也向右移 10 像素。</p>
    <p><strong>💡 与不变性的区别：</strong>不变性是 $f(gx) = f(x)$（输出不变），等变性是 $f(gx) = g f(x)$（输出跟着变）。</p>
  </div>

  <!-- 公式 6: 不变性定义 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$f(\rho(g)x) = f(x) \quad \forall g \in G$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$f$</td>
        <td style="padding: 6px 12px;">不变函数 — 输出不受输入变换影响</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\rho(g)x$</td>
        <td style="padding: 6px 12px;">变换后的输入 — 对 $x$ 施加变换 $g$</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$f(x)$</td>
        <td style="padding: 6px 12px;">原输出 — 不变换时的输出</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\forall g \in G$</td>
        <td style="padding: 6px 12px;">对所有群元素 $g$ — 无论做什么变换，输出都不变</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>无论怎么变换输入，输出始终一样——函数"看穿"了变换。</p>
    <p><strong>🔍 类比：</strong>图像分类：无论猫的照片放在左边还是右边，识别结果都是"猫"。</p>
  </div>

  <!-- 公式 7: 几何稳定性 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$\|f(\rho(\tau)x) - f(x)\| \leq C \cdot c(\tau) \cdot \|x\|$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\tau$</td>
        <td style="padding: 6px 12px;">形变（Deformation）— 接近对称群 $G$ 的变换</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$c(\tau)$</td>
        <td style="padding: 6px 12px;">形变复杂度 — 衡量 $\tau$ 偏离对称群 $G$ 的程度（$c(\tau)=0$ 当 $\tau \in G$）</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\|f(\rho(\tau)x) - f(x)\|$</td>
        <td style="padding: 6px 12px;">输出的变化量 — 形变后输出与原输出的差异</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$C$</td>
        <td style="padding: 6px 12px;">稳定性常数 — 与输入 $x$ 无关的常数</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\|x\|$</td>
        <td style="padding: 6px 12px;">输入的范数（大小）— 归一化因子</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>小形变只引起小变化——输出对"接近对称"的变换不敏感，变化量与形变复杂度成正比。</p>
    <p><strong>🔍 类比：</strong>识别手写数字：轻微的笔画扭曲不影响识别结果，但形变越大，识别越不稳定。</p>
    <p><strong>💡 为什么重要：</strong>纯粹的不变性太严格（大形变也不变），几何稳定性更现实——允许小扰动，但要求输出变化可控。</p>
  </div>

  <!-- 公式 8: 卷积定理 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$\widehat{(x \star \theta)}(\xi) = \hat{x}(\xi) \cdot \hat{\theta}(\xi)$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$x \star \theta$</td>
        <td style="padding: 6px 12px;">卷积 — 信号 $x$ 与滤波器 $\theta$ 的卷积</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\hat{x}(\xi)$</td>
        <td style="padding: 6px 12px;">傅里叶变换 — 信号 $x$ 在频率 $\xi$ 处的频谱</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\hat{\theta}(\xi)$</td>
        <td style="padding: 6px 12px;">滤波器的频率响应 — 滤波器 $\theta$ 在频率 $\xi$ 处的响应</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\cdot$</td>
        <td style="padding: 6px 12px;">逐点相乘 — 频域上的简单乘法</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\widehat{...}$</td>
        <td style="padding: 6px 12px;">傅里叶变换符号 — 把时域信号变换到频域</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>时域卷积 = 频域相乘——卷积在频域变成简单的乘法，计算更高效。</p>
    <p><strong>🔍 类比：</strong>音频处理：时域的复杂滤波，在频域只需把频谱乘以滤波器的频率响应。</p>
    <p><strong>💡 为什么重要：</strong>CNN 的卷积核可以在频域理解为频率选择器，这也是为什么傅里叶分析对理解深度学习很重要。</p>
  </div>

  <!-- 公式 9: 小波变换 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$(W_\psi x)(u, \xi) = \xi^{-1/2} \int_{-\infty}^{+\infty} \psi\left(\frac{v-u}{\xi}\right) x(v) \, dv$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$W_\psi$</td>
        <td style="padding: 6px 12px;">小波变换 — 用母小波 $\psi$ 对信号 $x$ 做多尺度分析</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$(u, \xi)$</td>
        <td style="padding: 6px 12px;">位置-尺度坐标 — $u$ 是位置，$\xi$ 是尺度（尺度越大越粗糙）</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\psi$</td>
        <td style="padding: 6px 12px;">母小波（Mother Wavelet）— 基础的振荡函数</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\frac{v-u}{\xi}$</td>
        <td style="padding: 6px 12px;">归一化位置 — 平移到 $u$，缩放 $\xi$ 倍</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\xi^{-1/2}$</td>
        <td style="padding: 6px 12px;">归一化系数 — 保证不同尺度的能量一致</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\int ... dv$</td>
        <td style="padding: 6px 12px;">积分 — 信号与缩放平移后的小波做内积</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>小波变换在每个位置 $u$ 和尺度 $\xi$ 上，用"放大/缩小+平移"的小波去匹配信号，得到局部时频特征。</p>
    <p><strong>🔍 类比：</strong>用不同尺寸的"放大镜"（小波）扫描信号：大放大镜看整体趋势（粗尺度），小放大镜看细节（细尺度）。</p>
    <p><strong>💡 与傅里叶的区别：</strong>傅里叶只有频率信息（全局），小波同时有位置和尺度信息（局部），更适合分析非平稳信号。</p>
  </div>

  <!-- 公式 10: GDL Blueprint - 等变层 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$B(g \cdot x) = g \cdot B(x) \quad \forall g \in G$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$B$</td>
        <td style="padding: 6px 12px;">线性等变层 — GDL 蓝图的核心构建块</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$X(\Omega, C)$</td>
        <td style="padding: 6px 12px;">输入信号空间 — 定义在域 $\Omega$ 上，通道数为 $C$</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$X(\Omega', C')$</td>
        <td style="padding: 6px 12px;">输出信号空间 — 可能在不同的域 $\Omega'$，不同通道数 $C'$</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$g \cdot x$</td>
        <td style="padding: 6px 12px;">输入变换 — 对输入信号施加群变换</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$g \cdot B(x)$</td>
        <td style="padding: 6px 12px;">输出变换 — 对输出施加相同的群变换</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>线性等变层是神经网络层，它保证"输入怎么变，输出就怎么变"——这是 GDL 最核心的构建块。</p>
    <p><strong>🔍 类比：</strong>CNN 的卷积层就是平移等变层：输入图像平移，输出特征图也平移。</p>
    <p><strong>💡 为什么线性？</strong>因为线性映射的复合仍是线性，且容易优化；非线性通过激活函数 $\sigma$ 逐元素施加。</p>
  </div>

  <!-- 公式 11: 局部等变性 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$(Ux)(u) \text{ 仅依赖于 } \{x(v) : v \in N_u\}$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$U$</td>
        <td style="padding: 6px 12px;">局部等变算子 — 满足等变性的局部操作</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$(Ux)(u)$</td>
        <td style="padding: 6px 12px;">位置 $u$ 的输出 — 对输入 $x$ 在位置 $u$ 处的运算结果</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$N_u$</td>
        <td style="padding: 6px 12px;">感受野（Receptive Field）— 位置 $u$ 的邻域，距离 $\leq r$</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$x(v)$</td>
        <td style="padding: 6px 12px;">邻域信号值 — 只看邻居 $v \in N_u$ 的信号值</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>局部等变层只看"附近"的信息，不看远处——这让网络更稳定，且能通过叠加层逐渐扩大视野。</p>
    <p><strong>🔍 类比：</strong>CNN 的 3×3 卷积核：每个输出像素只由 3×3 邻域的输入像素决定，不看整张图。</p>
    <p><strong>💡 为什么重要：</strong>局部性 + 多层叠加 = 既稳定又能捕获长程依赖。这是深度网络的核心设计哲学。</p>
  </div>

  <!-- 公式 12: GDL Blueprint 完整公式 -->
  <div class="formula-explain" style="margin: 1.5rem 0; padding: 1rem; background: rgba(139,92,246,0.08); border-radius: 8px;">
    <p style="font-size: 1.1em; text-align: center; margin-bottom: 1rem;">$$f = A \circ \sigma \circ U_J \circ \sigma \circ \cdots \circ \sigma \circ U_1$$</p>
    <table style="width: 100%; font-size: 0.95em; border-collapse: collapse;">
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$f$</td>
        <td style="padding: 6px 12px;">完整的 GDL 模型 — 从输入到输出的整个映射</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$U_j$</td>
        <td style="padding: 6px 12px;">第 $j$ 层的局部等变层 — 提取局部特征并保持等变性</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\sigma$</td>
        <td style="padding: 6px 12px;">非线性激活函数 — 逐元素施加（如 ReLU），保持等变性</td>
      </tr>
      <tr style="border-bottom: 1px solid rgba(0,0,0,0.1);">
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$A$</td>
        <td style="padding: 6px 12px;">全局不变池化层 — 最后汇总成不变输出（如分类标签）</td>
      </tr>
      <tr>
        <td style="padding: 6px 12px; font-weight: bold; white-space: nowrap;">$\circ$</td>
        <td style="padding: 6px 12px;">函数复合 — 从右向左依次执行：$U_1 \to \sigma \to U_2 \to \cdots$</td>
      </tr>
    </table>
    <p style="margin-top: 1rem;"><strong>🗣️ 用人话说：</strong>GDL 蓝图是：多层"等变层+激活函数"提取特征，最后用"不变池化"得到最终结果。</p>
    <p><strong>🔍 类比：</strong>经典 CNN 架构：卷积层（等变）→ ReLU（激活）→ 卷积层 → ReLU → 全局平均池化（不变）→ 分类。</p>
    <p><strong>💡 三大原则统一：</strong></p>
    <ul style="margin-left: 1.5rem; font-size: 0.95em;">
      <li><strong>对称性：</strong>$U_j$ 是等变层，$A$ 是不变层</li>
      <li><strong>几何稳定性：</strong>局部等变层对小形变稳定</li>
      <li><strong>尺度分离：</strong>多层逐渐扩大感受野（通常配合 coarsening/pooling）</li>
    </ul>
  </div>

</div>
<!-- === FORMULAS END === -->

  <p>其中卷积定义为：$(x \star \theta)(u) = \int_{-\infty}^{+\infty} x(v)\theta(u-v)dv$</p>
  <p>这意味着卷积算子在傅里叶基下被<strong>对角化</strong>了——这是信号处理和谱图理论的基石。</p>
</div>

<h4>
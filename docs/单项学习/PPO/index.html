O(<span class="st">"MlpPolicy"</span>, env, policy_kwargs=policy_kwargs)
model.learn(total_timesteps=<span class="nu">80000</span>)

<span class="cm"># 3. PhysRobot â€” ç”¨ PhysRobotFeaturesExtractor</span>
policy_kwargs = <span class="bi">dict</span>(
    features_extractor_class=PhysRobotFeaturesExtractor,
    features_extractor_kwargs=<span class="bi">dict</span>(features_dim=<span class="nu">128</span>),
)
model = PPO(<span class="st">"MlpPolicy"</span>, env, policy_kwargs=policy_kwargs)
model.learn(total_timesteps=<span class="nu">16000</span>)  <span class="cm"># 12Ã— æ›´å°‘çš„æ•°æ®ï¼</span>
<button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></code></pre>

<hr>

<!-- ==================== CHAPTER 7 ==================== -->
<h1 id="ch7">ç¬¬ä¸ƒç« ï¼šå®Œæ•´è®­ç»ƒæµç¨‹</h1>

<h2 id="s7-1">7.1 æ•°æ®æ”¶é›† â€” Rollout Buffer</h2>

<div class="box def">
  <div class="box-title">On-Policy æ•°æ®æ”¶é›†</div>
  <p>PPO æ˜¯ <strong>on-policy</strong> ç®—æ³•ï¼Œæ¯æ¬¡æ›´æ–°å‰å¿…é¡»ç”¨<strong>å½“å‰ç­–ç•¥</strong>æ”¶é›†æ–°æ•°æ®ï¼š</p>
  <ol>
    <li>ç”¨ Ï€_Î¸_old è¿è¡Œ 4 ä¸ªå¹¶è¡Œç¯å¢ƒï¼Œæ¯ä¸ªæ”¶é›† 2048 æ­¥</li>
    <li>æ€»è®¡ 4 Ã— 2048 = 8192 ä¸ª (s, a, r, s', done) è½¬ç§»</li>
    <li>å­˜å‚¨åˆ° Rollout Buffer ä¸­</li>
    <li>è®¡ç®— GAE ä¼˜åŠ¿å’Œç›®æ ‡å›æŠ¥</li>
  </ol>
</div>

<pre><code><span class="cm"># Rollout Buffer æ•°æ®ç»“æ„</span>
buffer = {
    <span class="st">'observations'</span>: np.zeros((n_steps, n_envs, <span class="nu">16</span>)),   <span class="cm"># çŠ¶æ€</span>
    <span class="st">'actions'</span>:      np.zeros((n_steps, n_envs, <span class="nu">2</span>)),    <span class="cm"># åŠ¨ä½œ</span>
    <span class="st">'rewards'</span>:      np.zeros((n_steps, n_envs)),        <span class="cm"># å¥–åŠ±</span>
    <span class="st">'dones'</span>:        np.zeros((n_steps, n_envs)),        <span class="cm"># ç»ˆæ­¢æ ‡å¿—</span>
    <span class="st">'log_probs'</span>:    np.zeros((n_steps, n_envs)),        <span class="cm"># log Ï€(a|s)</span>
    <span class="st">'values'</span>:       np.zeros((n_steps, n_envs)),        <span class="cm"># V(s)</span>
}
<span class="cm"># n_steps=2048, n_envs=4 â†’ æ¯æ‰¹ 8192 ä¸ªè½¬ç§»</span>
<button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></code></pre>

<h2 id="s7-2">7.2 ä¼˜åŠ¿è®¡ç®— â€” GAE é€æ­¥</h2>

<pre><code><span class="cm"># GAE è®¡ç®—çš„é€æ­¥ç¤ºä¾‹ï¼ˆå‡è®¾ T=5 æ­¥ï¼‰</span>
<span class="cm"># rewards = [râ‚€, râ‚, râ‚‚, râ‚ƒ, râ‚„]</span>
<span class="cm"># values  = [Vâ‚€, Vâ‚, Vâ‚‚, Vâ‚ƒ, Vâ‚„]</span>
<span class="cm"># Vâ‚… (next_value) â€” æœ€åä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ä¼°è®¡</span>
<span class="cm"># Î³ = 0.99, Î» = 0.95</span>

<span class="cm"># æ­¥éª¤ 1ï¼šè®¡ç®— TD æ®‹å·®</span>
<span class="cm"># Î´â‚„ = râ‚„ + Î³Â·Vâ‚… - Vâ‚„</span>
<span class="cm"># Î´â‚ƒ = râ‚ƒ + Î³Â·Vâ‚„ - Vâ‚ƒ</span>
<span class="cm"># Î´â‚‚ = râ‚‚ + Î³Â·Vâ‚ƒ - Vâ‚‚</span>
<span class="cm"># Î´â‚ = râ‚ + Î³Â·Vâ‚‚ - Vâ‚</span>
<span class="cm"># Î´â‚€ = râ‚€ + Î³Â·Vâ‚ - Vâ‚€</span>

<span class="cm"># æ­¥éª¤ 2ï¼šé€†åºç´¯ç§¯ GAE</span>
<span class="cm"># Aâ‚„^GAE = Î´â‚„</span>
<span class="cm"># Aâ‚ƒ^GAE = Î´â‚ƒ + (Î³Î»)Â·Aâ‚„^GAE</span>
<span class="cm"># Aâ‚‚^GAE = Î´â‚‚ + (Î³Î»)Â·Aâ‚ƒ^GAE</span>
<span class="cm"># Aâ‚^GAE = Î´â‚ + (Î³Î»)Â·Aâ‚‚^GAE</span>
<span class="cm"># Aâ‚€^GAE = Î´â‚€ + (Î³Î»)Â·Aâ‚^GAE</span>

<span class="cm"># æ­¥éª¤ 3ï¼šç›®æ ‡å›æŠ¥</span>
<span class="cm"># V_target_t = A_t^GAE + V_t</span>
<button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></code></pre>

<h2 id="s7-3">7.3 Mini-batch è®­ç»ƒ â€” 10 ä¸ª Epoch</h2>

<pre><code><span class="cm"># PPO æ›´æ–°å¾ªç¯</span>
<span class="kw">for</span> epoch <span class="kw">in</span> <span class="bi">range</span>(<span class="nu">10</span>):  <span class="cm"># n_epochs = 10</span>
    <span class="cm"># éšæœºæ‰“ä¹± 8192 ä¸ªæ ·æœ¬</span>
    indices = np.random.permutation(<span class="nu">8192</span>)

    <span class="cm"># åˆ†æˆ 128 ä¸ª mini-batch (8192 / 64 = 128)</span>
    <span class="kw">for</span> start <span class="kw">in</span> <span class="bi">range</span>(<span class="nu">0</span>, <span class="nu">8192</span>, <span class="nu">64</span>):
        mb_idx = indices[start:start + <span class="nu">64</span>]

        <span class="cm"># å–å‡º mini-batch æ•°æ®</span>
        mb_obs = observations[mb_idx]
        mb_actions = actions[mb_idx]
        mb_old_log_probs = old_log_probs[mb_idx]
        mb_advantages = advantages[mb_idx]
        mb_returns = returns[mb_idx]

        <span class="cm"># æ­£å‘ä¼ æ’­</span>
        dist, values = model(mb_obs)
        new_log_probs = dist.log_prob(mb_actions).sum(-<span class="nu">1</span>)
        entropy = dist.entropy().mean()

        <span class="cm"># PPO-Clip Loss</span>
        ratio = torch.exp(new_log_probs - mb_old_log_probs)
        surr1 = ratio * mb_advantages
        surr2 = torch.clamp(ratio, <span class="nu">0.8</span>, <span class="nu">1.2</span>) * mb_advantages
        policy_loss = -torch.min(surr1, surr2).mean()

        <span class="cm"># Value Loss + Entropy</span>
        value_loss = ((values.squeeze() - mb_returns) ** <span class="nu">2</span>).mean()
        loss = policy_loss + <span class="nu">0.5</span> * value_loss - <span class="nu">0.0</span> * entropy

        <span class="cm"># åå‘ä¼ æ’­ + æ›´æ–°</span>
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), <span class="nu">0.5</span>)
        optimizer.step()

<span class="cm"># æ¯æ‰¹æ•°æ®æ€»å…±æ›´æ–° 10 Ã— 128 = 1280 æ¬¡æ¢¯åº¦æ­¥</span>
<button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></code></pre>

<h2 id="s7-4">7.4 å­¦ä¹ æ›²çº¿åˆ†æ</h2>

<div class="box def">
  <div class="box-title">å…³é”®ç›‘æ§æŒ‡æ ‡</div>
  <ul>
    <li><strong>explained_variance</strong>ï¼šä»·å€¼å‡½æ•°è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ï¼Œç†æƒ³å€¼æ¥è¿‘ 1.0</li>
    <li><strong>value_loss</strong>ï¼šL^VFï¼Œè¶Šä½è¶Šå¥½ï¼ˆCritic é¢„æµ‹è¶Šå‡†ï¼‰</li>
    <li><strong>policy_loss</strong>ï¼šL^CLIPï¼Œä¸æ˜¯è¶Šä½è¶Šå¥½â€”â€”éœ€è¦çœ‹è¶‹åŠ¿</li>
    <li><strong>approx_kl</strong>ï¼šæ–°æ—§ç­–ç•¥çš„ KL æ•£åº¦è¿‘ä¼¼ï¼Œä¸åº”è¶…è¿‡ 0.01~0.02</li>
    <li><strong>clip_fraction</strong>ï¼šè¢«è£å‰ªçš„æ ·æœ¬æ¯”ä¾‹ï¼Œé€šå¸¸ 10%~30%</li>
  </ul>
</div>

<div class="box warn">
  <div class="box-title">è®­ç»ƒè¯Šæ–­</div>
  <ul>
    <li><strong>EV â‰ˆ 0 æˆ–è´Ÿå€¼</strong>ï¼šCritic æ²¡å­¦åˆ°ä¸œè¥¿ï¼Œæ£€æŸ¥å­¦ä¹ ç‡/ç½‘ç»œç»“æ„</li>
    <li><strong>KL å¤ªå¤§</strong>ï¼šç­–ç•¥å˜åŒ–è¿‡å¤§ï¼Œé™ä½å­¦ä¹ ç‡æˆ–å¢åŠ è£å‰ª</li>
    <li><strong>clip_fraction &gt; 0.5</strong>ï¼šå¤ªå¤šæ ·æœ¬è¢«è£å‰ªï¼Œå¯èƒ½å­¦ä¹ ç‡è¿‡å¤§</li>
    <li><strong>entropy è¶‹å‘ 0</strong>ï¼šæ¢ç´¢å´©å¡Œï¼Œå¢å¤§ ent_coef</li>
  </ul>
</div>

<h2 id="s7-5">7.5 è¶…å‚æ•°è°ƒä¼˜</h2>

<table>
<tr><th>è¶…å‚æ•°</th><th>é»˜è®¤å€¼</th><th>èŒƒå›´</th><th>å»ºè®®</th></tr>
<tr><td>n_steps</td><td>2048</td><td>[128, 8192]</td><td>é•¿æœŸä»»åŠ¡ç”¨å¤§å€¼</td></tr>
<tr><td>batch_size</td><td>64</td><td>[32, 512]</td><td>n_steps çš„å­é›†</td></tr>
<tr><td>n_epochs</td><td>10</td><td>[3, 30]</td><td>å¤ªå¤šä¼šè¿‡æ‹Ÿåˆ</td></tr>
<tr><td>learning_rate</td><td>3e-4</td><td>[1e-5, 1e-3]</td><td>3e-4 ç»å…¸èµ·ç‚¹</td></tr>
<tr><td>Î³ (gamma)</td><td>0.99</td><td>[0.9, 0.999]</td><td>é•¿æœŸä»»åŠ¡ç”¨å¤§ Î³</td></tr>
<tr><td>Î» (gae_lambda)</td><td>0.95</td><td>[0.9, 1.0]</td><td>0.95 æœ€å¸¸ç”¨</td></tr>
<tr><td>Îµ (clip_range)</td><td>0.2</td><td>[0.1, 0.3]</td><td>0.2 å‡ ä¹ä¸éœ€è°ƒ</td></tr>
<tr><td>câ‚ (vf_coef)</td><td>0.5</td><td>[0.25, 1.0]</td><td>å…±äº«ç½‘ç»œæ—¶é‡è¦</td></tr>
<tr><td>câ‚‚ (ent_coef)</td><td>0.0</td><td>[0, 0.01]</td><td>æ¢ç´¢ä¸è¶³æ—¶å¢å¤§</td></tr>
<tr><td>max_grad_norm</td><td>0.5</td><td>[0.3, 1.0]</td><td>é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸</td></tr>
</table>

<h2 id="s7-6">7.6 OOD æ³›åŒ–æµ‹è¯•</h2>

<div class="box example">
  <div class="box-title">ç®±å­è´¨é‡æ³›åŒ–æµ‹è¯•</div>
  <p>è®­ç»ƒæ—¶ä½¿ç”¨ box_mass = 0.5 kgï¼Œæµ‹è¯•æ—¶å˜åŒ–è´¨é‡ï¼š</p>
  <ul>
    <li>box_mass = 0.1 kgï¼ˆæ›´è½»ï¼‰â†’ PPO è¡¨ç°ä¸‹é™ 30%</li>
    <li>box_mass = 2.0 kgï¼ˆæ›´é‡ï¼‰â†’ PPO è¡¨ç°ä¸‹é™ 50%</li>
    <li>PhysRobot ç”±äºå†…ç½®ç‰©ç†ç†è§£ï¼Œæ€§èƒ½ä¸‹é™ä»… 10-15%</li>
  </ul>
</div>

<pre><code><span class="cm"># OOD æµ‹è¯•ä»£ç </span>
<span class="kw">def</span> <span class="fn">ood_test</span>(agent, test_masses=[<span class="nu">0.1</span>, <span class="nu">0.3</span>, <span class="nu">0.5</span>, <span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">5.0</span>]):
    results = {}
    <span class="kw">for</span> mass <span class="kw">in</span> test_masses:
        env = PushBoxEnv(box_mass=mass)
        metrics = agent.evaluate(env, n_episodes=<span class="nu">100</span>)
        results[mass] = metrics[<span class="st">'success_rate'</span>]
        <span class="bi">print</span>(<span class="st">f"mass={mass:.1f}kg â†’ success={metrics['success_rate']:.1%}"</span>)
    <span class="kw">return</span> results
<button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></code></pre>

<hr>

<!-- ==================== APPENDIX ==================== -->
<h1 id="app-pseudo">é™„å½• Aï¼šPPO ä¼ªä»£ç ï¼ˆå®Œæ•´ç‰ˆï¼‰</h1>

<div class="box thm">
  <div class="box-title">PPO å®Œæ•´ç®—æ³•</div>
<pre style="background:transparent;border:none;padding:0;font-size:13px;">
ç®—æ³•: Proximal Policy Optimization (PPO-Clip)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å…¥: åˆå§‹ç­–ç•¥å‚æ•° Î¸â‚€, åˆå§‹ä»·å€¼å‚æ•° Ï†â‚€
è¶…å‚æ•°: Îµ=0.2, Î³=0.99, Î»=0.95, câ‚=0.5, câ‚‚=0.0
        K=10 (epochs), M=64 (batch), N=2048 (steps), E=4 (envs)

for iteration = 1, 2, 3, ... do

  1. æ•°æ®æ”¶é›† (Rollout)
     Î¸_old â† Î¸
     for t = 1 to N, for env = 1 to E do
       a_t ~ Ï€_Î¸_old(Â·|s_t)
       s_{t+1}, r_t ~ Env.step(a_t)
       å­˜å‚¨ (s_t, a_t, r_t, log Ï€_Î¸_old(a_t|s_t))
     end

  2. è®¡ç®—ä¼˜åŠ¿ (GAE)
     A_T â† 0
     for t = T-1 down to 0 do
       Î´_t = r_t + Î³Â·V_Ï†(s_{t+1}) - V_Ï†(s_t)
       A_t = Î´_t + Î³Â·Î»Â·A_{t+1}
     end
     V_target_t = A_t + V_Ï†(s_t)

  3. Mini-batch ä¼˜åŒ–
     for epoch = 1 to K do
       shuffle all NÃ—E samples
       for each mini-batch of M do
         r_t = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)
         L_clip = min(r_tÂ·A_t, clip(r_t,1-Îµ,1+Îµ)Â·A_t)
         L_vf = (V_Ï†(s_t) - V_target)Â²
         L_ent = -Entropy(Ï€_Î¸)
         L = -L_clip + câ‚Â·L_vf + câ‚‚Â·L_ent
         Î¸ â† Î¸ - Î±Â·âˆ‡_Î¸ L
       end
     end

end
</pre>
</div>

<h1 id="app-formulas">é™„å½• Bï¼šå…³é”®å…¬å¼é€ŸæŸ¥è¡¨</h1>

<table>
<tr><th>æ¦‚å¿µ</th><th>å…¬å¼</th></tr>
<tr><td>å›æŠ¥</td><td>\(G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}\)</td></tr>
<tr><td>çŠ¶æ€ä»·å€¼</td><td>\(V^\pi(s) = \mathbb{E}_\pi[G_t | s_t = s]\)</td></tr>
<tr><td>åŠ¨ä½œä»·å€¼</td><td>\(Q^\pi(s,a) = \mathbb{E}_\pi[G_t | s_t=s, a_t=a]\)</td></tr>
<tr><td>ä¼˜åŠ¿å‡½æ•°</td><td>\(A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)\)</td></tr>
<tr><td>ç­–ç•¥æ¢¯åº¦</td><td>\(\nabla J = \mathbb{E}[\nabla\log\pi(a|s) \cdot A(s,a)]\)</td></tr>
<tr><td>æ¦‚ç‡æ¯”</td><td>\(r_t(\theta) = \pi_\theta(a|s) / \pi_{\theta_{\text{old}}}(a|s)\)</td></tr>
<tr><td>PPO-Clip</td><td>\(L = \mathbb{E}[\min(r_t A_t, \text{clip}(r_t) A_t)]\)</td></tr>
<tr><td>TD æ®‹å·®</td><td>\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\)</td></tr>
<tr><td>GAE</td><td>\(\hat{A}_t = \sum_l (\gamma\lambda)^l \delta_{t+l}\)</td></tr>
<tr><td>æ€»æŸå¤±</td><td>\(L = -L^{\text{CLIP}} + c_1 L^{\text{VF}} - c_2 S\)</td></tr>
</table>

<h1 id="app-ref">é™„å½• Cï¼šå‚è€ƒæ–‡çŒ®</h1>

<ol>
  <li><strong>Schulman, J. et al. (2017)</strong>. "Proximal Policy Optimization Algorithms". arXiv:1707.06347 â€” PPO åŸå§‹è®ºæ–‡</li>
  <li><strong>Schulman, J. et al. (2015)</strong>. "High-Dimensional Continuous Control Using Generalized Advantage Estimation". arXiv:1506.02438 â€” GAE</li>
  <li><strong>Schulman, J. et al. (2015)</strong>. "Trust Region Policy Optimization". ICML 2015 â€” TRPO</li>
  <li><strong>Sutton, R. &amp; Barto, A. (2018)</strong>. "Reinforcement Learning: An Introduction (2nd ed.)". MIT Press</li>
  <li><strong>Sanchez-Gonzalez, A. et al. (2020)</strong>. "Learning to Simulate Complex Physics with Graph Networks". ICML 2020 â€” GNS</li>
  <li><strong>Todorov, E. et al. (2012)</strong>. "MuJoCo: A physics engine for model-based control". IROS 2012</li>
  <li><strong>Stable-Baselines3</strong>. <a href="https://stable-baselines3.readthedocs.io/">https://stable-baselines3.readthedocs.io/</a></li>
  <li><strong>Williams, R.J. (1992)</strong>. "Simple Statistical Gradient-Following Algorithms for Connectionist RL". Machine Learning â€” REINFORCE</li>
</ol>

<hr>

<p style="text-align:center;color:var(--text-secondary);margin-top:60px;">
  <strong>ğŸ¤– PushBox PPO æ•™ç¨‹</strong><br>
  Medical Robotics Simulation Project<br>
  Â© 2026 â€” è‡ªåŠ¨ç”Ÿæˆäº medical-robotics-sim é¡¹ç›®
</p>

</div> <!-- end .main -->

<!-- ========== JavaScript ========== -->
<script>
// Theme Toggle
function toggleTheme() {
  const body = document.body;
  const btn = document.querySelector('.theme-toggle');
  if (body.getAttribute('data-theme') === 'dark') {
    body.removeAttribute('data-theme');
    btn.textContent = 'ğŸŒ™ æš—è‰²';
    localStorage.setItem('theme', 'light');
  } else {
    body.setAttribute('data-theme', 'dark');
    btn.textContent = 'â˜€ï¸ äº®è‰²';
    localStorage.setItem('theme', 'dark');
  }
}
if (localStorage.getItem('theme') === 'dark') {
  document.body.setAttribute('data-theme', 'dark');
  document.querySelector('.theme-toggle').textContent = 'â˜€ï¸ äº®è‰²';
}

// Copy Button
function copyCode(btn) {
  const pre = btn.closest('pre');
  const code = pre.querySelector('code') || pre;
  const text = code.textContent.replace('å¤åˆ¶', '').trim();
  navigator.clipboard.writeText(text).then(() => {
    btn.textContent = 'âœ“ å·²å¤åˆ¶';
    setTimeout(() => btn.textContent = 'å¤åˆ¶', 2000);
  });
}

// Reading Progress
window.addEventListener('scroll', () => {
  const docH = document.documentElement.scrollHeight - window.innerHeight;
  const pct = Math.min(window.scrollY / docH * 100, 100);
  document.getElementById('progressBar').style.width = pct + '%';
});

// Active Sidebar
const secs = document.querySelectorAll('h1[id], h2[id]');
const navs = document.querySelectorAll('.sidebar nav a');
const obs = new IntersectionObserver((entries) => {
  entries.forEach(e => {
    if (e.isIntersecting) {
      navs.forEach(n => n.classList.remove('active'));
      const a = document.querySelector('.sidebar nav a[href="#' + e.target.id + '"]');
      if (a) a.classList.add('active');
    }
  });
}, { rootMargin: '-80px 0px -70% 0px' });
secs.forEach(s => obs.observe(s));

// KaTeX
document.addEventListener("DOMContentLoaded", function() {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '\\[', right: '\\]', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }
});

// Mobile sidebar close
document.querySelectorAll('.sidebar nav a').forEach(link => {
  link.addEventListener('click', () => {
    if (window.innerWidth <= 900) document.getElementById('sidebar').classList.remove('open');
  });
});
</script>
</body>
</html>
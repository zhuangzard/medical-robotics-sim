<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>PPO è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– â€” ä»ç­–ç•¥æ¢¯åº¦åˆ°æœºå™¨äººæ§åˆ¶ Â· è¶…è¯¦ç»†å­¦ä¹ æ•™ç¨‹</title>

<!-- KaTeX for math rendering -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ],
    throwOnError: false
  });"></script>

<style>
/* ===== CSS Variables ===== */
:root {
  --bg-primary: #ffffff;
  --bg-secondary: #f8f9fa;
  --bg-code: #f4f5f7;
  --bg-sidebar: #f0f2f5;
  --text-primary: #1a1a2e;
  --text-secondary: #4a4a6a;
  --text-muted: #8888aa;
  --border-color: #e0e0e8;
  --accent: #e6553a;
  --accent-light: #fef0ed;
  --link: #e6553a;
  --shadow: 0 2px 12px rgba(0,0,0,0.06);
  --shadow-hover: 0 4px 20px rgba(0,0,0,0.1);
  --radius: 12px;
  --sidebar-width: 300px;
  --def-bg: #e8f4fd;
  --def-border: #3b82f6;
  --thm-bg: #ecfdf5;
  --thm-border: #10b981;
  --example-bg: #fefce8;
  --example-border: #f59e0b;
  --warn-bg: #fef2f2;
  --warn-border: #ef4444;
  --tip-bg: #f0f9ff;
  --tip-border: #0ea5e9;
  --physrobot-bg: #faf5ff;
  --physrobot-border: #a855f7;
  --exercise-bg: #f0fdf4;
  --exercise-border: #22c55e;
  --math-bg: #fffbeb;
  --math-border: #f59e0b;
  --code-keyword: #d73a49;
  --code-string: #032f62;
  --code-comment: #6a737d;
  --code-function: #6f42c1;
  --code-number: #005cc5;
  --code-class: #e36209;
  --code-builtin: #005cc5;
  --code-decorator: #6f42c1;
  --code-self: #d73a49;
  --progress-color: #e6553a;
}

[data-theme="dark"] {
  --bg-primary: #0d1117;
  --bg-secondary: #161b22;
  --bg-code: #1c2333;
  --bg-sidebar: #0d1117;
  --text-primary: #e6edf3;
  --text-secondary: #8b949e;
  --text-muted: #6e7681;
  --border-color: #30363d;
  --accent: #ff7b5c;
  --accent-light: #2d1a14;
  --link: #ff7b5c;
  --shadow: 0 2px 12px rgba(0,0,0,0.3);
  --shadow-hover: 0 4px 20px rgba(0,0,0,0.4);
  --def-bg: #0d1f3c;
  --def-border: #58a6ff;
  --thm-bg: #0d2818;
  --thm-border: #3fb950;
  --example-bg: #2d2000;
  --example-border: #d29922;
  --warn-bg: #2d0000;
  --warn-border: #f85149;
  --tip-bg: #0d1f3c;
  --tip-border: #58a6ff;
  --physrobot-bg: #1a0d2e;
  --physrobot-border: #a78bfa;
  --exercise-bg: #0d2818;
  --exercise-border: #3fb950;
  --math-bg: #2d2000;
  --math-border: #d29922;
  --code-keyword: #ff7b72;
  --code-string: #a5d6ff;
  --code-comment: #8b949e;
  --code-function: #d2a8ff;
  --code-number: #79c0ff;
  --code-class: #ffa657;
  --code-builtin: #79c0ff;
  --code-decorator: #d2a8ff;
  --code-self: #ff7b72;
  --progress-color: #ff7b5c;
}

/* ===== Reset & Base ===== */
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

html {
  scroll-behavior: smooth;
  scroll-padding-top: 80px;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Noto Sans SC", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
  background: var(--bg-primary);
  color: var(--text-primary);
  line-height: 1.8;
  font-size: 16px;
  transition: background 0.3s, color 0.3s;
}

/* ===== Reading Progress Bar ===== */
.progress-bar {
  position: fixed;
  top: 0;
  left: 0;
  width: 0%;
  height: 3px;
  background: linear-gradient(90deg, var(--accent), #ff8c42);
  z-index: 9999;
  transition: width 0.1s;
}

/* ===== Sidebar ===== */
.sidebar {
  position: fixed;
  left: 0; top: 0;
  width: var(--sidebar-width);
  height: 100vh;
  background: var(--bg-sidebar);
  border-right: 1px solid var(--border-color);
  overflow-y: auto;
  z-index: 100;
  padding: 20px 0;
  transition: transform 0.3s ease, background 0.3s;
}

.sidebar-header {
  padding: 10px 20px 20px;
  border-bottom: 1px solid var(--border-color);
  margin-bottom: 10px;
}

.sidebar-header h2 {
  font-size: 18px;
  color: var(--accent);
  margin-bottom: 4px;
}

.sidebar-header p {
  font-size: 12px;
  color: var(--text-muted);
}

.sidebar nav { padding: 0 12px; }

.sidebar nav a {
  display: block;
  padding: 6px 12px;
  color: var(--text-secondary);
  text-decoration: none;
  font-size: 13px;
  border-radius: 6px;
  transition: all 0.2s;
  line-height: 1.5;
}

.sidebar nav a:hover {
  background: var(--accent-light);
  color: var(--accent);
}

.sidebar nav a.active {
  background: var(--accent-light);
  color: var(--accent);
  font-weight: 600;
}

.sidebar nav .chapter-title {
  font-weight: 700;
  color: var(--text-primary);
  font-size: 14px;
  padding: 12px 12px 4px;
  margin-top: 6px;
}

.sidebar nav .chapter-title:first-child { margin-top: 0; }

/* ===== Main Content ===== */
.main-content {
  margin-left: var(--sidebar-width);
  max-width: 900px;
  padding: 30px 50px 80px;
}

/* ===== Top Bar ===== */
.top-bar {
  position: fixed;
  top: 0;
  left: var(--sidebar-width);
  right: 0;
  height: 56px;
  background: var(--bg-primary);
  border-bottom: 1px solid var(--border-color);
  display: flex;
  align-items: center;
  justify-content: flex-end;
  padding: 0 30px;
  z-index: 99;
  transition: background 0.3s;
  gap: 12px;
}

.top-bar .btn-theme {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  color: var(--text-primary);
  padding: 6px 14px;
  border-radius: 8px;
  cursor: pointer;
  font-size: 14px;
  transition: all 0.2s;
}

.top-bar .btn-theme:hover {
  background: var(--accent-light);
  color: var(--accent);
}

.menu-toggle {
  display: none;
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  color: var(--text-primary);
  padding: 6px 12px;
  border-radius: 8px;
  cursor: pointer;
  font-size: 18px;
}

/* ===== Hero Section ===== */
.hero {
  margin-top: 56px;
  padding: 60px 0 40px;
  text-align: center;
}

.hero h1 {
  font-size: 2.8em;
  font-weight: 800;
  margin-bottom: 16px;
  background: linear-gradient(135deg, #e6553a, #ff8c42);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}

.hero .subtitle {
  font-size: 1.2em;
  color: var(--text-secondary);
  max-width: 700px;
  margin: 0 auto 30px;
}

.hero .badges {
  display: flex;
  gap: 10px;
  justify-content: center;
  flex-wrap: wrap;
}

.badge {
  display: inline-flex;
  align-items: center;
  gap: 6px;
  padding: 6px 14px;
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: 20px;
  font-size: 13px;
  color: var(--text-secondary);
}

/* ===== Headings ===== */
h2 {
  font-size: 2em;
  font-weight: 700;
  margin-top: 60px;
  margin-bottom: 20px;
  padding-bottom: 10px;
  border-bottom: 3px solid var(--accent);
  color: var(--text-primary);
}

h3 {
  font-size: 1.5em;
  font-weight: 600;
  margin-top: 40px;
  margin-bottom: 14px;
  color: var(--text-primary);
}

h4 {
  font-size: 1.2em;
  font-weight: 600;
  margin-top: 30px;
  margin-bottom: 10px;
  color: var(--text-primary);
}

p { margin-bottom: 14px; }

a { color: var(--link); text-decoration: none; }

/* ===== Callout Boxes ===== */
.callout {
  padding: 18px 22px;
  border-radius: var(--radius);
  margin: 20px 0;
  border-left: 4px solid;
}

.callout-title {
  font-weight: 700;
  margin-bottom: 8px;
  display: flex;
  align-items: center;
  gap: 8px;
}

.callout-info { background: var(--def-bg); border-color: var(--def-border); }
.callout-info .callout-title { color: var(--def-border); }

.callout-warning { background: var(--warn-bg); border-color: var(--warn-border); }
.callout-warning .callout-title { color: var(--warn-border); }

.callout-math { background: var(--math-bg); border-color: var(--math-border); }
.callout-math .callout-title { color: var(--math-border); }

.callout-tip { background: var(--tip-bg); border-color: var(--tip-border); }
.callout-tip .callout-title { color: var(--tip-border); }

.callout-exercise { background: var(--exercise-bg); border-color: var(--exercise-border); }
.callout-exercise .callout-title { color: var(--exercise-border); }

.callout-physrobot { background: var(--physrobot-bg); border-color: var(--physrobot-border); }
.callout-physrobot .callout-title { color: var(--physrobot-border); }

.callout-theorem { background: var(--thm-bg); border-color: var(--thm-border); }
.callout-theorem .callout-title { color: var(--thm-border); }

.callout-example { background: var(--example-bg); border-color: var(--example-border); }
.callout-example .callout-title { color: var(--example-border); }

/* ===== Code Blocks ===== */
.code-container {
  background: var(--bg-code);
  border: 1px solid var(--border-color);
  border-radius: var(--radius);
  margin: 20px 0;
  overflow: hidden;
}

.code-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 8px 16px;
  background: var(--bg-secondary);
  border-bottom: 1px solid var(--border-color);
  font-size: 13px;
  color: var(--text-muted);
}

.copy-btn {
  background: var(--bg-primary);
  border: 1px solid var(--border-color);
  color: var(--text-secondary);
  padding: 3px 10px;
  border-radius: 6px;
  cursor: pointer;
  font-size: 12px;
  transition: all 0.2s;
}

.copy-btn:hover { background: var(--accent-light); color: var(--accent); }

pre {
  padding: 16px 20px;
  overflow-x: auto;
  font-family: "SF Mono", "Fira Code", Consolas, monospace;
  font-size: 13.5px;
  line-height: 1.6;
}

code {
  font-family: "SF Mono", "Fira Code", Consolas, monospace;
}

/* Inline code */
p code, li code, td code {
  background: var(--bg-code);
  padding: 2px 6px;
  border-radius: 4px;
  font-size: 0.9em;
  border: 1px solid var(--border-color);
}

/* Syntax highlighting */
.kw { color: var(--code-keyword); font-weight: 600; }
.str { color: var(--code-string); }
.cmt { color: var(--code-comment); font-style: italic; }
.fn { color: var(--code-function); }
.num { color: var(--code-number); }
.cls { color: var(--code-class); font-weight: 600; }
.bi { color: var(--code-builtin); }
.dec { color: var(--code-decorator); }
.sf { color: var(--code-self); }

/* ===== Tables ===== */
table {
  width: 100%;
  border-collapse: collapse;
  margin: 20px 0;
  font-size: 14px;
}

th, td {
  padding: 10px 14px;
  border: 1px solid var(--border-color);
  text-align: left;
}

th {
  background: var(--bg-secondary);
  font-weight: 600;
  color: var(--text-primary);
}

tr:nth-child(even) { background: var(--bg-secondary); }

/* ===== Lists ===== */
ul, ol {
  margin: 10px 0 14px 24px;
}

li {
  margin-bottom: 6px;
}

/* ===== SVG Diagrams ===== */
.diagram-container {
  text-align: center;
  margin: 24px 0;
  padding: 20px;
  background: var(--bg-secondary);
  border-radius: var(--radius);
  border: 1px solid var(--border-color);
  overflow-x: auto;
}

.diagram-container svg {
  max-width: 100%;
  height: auto;
}

.diagram-caption {
  margin-top: 10px;
  font-size: 13px;
  color: var(--text-muted);
  font-style: italic;
}

/* ===== Responsive ===== */
@media (max-width: 900px) {
  .sidebar {
    transform: translateX(-100%);
  }
  .sidebar.open {
    transform: translateX(0);
  }
  .main-content {
    margin-left: 0;
    padding: 20px 20px 60px;
  }
  .top-bar {
    left: 0;
  }
  .menu-toggle {
    display: block;
  }
  .hero h1 { font-size: 2em; }
}

@media (max-width: 600px) {
  .hero h1 { font-size: 1.6em; }
  .hero .subtitle { font-size: 1em; }
  pre { font-size: 12px; padding: 12px; }
}

/* ===== Algorithm Box ===== */
.algorithm-box {
  background: var(--bg-secondary);
  border: 2px solid var(--accent);
  border-radius: var(--radius);
  padding: 20px 24px;
  margin: 24px 0;
}

.algorithm-box .algo-title {
  font-weight: 700;
  font-size: 1.1em;
  color: var(--accent);
  margin-bottom: 12px;
  padding-bottom: 8px;
  border-bottom: 1px solid var(--border-color);
}

.algorithm-box ol {
  margin-left: 20px;
}

.algorithm-box li {
  margin-bottom: 8px;
}

/* ===== Comparison Cards ===== */
.comparison-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 16px;
  margin: 20px 0;
}

.comparison-card {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius);
  padding: 18px;
  transition: box-shadow 0.2s;
}

.comparison-card:hover {
  box-shadow: var(--shadow-hover);
}

.comparison-card h4 {
  margin-top: 0;
  color: var(--accent);
  margin-bottom: 8px;
}

/* ===== Checklist ===== */
.checklist {
  list-style: none;
  margin-left: 0;
}

.checklist li {
  padding: 6px 0;
  padding-left: 28px;
  position: relative;
}

.checklist li::before {
  content: "â˜";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-size: 1.1em;
}

/* ===== Scrollbar ===== */
.sidebar::-webkit-scrollbar { width: 6px; }
.sidebar::-webkit-scrollbar-track { background: transparent; }
.sidebar::-webkit-scrollbar-thumb { background: var(--border-color); border-radius: 3px; }

/* ===== Print ===== */
@media print {
  .sidebar, .top-bar, .progress-bar, .menu-toggle { display: none !important; }
  .main-content { margin-left: 0; max-width: 100%; }
}
</style>
</head>
<body>

<!-- Reading Progress Bar -->
<div class="progress-bar" id="progressBar"></div>

<!-- Sidebar Navigation -->
<aside class="sidebar" id="sidebar">
  <div class="sidebar-header">
    <h2>ğŸ¯ PPO æ•™ç¨‹</h2>
    <p>ä»ç­–ç•¥æ¢¯åº¦åˆ°æœºå™¨äººæ§åˆ¶</p>
  </div>
  <nav>
    <div class="chapter-title">å¯¼è®º</div>
    <a href="#overview">æ•™ç¨‹æ¦‚è§ˆ</a>
    <a href="#prereqs">å‰ç½®çŸ¥è¯†</a>
    <a href="#roadmap">å­¦ä¹ è·¯çº¿å›¾</a>

    <div class="chapter-title">ç¬¬ä¸€ç« ï¼šç­–ç•¥æ¢¯åº¦åŸºç¡€</div>
    <a href="#s1-1">1.1 ä¸ºä»€ä¹ˆè¦å­¦ç­–ç•¥æ¢¯åº¦</a>
    <a href="#s1-2">1.2 MDP ä¸ç­–ç•¥å®šä¹‰</a>
    <a href="#s1-3">1.3 ç­–ç•¥æ¢¯åº¦å®šç†æ¨å¯¼</a>
    <a href="#s1-4">1.4 REINFORCE ç®—æ³•</a>
    <a href="#s1-5">1.5 ä»£ç ï¼šCartPole REINFORCE</a>
    <a href="#s1-6">1.6 æ–¹å·®é—®é¢˜ä¸ Baseline</a>

    <div class="chapter-title">ç¬¬äºŒç« ï¼šActor-Critic</div>
    <a href="#s2-1">2.1 ä» REINFORCE åˆ° AC</a>
    <a href="#s2-2">2.2 ä¼˜åŠ¿å‡½æ•° A(s,a)</a>
    <a href="#s2-3">2.3 GAE å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡</a>
    <a href="#s2-4">2.4 ä»£ç ï¼šA2C å®ç°</a>
    <a href="#s2-5">2.5 N-step ä¸ GAE å¯¹æ¯”</a>

    <div class="chapter-title">ç¬¬ä¸‰ç« ï¼šPPO æ ¸å¿ƒ</div>
    <a href="#s3-1">3.1 Trust Region æ€æƒ³</a>
    <a href="#s3-2">3.2 TRPO ç®—æ³•</a>
    <a href="#s3-3">3.3 PPO-Clip å®Œæ•´æ¨å¯¼</a>
    <a href="#s3-4">3.4 é€ç¬¦å·è§£è¯»</a>
    <a href="#s3-5">3.5 ä»£ç ï¼šPPO from Scratch</a>

    <div class="chapter-title">ç¬¬å››ç« ï¼šPPO å®ç°ç»†èŠ‚</div>
    <a href="#s4-1">4.1 Value Loss è®¾è®¡</a>
    <a href="#s4-2">4.2 Entropy Bonus</a>
    <a href="#s4-3">4.3 æ€»æŸå¤±å‡½æ•°</a>
    <a href="#s4-4">4.4 Mini-batch SGD</a>
    <a href="#s4-5">4.5 å½’ä¸€åŒ–ä¸é€€ç«æŠ€å·§</a>
    <a href="#s4-6">4.6 ä»£ç ï¼šæ‰‹å†™ vs SB3</a>

    <div class="chapter-title">ç¬¬äº”ç« ï¼šæœºå™¨äººæ§åˆ¶</div>
    <a href="#s5-1">5.1 è¿ç»­åŠ¨ä½œç©ºé—´</a>
    <a href="#s5-2">5.2 åŠ¨ä½œè£å‰ªä¸ Squashing</a>
    <a href="#s5-3">5.3 è§‚æµ‹å½’ä¸€åŒ–</a>
    <a href="#s5-4">5.4 Reward Shaping</a>
    <a href="#s5-5">5.5 PhysRobot PushBox</a>
    <a href="#s5-6">5.6 ä»£ç ï¼šè¿ç»­æ§åˆ¶ PPO</a>

    <div class="chapter-title">ç¬¬å…­ç« ï¼šé«˜çº§å˜ä½“</div>
    <a href="#s6-1">6.1 PPO-Penalty</a>
    <a href="#s6-2">6.2 MAPPO å¤šæ™ºèƒ½ä½“</a>
    <a href="#s6-3">6.3 PPO + RNN/Transformer</a>
    <a href="#s6-4">6.4 PPO + ç‰©ç†å…ˆéªŒ</a>
    <a href="#s6-5">6.5 PPO + è¯¾ç¨‹å­¦ä¹ </a>

    <div class="chapter-title">ç¬¬ä¸ƒç« ï¼šè¶…å‚æ•°è°ƒä¼˜</div>
    <a href="#s7-1">7.1 å­¦ä¹ ç‡</a>
    <a href="#s7-2">7.2 Batch Size ä¸ N-steps</a>
    <a href="#s7-3">7.3 Clip Range</a>
    <a href="#s7-4">7.4 Gamma ä¸ GAE Lambda</a>
    <a href="#s7-5">7.5 Entropy ç³»æ•°</a>
    <a href="#s7-6">7.6 Failure Modes è¯Šæ–­</a>
    <a href="#s7-7">7.7 å®æˆ˜ Checklist</a>

    <div class="chapter-title">é™„å½•</div>
    <a href="#appA">A: PPO è®ºæ–‡é€æ®µè§£è¯»</a>
    <a href="#appB">B: PPO vs å…¶ä»–ç®—æ³•</a>
    <a href="#references">å‚è€ƒæ–‡çŒ®</a>
  </nav>
</aside>

<!-- Top Bar -->
<div class="top-bar">
  <button class="menu-toggle" onclick="document.getElementById('sidebar').classList.toggle('open')">â˜°</button>
  <button class="btn-theme" onclick="toggleTheme()">ğŸŒ“ ä¸»é¢˜</button>
</div>

<!-- Main Content -->
<div class="main-content">

<!-- ============================================================ -->
<!-- HERO SECTION -->
<!-- ============================================================ -->
<section class="hero">
  <h1>PPO è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–</h1>
  <p class="subtitle">ä»ç­–ç•¥æ¢¯åº¦ç†è®ºåˆ°æœºå™¨äººæ§åˆ¶å®è·µ â€”â€” ä¸€ä»½è¶…è¯¦ç»†çš„ä¸­æ–‡å­¦ä¹ æ•™ç¨‹</p>
  <div class="badges">
    <span class="badge">ğŸ“ å®Œæ•´æ•°å­¦æ¨å¯¼</span>
    <span class="badge">ğŸ PyTorch ä»£ç </span>
    <span class="badge">ğŸ¤– æœºå™¨äººåº”ç”¨</span>
    <span class="badge">ğŸ“Š å¯è§†åŒ–å›¾è§£</span>
    <span class="badge">â± çº¦ 3-4 å°æ—¶</span>
  </div>
</section>

<!-- ============================================================ -->
<!-- OVERVIEW -->
<!-- ============================================================ -->
<h2 id="overview">æ•™ç¨‹æ¦‚è§ˆ</h2>

<p>Proximal Policy Optimization (PPO) æ˜¯ OpenAI åœ¨ 2017 å¹´æå‡ºçš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä¹Ÿæ˜¯å½“å‰å¼ºåŒ–å­¦ä¹ ä¸­åº”ç”¨æœ€å¹¿æ³›çš„ç®—æ³•ä¹‹ä¸€ã€‚ä» Atari æ¸¸æˆåˆ°æœºå™¨äººæ§åˆ¶ï¼Œä» ChatGPT çš„ RLHF åˆ°å¤æ‚çš„å¤šæ™ºèƒ½ä½“åšå¼ˆï¼ŒPPO ä»¥å…¶å‡ºè‰²çš„<strong>ç¨³å®šæ€§</strong>ã€<strong>å®ç°ç®€æ´æ€§</strong>å’Œ<strong>é€šç”¨æ€§</strong>æˆä¸ºäº†äº‹å®ä¸Šçš„è¡Œä¸šæ ‡å‡†ã€‚</p>

<p>æœ¬æ•™ç¨‹é¢å‘æœ‰æ·±åº¦å­¦ä¹ åŸºç¡€ä½†åˆå­¦å¼ºåŒ–å­¦ä¹ çš„è¯»è€…ï¼Œä»æœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦å®šç†å‡ºå‘ï¼Œé€šè¿‡ä¸¥è°¨çš„æ•°å­¦æ¨å¯¼å’Œä¸°å¯Œçš„ä»£ç å®ä¾‹ï¼Œä¸€æ­¥æ­¥æ„å»ºåˆ°å®Œæ•´çš„ PPO ç®—æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœºå™¨äººæ§åˆ¶ä»»åŠ¡ã€‚</p>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ æœ¬æ•™ç¨‹æ¶µç›–</div>
  <ul>
    <li><strong>7 ä¸ªå®Œæ•´ç« èŠ‚</strong>ï¼šç­–ç•¥æ¢¯åº¦ â†’ Actor-Critic â†’ PPO æ ¸å¿ƒ â†’ å®ç°ç»†èŠ‚ â†’ æœºå™¨äººæ§åˆ¶ â†’ é«˜çº§å˜ä½“ â†’ è¶…å‚æ•°è°ƒä¼˜</li>
    <li><strong>2 ä¸ªé™„å½•</strong>ï¼šPPO è®ºæ–‡é€æ®µè§£è¯» + ç®—æ³•å¯¹æ¯”åˆ†æ</li>
    <li><strong>å®Œæ•´ä»£ç </strong>ï¼šREINFORCEã€A2Cã€PPO from scratchã€SB3 å¯¹æ¯”ã€è¿ç»­æ§åˆ¶ç­‰</li>
    <li><strong>æ•°å­¦æ¨å¯¼</strong>ï¼šç­–ç•¥æ¢¯åº¦å®šç†ã€GAEã€PPO-Clip æ¯ä¸€æ­¥éƒ½æœ‰è¯¦ç»†å±•å¼€</li>
    <li><strong>é¡¹ç›®å…³è”</strong>ï¼šä¸ PhysRobot é¡¹ç›®çš„ PushBox è®­ç»ƒåˆ†æç»“åˆ</li>
  </ul>
</div>

<h3 id="prereqs">å‰ç½®çŸ¥è¯†</h3>

<table>
  <tr>
    <th>é¢†åŸŸ</th>
    <th>è¦æ±‚</th>
    <th>æ¨èèµ„æº</th>
  </tr>
  <tr>
    <td>æ·±åº¦å­¦ä¹ </td>
    <td>ç†è§£åå‘ä¼ æ’­ã€æŸå¤±å‡½æ•°ã€SGD</td>
    <td>PyTorch æ•™ç¨‹</td>
  </tr>
  <tr>
    <td>æ¦‚ç‡è®º</td>
    <td>æ¦‚ç‡åˆ†å¸ƒã€æœŸæœ›ã€æ¡ä»¶æ¦‚ç‡</td>
    <td>ç»Ÿè®¡å­¦åŸºç¡€</td>
  </tr>
  <tr>
    <td>Python</td>
    <td>ç±»ã€è£…é¥°å™¨ã€NumPy åŸºæœ¬æ“ä½œ</td>
    <td>Python å®˜æ–¹æ•™ç¨‹</td>
  </tr>
  <tr>
    <td>RL åŸºç¡€</td>
    <td>äº†è§£ MDPã€rewardã€episode æ¦‚å¿µï¼ˆæœ¬æ•™ç¨‹ä¹Ÿä¼šå¤ä¹ ï¼‰</td>
    <td>Sutton & Barto Ch.1-3</td>
  </tr>
</table>

<h3 id="roadmap">å­¦ä¹ è·¯çº¿å›¾</h3>

<div class="diagram-container">
  <svg width="780" height="120" viewBox="0 0 780 120">
    <defs>
      <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
        <path d="M 0 0 L 10 5 L 0 10 Z" fill="var(--accent)"/>
      </marker>
    </defs>
    <!-- Boxes -->
    <rect x="5" y="30" width="95" height="50" rx="8" fill="var(--bg-code)" stroke="var(--accent)" stroke-width="2"/>
    <text x="52" y="60" text-anchor="middle" font-size="12" fill="var(--text-primary)" font-weight="600">ç­–ç•¥æ¢¯åº¦</text>
    <line x1="100" y1="55" x2="115" y2="55" stroke="var(--accent)" stroke-width="2" marker-end="url(#arrow)"/>

    <rect x="120" y="30" width="95" height="50" rx="8" fill="var(--bg-code)" stroke="var(--accent)" stroke-width="2"/>
    <text x="167" y="60" text-anchor="middle" font-size="12" fill="var(--text-primary)" font-weight="600">Actor-Critic</text>
    <line x1="215" y1="55" x2="230" y2="55" stroke="var(--accent)" stroke-width="2" marker-end="url(#arrow)"/>

    <rect x="235" y="30" width="95" height="50" rx="8" fill="var(--bg-code)" stroke="var(--accent)" stroke-width="2"/>
    <text x="282" y="60" text-anchor="middle" font-size="12" fill="var(--text-primary)" font-weight="600">PPO æ ¸å¿ƒ</text>
    <line x1="330" y1="55" x2="345" y2="55" stroke="var(--accent)" stroke-width="2" marker-end="url(#arrow)"/>

    <rect x="350" y="30" width="95" height="50" rx="8" fill="var(--bg-code)" stroke="var(--accent)" stroke-width="2"/>
    <text x="397" y="60" text-anchor="middle" font-size="12" fill="var(--text-primary)" font-weight="600">å®ç°ç»†èŠ‚</text>
    <line x1="445" y1="55" x2="460" y2="55" stroke="var(--accent)" stroke-width="2" marker-end="url(#arrow)"/>

    <rect x="465" y="30" width="95" height="50" rx="8" fill="var(--bg-code)" stroke="var(--accent)" stroke-width="2"/>
    <text x="512" y="60" text-anchor="middle" font-size="12" fill="var(--text-primary)" font-weight="600">æœºå™¨äººæ§åˆ¶</text>
    <line x1="560" y1="55" x2="575" y2="55" stroke="var(--accent)" stroke-width="2" marker-end="url(#arrow)"/>

    <rect x="580" y="30" width="95" height="50" rx="8" fill="var(--bg-code)" stroke="var(--accent)" stroke-width="2"/>
    <text x="627" y="60" text-anchor="middle" font-size="12" fill="var(--text-primary)" font-weight="600">é«˜çº§å˜ä½“</text>
    <line x1="675" y1="55" x2="690" y2="55" stroke="var(--accent)" stroke-width="2" marker-end="url(#arrow)"/>

    <rect x="695" y="30" width="80" height="50" rx="8" fill="var(--accent-light)" stroke="var(--accent)" stroke-width="2"/>
    <text x="735" y="60" text-anchor="middle" font-size="12" fill="var(--accent)" font-weight="700">è°ƒå‚å®æˆ˜</text>
  </svg>
  <div class="diagram-caption">å›¾ï¼šPPO å­¦ä¹ è·¯çº¿ â€” ä»ç†è®ºåŸºç¡€åˆ°å®æˆ˜è°ƒå‚</div>
</div>


<!-- ============================================================ -->
<!-- CHAPTER 1: ç­–ç•¥æ¢¯åº¦åŸºç¡€ -->
<!-- ============================================================ -->
<h2 id="ch1">ç¬¬ä¸€ç« ï¼šç­–ç•¥æ¢¯åº¦åŸºç¡€</h2>

<p>ç­–ç•¥æ¢¯åº¦æ˜¯ä¸€å¤§ç±»å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç»Ÿç§°ã€‚ä¸ DQN è¿™ç±»<strong>åŸºäºå€¼å‡½æ•°</strong>çš„æ–¹æ³•ä¸åŒï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•<strong>ç›´æ¥å‚æ•°åŒ–ç­–ç•¥</strong>ï¼Œå¹¶é€šè¿‡æ¢¯åº¦ä¸Šå‡æ¥æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ã€‚PPO æ­£æ˜¯ç­–ç•¥æ¢¯åº¦å®¶æ—ä¸­æœ€æˆåŠŸçš„æˆå‘˜ä¹‹ä¸€ã€‚</p>

<h3 id="s1-1">1.1 ä¸ºä»€ä¹ˆè¦å­¦ç­–ç•¥æ¢¯åº¦ï¼Ÿ</h3>

<p>åœ¨å¼€å§‹æ•°å­¦æ¨å¯¼ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç†è§£ä¸ºä»€ä¹ˆéœ€è¦ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œä»¥åŠå®ƒç›¸å¯¹äºå€¼å‡½æ•°æ–¹æ³•çš„ä¼˜åŠ¿åœ¨å“ªé‡Œã€‚</p>

<div class="comparison-grid">
  <div class="comparison-card">
    <h4>å€¼å‡½æ•°æ–¹æ³•ï¼ˆDQN ç³»åˆ—ï¼‰</h4>
    <ul>
      <li>å­¦ä¹  $Q(s,a)$ï¼Œç„¶å $\arg\max_a Q(s,a)$</li>
      <li>âœ… é‡‡æ ·æ•ˆç‡è¾ƒé«˜</li>
      <li>âŒ åªèƒ½å¤„ç†ç¦»æ•£åŠ¨ä½œç©ºé—´</li>
      <li>âŒ æ— æ³•å­¦ä¹ éšæœºç­–ç•¥</li>
      <li>âŒ $\arg\max$ å¯èƒ½å¯¼è‡´ç­–ç•¥çªå˜</li>
    </ul>
  </div>
  <div class="comparison-card">
    <h4>ç­–ç•¥æ¢¯åº¦æ–¹æ³•</h4>
    <ul>
      <li>ç›´æ¥å­¦ä¹  $\pi(a|s;\theta)$</li>
      <li>âœ… è‡ªç„¶æ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´</li>
      <li>âœ… å¯ä»¥å­¦ä¹ éšæœºç­–ç•¥ï¼ˆæ¢ç´¢ï¼‰</li>
      <li>âœ… ç­–ç•¥å‚æ•°åŒ–ä¿è¯å¹³æ»‘æ›´æ–°</li>
      <li>âŒ é«˜æ–¹å·®ï¼Œéœ€è¦æ›´å¤šæ ·æœ¬</li>
    </ul>
  </div>
</div>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ å…³é”®ç›´è§‰</div>
  <p>æƒ³è±¡ä½ åœ¨å­¦æ‰“ç¯®çƒã€‚<strong>å€¼å‡½æ•°æ–¹æ³•</strong>ç›¸å½“äºå…ˆè¯„ä¼°æ¯ä¸ªæŠ•ç¯®è§’åº¦çš„å¾—åˆ†æœŸæœ›ï¼Œç„¶åé€‰æœ€é«˜åˆ†çš„è§’åº¦ï¼›<strong>ç­–ç•¥æ¢¯åº¦æ–¹æ³•</strong>åˆ™æ˜¯ç›´æ¥è°ƒæ•´ä½ çš„æŠ•ç¯®åŠ¨ä½œï¼ˆæ‰‹è…•è§’åº¦ã€åŠ›åº¦ç­‰è¿ç»­å‚æ•°ï¼‰ï¼Œé€šè¿‡åå¤ç»ƒä¹ è®©å¥½åŠ¨ä½œå‡ºç°æ¦‚ç‡æ›´é«˜ã€‚å¯¹äºè¿ç»­ã€é«˜ç»´çš„åŠ¨ä½œç©ºé—´ï¼Œåè€…æ˜æ˜¾æ›´è‡ªç„¶ã€‚</p>
</div>

<p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨ä»¥ä¸‹åœºæ™¯ç‰¹åˆ«æœ‰ä¼˜åŠ¿ï¼š</p>

<ul>
  <li><strong>è¿ç»­æ§åˆ¶</strong>ï¼šæœºå™¨äººå…³èŠ‚è§’åº¦ã€åŠ›çŸ©ç­‰éƒ½æ˜¯è¿ç»­å€¼ï¼Œæ— æ³•æšä¸¾æ‰€æœ‰åŠ¨ä½œ</li>
  <li><strong>éšæœºç­–ç•¥</strong>ï¼šæŸäº›åšå¼ˆåœºæ™¯éœ€è¦æ··åˆç­–ç•¥ï¼ˆå¦‚çŸ³å¤´å‰ªåˆ€å¸ƒï¼‰</li>
  <li><strong>é«˜ç»´åŠ¨ä½œç©ºé—´</strong>ï¼šåŠ¨ä½œç»´åº¦å¾ˆé«˜æ—¶ï¼Œ$\arg\max$ è®¡ç®—ä¸å¯è¡Œ</li>
  <li><strong>ç­–ç•¥ç»“æ„å…ˆéªŒ</strong>ï¼šå¯ä»¥è®¾è®¡ç‰¹å®šçš„ç­–ç•¥ç½‘ç»œç»“æ„ï¼ˆå¦‚é«˜æ–¯åˆ†å¸ƒè¾“å‡ºï¼‰</li>
</ul>

<h3 id="s1-2">1.2 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP) ä¸ç­–ç•¥å®šä¹‰</h3>

<p>å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦æ¡†æ¶æ˜¯<strong>é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹</strong> (Markov Decision Process, MDP)ã€‚æˆ‘ä»¬å…ˆç²¾ç¡®å®šä¹‰ MDPï¼Œç„¶åå¼•å‡ºç­–ç•¥çš„å½¢å¼åŒ–å®šä¹‰ã€‚</p>

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ å®šä¹‰ï¼šMDP äº”å…ƒç»„</div>
  <p>ä¸€ä¸ª MDP ç”±äº”å…ƒç»„ $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ å®šä¹‰ï¼š</p>
  <ul>
    <li>$\mathcal{S}$ï¼š<strong>çŠ¶æ€ç©ºé—´</strong>ï¼ˆå¯ä»¥æ˜¯ç¦»æ•£æˆ–è¿ç»­çš„ï¼‰</li>
    <li>$\mathcal{A}$ï¼š<strong>åŠ¨ä½œç©ºé—´</strong>ï¼ˆå¯ä»¥æ˜¯ç¦»æ•£æˆ–è¿ç»­çš„ï¼‰</li>
    <li>$P(s'|s,a)$ï¼š<strong>çŠ¶æ€è½¬ç§»æ¦‚ç‡</strong>ï¼Œæè¿°ç¯å¢ƒçš„åŠ¨æ€</li>
    <li>$R(s,a,s')$ï¼š<strong>å¥–åŠ±å‡½æ•°</strong>ï¼Œä¹Ÿå¸¸å†™æˆ $r(s,a)$</li>
    <li>$\gamma \in [0,1)$ï¼š<strong>æŠ˜æ‰£å› å­</strong>ï¼Œå¹³è¡¡å³æ—¶å›æŠ¥ä¸è¿œæœŸå›æŠ¥</li>
  </ul>
</div>

<p><strong>ç­–ç•¥ (Policy)</strong> æ˜¯æ™ºèƒ½ä½“å†³ç­–çš„æ ¸å¿ƒã€‚å½¢å¼ä¸Šï¼Œç­–ç•¥æ˜¯ä¸€ä¸ªä»çŠ¶æ€åˆ°åŠ¨ä½œåˆ†å¸ƒçš„æ˜ å°„ï¼š</p>

$$\pi(a|s;\theta): \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$$

<p>å…¶ä¸­ $\theta$ æ˜¯ç­–ç•¥çš„å‚æ•°ï¼ˆé€šå¸¸æ˜¯ç¥ç»ç½‘ç»œçš„æƒé‡ï¼‰ã€‚$\pi(a|s;\theta)$ è¡¨ç¤ºåœ¨çŠ¶æ€ $s$ ä¸‹é€‰æ‹©åŠ¨ä½œ $a$ çš„æ¦‚ç‡ã€‚</p>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ ç­–ç•¥çš„ä¸¤ç§ç±»å‹</div>
  <ul>
    <li><strong>ç¡®å®šæ€§ç­–ç•¥</strong>ï¼š$a = \mu(s;\theta)$ï¼Œç»™å®šçŠ¶æ€ç›´æ¥è¾“å‡ºåŠ¨ä½œ</li>
    <li><strong>éšæœºç­–ç•¥</strong>ï¼š$a \sim \pi(\cdot|s;\theta)$ï¼Œè¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»ä¸­é‡‡æ ·</li>
  </ul>
  <p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•é€šå¸¸ä½¿ç”¨<strong>éšæœºç­–ç•¥</strong>ï¼Œå› ä¸ºéšæœºæ€§æä¾›äº†å¤©ç„¶çš„æ¢ç´¢æœºåˆ¶ã€‚</p>
</div>

<h4>è½¨è¿¹ä¸å›æŠ¥</h4>

<p>æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿä¸€æ¡<strong>è½¨è¿¹</strong> (trajectory)ï¼š</p>

$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T)$$

<p>è½¨è¿¹çš„æ¦‚ç‡ç”±ç­–ç•¥å’Œç¯å¢ƒåŠ¨æ€å…±åŒå†³å®šï¼š</p>

$$P(\tau|\theta) = \rho(s_0) \prod_{t=0}^{T-1} \pi(a_t|s_t;\theta) \cdot P(s_{t+1}|s_t,a_t)$$

<p>å…¶ä¸­ $\rho(s_0)$ æ˜¯åˆå§‹çŠ¶æ€åˆ†å¸ƒã€‚è½¨è¿¹çš„<strong>æŠ˜æ‰£å›æŠ¥</strong>å®šä¹‰ä¸ºï¼š</p>

$$R(\tau) = \sum_{t=0}^{T-1} \gamma^t r_t$$

<h4>å…³é”®ä»·å€¼å‡½æ•°</h4>

<p>ä¸‰ä¸ªæ ¸å¿ƒä»·å€¼å‡½æ•°æ„æˆäº†ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„åŸºçŸ³ï¼š</p>

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ ä»·å€¼å‡½æ•°å®šä¹‰</div>
  <p><strong>çŠ¶æ€ä»·å€¼å‡½æ•°</strong>ï¼ˆä»çŠ¶æ€ $s$ å‡ºå‘ã€éµå¾ªç­–ç•¥ $\pi$ èƒ½è·å¾—çš„æœŸæœ›å›æŠ¥ï¼‰ï¼š</p>
  $$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t r_t \;\middle|\; s_0 = s\right]$$
  <p><strong>åŠ¨ä½œä»·å€¼å‡½æ•°</strong>ï¼ˆåœ¨çŠ¶æ€ $s$ æ‰§è¡ŒåŠ¨ä½œ $a$ åã€éµå¾ªç­–ç•¥ $\pi$ çš„æœŸæœ›å›æŠ¥ï¼‰ï¼š</p>
  $$Q^\pi(s,a) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t r_t \;\middle|\; s_0 = s, a_0 = a\right]$$
  <p><strong>ä¼˜åŠ¿å‡½æ•°</strong>ï¼ˆåŠ¨ä½œ $a$ æ¯”"å¹³å‡åŠ¨ä½œ"å¥½å¤šå°‘ï¼‰ï¼š</p>
  $$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$$
</div>

<p>$V^\pi(s)$ è¡¡é‡ä¸€ä¸ªçŠ¶æ€æœ‰å¤š"å¥½"ï¼Œ$Q^\pi(s,a)$ è¡¡é‡ä¸€ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹æœ‰å¤šå¥½ï¼Œ$A^\pi(s,a)$ è¡¡é‡æŸä¸ªç‰¹å®šåŠ¨ä½œç›¸æ¯”å¹³å‡æ°´å¹³çš„<strong>ç›¸å¯¹ä¼˜åŠ¿</strong>ã€‚ä¼˜åŠ¿å‡½æ•°åœ¨åç»­ PPO çš„æ¨å¯¼ä¸­èµ·æ ¸å¿ƒä½œç”¨ã€‚</p>

<h3 id="s1-3">1.3 ç­–ç•¥æ¢¯åº¦å®šç†æ¨å¯¼</h3>

<p>ç°åœ¨æˆ‘ä»¬è¿›å…¥æœ¬ç« çš„æ ¸å¿ƒï¼š<strong>ç­–ç•¥æ¢¯åº¦å®šç†</strong> (Policy Gradient Theorem)ã€‚è¿™ä¸ªå®šç†å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºç­–ç•¥å‚æ•°çš„æ¢¯åº¦ï¼Œæ˜¯æ‰€æœ‰ç­–ç•¥æ¢¯åº¦ç®—æ³•çš„ç†è®ºåŸºç¡€ã€‚</p>

<h4>Step 1: å®šä¹‰ç›®æ ‡å‡½æ•°</h4>

<p>æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥å‚æ•° $\theta^*$ï¼Œä½¿å¾—æœŸæœ›å›æŠ¥æœ€å¤§ï¼š</p>

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T-1} \gamma^t r_t\right]$$

<p>å±•å¼€æœŸæœ›çš„å®šä¹‰ï¼Œè¿™ç­‰ä»·äºå¯¹æ‰€æœ‰å¯èƒ½è½¨è¿¹çš„åŠ æƒæ±‚å’Œï¼š</p>

$$J(\theta) = \int_\tau P(\tau|\theta) R(\tau) \, d\tau$$

<p>å…¶ä¸­ $P(\tau|\theta)$ æ˜¯ç­–ç•¥ $\pi_\theta$ ä¸‹äº§ç”Ÿè½¨è¿¹ $\tau$ çš„æ¦‚ç‡ã€‚</p>

<h4>Step 2: è®¡ç®—æ¢¯åº¦</h4>

<p>æˆ‘ä»¬éœ€è¦æ±‚ $\nabla_\theta J(\theta)$ï¼š</p>

$$\nabla_\theta J(\theta) = \nabla_\theta \int_\tau P(\tau|\theta) R(\tau) \, d\tau = \int_\tau \nabla_\theta P(\tau|\theta) R(\tau) \, d\tau$$

<p>ç›´æ¥è®¡ç®— $\nabla_\theta P(\tau|\theta)$ å¾ˆå›°éš¾ï¼Œå› ä¸ºè½¨è¿¹æ¦‚ç‡æ¶‰åŠå¤æ‚çš„è¿ä¹˜ã€‚è¿™é‡Œå°±éœ€è¦ä¸€ä¸ªå·§å¦™çš„æ•°å­¦æŠ€å·§ã€‚</p>

<h4>Step 3: Log-Derivative Trick</h4>

<p>è¿™æ˜¯ç­–ç•¥æ¢¯åº¦æ¨å¯¼ä¸­æœ€å…³é”®çš„ä¸€æ­¥ã€‚åˆ©ç”¨æ’ç­‰å¼ï¼š</p>

$$\nabla_\theta P(\tau|\theta) = P(\tau|\theta) \cdot \nabla_\theta \log P(\tau|\theta)$$

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ è¯æ˜ï¼šLog-Derivative Trick</div>
  <p>ç”±é“¾å¼æ³•åˆ™ï¼š</p>
  $$\nabla_\theta \log P(\tau|\theta) = \frac{\nabla_\theta P(\tau|\theta)}{P(\tau|\theta)}$$
  <p>å› æ­¤ï¼š</p>
  $$\nabla_\theta P(\tau|\theta) = P(\tau|\theta) \cdot \nabla_\theta \log P(\tau|\theta)$$
  <p>è¿™ä¸ªæ’ç­‰å¼ä¹‹æ‰€ä»¥æœ‰ç”¨ï¼Œæ˜¯å› ä¸ºå®ƒæŠŠå¯¹æ¦‚ç‡çš„æ¢¯åº¦è½¬åŒ–ä¸ºå¯¹ log æ¦‚ç‡çš„æ¢¯åº¦ï¼Œåè€…çš„å½¢å¼æ›´åŠ å‹å¥½ã€‚</p>
</div>

<p>ä»£å…¥æ¢¯åº¦è¡¨è¾¾å¼ï¼š</p>

$$\nabla_\theta J(\theta) = \int_\tau P(\tau|\theta) \cdot \nabla_\theta \log P(\tau|\theta) \cdot R(\tau) \, d\tau$$

$$= \mathbb{E}_{\tau \sim \pi_\theta}\left[\nabla_\theta \log P(\tau|\theta) \cdot R(\tau)\right]$$

<h4>Step 4: å±•å¼€ $\log P(\tau|\theta)$</h4>

<p>å›å¿†è½¨è¿¹æ¦‚ç‡çš„å®šä¹‰ï¼š</p>

$$P(\tau|\theta) = \rho(s_0) \prod_{t=0}^{T-1} \pi(a_t|s_t;\theta) \cdot P(s_{t+1}|s_t,a_t)$$

<p>å–å¯¹æ•°ï¼š</p>

$$\log P(\tau|\theta) = \log \rho(s_0) + \sum_{t=0}^{T-1}\left[\log \pi(a_t|s_t;\theta) + \log P(s_{t+1}|s_t,a_t)\right]$$

<p>å¯¹ $\theta$ æ±‚æ¢¯åº¦æ—¶ï¼Œ$\log \rho(s_0)$ å’Œ $\log P(s_{t+1}|s_t,a_t)$ éƒ½ä¸ $\theta$ æ— å…³ï¼Œå› æ­¤ï¼š</p>

$$\nabla_\theta \log P(\tau|\theta) = \sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t|s_t;\theta)$$

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ é‡è¦æ„ä¹‰</div>
  <p>ç¯å¢ƒåŠ¨æ€ $P(s'|s,a)$ åœ¨æ¢¯åº¦ä¸­æ¶ˆå¤±äº†ï¼è¿™æ„å‘³ç€ç­–ç•¥æ¢¯åº¦æ˜¯ <strong>model-free</strong> çš„ â€”â€” æˆ‘ä»¬ä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„è½¬ç§»æ¦‚ç‡ï¼Œåªéœ€è¦èƒ½ä»ç¯å¢ƒä¸­é‡‡æ ·è½¨è¿¹å³å¯ã€‚è¿™å¯¹äºå¤æ‚ç¯å¢ƒï¼ˆå¦‚çœŸå®æœºå™¨äººï¼‰è‡³å…³é‡è¦ã€‚</p>
</div>

<h4>Step 5: ç­–ç•¥æ¢¯åº¦å®šç†çš„æœ€ç»ˆå½¢å¼</h4>

<p>å°† Step 3 å’Œ Step 4 çš„ç»“æœåˆå¹¶ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š</p>

<div class="callout callout-theorem">
  <div class="callout-title">ğŸ¯ å®šç†ï¼šç­–ç•¥æ¢¯åº¦å®šç† (Policy Gradient Theorem)</div>
  $$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t|s_t;\theta) \cdot R(\tau)\right]$$
  <p>æ›´å¸¸è§çš„å•æ­¥å½¢å¼ï¼ˆåˆ©ç”¨å› æœæ€§ â€”â€” å½“å‰åŠ¨ä½œä¸å½±å“è¿‡å»çš„å›æŠ¥ï¼‰ï¼š</p>
  $$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t|s_t;\theta) \cdot \hat{Q}_t\right]$$
  <p>å…¶ä¸­ $\hat{Q}_t = \sum_{t'=t}^{T-1} \gamma^{t'-t} r_{t'}$ æ˜¯ä»æ—¶é—´æ­¥ $t$ å¼€å§‹çš„ reward-to-goã€‚</p>
</div>

<p><strong>ç›´è§‰è§£é‡Š</strong>ï¼šè¿™ä¸ªå…¬å¼çš„å«ä¹‰éå¸¸ä¼˜ç¾ â€”â€”</p>
<ul>
  <li>$\nabla_\theta \log \pi(a_t|s_t;\theta)$ï¼šå¢åŠ åŠ¨ä½œ $a_t$ æ¦‚ç‡çš„æ–¹å‘</li>
  <li>$\hat{Q}_t$ï¼šè¿™ä¸ªåŠ¨ä½œåç»­è·å¾—çš„å›æŠ¥</li>
  <li>ä¸¤è€…ç›¸ä¹˜ï¼šå¦‚æœä¸€ä¸ªåŠ¨ä½œå¸¦æ¥äº†é«˜å›æŠ¥ï¼Œå°±å¢åŠ å®ƒçš„æ¦‚ç‡ï¼›å¦‚æœå¸¦æ¥ä½å›æŠ¥ï¼Œå°±é™ä½æ¦‚ç‡</li>
</ul>

<p>è¿™æ­£æ˜¯å¼ºåŒ–å­¦ä¹ "è¯•é”™"æ€æƒ³çš„æ•°å­¦è¡¨è¾¾ã€‚</p>

<h4>å› æœæ€§ (Causality) çš„æ¨å¯¼</h4>

<p>ä¸ºä»€ä¹ˆå¯ä»¥ç”¨ reward-to-go $\hat{Q}_t$ ä»£æ›¿å®Œæ•´å›æŠ¥ $R(\tau)$ï¼Ÿå› ä¸ºæ—¶é—´æ­¥ $t$ çš„åŠ¨ä½œä¸å¯èƒ½å½±å“ $t$ ä¹‹å‰çš„å¥–åŠ±ï¼š</p>

$$\mathbb{E}\left[\nabla_\theta \log \pi(a_t|s_t;\theta) \cdot \sum_{t'=0}^{t-1} \gamma^{t'} r_{t'}\right] = 0$$

<p>è¿™å¯ä»¥é€šè¿‡æ¡ä»¶æœŸæœ›æ¥è¯æ˜ï¼šç»™å®š $s_t$ï¼Œ$\nabla_\theta \log \pi(a_t|s_t;\theta)$ çš„æœŸæœ›ä¸ºé›¶ï¼ˆå› ä¸ºæ¦‚ç‡çš„æ¢¯åº¦å’Œä¸ºé›¶ï¼‰ï¼Œè€Œ $t$ ä¹‹å‰çš„å¥–åŠ±åªä¾èµ–äº $s_t$ ä¹‹å‰çš„è½¨è¿¹ã€‚å› æ­¤ï¼Œä½¿ç”¨ reward-to-go å¯ä»¥åœ¨ä¸å¼•å…¥åå·®çš„æƒ…å†µä¸‹é™ä½æ–¹å·®ã€‚</p>

<h3 id="s1-4">1.4 REINFORCE ç®—æ³•</h3>

<p>REINFORCE (Williams, 1992) æ˜¯æœ€ç®€å•çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œç›´æ¥ä½¿ç”¨ Monte Carlo é‡‡æ ·æ¥ä¼°è®¡ç­–ç•¥æ¢¯åº¦ã€‚</p>

<div class="algorithm-box">
  <div class="algo-title">ç®—æ³• 1: REINFORCE (å¸¦ Baseline)</div>
  <ol>
    <li><strong>åˆå§‹åŒ–</strong>ï¼šç­–ç•¥å‚æ•° $\theta$ï¼Œbaseline ç½‘ç»œå‚æ•° $\phi$ï¼ˆå¯é€‰ï¼‰</li>
    <li><strong>å¾ªç¯</strong> æ¯ä¸ª episodeï¼š
      <ol type="a">
        <li>ç”¨å½“å‰ç­–ç•¥ $\pi_\theta$ é‡‡é›†ä¸€æ¡å®Œæ•´è½¨è¿¹ $\tau = (s_0, a_0, r_0, \ldots, s_T)$</li>
        <li>å¯¹æ¯ä¸ªæ—¶é—´æ­¥ $t$ï¼Œè®¡ç®— reward-to-goï¼š$\hat{G}_t = \sum_{t'=t}^{T-1} \gamma^{t'-t} r_{t'}$</li>
        <li>ï¼ˆå¯é€‰ï¼‰è®¡ç®— baselineï¼š$b_t = V_\phi(s_t)$</li>
        <li>è®¡ç®—ç­–ç•¥æ¢¯åº¦ä¼°è®¡ï¼š$\hat{g} = \frac{1}{T}\sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t|s_t;\theta) \cdot (\hat{G}_t - b_t)$</li>
        <li>æ›´æ–°ç­–ç•¥ï¼š$\theta \leftarrow \theta + \alpha \hat{g}$</li>
        <li>ï¼ˆå¯é€‰ï¼‰æ›´æ–° baselineï¼š$\phi \leftarrow \phi - \beta \nabla_\phi \sum_t (V_\phi(s_t) - \hat{G}_t)^2$</li>
      </ol>
    </li>
  </ol>
</div>

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ REINFORCE çš„å±€é™</div>
  <ul>
    <li><strong>é«˜æ–¹å·®</strong>ï¼šMonte Carlo ä¼°è®¡çš„æ–¹å·®å¾ˆå¤§ï¼Œè®­ç»ƒä¸ç¨³å®š</li>
    <li><strong>On-policy</strong>ï¼šæ¯æ¬¡æ›´æ–°ç­–ç•¥åï¼Œä¹‹å‰çš„æ•°æ®å°±å¤±æ•ˆäº†ï¼Œé‡‡æ ·æ•ˆç‡ä½</li>
    <li><strong>éœ€è¦å®Œæ•´ episode</strong>ï¼šå¿…é¡»ç­‰åˆ° episode ç»“æŸæ‰èƒ½æ›´æ–°ï¼Œæ— æ³•åœ¨çº¿å­¦ä¹ </li>
    <li><strong>æ— æ³•åˆ©ç”¨ bootstrapping</strong>ï¼šä¸èƒ½ç”¨å½“å‰ value ä¼°è®¡æ¥è¾…åŠ©è®¡ç®—å›æŠ¥</li>
  </ul>
  <p>è¿™äº›é—®é¢˜å°†åœ¨åç»­çš„ Actor-Critic å’Œ PPO ä¸­å¾—åˆ°è§£å†³ã€‚</p>
</div>

<h3 id="s1-5">1.5 ä»£ç ï¼šCartPole REINFORCE å®Œæ•´å®ç°</h3>

<p>ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯è¿è¡Œçš„ REINFORCE å®ç°ï¼Œç”¨äºè§£å†³ç»å…¸çš„ CartPole é—®é¢˜ã€‚ä»£ç åŒ…å«è¯¦ç»†æ³¨é‡Šï¼Œè§£é‡Šæ¯ä¸€æ­¥ä¸ä¸Šé¢çš„æ•°å­¦å…¬å¼çš„å¯¹åº”å…³ç³»ã€‚</p>

<div class="code-container">
  <div class="code-header">
    <span>reinforce_cartpole.py â€” å®Œæ•´ REINFORCE å®ç°</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.optim <span class="kw">as</span> optim
<span class="kw">from</span> torch.distributions <span class="kw">import</span> Categorical
<span class="kw">import</span> gymnasium <span class="kw">as</span> gym
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> matplotlib.pyplot <span class="kw">as</span> plt

<span class="cmt"># ============================================================</span>
<span class="cmt"># 1. ç­–ç•¥ç½‘ç»œï¼šÏ€(a|s;Î¸)</span>
<span class="cmt"># è¾“å…¥: çŠ¶æ€ s (4ç»´: ä½ç½®, é€Ÿåº¦, è§’åº¦, è§’é€Ÿåº¦)</span>
<span class="cmt"># è¾“å‡º: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ (2ç»´: å·¦, å³)</span>
<span class="cmt"># ============================================================</span>
<span class="kw">class</span> <span class="cls">PolicyNetwork</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, state_dim, action_dim, hidden=<span class="num">128</span>):
        <span class="bi">super</span>().__init__()
        <span class="sf">self</span>.network = nn.Sequential(
            nn.Linear(state_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, action_dim),
            nn.Softmax(dim=-<span class="num">1</span>)   <span class="cmt"># è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ</span>
        )

    <span class="kw">def</span> <span class="fn">forward</span>(<span class="sf">self</span>, state):
        <span class="kw">return</span> <span class="sf">self</span>.network(state)

    <span class="kw">def</span> <span class="fn">get_action</span>(<span class="sf">self</span>, state):
        <span class="str">"""æ ¹æ®ç­–ç•¥é‡‡æ ·åŠ¨ä½œï¼Œå¹¶è¿”å› log æ¦‚ç‡"""</span>
        state = torch.FloatTensor(state).unsqueeze(<span class="num">0</span>)
        probs = <span class="sf">self</span>.forward(state)
        dist = Categorical(probs)        <span class="cmt"># æ„å»ºåˆ†ç±»åˆ†å¸ƒ</span>
        action = dist.sample()           <span class="cmt"># ä» Ï€(Â·|s) é‡‡æ ·</span>
        log_prob = dist.log_prob(action)  <span class="cmt"># log Ï€(a|s;Î¸)</span>
        <span class="kw">return</span> action.item(), log_prob


<span class="cmt"># ============================================================</span>
<span class="cmt"># 2. Baseline ç½‘ç»œ V(s;Ï†)ï¼ˆå¯é€‰ï¼Œç”¨äºæ–¹å·®ç¼©å‡ï¼‰</span>
<span class="cmt"># ============================================================</span>
<span class="kw">class</span> <span class="cls">ValueNetwork</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, state_dim, hidden=<span class="num">128</span>):
        <span class="bi">super</span>().__init__()
        <span class="sf">self</span>.network = nn.Sequential(
            nn.Linear(state_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, <span class="num">1</span>)
        )

    <span class="kw">def</span> <span class="fn">forward</span>(<span class="sf">self</span>, state):
        <span class="kw">return</span> <span class="sf">self</span>.network(state).squeeze(-<span class="num">1</span>)


<span class="cmt"># ============================================================</span>
<span class="cmt"># 3. è®¡ç®—æŠ˜æ‰£ reward-to-go</span>
<span class="cmt"># G_t = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2} + ...</span>
<span class="cmt"># ============================================================</span>
<span class="kw">def</span> <span class="fn">compute_returns</span>(rewards, gamma=<span class="num">0.99</span>):
    <span class="str">"""ä»åå¾€å‰è®¡ç®—æŠ˜æ‰£å›æŠ¥ï¼ˆreward-to-goï¼‰"""</span>
    returns = []
    G = <span class="num">0.0</span>
    <span class="kw">for</span> r <span class="kw">in</span> <span class="bi">reversed</span>(rewards):
        G = r + gamma * G
        returns.insert(<span class="num">0</span>, G)
    returns = torch.FloatTensor(returns)
    <span class="cmt"># æ ‡å‡†åŒ–å¯ä»¥ç¨³å®šè®­ç»ƒï¼ˆä¸å½±å“æ¢¯åº¦æ–¹å‘ï¼Œåªå½±å“æ­¥é•¿ï¼‰</span>
    returns = (returns - returns.mean()) / (returns.std() + <span class="num">1e-8</span>)
    <span class="kw">return</span> returns


<span class="cmt"># ============================================================</span>
<span class="cmt"># 4. è®­ç»ƒå¾ªç¯</span>
<span class="cmt"># ============================================================</span>
<span class="kw">def</span> <span class="fn">train_reinforce</span>(
    env_name=<span class="str">"CartPole-v1"</span>,
    num_episodes=<span class="num">1000</span>,
    gamma=<span class="num">0.99</span>,
    lr_policy=<span class="num">1e-3</span>,
    lr_value=<span class="num">1e-3</span>,
    use_baseline=<span class="num">True</span>,
    seed=<span class="num">42</span>
):
    <span class="cmt"># ç¯å¢ƒè®¾ç½®</span>
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[<span class="num">0</span>]
    action_dim = env.action_space.n

    <span class="cmt"># ç½‘ç»œåˆå§‹åŒ–</span>
    policy = PolicyNetwork(state_dim, action_dim)
    optimizer_policy = optim.Adam(policy.parameters(), lr=lr_policy)

    value_net = <span class="num">None</span>
    optimizer_value = <span class="num">None</span>
    <span class="kw">if</span> use_baseline:
        value_net = ValueNetwork(state_dim)
        optimizer_value = optim.Adam(value_net.parameters(), lr=lr_value)

    <span class="cmt"># è®­ç»ƒè®°å½•</span>
    episode_rewards = []
    running_avg = []

    <span class="kw">for</span> ep <span class="kw">in</span> <span class="bi">range</span>(num_episodes):
        state, _ = env.reset(seed=seed + ep)
        log_probs = []
        rewards = []
        states = []

        <span class="cmt"># ---- é‡‡é›†ä¸€æ¡å®Œæ•´è½¨è¿¹ ----</span>
        done = <span class="num">False</span>
        <span class="kw">while</span> <span class="kw">not</span> done:
            action, log_prob = policy.get_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated <span class="kw">or</span> truncated

            states.append(state)
            log_probs.append(log_prob)
            rewards.append(reward)
            state = next_state

        <span class="cmt"># ---- è®¡ç®—å›æŠ¥ ----</span>
        returns = compute_returns(rewards, gamma)
        log_probs = torch.stack(log_probs)

        <span class="cmt"># ---- è®¡ç®—ç­–ç•¥æ¢¯åº¦ ----</span>
        <span class="kw">if</span> use_baseline <span class="kw">and</span> value_net <span class="kw">is</span> <span class="kw">not</span> <span class="num">None</span>:
            states_tensor = torch.FloatTensor(np.array(states))
            values = value_net(states_tensor).detach()
            advantages = returns - values   <span class="cmt"># A(s,a) â‰ˆ G_t - V(s_t)</span>

            <span class="cmt"># æ›´æ–° value baseline</span>
            v_pred = value_net(states_tensor)
            value_loss = nn.MSELoss()(v_pred, returns)
            optimizer_value.zero_grad()
            value_loss.backward()
            optimizer_value.step()
        <span class="kw">else</span>:
            advantages = returns

        <span class="cmt"># ç­–ç•¥æŸå¤±: -1/T * Î£ log Ï€(a|s) * A</span>
        <span class="cmt"># æ³¨æ„: å–è´Ÿå·æ˜¯å› ä¸º PyTorch é»˜è®¤æœ€å°åŒ–æŸå¤±ï¼Œè€Œæˆ‘ä»¬è¦æœ€å¤§åŒ–å›æŠ¥</span>
        policy_loss = -(log_probs * advantages).mean()

        optimizer_policy.zero_grad()
        policy_loss.backward()
        optimizer_policy.step()

        <span class="cmt"># è®°å½•</span>
        total_reward = <span class="bi">sum</span>(rewards)
        episode_rewards.append(total_reward)
        avg = np.mean(episode_rewards[-<span class="num">100</span>:])
        running_avg.append(avg)

        <span class="kw">if</span> (ep + <span class="num">1</span>) % <span class="num">50</span> == <span class="num">0</span>:
            <span class="bi">print</span>(<span class="str">f"Episode {ep+1:4d} | Reward: {total_reward:6.1f} | Avg100: {avg:6.1f}"</span>)

        <span class="kw">if</span> avg >= <span class="num">475</span>:
            <span class="bi">print</span>(<span class="str">f"\nâœ… ç¯å¢ƒè§£å†³! Episode {ep+1}, Avg100 = {avg:.1f}"</span>)
            <span class="kw">break</span>

    env.close()
    <span class="kw">return</span> episode_rewards, running_avg


<span class="cmt"># ============================================================</span>
<span class="cmt"># 5. è¿è¡Œå¹¶å¯è§†åŒ–</span>
<span class="cmt"># ============================================================</span>
<span class="kw">if</span> __name__ == <span class="str">"__main__"</span>:
    <span class="cmt"># å¯¹æ¯”: æœ‰ baseline vs æ—  baseline</span>
    <span class="bi">print</span>(<span class="str">"=== æ—  Baseline ==="</span>)
    rewards_no_bl, avg_no_bl = train_reinforce(use_baseline=<span class="num">False</span>)

    <span class="bi">print</span>(<span class="str">"\n=== æœ‰ Baseline ==="</span>)
    rewards_bl, avg_bl = train_reinforce(use_baseline=<span class="num">True</span>)

    <span class="cmt"># ç»˜å›¾å¯¹æ¯”</span>
    plt.figure(figsize=(<span class="num">12</span>, <span class="num">5</span>))
    plt.plot(avg_no_bl, label=<span class="str">"REINFORCE (no baseline)"</span>, alpha=<span class="num">0.8</span>)
    plt.plot(avg_bl, label=<span class="str">"REINFORCE (with baseline)"</span>, alpha=<span class="num">0.8</span>)
    plt.xlabel(<span class="str">"Episode"</span>)
    plt.ylabel(<span class="str">"Average Reward (100 ep)"</span>)
    plt.title(<span class="str">"REINFORCE on CartPole-v1"</span>)
    plt.legend()
    plt.grid(<span class="num">True</span>, alpha=<span class="num">0.3</span>)
    plt.tight_layout()
    plt.savefig(<span class="str">"reinforce_cartpole.png"</span>, dpi=<span class="num">150</span>)
    plt.show()
</code></pre>
</div>

<div class="callout callout-exercise">
  <div class="callout-title">ğŸ‹ï¸ ç»ƒä¹  1</div>
  <ol>
    <li>è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œå¯¹æ¯”æœ‰/æ—  baseline çš„è®­ç»ƒæ›²çº¿å·®å¼‚</li>
    <li>å°è¯•è°ƒæ•´ <code>hidden</code> å±‚å¤§å°ï¼ˆ64, 128, 256ï¼‰ï¼Œè§‚å¯Ÿå¯¹æ”¶æ•›é€Ÿåº¦çš„å½±å“</li>
    <li>ä¿®æ”¹ <code>gamma</code> å€¼ï¼ˆ0.9, 0.99, 0.999ï¼‰ï¼Œç†è§£æŠ˜æ‰£å› å­å¯¹é•¿æœŸå›æŠ¥æƒè¡¡çš„ä½œç”¨</li>
    <li>å°†ç¯å¢ƒæ›¿æ¢ä¸º <code>LunarLander-v2</code>ï¼ˆç¦»æ•£åŠ¨ä½œï¼‰ï¼Œè§‚å¯Ÿ REINFORCE çš„è¡¨ç°</li>
  </ol>
</div>

<h3 id="s1-6">1.6 æ–¹å·®é—®é¢˜ä¸ Baseline</h3>

<p>REINFORCE çš„æ ¸å¿ƒé—®é¢˜æ˜¯<strong>é«˜æ–¹å·®</strong>ã€‚ç›´è§‰ä¸Šï¼ŒMonte Carlo é‡‡æ ·çš„å›æŠ¥å¯èƒ½å·®å¼‚å¾ˆå¤§â€”â€”åŒä¸€ä¸ªçŠ¶æ€ï¼Œä¸åŒçš„é‡‡æ ·è½¨è¿¹å¯èƒ½äº§ç”Ÿæˆªç„¶ä¸åŒçš„å›æŠ¥å€¼ã€‚è¿™å¯¼è‡´æ¢¯åº¦ä¼°è®¡æ³¢åŠ¨å‰§çƒˆï¼Œè®­ç»ƒä¸ç¨³å®šã€‚</p>

<h4>æ–¹å·®çš„æ•°å­¦åˆ†æ</h4>

<p>ç­–ç•¥æ¢¯åº¦ä¼°è®¡ $\hat{g} = \nabla_\theta \log \pi(a|s;\theta) \cdot R(\tau)$ çš„æ–¹å·®ä¸ºï¼š</p>

$$\text{Var}[\hat{g}] = \mathbb{E}[\hat{g}^2] - (\mathbb{E}[\hat{g}])^2$$

<p>$R(\tau)$ å¯èƒ½å¾ˆå¤§ï¼ˆæ¯”å¦‚ CartPole çš„ 500 æ­¥ï¼‰ï¼Œå¯¼è‡´ $\hat{g}$ çš„é‡çº§æ³¢åŠ¨å¾ˆå¤§ã€‚</p>

<h4>Baseline çš„å¼•å…¥</h4>

<p>ä¸€ä¸ªå…³é”®çš„æ–¹å·®ç¼©å‡æŠ€å·§æ˜¯å¼•å…¥ <strong>baseline</strong> $b(s)$ï¼š</p>

$$\nabla_\theta J(\theta) = \mathbb{E}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t|s_t;\theta) \cdot \left(\hat{Q}_t - b(s_t)\right)\right]$$

<h4>è¯æ˜ï¼šä»»æ„ baseline ä¸å¼•å…¥åå·®</h4>

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ å®šç†ï¼šBaseline çš„æ— åæ€§</div>
  <p>å¯¹äºä»»ä½•åªä¾èµ–çŠ¶æ€ $s$ çš„å‡½æ•° $b(s)$ï¼Œæœ‰ï¼š</p>
  $$\mathbb{E}_{a \sim \pi}\left[\nabla_\theta \log \pi(a|s;\theta) \cdot b(s)\right] = 0$$
  <p><strong>è¯æ˜</strong>ï¼š</p>
  $$\mathbb{E}_{a \sim \pi}\left[\nabla_\theta \log \pi(a|s;\theta) \cdot b(s)\right] = b(s) \cdot \sum_a \pi(a|s;\theta) \cdot \frac{\nabla_\theta \pi(a|s;\theta)}{\pi(a|s;\theta)}$$
  $$= b(s) \cdot \sum_a \nabla_\theta \pi(a|s;\theta) = b(s) \cdot \nabla_\theta \sum_a \pi(a|s;\theta) = b(s) \cdot \nabla_\theta 1 = 0$$
  <p>ç”±äº $\sum_a \pi(a|s;\theta) = 1$ï¼ˆæ¦‚ç‡å½’ä¸€åŒ–ï¼‰ï¼Œå…¶æ¢¯åº¦ä¸ºé›¶ã€‚$\square$</p>
</div>

<h4>æœ€ä¼˜ Baseline</h4>

<p>ç†è®ºä¸Šï¼Œæœ€ä¼˜ baselineï¼ˆä½¿æ–¹å·®æœ€å°çš„ baselineï¼‰ä¸ºï¼š</p>

$$b^*(s) = \frac{\mathbb{E}\left[\|\nabla_\theta \log \pi\|^2 \cdot R(\tau) \;\middle|\; s\right]}{\mathbb{E}\left[\|\nabla_\theta \log \pi\|^2 \;\middle|\; s\right]}$$

<p>å®é™…ä¸­ï¼Œä½¿ç”¨ $b(s) = V(s)$ï¼ˆçŠ¶æ€ä»·å€¼å‡½æ•°çš„ä¼°è®¡ï¼‰å·²ç»èƒ½æä¾›å¾ˆå¥½çš„æ–¹å·®ç¼©å‡æ•ˆæœï¼Œè€Œä¸”æ›´å®¹æ˜“å®ç°ã€‚æ­¤æ—¶æ¢¯åº¦ä¼°è®¡å˜ä¸ºï¼š</p>

$$\hat{g} = \nabla_\theta \log \pi(a_t|s_t;\theta) \cdot \underbrace{(\hat{Q}_t - V(s_t))}_{\approx A(s_t, a_t)}$$

<p>è¿™å°±æ˜¯<strong>ä¼˜åŠ¿å‡½æ•° (Advantage Function)</strong> çš„é›å½¢ï¼Œä¹Ÿæ˜¯é€šå¾€ Actor-Critic çš„æ¡¥æ¢ã€‚</p>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ æ–¹å·®ç¼©å‡æŠ€å·§æ±‡æ€»</div>
  <table>
    <tr><th>æŠ€å·§</th><th>æ–¹æ³•</th><th>ä»£ä»·</th></tr>
    <tr><td>Reward-to-go</td><td>ç”¨ $\hat{Q}_t$ æ›¿ä»£ $R(\tau)$</td><td>æ— ï¼ˆå‡å°‘æ–¹å·®ä¸”æ— åï¼‰</td></tr>
    <tr><td>Baseline</td><td>å‡å» $b(s)$ï¼ˆé€šå¸¸æ˜¯ $V(s)$ï¼‰</td><td>éœ€è¦é¢å¤–å­¦ä¹  $V(s)$</td></tr>
    <tr><td>æ ‡å‡†åŒ–</td><td>å¯¹å›æŠ¥åš z-score æ ‡å‡†åŒ–</td><td>è½»å¾®åå·®</td></tr>
    <tr><td>å¤šè½¨è¿¹å¹³å‡</td><td>å¢åŠ æ¯æ¬¡æ›´æ–°çš„é‡‡æ ·é‡</td><td>è®¡ç®—æˆæœ¬å¢åŠ </td></tr>
  </table>
</div>


<!-- ============================================================ -->
<!-- CHAPTER 2: ACTOR-CRITIC -->
<!-- ============================================================ -->
<h2 id="ch2">ç¬¬äºŒç« ï¼šActor-Critic æ–¹æ³•</h2>

<p>ä¸Šä¸€ç« æˆ‘ä»¬çœ‹åˆ°ï¼ŒREINFORCE ä½¿ç”¨å®Œæ•´è½¨è¿¹çš„ Monte Carlo å›æŠ¥æ¥ä¼°è®¡ç­–ç•¥æ¢¯åº¦ï¼Œæ–¹å·®å¾ˆå¤§ã€‚<strong>Actor-Critic</strong> æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ä¸ªå­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°æ¥è¾…åŠ©ç­–ç•¥æ¢¯åº¦ä¼°è®¡ï¼Œåœ¨æ–¹å·®å’Œåå·®ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚</p>

<h3 id="s2-1">2.1 ä» REINFORCE åˆ° Actor-Critic</h3>

<p>Actor-Critic æ¶æ„åŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼š</p>

<ul>
  <li><strong>Actorï¼ˆæ¼”å‘˜ï¼‰</strong>ï¼šç­–ç•¥ç½‘ç»œ $\pi(a|s;\theta)$ï¼Œè´Ÿè´£å†³å®šåœ¨æ¯ä¸ªçŠ¶æ€é‡‡å–ä»€ä¹ˆåŠ¨ä½œ</li>
  <li><strong>Criticï¼ˆè¯„è®ºå®¶ï¼‰</strong>ï¼šä»·å€¼ç½‘ç»œ $V(s;\phi)$ï¼Œè¯„ä¼°å½“å‰çŠ¶æ€çš„å¥½åï¼Œä¸º Actor æä¾›æ¢¯åº¦ä¿¡å·</li>
</ul>

<div class="diagram-container">
  <svg width="500" height="280" viewBox="0 0 500 280">
    <!-- Environment -->
    <rect x="180" y="10" width="140" height="50" rx="10" fill="var(--tip-bg)" stroke="var(--tip-border)" stroke-width="2"/>
    <text x="250" y="40" text-anchor="middle" font-size="14" fill="var(--text-primary)" font-weight="600">ç¯å¢ƒ (Environment)</text>

    <!-- State arrow down -->
    <line x1="210" y1="60" x2="210" y2="100" stroke="var(--text-muted)" stroke-width="2" marker-end="url(#arrow)"/>
    <text x="195" y="85" font-size="11" fill="var(--text-muted)">s, r</text>

    <!-- Action arrow up -->
    <line x1="290" y1="100" x2="290" y2="60" stroke="var(--accent)" stroke-width="2" marker-end="url(#arrow)"/>
    <text x="300" y="85" font-size="11" fill="var(--accent)">a</text>

    <!-- Actor -->
    <rect x="40" y="110" width="180" height="60" rx="10" fill="var(--example-bg)" stroke="var(--example-border)" stroke-width="2"/>
    <text x="130" y="137" text-anchor="middle" font-size="13" fill="var(--text-primary)" font-weight="600">Actor: Ï€(a|s;Î¸)</text>
    <text x="130" y="157" text-anchor="middle" font-size="11" fill="var(--text-secondary)">é€‰æ‹©åŠ¨ä½œ</text>

    <!-- Critic -->
    <rect x="280" y="110" width="180" height="60" rx="10" fill="var(--def-bg)" stroke="var(--def-border)" stroke-width="2"/>
    <text x="370" y="137" text-anchor="middle" font-size="13" fill="var(--text-primary)" font-weight="600">Critic: V(s;Ï†)</text>
    <text x="370" y="157" text-anchor="middle" font-size="11" fill="var(--text-secondary)">è¯„ä¼°çŠ¶æ€</text>

    <!-- TD Error -->
    <rect x="160" y="210" width="180" height="50" rx="10" fill="var(--warn-bg)" stroke="var(--warn-border)" stroke-width="2"/>
    <text x="250" y="233" text-anchor="middle" font-size="12" fill="var(--text-primary)" font-weight="600">Î´ = r + Î³V(s') - V(s)</text>
    <text x="250" y="250" text-anchor="middle" font-size="11" fill="var(--text-secondary)">TD Error â†’ ä¼˜åŠ¿ä¼°è®¡</text>

    <!-- Arrows from Actor/Critic to TD -->
    <line x1="130" y1="170" x2="200" y2="210" stroke="var(--text-muted)" stroke-width="1.5" stroke-dasharray="4"/>
    <line x1="370" y1="170" x2="300" y2="210" stroke="var(--text-muted)" stroke-width="1.5" stroke-dasharray="4"/>
  </svg>
  <div class="diagram-caption">å›¾ 2.1: Actor-Critic æ¶æ„ â€” Actor é€‰æ‹©åŠ¨ä½œï¼ŒCritic è¯„ä¼°çŠ¶æ€ï¼ŒTD Error è¿æ¥äºŒè€…</div>
</div>

<p>REINFORCE ä¸ Actor-Critic çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºå¦‚ä½•ä¼°è®¡"åŠ¨ä½œçš„å¥½å"ï¼š</p>

<table>
  <tr><th>æ–¹æ³•</th><th>æ¢¯åº¦ä¸­çš„ä¿¡å·</th><th>åå·®</th><th>æ–¹å·®</th></tr>
  <tr><td>REINFORCE</td><td>$\hat{G}_t = \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}$ï¼ˆMonte Carlo å›æŠ¥ï¼‰</td><td>æ— å</td><td>é«˜</td></tr>
  <tr><td>Actor-Critic</td><td>$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ï¼ˆTD errorï¼‰</td><td>æœ‰åï¼ˆä¾èµ– $V$ çš„å‡†ç¡®æ€§ï¼‰</td><td>ä½</td></tr>
</table>

<h4>Actor-Critic çš„æ¢¯åº¦æ›´æ–°</h4>

<p>ç­–ç•¥æ¢¯åº¦ä½¿ç”¨ TD error ä½œä¸ºä¼˜åŠ¿ä¼°è®¡ï¼š</p>

$$\nabla_\theta J(\theta) \approx \mathbb{E}\left[\nabla_\theta \log \pi(a_t|s_t;\theta) \cdot \delta_t\right]$$

<p>å…¶ä¸­ <strong>TD error</strong> $\delta_t$ æ˜¯ Critic æä¾›çš„"åŠ¨ä½œå¥½å"çš„ä¿¡å·ï¼š</p>

$$\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)$$

<h4>ä¸ºä»€ä¹ˆ TD error å¯ä»¥ä½œä¸ºä¼˜åŠ¿ä¼°è®¡ï¼Ÿ</h4>

<p>å½“ $V_\phi \approx V^\pi$ æ—¶ï¼š</p>

$$\mathbb{E}[\delta_t | s_t, a_t] = \mathbb{E}[r_t + \gamma V^\pi(s_{t+1}) | s_t, a_t] - V^\pi(s_t) = Q^\pi(s_t, a_t) - V^\pi(s_t) = A^\pi(s_t, a_t)$$

<p>å³ TD error çš„æ¡ä»¶æœŸæœ›å°±æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼è™½ç„¶å•ä¸ª TD error æ˜¯æœ‰åçš„ï¼ˆå› ä¸º $V_\phi$ ä¸å®Œç¾ï¼‰ï¼Œä½†å®ƒæ˜¯ä¼˜åŠ¿å‡½æ•°çš„æ— åä¼°è®¡ï¼ˆåœ¨ Critic å®Œç¾çš„å‡è®¾ä¸‹ï¼‰ã€‚</p>

<h3 id="s2-2">2.2 ä¼˜åŠ¿å‡½æ•° $A(s,a)$ æ·±å…¥åˆ†æ</h3>

<p>ä¼˜åŠ¿å‡½æ•°åœ¨ PPO ä¸­èµ·æ ¸å¿ƒä½œç”¨ï¼Œå€¼å¾—æ·±å…¥ç†è§£ã€‚</p>

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ ä¼˜åŠ¿å‡½æ•°çš„æ€§è´¨</div>
  <ol>
    <li><strong>å®šä¹‰</strong>ï¼š$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$</li>
    <li><strong>é›¶å‡å€¼</strong>ï¼š$\mathbb{E}_{a \sim \pi}[A^\pi(s,a)] = \sum_a \pi(a|s)[Q^\pi(s,a) - V^\pi(s)] = V^\pi(s) - V^\pi(s) = 0$</li>
    <li><strong>æ­£å€¼ â†’ å¥½åŠ¨ä½œ</strong>ï¼š$A > 0$ è¡¨ç¤ºè¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å¥½</li>
    <li><strong>è´Ÿå€¼ â†’ å·®åŠ¨ä½œ</strong>ï¼š$A < 0$ è¡¨ç¤ºè¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å·®</li>
  </ol>
</div>

<p>ä½¿ç”¨ä¼˜åŠ¿å‡½æ•°ä»£æ›¿ Q å€¼æˆ–å®Œæ•´å›æŠ¥ï¼Œæœ‰ä¸¤ä¸ªå¥½å¤„ï¼š</p>

<ul>
  <li><strong>æ–¹å·®æ›´ä½</strong>ï¼šå‡å» $V(s)$ ç›¸å½“äºä½¿ç”¨äº†ä¸€ä¸ªå¥½çš„ baseline</li>
  <li><strong>æ¢¯åº¦ä¿¡å·æ›´æ¸…æ™°</strong>ï¼šç›´æ¥å‘Šè¯‰æˆ‘ä»¬"è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½å¤šå°‘"ï¼Œè€Œä¸æ˜¯ç»å¯¹å¥½å</li>
</ul>

<h4>ä¸åŒçš„ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•</h4>

<table>
  <tr><th>æ–¹æ³•</th><th>å…¬å¼</th><th>åå·®</th><th>æ–¹å·®</th></tr>
  <tr><td>Monte Carlo</td><td>$\hat{A}_t^{MC} = \hat{G}_t - V(s_t)$</td><td>ä½ï¼ˆV çš„åå·®ï¼‰</td><td>é«˜</td></tr>
  <tr><td>1-step TD</td><td>$\hat{A}_t^{(1)} = r_t + \gamma V(s_{t+1}) - V(s_t)$</td><td>é«˜ï¼ˆå¼ºä¾èµ– V å‡†ç¡®æ€§ï¼‰</td><td>ä½</td></tr>
  <tr><td>N-step TD</td><td>$\hat{A}_t^{(n)} = \sum_{k=0}^{n-1}\gamma^k r_{t+k} + \gamma^n V(s_{t+n}) - V(s_t)$</td><td>ä¸­</td><td>ä¸­</td></tr>
  <tr><td>GAE</td><td>$\hat{A}_t^{GAE} = \sum_{l=0}^{\infty}(\gamma\lambda)^l \delta_{t+l}$</td><td>å¯è°ƒï¼ˆ$\lambda$ï¼‰</td><td>å¯è°ƒï¼ˆ$\lambda$ï¼‰</td></tr>
</table>

<h4>TD æ®‹å·®ä½œä¸ºä¼˜åŠ¿ä¼°è®¡</h4>

<p>1-step TD æ®‹å·® (Temporal Difference residual) æ˜¯æœ€ç®€å•çš„ä¼˜åŠ¿ä¼°è®¡ï¼š</p>

$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

<p>ç‰©ç†å«ä¹‰ï¼š$V(s_t)$ æ˜¯ Critic å¯¹çŠ¶æ€ $s_t$ çš„æœŸæœ›å›æŠ¥é¢„æµ‹ï¼Œ$r_t + \gamma V(s_{t+1})$ æ˜¯"å®é™…ä¸€æ­¥å›æŠ¥ + å¯¹ä¸‹ä¸€çŠ¶æ€çš„é¢„æµ‹"ã€‚ä¸¤è€…ä¹‹å·®åæ˜ äº†å®é™…ä½“éªŒä¸é¢„æœŸä¹‹é—´çš„<strong>æƒŠè®¶ç¨‹åº¦</strong>ã€‚</p>

<ul>
  <li>$\delta_t > 0$ï¼šå®é™…ä½“éªŒæ¯”é¢„æœŸå¥½ â†’ è¿™ä¸ªåŠ¨ä½œæ˜¯å¥½çš„</li>
  <li>$\delta_t < 0$ï¼šå®é™…ä½“éªŒæ¯”é¢„æœŸå·® â†’ è¿™ä¸ªåŠ¨ä½œæ˜¯å·®çš„</li>
  <li>$\delta_t \approx 0$ï¼šä¸é¢„æœŸä¸€è‡´ â†’ è¿™æ˜¯ä¸€ä¸ª"å¹³å‡"åŠ¨ä½œ</li>
</ul>

<h3 id="s2-3">2.3 GAEï¼šå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡</h3>

<p>GAE (Generalized Advantage Estimation, Schulman et al., 2015) æ˜¯ PPO ä¸­é»˜è®¤ä½¿ç”¨çš„ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•ã€‚å®ƒæä¾›äº†ä¸€ç§ä¼˜é›…çš„æ–¹å¼æ¥åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</p>

<h4>Step 1: å®šä¹‰ä¸åŒæ­¥æ•°çš„ä¼˜åŠ¿ä¼°è®¡å™¨</h4>

<p>é¦–å…ˆï¼Œå®šä¹‰ $n$-step ä¼˜åŠ¿ä¼°è®¡ï¼š</p>

$$\hat{A}_t^{(1)} = \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$
$$\hat{A}_t^{(2)} = \delta_t + \gamma\delta_{t+1} = r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) - V(s_t)$$
$$\hat{A}_t^{(3)} = \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2}$$

<p>ä¸€èˆ¬åœ°ï¼š</p>

$$\hat{A}_t^{(n)} = \sum_{l=0}^{n-1} \gamma^l \delta_{t+l}$$

<h4>Step 2: æŒ‡æ•°åŠ æƒå¹³å‡</h4>

<p>GAE æ˜¯æ‰€æœ‰ $n$-step ä¼°è®¡çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼š</p>

$$\hat{A}_t^{GAE(\gamma,\lambda)} = (1-\lambda)\left[\hat{A}_t^{(1)} + \lambda\hat{A}_t^{(2)} + \lambda^2\hat{A}_t^{(3)} + \cdots\right]$$

<p>$\lambda \in [0, 1]$ æ˜¯æƒè¡¡å‚æ•°ï¼Œ$(1-\lambda)$ æ˜¯å½’ä¸€åŒ–ç³»æ•°ã€‚</p>

<h4>Step 3: åŒ–ç®€ä¸ºé€’æ¨å½¢å¼</h4>

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ GAE çš„ç­‰ä»·é€’æ¨å½¢å¼</div>
  <p>ç»è¿‡å±•å¼€å’ŒåŒ–ç®€ï¼ŒGAE å¯ä»¥è¡¨è¾¾ä¸º TD æ®‹å·®çš„æŒ‡æ•°åŠ æƒå’Œï¼š</p>
  $$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$
  <p>æˆ–ç­‰ä»·çš„é€’æ¨å½¢å¼ï¼ˆä»åå‘å‰è®¡ç®—ï¼‰ï¼š</p>
  $$\hat{A}_t^{GAE} = \delta_t + \gamma\lambda \cdot \hat{A}_{t+1}^{GAE}$$
</div>

<h4>å®Œæ•´æ¨å¯¼</h4>

<p>å°†å®šä¹‰å±•å¼€ï¼š</p>

$$\hat{A}_t^{GAE} = (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}\hat{A}_t^{(n)}$$

$$= (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}\sum_{l=0}^{n-1}\gamma^l\delta_{t+l}$$

<p>äº¤æ¢æ±‚å’Œé¡ºåºï¼ˆå…ˆå¯¹ $l$ æ±‚å’Œï¼‰ï¼š</p>

$$= (1-\lambda)\sum_{l=0}^{\infty}\gamma^l\delta_{t+l}\sum_{n=l+1}^{\infty}\lambda^{n-1}$$

$$= (1-\lambda)\sum_{l=0}^{\infty}\gamma^l\delta_{t+l}\cdot\frac{\lambda^l}{1-\lambda}$$

$$= \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}$$

<p>è¿™ä¸ªç»“æœéå¸¸ä¼˜é›…ï¼šGAE å°±æ˜¯ TD æ®‹å·®ä»¥ $\gamma\lambda$ ä¸ºè¡°å‡å› å­çš„åŠ æƒå’Œã€‚</p>

<h4>$\lambda$ çš„ç›´è§‰ç†è§£</h4>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ $\lambda$ çš„åå·®-æ–¹å·®æƒè¡¡</div>
  <ul>
    <li><strong>$\lambda = 0$</strong>ï¼š$\hat{A}_t = \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$<br>
      â†’ åªç”¨ 1-step TDï¼Œ<strong>ä½æ–¹å·®ä½†é«˜åå·®</strong>ï¼ˆå®Œå…¨ä¾èµ– V çš„å‡†ç¡®æ€§ï¼‰</li>
    <li><strong>$\lambda = 1$</strong>ï¼š$\hat{A}_t = \sum_{l=0}^{\infty}\gamma^l\delta_{t+l} = \hat{G}_t - V(s_t)$<br>
      â†’ ç­‰ä»·äº Monte Carloï¼Œ<strong>æ— åä½†é«˜æ–¹å·®</strong></li>
    <li><strong>$\lambda \in (0,1)$</strong>ï¼šåœ¨ä¸¤ä¸ªæç«¯ä¹‹é—´å¹³æ»‘æ’å€¼<br>
      â†’ <strong>PPO é»˜è®¤ $\lambda = 0.95$</strong>ï¼Œæ¥è¿‘ Monte Carlo ä½†æœ‰ä¸€å®šæ–¹å·®ç¼©å‡</li>
  </ul>
</div>

<h4>é€’æ¨è®¡ç®—å®ç°</h4>

<p>åœ¨å®é™…å®ç°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨é€’æ¨å…¬å¼ä»åå‘å‰é«˜æ•ˆè®¡ç®— GAEï¼š</p>

<div class="code-container">
  <div class="code-header">
    <span>gae_computation.py â€” GAE é€’æ¨è®¡ç®—</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">def</span> <span class="fn">compute_gae</span>(rewards, values, dones, next_value, gamma=<span class="num">0.99</span>, lam=<span class="num">0.95</span>):
    <span class="str">"""
    è®¡ç®— Generalized Advantage Estimation (GAE)

    å‚æ•°:
        rewards:    [T] æ¯æ­¥å¥–åŠ±
        values:     [T] V(s_t) çš„ä¼°è®¡å€¼
        dones:      [T] æ˜¯å¦ç»ˆæ­¢ (0 æˆ– 1)
        next_value: V(s_T) æœ€åä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼
        gamma:      æŠ˜æ‰£å› å­
        lam:        GAE lambda (åå·®-æ–¹å·®æƒè¡¡)

    è¿”å›:
        advantages: [T] GAE ä¼˜åŠ¿ä¼°è®¡
        returns:    [T] ç›®æ ‡å›æŠ¥ (advantages + values)
    """</span>
    T = <span class="bi">len</span>(rewards)
    advantages = np.zeros(T)
    last_gae = <span class="num">0.0</span>

    <span class="kw">for</span> t <span class="kw">in</span> <span class="bi">reversed</span>(<span class="bi">range</span>(T)):
        <span class="cmt"># å¤„ç† episode è¾¹ç•Œ</span>
        <span class="kw">if</span> t == T - <span class="num">1</span>:
            next_val = next_value
        <span class="kw">else</span>:
            next_val = values[t + <span class="num">1</span>]

        non_terminal = <span class="num">1.0</span> - dones[t]

        <span class="cmt"># Î´_t = r_t + Î³Â·V(s_{t+1}) - V(s_t)</span>
        delta = rewards[t] + gamma * next_val * non_terminal - values[t]

        <span class="cmt"># A_t^GAE = Î´_t + Î³Î» Â· A_{t+1}^GAE</span>
        advantages[t] = last_gae = delta + gamma * lam * non_terminal * last_gae

    returns = advantages + values  <span class="cmt"># ç›®æ ‡å›æŠ¥ = ä¼˜åŠ¿ + ä»·å€¼</span>
    <span class="kw">return</span> advantages, returns


<span class="cmt"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cmt"># å‡è®¾ä¸€ä¸ª 5 æ­¥çš„å° episode</span>
rewards = np.array([<span class="num">1.0</span>, <span class="num">0.0</span>, <span class="num">-1.0</span>, <span class="num">2.0</span>, <span class="num">1.0</span>])
values  = np.array([<span class="num">0.5</span>, <span class="num">0.8</span>, <span class="num">0.3</span>, <span class="num">1.5</span>, <span class="num">0.7</span>])
dones   = np.array([<span class="num">0.0</span>, <span class="num">0.0</span>, <span class="num">0.0</span>, <span class="num">0.0</span>, <span class="num">1.0</span>])
next_value = <span class="num">0.0</span>  <span class="cmt"># ç»ˆæ­¢çŠ¶æ€ä»·å€¼ä¸º 0</span>

advantages, returns = compute_gae(rewards, values, dones, next_value)
<span class="bi">print</span>(<span class="str">"Advantages:"</span>, advantages)
<span class="bi">print</span>(<span class="str">"Returns:"</span>, returns)
</code></pre>
</div>

<h3 id="s2-4">2.4 ä»£ç ï¼šA2C å®Œæ•´å®ç°</h3>

<p>Advantage Actor-Critic (A2C) æ˜¯ Actor-Critic çš„åŒæ­¥ç‰ˆæœ¬ï¼Œä¹Ÿæ˜¯ PPO çš„ç›´æ¥å‰èº«ã€‚ç†è§£ A2C çš„å®ç°å¯¹äºç†è§£ PPO è‡³å…³é‡è¦ã€‚</p>

<div class="code-container">
  <div class="code-header">
    <span>a2c_cartpole.py â€” å®Œæ•´ A2C å®ç°</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.optim <span class="kw">as</span> optim
<span class="kw">from</span> torch.distributions <span class="kw">import</span> Categorical
<span class="kw">import</span> gymnasium <span class="kw">as</span> gym
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cmt"># ============================================================</span>
<span class="cmt"># 1. Actor-Critic å…±äº«ç½‘ç»œ</span>
<span class="cmt"># å…±äº«åº•å±‚ç‰¹å¾æå–å™¨ï¼Œåˆ†åˆ«è¾“å‡ºç­–ç•¥å’Œä»·å€¼</span>
<span class="cmt"># ============================================================</span>
<span class="kw">class</span> <span class="cls">ActorCritic</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, state_dim, action_dim, hidden=<span class="num">128</span>):
        <span class="bi">super</span>().__init__()
        <span class="cmt"># å…±äº«ç‰¹å¾æå–å™¨</span>
        <span class="sf">self</span>.shared = nn.Sequential(
            nn.Linear(state_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU()
        )
        <span class="cmt"># Actor å¤´ï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡</span>
        <span class="sf">self</span>.actor_head = nn.Sequential(
            nn.Linear(hidden, action_dim),
            nn.Softmax(dim=-<span class="num">1</span>)
        )
        <span class="cmt"># Critic å¤´ï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼</span>
        <span class="sf">self</span>.critic_head = nn.Linear(hidden, <span class="num">1</span>)

    <span class="kw">def</span> <span class="fn">forward</span>(<span class="sf">self</span>, state):
        features = <span class="sf">self</span>.shared(state)
        policy = <span class="sf">self</span>.actor_head(features)
        value = <span class="sf">self</span>.critic_head(features).squeeze(-<span class="num">1</span>)
        <span class="kw">return</span> policy, value

    <span class="kw">def</span> <span class="fn">get_action</span>(<span class="sf">self</span>, state):
        state = torch.FloatTensor(state).unsqueeze(<span class="num">0</span>)
        probs, value = <span class="sf">self</span>.forward(state)
        dist = Categorical(probs)
        action = dist.sample()
        <span class="kw">return</span> (action.item(), dist.log_prob(action),
                dist.entropy(), value)


<span class="cmt"># ============================================================</span>
<span class="cmt"># 2. A2C Agent</span>
<span class="cmt"># ============================================================</span>
<span class="kw">class</span> <span class="cls">A2CAgent</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, state_dim, action_dim,
                 lr=<span class="num">7e-4</span>, gamma=<span class="num">0.99</span>, lam=<span class="num">0.95</span>,
                 value_coef=<span class="num">0.5</span>, entropy_coef=<span class="num">0.01</span>,
                 n_steps=<span class="num">5</span>, max_grad_norm=<span class="num">0.5</span>):
        <span class="sf">self</span>.model = ActorCritic(state_dim, action_dim)
        <span class="sf">self</span>.optimizer = optim.Adam(<span class="sf">self</span>.model.parameters(), lr=lr)
        <span class="sf">self</span>.gamma = gamma
        <span class="sf">self</span>.lam = lam
        <span class="sf">self</span>.value_coef = value_coef
        <span class="sf">self</span>.entropy_coef = entropy_coef
        <span class="sf">self</span>.n_steps = n_steps
        <span class="sf">self</span>.max_grad_norm = max_grad_norm

    <span class="kw">def</span> <span class="fn">compute_gae</span>(<span class="sf">self</span>, rewards, values, dones, next_value):
        <span class="str">"""è®¡ç®— GAE ä¼˜åŠ¿ä¼°è®¡"""</span>
        T = <span class="bi">len</span>(rewards)
        advantages = np.zeros(T)
        last_gae = <span class="num">0.0</span>
        <span class="kw">for</span> t <span class="kw">in</span> <span class="bi">reversed</span>(<span class="bi">range</span>(T)):
            next_val = next_value <span class="kw">if</span> t == T - <span class="num">1</span> <span class="kw">else</span> values[t + <span class="num">1</span>]
            non_terminal = <span class="num">1.0</span> - dones[t]
            delta = rewards[t] + <span class="sf">self</span>.gamma * next_val * non_terminal - values[t]
            advantages[t] = last_gae = delta + <span class="sf">self</span>.gamma * <span class="sf">self</span>.lam * non_terminal * last_gae
        returns = advantages + np.array(values)
        <span class="kw">return</span> advantages, returns

    <span class="kw">def</span> <span class="fn">update</span>(<span class="sf">self</span>, states, actions, log_probs, rewards, dones,
               values, entropies, next_value):
        <span class="str">"""A2C å•æ­¥æ›´æ–°"""</span>
        <span class="cmt"># è®¡ç®— GAE</span>
        values_np = [v.item() <span class="kw">for</span> v <span class="kw">in</span> values]
        advantages, returns = <span class="sf">self</span>.compute_gae(
            rewards, values_np, dones, next_value
        )

        <span class="cmt"># è½¬ä¸º tensor</span>
        advantages = torch.FloatTensor(advantages)
        returns = torch.FloatTensor(returns)
        log_probs = torch.stack(log_probs)
        entropies = torch.stack(entropies)
        values_t = torch.stack(values)

        <span class="cmt"># æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼ˆå…³é”®æŠ€å·§ï¼ï¼‰</span>
        advantages = (advantages - advantages.mean()) / (advantages.std() + <span class="num">1e-8</span>)

        <span class="cmt"># ä¸‰ä¸ªæŸå¤±åˆ†é‡</span>
        <span class="cmt"># 1. ç­–ç•¥æŸå¤±: -E[log Ï€(a|s) Â· A(s,a)]</span>
        policy_loss = -(log_probs * advantages.detach()).mean()

        <span class="cmt"># 2. ä»·å€¼æŸå¤±: E[(V(s) - R)Â²]</span>
        value_loss = nn.MSELoss()(values_t, returns.detach())

        <span class="cmt"># 3. ç†µå¥–åŠ±: -E[H(Ï€)]  (é¼“åŠ±æ¢ç´¢)</span>
        entropy_loss = -entropies.mean()

        <span class="cmt"># æ€»æŸå¤± = ç­–ç•¥æŸå¤± + câ‚Â·ä»·å€¼æŸå¤± + câ‚‚Â·ç†µæŸå¤±</span>
        total_loss = (policy_loss
                      + <span class="sf">self</span>.value_coef * value_loss
                      + <span class="sf">self</span>.entropy_coef * entropy_loss)

        <span class="cmt"># åå‘ä¼ æ’­</span>
        <span class="sf">self</span>.optimizer.zero_grad()
        total_loss.backward()
        nn.utils.clip_grad_norm_(<span class="sf">self</span>.model.parameters(), <span class="sf">self</span>.max_grad_norm)
        <span class="sf">self</span>.optimizer.step()

        <span class="kw">return</span> {
            <span class="str">"policy_loss"</span>: policy_loss.item(),
            <span class="str">"value_loss"</span>: value_loss.item(),
            <span class="str">"entropy"</span>: -entropy_loss.item()
        }


<span class="cmt"># ============================================================</span>
<span class="cmt"># 3. è®­ç»ƒå¾ªç¯</span>
<span class="cmt"># ============================================================</span>
<span class="kw">def</span> <span class="fn">train_a2c</span>(env_name=<span class="str">"CartPole-v1"</span>, total_steps=<span class="num">200000</span>, n_steps=<span class="num">5</span>):
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[<span class="num">0</span>]
    action_dim = env.action_space.n

    agent = A2CAgent(state_dim, action_dim, n_steps=n_steps)

    state, _ = env.reset()
    episode_reward = <span class="num">0</span>
    episode_rewards = []
    step = <span class="num">0</span>

    <span class="kw">while</span> step < total_steps:
        <span class="cmt"># æ”¶é›† n_steps çš„ç»éªŒ</span>
        states, actions, log_probs = [], [], []
        rewards, dones, values, entropies = [], [], [], []

        <span class="kw">for</span> _ <span class="kw">in</span> <span class="bi">range</span>(n_steps):
            action, log_prob, entropy, value = agent.model.get_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated <span class="kw">or</span> truncated

            states.append(state)
            actions.append(action)
            log_probs.append(log_prob)
            rewards.append(reward)
            dones.append(<span class="bi">float</span>(done))
            values.append(value)
            entropies.append(entropy)

            episode_reward += reward
            state = next_state
            step += <span class="num">1</span>

            <span class="kw">if</span> done:
                episode_rewards.append(episode_reward)
                episode_reward = <span class="num">0</span>
                state, _ = env.reset()

        <span class="cmt"># è®¡ç®— next_value ç”¨äº bootstrap</span>
        <span class="kw">with</span> torch.no_grad():
            _, next_value = agent.model(
                torch.FloatTensor(state).unsqueeze(<span class="num">0</span>))
            next_value = next_value.item()

        <span class="cmt"># æ›´æ–°ç½‘ç»œ</span>
        info = agent.update(states, actions, log_probs, rewards,
                           dones, values, entropies, next_value)

        <span class="kw">if</span> <span class="bi">len</span>(episode_rewards) % <span class="num">50</span> == <span class="num">0</span> <span class="kw">and</span> <span class="bi">len</span>(episode_rewards) > <span class="num">0</span>:
            avg = np.mean(episode_rewards[-<span class="num">100</span>:])
            <span class="bi">print</span>(<span class="str">f"Step {step:6d} | Episodes: {len(episode_rewards):4d} | "
                  f"Avg100: {avg:6.1f} | Entropy: {info['entropy']:.3f}"</span>)

    env.close()
    <span class="kw">return</span> episode_rewards

<span class="kw">if</span> __name__ == <span class="str">"__main__"</span>:
    rewards = train_a2c()
</code></pre>
</div>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ A2C vs A3C</div>
  <p><strong>A3C</strong> (Asynchronous Advantage Actor-Critic) ä½¿ç”¨å¤šä¸ªå¼‚æ­¥ worker å¹¶è¡Œé‡‡æ ·ã€‚<strong>A2C</strong> æ˜¯å…¶åŒæ­¥ç‰ˆæœ¬ï¼Œæ‰€æœ‰ worker åŒæ­¥æ›´æ–°ã€‚å®è·µä¸­ A2C ä¸ A3C æ€§èƒ½ç›¸å½“ï¼Œä½†å®ç°æ›´ç®€å•ï¼ŒGPU åˆ©ç”¨ç‡æ›´é«˜ã€‚PPO åœ¨ A2C çš„åŸºç¡€ä¸Šå¢åŠ äº† clipped objectiveï¼Œè¿›ä¸€æ­¥æå‡äº†è®­ç»ƒç¨³å®šæ€§ã€‚</p>
</div>

<h3 id="s2-5">2.5 N-step Returns ä¸ GAE å¯¹æ¯”</h3>

<p>ä¸ºäº†åŠ æ·±ç†è§£ï¼Œè®©æˆ‘ä»¬è¯¦ç»†å¯¹æ¯” N-step returns å’Œ GAE çš„å…³ç³»ã€‚</p>

<h4>N-step Return</h4>

<p>$n$-step return ä½¿ç”¨å‰ $n$ æ­¥çš„å®é™…å¥–åŠ±ï¼ŒåŠ ä¸Šç¬¬ $n$ æ­¥çš„ä»·å€¼ä¼°è®¡è¿›è¡Œ bootstrapï¼š</p>

$$G_t^{(n)} = r_t + \gamma r_{t+1} + \cdots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})$$

<table>
  <tr><th>$n$</th><th>åå·®</th><th>æ–¹å·®</th><th>é€‚ç”¨åœºæ™¯</th></tr>
  <tr><td>$n=1$ (TD(0))</td><td>é«˜åå·®</td><td>ä½æ–¹å·®</td><td>V è¾ƒå‡†ç¡®æ—¶</td></tr>
  <tr><td>$n=5$</td><td>ä¸­ç­‰</td><td>ä¸­ç­‰</td><td>é€šç”¨é»˜è®¤</td></tr>
  <tr><td>$n=20$</td><td>ä½åå·®</td><td>è¾ƒé«˜æ–¹å·®</td><td>å¥–åŠ±å»¶è¿Ÿå¤§æ—¶</td></tr>
  <tr><td>$n=\infty$ (MC)</td><td>æ— å</td><td>æœ€é«˜æ–¹å·®</td><td>episode çŸ­æ—¶</td></tr>
</table>

<h4>GAE ä½œä¸º N-step çš„è½¯ç‰ˆæœ¬</h4>

<p>N-step return åœ¨ç¬¬ $n$ æ­¥æœ‰ä¸€ä¸ª"ç¡¬æˆªæ–­"ï¼šå®Œå…¨ä¿¡ä»»å®é™…å¥–åŠ± $n$ æ­¥ï¼Œç„¶åå®Œå…¨ä¿¡ä»» $V$ã€‚GAE åˆ™æ˜¯"è½¯æˆªæ–­"ï¼šé€šè¿‡ $\lambda$ å¹³æ»‘åœ°è¡°å‡å¯¹è¿œå¤„ TD æ®‹å·®çš„ä¿¡ä»»åº¦ã€‚</p>

$$\text{N-step: } \hat{A}_t^{(n)} = \sum_{l=0}^{n-1}\gamma^l\delta_{t+l} \quad\quad \text{GAE: } \hat{A}_t^{GAE} = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}$$

<p>GAE çš„æŒ‡æ•°è¡°å‡æ¯” N-step çš„ç¡¬æˆªæ–­æ›´åŠ å¹³æ»‘ï¼Œé€šå¸¸èƒ½å¾—åˆ°æ›´å¥½çš„æ¢¯åº¦ä¼°è®¡ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ PPO é»˜è®¤ä½¿ç”¨ GAE ($\lambda = 0.95$) è€Œéå›ºå®š N-stepã€‚</p>


<!-- ============================================================ -->
<!-- CHAPTER 3: PPO æ ¸å¿ƒ -->
<!-- ============================================================ -->
<h2 id="ch3">ç¬¬ä¸‰ç« ï¼šPPO æ ¸å¿ƒç®—æ³•</h2>

<p>ç»ˆäºæ¥åˆ°æœ¬æ•™ç¨‹çš„æ ¸å¿ƒâ€”â€”PPOã€‚æˆ‘ä»¬å°†ä» Trust Region çš„æ€æƒ³å‡ºå‘ï¼Œç»è¿‡ TRPOï¼Œæœ€ç»ˆæ¨å¯¼å‡º PPO çš„ clipped objectiveã€‚</p>

<h3 id="s3-1">3.1 Trust Region æ€æƒ³</h3>

<p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•æœ‰ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼š<strong>æ­¥é•¿é€‰æ‹©</strong>ã€‚å¦‚æœå­¦ä¹ ç‡å¤ªå¤§ï¼Œä¸€æ¬¡æ›´æ–°å¯èƒ½è®©ç­–ç•¥å˜å·®å¾ˆå¤šï¼Œè€Œå˜å·®çš„ç­–ç•¥åˆä¼šäº§ç”Ÿæ›´å·®çš„æ•°æ®ï¼Œå¯¼è‡´æ¶æ€§å¾ªç¯ï¼ˆ<strong>performance collapse</strong>ï¼‰ã€‚</p>

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ ç­–ç•¥å´©æºƒçš„ç›´è§‰</div>
  <p>æƒ³è±¡ä½ åœ¨æ‚¬å´–è¾¹èµ°è·¯ã€‚æ™®é€šæ¢¯åº¦ä¸‹é™å°±åƒé—­ç€çœ¼å‘å‰è·¨ä¸€å¤§æ­¥â€”â€”å¯èƒ½ç›´æ¥æ‰ä¸‹æ‚¬å´–ã€‚Trust Region æ–¹æ³•åˆ™æ˜¯åœ¨è„šä¸‹ç”»ä¸€ä¸ªå°åœ†åœˆï¼Œåªåœ¨åœ†åœˆå†…é€‰æ‹©æœ€ä¼˜æ–¹å‘â€”â€”æ¯ä¸€æ­¥éƒ½æ˜¯å®‰å…¨çš„ã€‚</p>
</div>

<p><strong>Trust Regionï¼ˆä¿¡èµ–åŸŸï¼‰</strong>çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ¯æ¬¡æ›´æ–°ç­–ç•¥æ—¶ï¼Œé™åˆ¶æ–°ç­–ç•¥ä¸æ—§ç­–ç•¥çš„å·®å¼‚ä¸è¶…è¿‡ä¸€ä¸ªå°çš„èŒƒå›´ã€‚è¿™æ ·å³ä½¿ä¼˜åŒ–æ–¹å‘ä¸å®Œç¾ï¼Œä¹Ÿä¸ä¼šè®©ç­–ç•¥æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚</p>

<h4>æ•°å­¦è¡¨è¾¾</h4>

<p>æˆ‘ä»¬å¸Œæœ›åœ¨æ¯æ¬¡æ›´æ–°ä¸­ï¼Œæœ€å¤§åŒ–ä¸€ä¸ªå…³äºæ–°ç­–ç•¥æ€§èƒ½çš„æ›¿ä»£ç›®æ ‡ï¼ˆsurrogate objectiveï¼‰ï¼ŒåŒæ—¶çº¦æŸæ–°ç­–ç•¥ä¸è¦åç¦»æ—§ç­–ç•¥å¤ªè¿œï¼š</p>

$$\max_\theta \; L(\theta) \quad \text{s.t.} \quad D_{KL}(\pi_{\theta_{old}} \| \pi_\theta) \leq \delta$$

<p>å…¶ä¸­ $D_{KL}$ æ˜¯ KL æ•£åº¦ï¼Œè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ã€‚</p>

<h4>ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨ç­–ç•¥æ¢¯åº¦ï¼Ÿ</h4>

<p>ç­–ç•¥æ¢¯åº¦ $\nabla_\theta J(\theta)$ åªåœ¨å½“å‰å‚æ•° $\theta$ é™„è¿‘æ˜¯å‡†ç¡®çš„â€”â€”å®ƒæ˜¯ä¸€ä¸ª<strong>å±€éƒ¨çº¿æ€§è¿‘ä¼¼</strong>ã€‚æ²¿æ¢¯åº¦æ–¹å‘èµ°å¤ªè¿œï¼ŒçœŸå®ç›®æ ‡å‡½æ•°å¯èƒ½å·²ç»å®Œå…¨ä¸åŒäº†ã€‚è¿™ç±»ä¼¼äºæ³°å‹’å±•å¼€åªåœ¨å±•å¼€ç‚¹é™„è¿‘æœ‰æ•ˆã€‚</p>

<p>Trust Region æ–¹æ³•é€šè¿‡æ˜¾å¼é™åˆ¶æ›´æ–°çš„å¹…åº¦ï¼Œä¿è¯äº†æ›´æ–°åçš„ç­–ç•¥æ€§èƒ½æœ‰ç†è®ºä¸‹ç•Œä¿è¯ï¼ˆmonotonic improvement guaranteeï¼‰ã€‚</p>

<h3 id="s3-2">3.2 TRPO ç®—æ³•</h3>

<p>TRPO (Trust Region Policy Optimization, Schulman et al., 2015) æ˜¯ PPO çš„ç›´æ¥å‰èº«ï¼Œæä¾›äº†ç­–ç•¥å•è°ƒæ”¹å–„çš„ç†è®ºä¿è¯ã€‚</p>

<h4>Surrogate Objectiveï¼ˆæ›¿ä»£ç›®æ ‡ï¼‰</h4>

<p>TRPO çš„ä¼˜åŒ–ç›®æ ‡æ˜¯æ–°ç­–ç•¥åœ¨æ—§ç­–ç•¥æ•°æ®ä¸Šçš„æœŸæœ›å›æŠ¥æ”¹å–„ï¼š</p>

$$L^{CPI}(\theta) = \mathbb{E}_{s,a \sim \pi_{\theta_{old}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} \hat{A}^{\pi_{\theta_{old}}}(s,a)\right]$$

<p>å…¶ä¸­ $\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}$ æ˜¯<strong>é‡è¦æ€§é‡‡æ ·æ¯”ç‡</strong> (importance sampling ratio)ï¼Œé€šå¸¸è®°ä½œ $r_t(\theta)$ï¼š</p>

$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

<p>è¿™ä¸ªæ¯”ç‡è¡¡é‡äº†æ–°ç­–ç•¥ç›¸å¯¹äºæ—§ç­–ç•¥å¯¹åŒä¸€åŠ¨ä½œçš„æ¦‚ç‡å˜åŒ–ï¼š</p>
<ul>
  <li>$r_t(\theta) = 1$ï¼šæ–°æ—§ç­–ç•¥å¯¹è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ç›¸åŒ</li>
  <li>$r_t(\theta) > 1$ï¼šæ–°ç­–ç•¥æ›´å€¾å‘äºé€‰æ‹©è¿™ä¸ªåŠ¨ä½œ</li>
  <li>$r_t(\theta) < 1$ï¼šæ–°ç­–ç•¥æ›´ä¸å€¾å‘äºé€‰æ‹©è¿™ä¸ªåŠ¨ä½œ</li>
</ul>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ ä¸ºä»€ä¹ˆå« CPIï¼Ÿ</div>
  <p>CPI = Conservative Policy Iterationã€‚è¿™ä¸ªç›®æ ‡å‡½æ•°æ¥è‡ª Kakade & Langford (2002) çš„ä¿å®ˆç­–ç•¥è¿­ä»£ç†è®ºã€‚å½“ $\theta = \theta_{old}$ æ—¶ï¼Œ$L^{CPI} = 0$ï¼Œä¸” $\nabla_\theta L^{CPI}|_{\theta=\theta_{old}} = \nabla_\theta J(\theta)|_{\theta=\theta_{old}}$ï¼Œå³åœ¨æ—§å‚æ•°å¤„çš„æ¢¯åº¦ç­‰äºçœŸå®ç­–ç•¥æ¢¯åº¦ã€‚</p>
</div>

<h4>TRPO çš„çº¦æŸä¼˜åŒ–</h4>

<p>TRPO åœ¨ surrogate objective ä¸ŠåŠ  KL æ•£åº¦çº¦æŸï¼š</p>

$$\max_\theta \; \mathbb{E}\left[r_t(\theta) \hat{A}_t\right] \quad \text{s.t.} \quad \mathbb{E}\left[D_{KL}(\pi_{\theta_{old}}(\cdot|s) \| \pi_\theta(\cdot|s))\right] \leq \delta$$

<p>ç”¨æ‹‰æ ¼æœ—æ—¥å¯¹å¶å’Œ Fisher Information Matrix çš„äºŒé˜¶å±•å¼€ï¼ŒTRPO çš„æ›´æ–°æ­¥éª¤æ˜¯ï¼š</p>

$$\theta_{new} = \theta_{old} + \alpha \sqrt{\frac{2\delta}{g^T F^{-1} g}} F^{-1} g$$

<p>å…¶ä¸­ $g = \nabla_\theta L^{CPI}$ æ˜¯ç­–ç•¥æ¢¯åº¦ï¼Œ$F$ æ˜¯ Fisher ä¿¡æ¯çŸ©é˜µã€‚</p>

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ TRPO çš„ç¼ºç‚¹</div>
  <ul>
    <li><strong>è®¡ç®—æ˜‚è´µ</strong>ï¼šéœ€è¦è®¡ç®—å’Œæ±‚é€† Fisher ä¿¡æ¯çŸ©é˜µï¼ˆå³ä½¿ç”¨å…±è½­æ¢¯åº¦è¿‘ä¼¼ä¹Ÿå¾ˆæ…¢ï¼‰</li>
    <li><strong>å®ç°å¤æ‚</strong>ï¼šline searchã€å…±è½­æ¢¯åº¦ã€Hessian-vector product</li>
    <li><strong>ä¸å…¼å®¹å…±äº«ç½‘ç»œ</strong>ï¼šç­–ç•¥å’Œå€¼å‡½æ•°å…±äº«å‚æ•°æ—¶ KL çº¦æŸå¾ˆéš¾å¤„ç†</li>
    <li><strong>ä¸å…¼å®¹ dropout/BatchNorm</strong>ï¼šäºŒé˜¶æ–¹æ³•å¯¹è¿™äº›æ“ä½œä¸å‹å¥½</li>
  </ul>
  <p>PPO çš„å‡ºç°å°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜â€”â€”ç”¨ä¸€é˜¶æ–¹æ³•å®ç°ç±»ä¼¼ Trust Region çš„æ•ˆæœã€‚</p>
</div>

<h3 id="s3-3">3.3 PPO-Clip å®Œæ•´æ¨å¯¼</h3>

<p>PPO (Schulman et al., 2017) çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ç”¨ <strong>clipping</strong> ä»£æ›¿ KL çº¦æŸï¼Œä¿ç•™äº† Trust Region çš„æ€æƒ³ç²¾é«“ï¼ŒåŒæ—¶å¤§å¹…ç®€åŒ–äº†å®ç°ã€‚</p>

<h4>Step 1: ä» TRPO çš„åŠ¨æœºå‡ºå‘</h4>

<p>æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ– $\mathbb{E}[r_t(\theta)\hat{A}_t]$ï¼Œä½†è¦é™åˆ¶ $r_t(\theta)$ ä¸è¦åç¦» 1 å¤ªè¿œã€‚TRPO ç”¨ KL çº¦æŸå®ç°è¿™ä¸€ç‚¹ã€‚PPO åˆ™ç›´æ¥å¯¹ $r_t(\theta)$ è¿›è¡Œè£å‰ªã€‚</p>

<h4>Step 2: Clipped Surrogate Objective</h4>

<div class="callout callout-theorem">
  <div class="callout-title">ğŸ¯ PPO-Clip ç›®æ ‡å‡½æ•°</div>
  $$L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \;\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$
  <p>å…¶ä¸­ $\epsilon$ æ˜¯è£å‰ªå‚æ•°ï¼ˆé»˜è®¤ $\epsilon = 0.2$ï¼‰ï¼Œ$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ã€‚</p>
</div>

<h4>Step 3: é€ç¬¦å·è§£è¯»</h4>

<table>
  <tr><th>ç¬¦å·</th><th>å«ä¹‰</th><th>è¯¦ç»†è¯´æ˜</th></tr>
  <tr>
    <td>$r_t(\theta)$</td>
    <td>æ¦‚ç‡æ¯”ç‡</td>
    <td>$\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ï¼Œæ–°æ—§ç­–ç•¥å¯¹åŒä¸€åŠ¨ä½œçš„æ¦‚ç‡ä¹‹æ¯”ã€‚åˆå§‹ä¸º 1ï¼Œæ›´æ–°ååç¦» 1ã€‚</td>
  </tr>
  <tr>
    <td>$\hat{A}_t$</td>
    <td>ä¼˜åŠ¿ä¼°è®¡</td>
    <td>é€šè¿‡ GAE è®¡ç®—çš„ä¼˜åŠ¿å‡½æ•°ä¼°è®¡å€¼ã€‚æ­£å€¼=å¥½åŠ¨ä½œï¼Œè´Ÿå€¼=å·®åŠ¨ä½œã€‚</td>
  </tr>
  <tr>
    <td>$\epsilon$</td>
    <td>è£å‰ªèŒƒå›´</td>
    <td>æ§åˆ¶ç­–ç•¥å…è®¸å˜åŒ–çš„å¹…åº¦ã€‚$\epsilon=0.2$ æ„å‘³ç€ $r_t$ è¢«é™åˆ¶åœ¨ $[0.8, 1.2]$ èŒƒå›´å†…ã€‚</td>
  </tr>
  <tr>
    <td>$\text{clip}(\cdot)$</td>
    <td>è£å‰ªå‡½æ•°</td>
    <td>$\text{clip}(x, a, b) = \max(a, \min(x, b))$ï¼Œå°† $x$ é™åˆ¶åœ¨ $[a, b]$ èŒƒå›´å†…ã€‚</td>
  </tr>
  <tr>
    <td>$\min(\cdot, \cdot)$</td>
    <td>å–è¾ƒå°å€¼</td>
    <td>åœ¨åŸå§‹ç›®æ ‡å’Œè£å‰ªåç›®æ ‡ä¹‹é—´å–è¾ƒå°å€¼ï¼Œå½¢æˆ<strong>æ‚²è§‚ä¸‹ç•Œ</strong>ã€‚</td>
  </tr>
</table>

<h4>Step 4: åˆ†æƒ…å†µåˆ†æï¼ˆæ ¸å¿ƒç›´è§‰ï¼‰</h4>

<p>ç†è§£ PPO-Clip çš„å…³é”®æ˜¯åˆ†ä¸¤ç§æƒ…å†µåˆ†æ $\min$ æ“ä½œçš„è¡Œä¸ºï¼š</p>

<div class="comparison-grid">
  <div class="comparison-card">
    <h4>æƒ…å†µ 1: $\hat{A}_t > 0$ï¼ˆå¥½åŠ¨ä½œï¼‰</h4>
    <p>è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å¥½ï¼Œæˆ‘ä»¬å¸Œæœ›å¢åŠ å®ƒçš„æ¦‚ç‡ï¼Œå³ $r_t(\theta) > 1$ã€‚</p>
    <p>$L^{CLIP} = \min(r_t\hat{A}_t, \; (1+\epsilon)\hat{A}_t)$</p>
    <ul>
      <li>å½“ $r_t \leq 1+\epsilon$ æ—¶ï¼šä½¿ç”¨åŸå§‹ç›®æ ‡ $r_t\hat{A}_t$ï¼ˆæ­£å¸¸ä¼˜åŒ–ï¼‰</li>
      <li>å½“ $r_t > 1+\epsilon$ æ—¶ï¼šä½¿ç”¨ $(1+\epsilon)\hat{A}_t$ï¼ˆæ¢¯åº¦ä¸ºé›¶ï¼Œåœæ­¢å¢åŠ æ¦‚ç‡ï¼‰</li>
    </ul>
    <p>ğŸ”‘ <strong>æ•ˆæœ</strong>ï¼šå…è®¸å¢åŠ å¥½åŠ¨ä½œçš„æ¦‚ç‡ï¼Œä½†ä¸è¶…è¿‡ $1+\epsilon$ å€ã€‚</p>
  </div>
  <div class="comparison-card">
    <h4>æƒ…å†µ 2: $\hat{A}_t < 0$ï¼ˆå·®åŠ¨ä½œï¼‰</h4>
    <p>è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å·®ï¼Œæˆ‘ä»¬å¸Œæœ›å‡å°‘å®ƒçš„æ¦‚ç‡ï¼Œå³ $r_t(\theta) < 1$ã€‚</p>
    <p>$L^{CLIP} = \min(r_t\hat{A}_t, \; (1-\epsilon)\hat{A}_t)$</p>
    <ul>
      <li>å½“ $r_t \geq 1-\epsilon$ æ—¶ï¼šä½¿ç”¨åŸå§‹ç›®æ ‡ $r_t\hat{A}_t$ï¼ˆæ­£å¸¸ä¼˜åŒ–ï¼‰</li>
      <li>å½“ $r_t < 1-\epsilon$ æ—¶ï¼šä½¿ç”¨ $(1-\epsilon)\hat{A}_t$ï¼ˆæ¢¯åº¦ä¸ºé›¶ï¼Œåœæ­¢å‡å°‘æ¦‚ç‡ï¼‰</li>
    </ul>
    <p>ğŸ”‘ <strong>æ•ˆæœ</strong>ï¼šå…è®¸å‡å°‘å·®åŠ¨ä½œçš„æ¦‚ç‡ï¼Œä½†ä¸ä½äº $1-\epsilon$ å€ã€‚</p>
  </div>
</div>

<h4>Step 5: å¯è§†åŒ–ç†è§£</h4>

<p>ä¸‹é¢çš„å›¾å±•ç¤ºäº† PPO-Clip ç›®æ ‡å‡½æ•°éš $r_t$ å˜åŒ–çš„å½¢çŠ¶ï¼š</p>

<div class="diagram-container">
  <svg width="650" height="320" viewBox="0 0 650 320">
    <defs>
      <marker id="arrowAxis" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto">
        <path d="M 0 0 L 10 5 L 0 10 Z" fill="var(--text-muted)"/>
      </marker>
    </defs>
    <!-- Title -->
    <text x="160" y="20" font-size="13" fill="var(--accent)" font-weight="700">A > 0ï¼ˆå¥½åŠ¨ä½œï¼‰</text>
    <text x="480" y="20" font-size="13" fill="var(--def-border)" font-weight="700">A &lt; 0ï¼ˆå·®åŠ¨ä½œï¼‰</text>

    <!-- Left plot: A > 0 -->
    <g transform="translate(30, 40)">
      <!-- Axes -->
      <line x1="20" y1="230" x2="270" y2="230" stroke="var(--text-muted)" stroke-width="1.5" marker-end="url(#arrowAxis)"/>
      <line x1="20" y1="230" x2="20" y2="20" stroke="var(--text-muted)" stroke-width="1.5" marker-end="url(#arrowAxis)"/>
      <text x="270" y="250" font-size="11" fill="var(--text-muted)">r(Î¸)</text>
      <text x="5" y="18" font-size="11" fill="var(--text-muted)">L</text>

      <!-- Clipped region shading -->
      <rect x="100" y="30" width="100" height="200" fill="var(--accent-light)" opacity="0.3"/>

      <!-- Unclipped line (dashed) -->
      <line x1="20" y1="230" x2="250" y2="40" stroke="var(--text-muted)" stroke-width="1.5" stroke-dasharray="5,3"/>

      <!-- Clipped line (solid) -->
      <line x1="20" y1="230" x2="200" y2="65" stroke="var(--accent)" stroke-width="2.5"/>
      <line x1="200" y1="65" x2="260" y2="65" stroke="var(--accent)" stroke-width="2.5"/>

      <!-- Labels -->
      <text x="95" y="255" font-size="10" fill="var(--text-muted)">1-Îµ</text>
      <text x="195" y="255" font-size="10" fill="var(--text-muted)">1+Îµ</text>
      <line x1="100" y1="225" x2="100" y2="235" stroke="var(--text-muted)" stroke-width="1"/>
      <line x1="200" y1="225" x2="200" y2="235" stroke="var(--text-muted)" stroke-width="1"/>

      <text x="210" y="55" font-size="10" fill="var(--accent)" font-weight="600">clip!</text>
    </g>

    <!-- Right plot: A < 0 -->
    <g transform="translate(350, 40)">
      <!-- Axes -->
      <line x1="20" y1="120" x2="270" y2="120" stroke="var(--text-muted)" stroke-width="1.5" marker-end="url(#arrowAxis)"/>
      <line x1="20" y1="230" x2="20" y2="20" stroke="var(--text-muted)" stroke-width="1.5" marker-end="url(#arrowAxis)"/>
      <text x="270" y="140" font-size="11" fill="var(--text-muted)">r(Î¸)</text>
      <text x="5" y="18" font-size="11" fill="var(--text-muted)">L</text>

      <!-- Clipped region shading -->
      <rect x="100" y="30" width="100" height="200" fill="var(--def-bg)" opacity="0.3"/>

      <!-- Unclipped line (dashed) -->
      <line x1="20" y1="30" x2="250" y2="210" stroke="var(--text-muted)" stroke-width="1.5" stroke-dasharray="5,3"/>

      <!-- Clipped line (solid) -->
      <line x1="20" y1="95" x2="100" y2="95" stroke="var(--def-border)" stroke-width="2.5"/>
      <line x1="100" y1="95" x2="250" y2="210" stroke="var(--def-border)" stroke-width="2.5"/>

      <!-- Labels -->
      <text x="95" y="255" font-size="10" fill="var(--text-muted)">1-Îµ</text>
      <text x="195" y="255" font-size="10" fill="var(--text-muted)">1+Îµ</text>
      <line x1="100" y1="115" x2="100" y2="125" stroke="var(--text-muted)" stroke-width="1"/>
      <line x1="200" y1="115" x2="200" y2="125" stroke="var(--text-muted)" stroke-width="1"/>

      <text x="30" y="85" font-size="10" fill="var(--def-border)" font-weight="600">clip!</text>
    </g>
  </svg>
  <div class="diagram-caption">å›¾ 3.1: PPO-Clip ç›®æ ‡å‡½æ•° â€” å·¦å›¾ï¼šå¥½åŠ¨ä½œ (A>0) æ—¶ä¸Šé™è£å‰ªï¼›å³å›¾ï¼šå·®åŠ¨ä½œ (A&lt;0) æ—¶ä¸‹é™è£å‰ª</div>
</div>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ PPO-Clip çš„æ ¸å¿ƒç›´è§‰</div>
  <p>PPO-Clip çš„ $\min$ æ“ä½œæ„æˆäº†ä¸€ä¸ª<strong>æ‚²è§‚ä¸‹ç•Œ</strong>ï¼ˆpessimistic lower boundï¼‰ï¼š</p>
  <ul>
    <li>å¯¹äºå¥½åŠ¨ä½œï¼šé™åˆ¶å¢åŠ æ¦‚ç‡çš„å¹…åº¦ï¼ˆä¸è¦å¤ªè´ªå¿ƒï¼‰</li>
    <li>å¯¹äºå·®åŠ¨ä½œï¼šé™åˆ¶å‡å°‘æ¦‚ç‡çš„å¹…åº¦ï¼ˆä¸è¦å¤ªæ¿€è¿›ï¼‰</li>
  </ul>
  <p>è¿™ä¿è¯äº†ç­–ç•¥åœ¨æ¯æ¬¡æ›´æ–°ä¸­ä¸ä¼šå‘ç”Ÿå‰§çƒˆå˜åŒ–ï¼Œä»è€Œå®ç°äº†ç±»ä¼¼ Trust Region çš„ç¨³å®šæ€§ï¼Œä½†<strong>åªç”¨äº†ä¸€é˜¶ä¼˜åŒ–</strong>ï¼ˆæ™®é€š SGD å³å¯ï¼‰ã€‚</p>
</div>

<h3 id="s3-4">3.4 PPO ç›®æ ‡å‡½æ•°é€ç¬¦å·è§£è¯»</h3>

<p>è®©æˆ‘ä»¬å†æ¬¡å†™å‡ºå®Œæ•´çš„ PPO ç›®æ ‡å‡½æ•°ï¼Œå¹¶å¯¹æ¯ä¸ªç»„æˆéƒ¨åˆ†ç»™å‡ºè¯¦ç»†çš„è§£é‡Šï¼š</p>

$$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t, \;\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ å®Œæ•´åˆ†è§£</div>
  <p><strong>1. æ¦‚ç‡æ¯”ç‡ $r_t(\theta)$</strong></p>
  $$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} = \exp\left(\log\pi_\theta(a_t|s_t) - \log\pi_{\theta_{old}}(a_t|s_t)\right)$$
  <p>å®é™…å®ç°ä¸­ç”¨ log ç©ºé—´è®¡ç®—æ›´ç¨³å®šã€‚åˆå§‹æ—¶ $r_t = 1$ï¼Œæ›´æ–°å $r_t$ åç¦» 1ã€‚</p>

  <p><strong>2. åŸå§‹ç›®æ ‡é¡¹ $r_t(\theta)\hat{A}_t$</strong></p>
  <p>è¿™å°±æ˜¯ TRPO çš„ surrogate objectiveï¼šå¦‚æœå¥½åŠ¨ä½œï¼ˆ$\hat{A}_t > 0$ï¼‰çš„æ¦‚ç‡å¢åŠ ï¼ˆ$r_t > 1$ï¼‰ï¼Œç›®æ ‡å€¼å¢å¤§ã€‚</p>

  <p><strong>3. è£å‰ªç›®æ ‡é¡¹ $\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t$</strong></p>
  <p>å…ˆå°† $r_t$ é™åˆ¶åœ¨ $[1-\epsilon, 1+\epsilon]$ èŒƒå›´å†…ï¼Œå†ä¸ $\hat{A}_t$ ç›¸ä¹˜ã€‚è¿™ä¸ªé¡¹çš„æ¢¯åº¦åœ¨ $r_t$ è¶…å‡ºèŒƒå›´æ—¶ä¸ºé›¶ã€‚</p>

  <p><strong>4. $\min(\cdot, \cdot)$ æ“ä½œ</strong></p>
  <p>å–ä¸¤ä¸ªç›®æ ‡ä¸­çš„è¾ƒå°å€¼ã€‚è¿™ç¡®ä¿äº†ï¼š</p>
  <ul>
    <li>å½“ $r_t$ åœ¨ $[1-\epsilon, 1+\epsilon]$ å†…æ—¶ï¼Œä¸¤é¡¹ç›¸ç­‰ï¼Œæ¢¯åº¦æ­£å¸¸æµåŠ¨</li>
    <li>å½“ $r_t$ è¶…å‡ºèŒƒå›´ä¸”ç›®æ ‡åœ¨"æ”¹å–„"æ—¶ï¼ˆå¥½åŠ¨ä½œæ¦‚ç‡å¤ªé«˜æˆ–å·®åŠ¨ä½œæ¦‚ç‡å¤ªä½ï¼‰ï¼Œ$\min$ ä¼šé€‰æ‹©è¾ƒå°çš„è£å‰ªå€¼ï¼Œæ¢¯åº¦ä¸ºé›¶ï¼Œé˜»æ­¢è¿›ä¸€æ­¥æ›´æ–°</li>
  </ul>
</div>

<h3 id="s3-5">3.5 ä»£ç ï¼šPPO from Scratch</h3>

<p>ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ PPO å®ç°ï¼ŒåŒ…å«æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ã€‚æˆ‘ä»¬å…ˆå®šä¹‰ç½‘ç»œç»“æ„ï¼Œå†å®ç°æ•°æ®æ”¶é›†å’Œæ›´æ–°å¾ªç¯ã€‚</p>

<div class="code-container">
  <div class="code-header">
    <span>ppo_from_scratch.py â€” å®Œæ•´ PPO å®ç° (ç¦»æ•£åŠ¨ä½œ)</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.optim <span class="kw">as</span> optim
<span class="kw">from</span> torch.distributions <span class="kw">import</span> Categorical
<span class="kw">import</span> gymnasium <span class="kw">as</span> gym
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cmt"># ============================================================</span>
<span class="cmt"># 1. Actor-Critic ç½‘ç»œ</span>
<span class="cmt"># ============================================================</span>
<span class="kw">class</span> <span class="cls">PPOActorCritic</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, state_dim, action_dim, hidden=<span class="num">64</span>):
        <span class="bi">super</span>().__init__()
        <span class="cmt"># Actor å’Œ Critic ä½¿ç”¨ç‹¬ç«‹ç½‘ç»œ (PPO æ¨èåšæ³•)</span>
        <span class="sf">self</span>.actor = nn.Sequential(
            nn.Linear(state_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, action_dim), nn.Softmax(dim=-<span class="num">1</span>)
        )
        <span class="sf">self</span>.critic = nn.Sequential(
            nn.Linear(state_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, <span class="num">1</span>)
        )

    <span class="kw">def</span> <span class="fn">get_action_and_value</span>(<span class="sf">self</span>, state):
        <span class="str">"""è·å–åŠ¨ä½œã€logæ¦‚ç‡ã€ç†µå’ŒçŠ¶æ€ä»·å€¼"""</span>
        probs = <span class="sf">self</span>.actor(state)
        dist = Categorical(probs)
        action = dist.sample()
        <span class="kw">return</span> action, dist.log_prob(action), dist.entropy(), <span class="sf">self</span>.critic(state).squeeze(-<span class="num">1</span>)

    <span class="kw">def</span> <span class="fn">evaluate_actions</span>(<span class="sf">self</span>, states, actions):
        <span class="str">"""è¯„ä¼°å·²æœ‰åŠ¨ä½œçš„ logæ¦‚ç‡ã€ç†µå’Œä»·å€¼ï¼ˆç”¨äº PPO æ›´æ–°ï¼‰"""</span>
        probs = <span class="sf">self</span>.actor(states)
        dist = Categorical(probs)
        <span class="kw">return</span> dist.log_prob(actions), dist.entropy(), <span class="sf">self</span>.critic(states).squeeze(-<span class="num">1</span>)

<span class="cmt"># ============================================================</span>
<span class="cmt"># 2. Rollout Buffer</span>
<span class="cmt"># ============================================================</span>
<span class="kw">class</span> <span class="cls">RolloutBuffer</span>:
    <span class="str">"""å­˜å‚¨ rollout æ•°æ®çš„ç¼“å†²åŒº"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>):
        <span class="sf">self</span>.states, <span class="sf">self</span>.actions, <span class="sf">self</span>.rewards = [], [], []
        <span class="sf">self</span>.dones, <span class="sf">self</span>.log_probs, <span class="sf">self</span>.values = [], [], []

    <span class="kw">def</span> <span class="fn">add</span>(<span class="sf">self</span>, state, action, reward, done, log_prob, value):
        <span class="sf">self</span>.states.append(state)
        <span class="sf">self</span>.actions.append(action)
        <span class="sf">self</span>.rewards.append(reward)
        <span class="sf">self</span>.dones.append(done)
        <span class="sf">self</span>.log_probs.append(log_prob)
        <span class="sf">self</span>.values.append(value)

    <span class="kw">def</span> <span class="fn">compute_returns_and_advantages</span>(<span class="sf">self</span>, next_value, gamma=<span class="num">0.99</span>, lam=<span class="num">0.95</span>):
        <span class="str">"""è®¡ç®— GAE ä¼˜åŠ¿å’Œç›®æ ‡å›æŠ¥"""</span>
        T = <span class="bi">len</span>(<span class="sf">self</span>.rewards)
        advantages = np.zeros(T)
        last_gae = <span class="num">0.0</span>
        values_np = np.array([v.item() <span class="kw">for</span> v <span class="kw">in</span> <span class="sf">self</span>.values])

        <span class="kw">for</span> t <span class="kw">in</span> <span class="bi">reversed</span>(<span class="bi">range</span>(T)):
            next_val = next_value <span class="kw">if</span> t == T - <span class="num">1</span> <span class="kw">else</span> values_np[t + <span class="num">1</span>]
            non_terminal = <span class="num">1.0</span> - <span class="sf">self</span>.dones[t]
            delta = <span class="sf">self</span>.rewards[t] + gamma * next_val * non_terminal - values_np[t]
            advantages[t] = last_gae = delta + gamma * lam * non_terminal * last_gae

        returns = advantages + values_np
        <span class="sf">self</span>.advantages = torch.FloatTensor(advantages)
        <span class="sf">self</span>.returns = torch.FloatTensor(returns)

    <span class="kw">def</span> <span class="fn">get_batches</span>(<span class="sf">self</span>, batch_size):
        <span class="str">"""ç”Ÿæˆéšæœº mini-batch"""</span>
        states = torch.FloatTensor(np.array(<span class="sf">self</span>.states))
        actions = torch.LongTensor(<span class="sf">self</span>.actions)
        old_log_probs = torch.stack(<span class="sf">self</span>.log_probs).detach()
        T = <span class="bi">len</span>(<span class="sf">self</span>.rewards)
        indices = np.random.permutation(T)
        <span class="kw">for</span> start <span class="kw">in</span> <span class="bi">range</span>(<span class="num">0</span>, T, batch_size):
            end = start + batch_size
            idx = indices[start:end]
            <span class="kw">yield</span> (states[idx], actions[idx], old_log_probs[idx],
                   <span class="sf">self</span>.advantages[idx], <span class="sf">self</span>.returns[idx])

    <span class="kw">def</span> <span class="fn">clear</span>(<span class="sf">self</span>):
        <span class="sf">self</span>.states.clear(); <span class="sf">self</span>.actions.clear()
        <span class="sf">self</span>.rewards.clear(); <span class="sf">self</span>.dones.clear()
        <span class="sf">self</span>.log_probs.clear(); <span class="sf">self</span>.values.clear()

<span class="cmt"># ============================================================</span>
<span class="cmt"># 3. PPO Agent</span>
<span class="cmt"># ============================================================</span>
<span class="kw">class</span> <span class="cls">PPOAgent</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, state_dim, action_dim,
                 lr=<span class="num">3e-4</span>, gamma=<span class="num">0.99</span>, lam=<span class="num">0.95</span>,
                 clip_range=<span class="num">0.2</span>, value_coef=<span class="num">0.5</span>,
                 entropy_coef=<span class="num">0.01</span>, max_grad_norm=<span class="num">0.5</span>,
                 n_epochs=<span class="num">4</span>, batch_size=<span class="num">64</span>):

        <span class="sf">self</span>.model = PPOActorCritic(state_dim, action_dim)
        <span class="sf">self</span>.optimizer = optim.Adam(<span class="sf">self</span>.model.parameters(), lr=lr)
        <span class="sf">self</span>.gamma = gamma
        <span class="sf">self</span>.lam = lam
        <span class="sf">self</span>.clip_range = clip_range
        <span class="sf">self</span>.value_coef = value_coef
        <span class="sf">self</span>.entropy_coef = entropy_coef
        <span class="sf">self</span>.max_grad_norm = max_grad_norm
        <span class="sf">self</span>.n_epochs = n_epochs
        <span class="sf">self</span>.batch_size = batch_size
        <span class="sf">self</span>.buffer = RolloutBuffer()

    <span class="kw">def</span> <span class="fn">update</span>(<span class="sf">self</span>, next_value):
        <span class="str">"""PPO æ ¸å¿ƒæ›´æ–°é€»è¾‘"""</span>
        <span class="sf">self</span>.buffer.compute_returns_and_advantages(next_value, <span class="sf">self</span>.gamma, <span class="sf">self</span>.lam)

        <span class="cmt"># Advantage æ ‡å‡†åŒ–</span>
        adv = <span class="sf">self</span>.buffer.advantages
        <span class="sf">self</span>.buffer.advantages = (adv - adv.mean()) / (adv.std() + <span class="num">1e-8</span>)

        total_pg_loss, total_vf_loss, total_entropy = <span class="num">0</span>, <span class="num">0</span>, <span class="num">0</span>
        n_updates = <span class="num">0</span>

        <span class="cmt"># å¤šè½® epochï¼ˆåŒä¸€æ‰¹æ•°æ®é‡å¤åˆ©ç”¨ï¼‰</span>
        <span class="kw">for</span> epoch <span class="kw">in</span> <span class="bi">range</span>(<span class="sf">self</span>.n_epochs):
            <span class="kw">for</span> batch <span class="kw">in</span> <span class="sf">self</span>.buffer.get_batches(<span class="sf">self</span>.batch_size):
                states, actions, old_log_probs, advantages, returns = batch

                <span class="cmt"># ç”¨å½“å‰ç­–ç•¥é‡æ–°è¯„ä¼°æ—§åŠ¨ä½œ</span>
                new_log_probs, entropy, values = <span class="sf">self</span>.model.evaluate_actions(states, actions)

                <span class="cmt"># ---- PPO-Clip æ ¸å¿ƒ ----</span>
                <span class="cmt"># æ¦‚ç‡æ¯”ç‡ r(Î¸)</span>
                ratio = torch.exp(new_log_probs - old_log_probs)

                <span class="cmt"># è£å‰ªç›®æ ‡</span>
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, <span class="num">1</span> - <span class="sf">self</span>.clip_range,
                                           <span class="num">1</span> + <span class="sf">self</span>.clip_range) * advantages
                policy_loss = -torch.min(surr1, surr2).mean()

                <span class="cmt"># ä»·å€¼æŸå¤±</span>
                value_loss = nn.MSELoss()(values, returns)

                <span class="cmt"># ç†µå¥–åŠ±</span>
                entropy_loss = -entropy.mean()

                <span class="cmt"># æ€»æŸå¤±</span>
                loss = (policy_loss
                        + <span class="sf">self</span>.value_coef * value_loss
                        + <span class="sf">self</span>.entropy_coef * entropy_loss)

                <span class="sf">self</span>.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(<span class="sf">self</span>.model.parameters(), <span class="sf">self</span>.max_grad_norm)
                <span class="sf">self</span>.optimizer.step()

                total_pg_loss += policy_loss.item()
                total_vf_loss += value_loss.item()
                total_entropy += -entropy_loss.item()
                n_updates += <span class="num">1</span>

        <span class="sf">self</span>.buffer.clear()
        <span class="kw">return</span> {
            <span class="str">"policy_loss"</span>: total_pg_loss / n_updates,
            <span class="str">"value_loss"</span>: total_vf_loss / n_updates,
            <span class="str">"entropy"</span>: total_entropy / n_updates
        }


<span class="cmt"># ============================================================</span>
<span class="cmt"># 4. è®­ç»ƒå¾ªç¯</span>
<span class="cmt"># ============================================================</span>
<span class="kw">def</span> <span class="fn">train_ppo</span>(env_name=<span class="str">"CartPole-v1"</span>, total_steps=<span class="num">200000</span>, n_steps=<span class="num">2048</span>):
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[<span class="num">0</span>]
    action_dim = env.action_space.n

    agent = PPOAgent(state_dim, action_dim)
    state, _ = env.reset()
    episode_reward = <span class="num">0</span>
    episode_rewards = []
    step = <span class="num">0</span>

    <span class="kw">while</span> step < total_steps:
        <span class="cmt"># æ”¶é›† n_steps çš„æ•°æ®</span>
        <span class="kw">for</span> _ <span class="kw">in</span> <span class="bi">range</span>(n_steps):
            state_t = torch.FloatTensor(state).unsqueeze(<span class="num">0</span>)
            <span class="kw">with</span> torch.no_grad():
                action, log_prob, _, value = agent.model.get_action_and_value(state_t)

            next_state, reward, terminated, truncated, _ = env.step(action.item())
            done = terminated <span class="kw">or</span> truncated

            agent.buffer.add(state, action.item(), reward, <span class="bi">float</span>(done),
                            log_prob, value)
            episode_reward += reward
            state = next_state
            step += <span class="num">1</span>

            <span class="kw">if</span> done:
                episode_rewards.append(episode_reward)
                episode_reward = <span class="num">0</span>
                state, _ = env.reset()

        <span class="cmt"># è®¡ç®— bootstrap value</span>
        <span class="kw">with</span> torch.no_grad():
            _, _, _, next_value = agent.model.get_action_and_value(
                torch.FloatTensor(state).unsqueeze(<span class="num">0</span>))
            next_value = next_value.item()

        <span class="cmt"># PPO æ›´æ–°</span>
        info = agent.update(next_value)

        <span class="kw">if</span> <span class="bi">len</span>(episode_rewards) > <span class="num">0</span>:
            avg = np.mean(episode_rewards[-<span class="num">100</span>:])
            <span class="bi">print</span>(<span class="str">f"Step {step:6d} | Episodes: {len(episode_rewards):4d} | "
                  f"Avg100: {avg:.1f} | PG Loss: {info['policy_loss']:.4f} | "
                  f"VF Loss: {info['value_loss']:.4f} | Entropy: {info['entropy']:.3f}"</span>)

    env.close()
    <span class="kw">return</span> episode_rewards

<span class="kw">if</span> __name__ == <span class="str">"__main__"</span>:
    rewards = train_ppo()
</code></pre>
</div>

<div class="callout callout-exercise">
  <div class="callout-title">ğŸ‹ï¸ ç»ƒä¹  2</div>
  <ol>
    <li>è¿è¡Œ PPO from scratch ä»£ç ï¼ŒéªŒè¯åœ¨ CartPole ä¸Šçš„æ”¶æ•›é€Ÿåº¦ï¼ˆåº”è¯¥æ¯” REINFORCE å’Œ A2C éƒ½å¿«ï¼‰</li>
    <li>å°† <code>clip_range</code> ä» 0.2 æ”¹ä¸º 0.1 å’Œ 0.3ï¼Œè§‚å¯Ÿè®­ç»ƒæ›²çº¿çš„å˜åŒ–</li>
    <li>å°† <code>n_epochs</code> ä» 4 æ”¹ä¸º 1 å’Œ 10ï¼Œç†è§£å¤šæ¬¡é‡ç”¨æ•°æ®çš„å¥½å¤„å’Œé£é™©</li>
    <li>å»æ‰ advantage normalizationï¼ˆæ³¨é‡Šæ‰æ ‡å‡†åŒ–é‚£ä¸€è¡Œï¼‰ï¼Œè§‚å¯Ÿè®­ç»ƒç¨³å®šæ€§çš„å˜åŒ–</li>
  </ol>
</div>


<!-- ============================================================ -->
<!-- CHAPTER 4: PPO å®ç°ç»†èŠ‚ -->
<!-- ============================================================ -->
<h2 id="ch4">ç¬¬å››ç« ï¼šPPO å®ç°ç»†èŠ‚</h2>

<p>ä¸Šä¸€ç« ä»‹ç»äº† PPO çš„æ ¸å¿ƒç®—æ³•â€”â€”clipped surrogate objectiveã€‚ä½†åœ¨å®é™…å®ç°ä¸­ï¼Œè¿˜æœ‰å¾ˆå¤šå…³é”®çš„"å·¥ç¨‹ç»†èŠ‚"ç›´æ¥å½±å“è®­ç»ƒæ•ˆæœã€‚æœ¬ç« æ·±å…¥æ¢è®¨è¿™äº›ç»†èŠ‚ï¼Œå®ƒä»¬å¾€å¾€æ˜¯ PPO æˆåŠŸçš„å…³é”®ï¼Œå´ç»å¸¸è¢«æ•™ç§‘ä¹¦å¿½ç•¥ã€‚</p>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ å®ç°ç»†èŠ‚çš„é‡è¦æ€§</div>
  <p>Engstrom et al. (2020) çš„ç ”ç©¶è¡¨æ˜ï¼ŒPPO çš„è®¸å¤šæ€§èƒ½æå‡å®é™…ä¸Šæ¥è‡ªå®ç°ç»†èŠ‚ï¼ˆå¦‚ advantage normalizationã€gradient clippingï¼‰ï¼Œè€Œéç®—æ³•æœ¬èº«ã€‚å› æ­¤ç†è§£è¿™äº›ç»†èŠ‚å¯¹äºæˆåŠŸè®­ç»ƒè‡³å…³é‡è¦ã€‚</p>
</div>

<h3 id="s4-1">4.1 Value Function Loss è®¾è®¡</h3>

<p>Critic ç½‘ç»œ $V_\phi(s)$ çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–ä¸ç›®æ ‡å›æŠ¥ä¹‹é—´çš„è¯¯å·®ã€‚PPO è®ºæ–‡ä¸­æåˆ°äº†ä¸¤ç§ value loss çš„è®¾è®¡æ–¹å¼ã€‚</p>

<h4>æ ‡å‡† Value Lossï¼ˆæœ€å¸¸ç”¨ï¼‰</h4>

$$L^{VF}(\phi) = \mathbb{E}_t\left[(V_\phi(s_t) - \hat{R}_t)^2\right]$$

<p>å…¶ä¸­ $\hat{R}_t = \hat{A}_t^{GAE} + V_{\phi_{old}}(s_t)$ æ˜¯ç›®æ ‡å›æŠ¥å€¼ã€‚è¿™å°±æ˜¯ç®€å•çš„ MSE æŸå¤±ã€‚</p>

<h4>Clipped Value Lossï¼ˆå¯é€‰ï¼‰</h4>

<p>ç±»ä¼¼äº policy loss çš„ clippingï¼ŒPPO ä¹Ÿå¯ä»¥å¯¹ value loss è¿›è¡Œè£å‰ªï¼š</p>

$$V_{clip}(s_t) = V_{\phi_{old}}(s_t) + \text{clip}(V_\phi(s_t) - V_{\phi_{old}}(s_t), -\epsilon, \epsilon)$$

$$L^{VF}_{clip}(\phi) = \mathbb{E}_t\left[\max\left((V_\phi(s_t) - \hat{R}_t)^2, \;(V_{clip}(s_t) - \hat{R}_t)^2\right)\right]$$

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ Clipped Value Loss çš„äº‰è®®</div>
  <p>Clipped value loss åœ¨ PPO è®ºæ–‡ä¸­è¢«æåŠï¼Œä½†åç»­ç ”ç©¶ï¼ˆå¦‚ Andrychowicz et al., 2021ï¼‰å‘ç°å®ƒæœ‰æ—¶åè€Œ<strong>æŸå®³æ€§èƒ½</strong>ã€‚åŸå› æ˜¯ï¼šå½“ value ä¼°è®¡ä¸å‡†ç¡®æ—¶ï¼Œclipping ä¼šé˜»æ­¢ critic å¿«é€Ÿä¿®æ­£ã€‚<strong>SB3 é»˜è®¤ä¸ä½¿ç”¨ clipped value loss</strong>ã€‚</p>
  <p>å»ºè®®ï¼šé™¤éæœ‰ç‰¹æ®ŠåŸå› ï¼Œä½¿ç”¨æ ‡å‡† MSE value loss å³å¯ã€‚</p>
</div>

<h4>Huber Loss æ›¿ä»£æ–¹æ¡ˆ</h4>

<p>æŸäº›å®ç°ä½¿ç”¨ Huber loss ä»£æ›¿ MSEï¼Œå¯¹å¼‚å¸¸å€¼æ›´åŠ é²æ£’ï¼š</p>

$$L^{VF}_{Huber}(\phi) = \begin{cases} \frac{1}{2}(V_\phi - \hat{R}_t)^2 & \text{if } |V_\phi - \hat{R}_t| \leq \delta \\ \delta(|V_\phi - \hat{R}_t| - \frac{1}{2}\delta) & \text{otherwise} \end{cases}$$

<h3 id="s4-2">4.2 Entropy Bonus é¼“åŠ±æ¢ç´¢</h3>

<p>ç­–ç•¥çš„<strong>ç†µ</strong> (entropy) è¡¡é‡äº†ç­–ç•¥çš„"éšæœºç¨‹åº¦"ã€‚é«˜ç†µç­–ç•¥æ›´å‡åŒ€åœ°åˆ†é…æ¦‚ç‡ç»™ä¸åŒåŠ¨ä½œï¼ˆæ›´å¤šæ¢ç´¢ï¼‰ï¼Œä½ç†µç­–ç•¥é›†ä¸­æ¦‚ç‡ç»™å°‘æ•°åŠ¨ä½œï¼ˆæ›´å¤šåˆ©ç”¨ï¼‰ã€‚</p>

<h4>ç¦»æ•£åŠ¨ä½œçš„ç†µ</h4>

$$H(\pi(\cdot|s)) = -\sum_a \pi(a|s) \log \pi(a|s)$$

<h4>è¿ç»­åŠ¨ä½œçš„ç†µï¼ˆé«˜æ–¯ç­–ç•¥ï¼‰</h4>

$$H(\mathcal{N}(\mu, \sigma^2)) = \frac{1}{2}\log(2\pi e \sigma^2) = \frac{1}{2}(1 + \log(2\pi\sigma^2))$$

<p>åœ¨ PPO çš„æ€»æŸå¤±å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ª<strong>ç†µå¥–åŠ±é¡¹</strong>ï¼Œé¼“åŠ±ç­–ç•¥ä¿æŒä¸€å®šçš„éšæœºæ€§ï¼š</p>

$$L_{entropy} = -H(\pi_\theta(\cdot|s_t))$$

<p>ç”±äºæˆ‘ä»¬åœ¨æœ€å°åŒ–æ€»æŸå¤±ï¼Œå‡å»ç†µï¼ˆç­‰ä»·äºæœ€å¤§åŒ–ç†µï¼‰å¯ä»¥ï¼š</p>
<ul>
  <li><strong>é˜²æ­¢ç­–ç•¥è¿‡æ—©æ”¶æ•›</strong>åˆ°ç¡®å®šæ€§ç­–ç•¥</li>
  <li><strong>é¼“åŠ±æ¢ç´¢</strong>æœªçŸ¥çš„åŠ¨ä½œå’ŒçŠ¶æ€</li>
  <li><strong>å¹³æ»‘ä¼˜åŒ–æ™¯è§‚</strong>ï¼šç†µæ­£åˆ™åŒ–ä½¿å¾—æŸå¤±è¡¨é¢æ›´å¹³æ»‘</li>
</ul>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ Entropy ç³»æ•°çš„é€‰æ‹©</div>
  <p><strong>å…¸å‹èŒƒå›´</strong>ï¼š$c_2 \in [0.0, 0.05]$</p>
  <ul>
    <li><strong>$c_2 = 0$</strong>ï¼šä¸ä½¿ç”¨ç†µå¥–åŠ±ã€‚é€‚ç”¨äºåŠ¨ä½œç©ºé—´ç®€å•æˆ–ä¸éœ€è¦é¢å¤–æ¢ç´¢çš„ä»»åŠ¡</li>
    <li><strong>$c_2 = 0.01$</strong>ï¼šPPO è®ºæ–‡é»˜è®¤å€¼ã€‚é€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡</li>
    <li><strong>$c_2 = 0.05$</strong>ï¼šæ›´å¼ºçš„æ¢ç´¢ã€‚é€‚ç”¨äºéœ€è¦å¤§é‡æ¢ç´¢çš„å¤æ‚ç¯å¢ƒ</li>
    <li><strong>$c_2$ å¤ªå¤§</strong>ï¼šç­–ç•¥å˜å¾—å¤ªéšæœºï¼Œæ— æ³•æ”¶æ•›åˆ°å¥½çš„ç­–ç•¥</li>
  </ul>
</div>

<h3 id="s4-3">4.3 æ€»æŸå¤±å‡½æ•°</h3>

<p>PPO çš„å®Œæ•´æŸå¤±å‡½æ•°ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼š</p>

<div class="callout callout-theorem">
  <div class="callout-title">ğŸ¯ PPO æ€»æŸå¤±å‡½æ•°</div>
  $$L(\theta, \phi) = \underbrace{L^{CLIP}(\theta)}_{\text{ç­–ç•¥æŸå¤±}} + \underbrace{c_1 \cdot L^{VF}(\phi)}_{\text{ä»·å€¼æŸå¤±}} - \underbrace{c_2 \cdot H(\pi_\theta)}_{\text{ç†µå¥–åŠ±}}$$
  <p>æ³¨æ„ç¬¦å·ï¼šç­–ç•¥æŸå¤± $L^{CLIP}$ æ˜¯è¦æœ€å¤§åŒ–çš„ï¼Œæ‰€ä»¥åœ¨ä»£ç ä¸­å–è´Ÿå·ï¼›ä»·å€¼æŸå¤±æœ€å°åŒ–ï¼›ç†µè¦æœ€å¤§åŒ–ï¼ˆæ‰€ä»¥å‡å»ï¼‰ã€‚</p>
  <p>åœ¨ä»£ç ä¸­ï¼Œæ€»æŸå¤±ä¸ºï¼š</p>
  <p><code>loss = -policy_loss + c1 * value_loss - c2 * entropy</code></p>
</div>

<table>
  <tr><th>ç³»æ•°</th><th>ç¬¦å·</th><th>é»˜è®¤å€¼</th><th>ä½œç”¨</th></tr>
  <tr><td>Value coefficient</td><td>$c_1$</td><td>0.5</td><td>å¹³è¡¡ç­–ç•¥å’Œä»·å€¼å­¦ä¹ çš„æƒé‡</td></tr>
  <tr><td>Entropy coefficient</td><td>$c_2$</td><td>0.01</td><td>æ§åˆ¶æ¢ç´¢ç¨‹åº¦</td></tr>
</table>

<h4>å…±äº«ç½‘ç»œ vs ç‹¬ç«‹ç½‘ç»œ</h4>

<p>Actor å’Œ Critic å¯ä»¥å…±äº«åº•å±‚ç‰¹å¾æå–å™¨ï¼ˆå…±äº«ç½‘ç»œï¼‰æˆ–ä½¿ç”¨ç‹¬ç«‹ç½‘ç»œï¼š</p>

<table>
  <tr><th>æ¶æ„</th><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th><th>é€‚ç”¨åœºæ™¯</th></tr>
  <tr>
    <td>å…±äº«ç½‘ç»œ</td>
    <td>å‚æ•°æ•ˆç‡é«˜ã€ç‰¹å¾å…±äº«</td>
    <td>ä¸¤ä¸ªæŸå¤±å¯èƒ½å†²çªã€éœ€è¦ä»”ç»†è°ƒèŠ‚ $c_1$</td>
    <td>ç®€å•ä»»åŠ¡ã€Atari æ¸¸æˆ</td>
  </tr>
  <tr>
    <td>ç‹¬ç«‹ç½‘ç»œ</td>
    <td>é¿å…æ¢¯åº¦å†²çªã€æ›´ç¨³å®š</td>
    <td>å‚æ•°é‡æ›´å¤§</td>
    <td>å¤æ‚è¿ç»­æ§åˆ¶ã€æœºå™¨äººä»»åŠ¡</td>
  </tr>
</table>

<div class="callout callout-physrobot">
  <div class="callout-title">ğŸ¤– PhysRobot å…³è”</div>
  <p>åœ¨ PhysRobot é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>ç‹¬ç«‹çš„ Actor å’Œ Critic ç½‘ç»œ</strong>ï¼Œå› ä¸ºç‰©ç†ä»¿çœŸä»»åŠ¡ï¼ˆå¦‚ PushBoxï¼‰çš„è§‚æµ‹ç©ºé—´å¤æ‚ï¼Œç­–ç•¥å’Œä»·å€¼å‡½æ•°å¯èƒ½éœ€è¦æå–ä¸åŒçš„ç‰¹å¾ã€‚</p>
</div>

<h3 id="s4-4">4.4 Mini-batch SGD + Multiple Epochs</h3>

<p>PPO çš„ä¸€ä¸ªå…³é”®å·¥ç¨‹åˆ›æ–°æ˜¯åœ¨åŒä¸€æ‰¹æ•°æ®ä¸Šè¿›è¡Œ<strong>å¤šè½®</strong> (multiple epochs) çš„ mini-batch SGD æ›´æ–°ã€‚</p>

<h4>æ•°æ®æ”¶é›†ä¸æ›´æ–°æµç¨‹</h4>

<div class="algorithm-box">
  <div class="algo-title">ç®—æ³• 2: PPO çš„æ•°æ®æ”¶é›†ä¸æ›´æ–°å¾ªç¯</div>
  <ol>
    <li>ç”¨å½“å‰ç­–ç•¥æ”¶é›† $N$ æ­¥ (n_steps) çš„è½¨è¿¹æ•°æ®</li>
    <li>è®¡ç®— GAE ä¼˜åŠ¿ä¼°è®¡å’Œç›®æ ‡å›æŠ¥</li>
    <li>å°†æ•°æ®å­˜å…¥ buffer</li>
    <li><strong>å¾ªç¯ $K$ æ¬¡ (n_epochsï¼Œé€šå¸¸ $K=3 \sim 10$)</strong>ï¼š
      <ol type="a">
        <li>å°† buffer ä¸­çš„ $N$ ä¸ªæ ·æœ¬éšæœºæ‰“ä¹±</li>
        <li>åˆ†æˆ $N/M$ ä¸ª mini-batchï¼ˆæ¯ä¸ªå¤§å° $M$ï¼‰</li>
        <li>å¯¹æ¯ä¸ª mini-batch è®¡ç®— PPO æŸå¤±å¹¶æ›´æ–°å‚æ•°</li>
      </ol>
    </li>
    <li>æ¸…ç©º bufferï¼Œå›åˆ°æ­¥éª¤ 1</li>
  </ol>
</div>

<h4>ä¸ºä»€ä¹ˆå¯ä»¥é‡ç”¨æ•°æ®ï¼Ÿ</h4>

<p>ä¼ ç»Ÿç­–ç•¥æ¢¯åº¦ï¼ˆå¦‚ REINFORCEã€A2Cï¼‰æ˜¯ä¸¥æ ¼ on-policy çš„ï¼šæ¯æ¬¡æ›´æ–°åªèƒ½ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆçš„æ•°æ®ã€‚PPO ä¹‹æ‰€ä»¥èƒ½é‡ç”¨æ•°æ®ï¼Œæ˜¯å› ä¸ºï¼š</p>

<ol>
  <li><strong>é‡è¦æ€§é‡‡æ ·</strong>ï¼š$r_t(\theta) = \pi_\theta / \pi_{\theta_{old}}$ ä¿®æ­£äº†æ–°æ—§ç­–ç•¥çš„åˆ†å¸ƒå·®å¼‚</li>
  <li><strong>Clipping</strong>ï¼šé™åˆ¶äº† $r_t$ çš„èŒƒå›´ï¼Œé˜²æ­¢é‡è¦æ€§é‡‡æ ·æ¯”ç‡è¿‡å¤§å¯¼è‡´çš„é«˜æ–¹å·®</li>
</ol>

<p>ä½†æ³¨æ„ï¼šepoch æ•°ä¸èƒ½å¤ªå¤šã€‚å¦‚æœæ•°æ®è¢«é‡ç”¨å¤ªå¤šæ¬¡ï¼Œç­–ç•¥å·²ç»å˜åŒ–è¾ƒå¤§ï¼Œæ—§æ•°æ®çš„ advantage ä¼°è®¡å°±ä¸å†å‡†ç¡®äº†ã€‚</p>

<h4>n_steps ä¸ batch_size çš„å…³ç³»</h4>

<table>
  <tr><th>å‚æ•°</th><th>å«ä¹‰</th><th>å…¸å‹å€¼</th></tr>
  <tr><td><code>n_steps</code></td><td>æ¯æ¬¡æ”¶é›†çš„æ•°æ®æ­¥æ•°ï¼ˆbuffer å¤§å°ï¼‰</td><td>2048</td></tr>
  <tr><td><code>batch_size</code></td><td>æ¯ä¸ª mini-batch çš„å¤§å°</td><td>64</td></tr>
  <tr><td><code>n_epochs</code></td><td>æ¯æ‰¹æ•°æ®é‡ç”¨çš„è½®æ¬¡</td><td>4 ~ 10</td></tr>
  <tr><td>æ¯æ¬¡æ›´æ–°çš„æ¢¯åº¦æ­¥æ•°</td><td>$K \times \lceil N/M \rceil$</td><td>$4 \times 32 = 128$</td></tr>
</table>

<h3 id="s4-5">4.5 å½’ä¸€åŒ–ä¸é€€ç«æŠ€å·§</h3>

<h4>Advantage Normalization</h4>

<p>åœ¨æ¯æ¬¡æ›´æ–°å‰ï¼Œå¯¹ advantage è¿›è¡Œ z-score æ ‡å‡†åŒ–ï¼š</p>

$$\hat{A}_t \leftarrow \frac{\hat{A}_t - \mu(\hat{A})}{\sigma(\hat{A}) + \epsilon}$$

<p>è¿™æ˜¯ä¸€ä¸ªå‡ ä¹<strong>å…è´¹</strong>çš„æŠ€å·§ï¼Œä½†å¯¹è®­ç»ƒç¨³å®šæ€§æœ‰å·¨å¤§å¸®åŠ©ï¼š</p>
<ul>
  <li>æ¶ˆé™¤å›æŠ¥å°ºåº¦çš„å½±å“ï¼ˆä¸åŒç¯å¢ƒçš„ reward é‡çº§å¯èƒ½å·®å¼‚å¾ˆå¤§ï¼‰</li>
  <li>ä½¿æ¢¯åº¦ä¿¡å·æ›´ä¸€è‡´</li>
  <li>æ ‡å‡†åŒ–åå¤§çº¦ä¸€åŠçš„ advantage æ˜¯æ­£çš„ï¼Œä¸€åŠæ˜¯è´Ÿçš„</li>
</ul>

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ æ³¨æ„ï¼šbatch çº§åˆ« vs mini-batch çº§åˆ«</div>
  <p>Advantage normalization åº”è¯¥åœ¨<strong>æ•´ä¸ª batch çº§åˆ«</strong>ï¼ˆæ‰€æœ‰ n_steps ä¸ªæ ·æœ¬ï¼‰è¿›è¡Œï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸ª mini-batch å†…ç‹¬ç«‹å½’ä¸€åŒ–ã€‚åœ¨ mini-batch å†…å½’ä¸€åŒ–ä¼šå¼•å…¥å™ªå£°ï¼ˆå› ä¸º mini-batch å¤ªå°ï¼Œå‡å€¼å’Œæ–¹å·®ä¸å‡†ç¡®ï¼‰ã€‚</p>
</div>

<h4>å­¦ä¹ ç‡é€€ç« (Learning Rate Annealing)</h4>

<p>çº¿æ€§é€€ç«æ˜¯ PPO ä¸­å¸¸ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼š</p>

$$\text{lr}(t) = \text{lr}_0 \times \left(1 - \frac{t}{T_{total}}\right)$$

<p>å­¦ä¹ ç‡ä»åˆå§‹å€¼çº¿æ€§è¡°å‡åˆ° 0ã€‚ç›´è§‰ï¼š</p>
<ul>
  <li>è®­ç»ƒåˆæœŸï¼šè¾ƒå¤§çš„å­¦ä¹ ç‡å¸®åŠ©å¿«é€Ÿæ¢ç´¢</li>
  <li>è®­ç»ƒåæœŸï¼šè¾ƒå°çš„å­¦ä¹ ç‡å¸®åŠ©ç²¾ç»†è°ƒæ•´ï¼Œé¿å…åœ¨æœ€ä¼˜ç­–ç•¥é™„è¿‘æŒ¯è¡</li>
</ul>

<div class="code-container">
  <div class="code-header">
    <span>lr_annealing.py â€” å­¦ä¹ ç‡é€€ç«å®ç°</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">def</span> <span class="fn">linear_schedule</span>(initial_lr, total_timesteps):
    <span class="str">"""åˆ›å»ºçº¿æ€§é€€ç«çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""</span>
    <span class="kw">def</span> <span class="fn">schedule</span>(progress_remaining):
        <span class="cmt"># progress_remaining: ä» 1.0 çº¿æ€§è¡°å‡åˆ° 0.0</span>
        <span class="kw">return</span> initial_lr * progress_remaining
    <span class="kw">return</span> schedule

<span class="cmt"># åœ¨ PPO è®­ç»ƒä¸­ä½¿ç”¨</span>
<span class="kw">def</span> <span class="fn">update_learning_rate</span>(optimizer, current_step, total_steps, initial_lr):
    <span class="str">"""æ‰‹åŠ¨æ›´æ–°å­¦ä¹ ç‡"""</span>
    frac = <span class="num">1.0</span> - current_step / total_steps
    lr = initial_lr * frac
    <span class="kw">for</span> param_group <span class="kw">in</span> optimizer.param_groups:
        param_group[<span class="str">'lr'</span>] = lr
    <span class="kw">return</span> lr

<span class="cmt"># SB3 ä¸­ä½¿ç”¨çº¿æ€§é€€ç«</span>
<span class="kw">from</span> stable_baselines3 <span class="kw">import</span> PPO
model = PPO(<span class="str">"MlpPolicy"</span>, env,
            learning_rate=linear_schedule(<span class="num">3e-4</span>, <span class="num">1000000</span>))
</code></pre>
</div>

<h4>æ¢¯åº¦è£å‰ª (Gradient Clipping)</h4>

<p>é™åˆ¶æ¢¯åº¦çš„æœ€å¤§èŒƒæ•°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼š</p>

<div class="code-container">
  <div class="code-header">
    <span>gradient_clipping.py</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="cmt"># PPO é»˜è®¤ max_grad_norm = 0.5</span>
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="num">0.5</span>)
</code></pre>
</div>

<h4>æƒé‡åˆå§‹åŒ–</h4>

<p>PPO å¯¹ç½‘ç»œåˆå§‹åŒ–æ¯”è¾ƒæ•æ„Ÿã€‚æ¨èçš„åˆå§‹åŒ–æ–¹æ¡ˆï¼š</p>

<div class="code-container">
  <div class="code-header">
    <span>weight_init.py â€” PPO æ¨èçš„æƒé‡åˆå§‹åŒ–</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">def</span> <span class="fn">layer_init</span>(layer, std=np.sqrt(<span class="num">2</span>), bias_const=<span class="num">0.0</span>):
    <span class="str">"""æ­£äº¤åˆå§‹åŒ–ï¼ˆorthogonal initializationï¼‰"""</span>
    nn.init.orthogonal_(layer.weight, std)
    nn.init.constant_(layer.bias, bias_const)
    <span class="kw">return</span> layer

<span class="cmt"># ä½¿ç”¨æ–¹å¼</span>
<span class="kw">class</span> <span class="cls">PPONetwork</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, obs_dim, act_dim):
        <span class="bi">super</span>().__init__()
        <span class="sf">self</span>.critic = nn.Sequential(
            layer_init(nn.Linear(obs_dim, <span class="num">64</span>)),
            nn.Tanh(),
            layer_init(nn.Linear(<span class="num">64</span>, <span class="num">64</span>)),
            nn.Tanh(),
            layer_init(nn.Linear(<span class="num">64</span>, <span class="num">1</span>), std=<span class="num">1.0</span>),  <span class="cmt"># critic è¾“å‡ºå±‚è¾ƒå°åˆå§‹åŒ–</span>
        )
        <span class="sf">self</span>.actor = nn.Sequential(
            layer_init(nn.Linear(obs_dim, <span class="num">64</span>)),
            nn.Tanh(),
            layer_init(nn.Linear(<span class="num">64</span>, <span class="num">64</span>)),
            nn.Tanh(),
            layer_init(nn.Linear(<span class="num">64</span>, act_dim), std=<span class="num">0.01</span>),  <span class="cmt"># actor è¾“å‡ºå±‚æ›´å°ï¼</span>
        )
</code></pre>
</div>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ ä¸ºä»€ä¹ˆ Actor è¾“å‡ºå±‚ç”¨ std=0.01ï¼Ÿ</div>
  <p>è¿™ä½¿å¾—åˆå§‹ç­–ç•¥æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼ˆæ‰€æœ‰åŠ¨ä½œæ¦‚ç‡æ¥è¿‘ï¼‰ï¼Œä¿è¯åˆå§‹é˜¶æ®µæœ‰è¶³å¤Ÿçš„æ¢ç´¢ã€‚å¦‚æœ Actor è¾“å‡ºå±‚åˆå§‹åŒ–å¤ªå¤§ï¼Œåˆå§‹ç­–ç•¥å¯èƒ½å°±éå¸¸é›†ä¸­åœ¨å°‘æ•°åŠ¨ä½œä¸Šï¼Œå¯¼è‡´æ¢ç´¢ä¸è¶³ã€‚</p>
</div>

<h3 id="s4-6">4.6 ä»£ç ï¼šæ‰‹å†™ PPO vs SB3 PPO å¯¹æ¯”</h3>

<p>è®©æˆ‘ä»¬ç›´æ¥å¯¹æ¯”æ‰‹å†™å®ç°å’Œ SB3ï¼ˆStable-Baselines3ï¼‰çš„ PPOã€‚</p>

<div class="code-container">
  <div class="code-header">
    <span>ppo_sb3_comparison.py â€” SB3 PPO ç­‰æ•ˆå®ç°</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">from</span> stable_baselines3 <span class="kw">import</span> PPO
<span class="kw">from</span> stable_baselines3.common.env_util <span class="kw">import</span> make_vec_env
<span class="kw">from</span> stable_baselines3.common.callbacks <span class="kw">import</span> EvalCallback
<span class="kw">import</span> gymnasium <span class="kw">as</span> gym

<span class="cmt"># ============================================================</span>
<span class="cmt"># SB3 PPO â€” å‡ è¡Œä»£ç æå®š</span>
<span class="cmt"># ============================================================</span>
env = make_vec_env(<span class="str">"CartPole-v1"</span>, n_envs=<span class="num">4</span>)  <span class="cmt"># 4 ä¸ªå¹¶è¡Œç¯å¢ƒ</span>

model = PPO(
    <span class="str">"MlpPolicy"</span>,
    env,
    <span class="cmt"># ---- è¿™äº›å‚æ•°å¯¹åº”æˆ‘ä»¬æ‰‹å†™ç‰ˆæœ¬çš„è®¾ç½® ----</span>
    learning_rate=<span class="num">3e-4</span>,       <span class="cmt"># å­¦ä¹ ç‡</span>
    n_steps=<span class="num">2048</span>,             <span class="cmt"># æ¯ä¸ªç¯å¢ƒæ”¶é›†çš„æ­¥æ•° (æ€»æ•°æ®é‡=n_steps*n_envs)</span>
    batch_size=<span class="num">64</span>,            <span class="cmt"># mini-batch å¤§å°</span>
    n_epochs=<span class="num">10</span>,              <span class="cmt"># æ¯æ‰¹æ•°æ®é‡ç”¨çš„è½®æ¬¡</span>
    gamma=<span class="num">0.99</span>,               <span class="cmt"># æŠ˜æ‰£å› å­</span>
    gae_lambda=<span class="num">0.95</span>,          <span class="cmt"># GAE lambda</span>
    clip_range=<span class="num">0.2</span>,           <span class="cmt"># Îµ (clipping å‚æ•°)</span>
    ent_coef=<span class="num">0.0</span>,             <span class="cmt"># ç†µç³»æ•°ï¼ˆCartPole ä¸éœ€è¦ï¼‰</span>
    vf_coef=<span class="num">0.5</span>,              <span class="cmt"># ä»·å€¼æŸå¤±ç³»æ•°</span>
    max_grad_norm=<span class="num">0.5</span>,        <span class="cmt"># æ¢¯åº¦è£å‰ª</span>
    verbose=<span class="num">1</span>,
)

<span class="cmt"># è®­ç»ƒ</span>
model.learn(total_timesteps=<span class="num">200_000</span>)

<span class="cmt"># è¯„ä¼°</span>
eval_env = gym.make(<span class="str">"CartPole-v1"</span>)
total_reward = <span class="num">0</span>
<span class="kw">for</span> _ <span class="kw">in</span> <span class="bi">range</span>(<span class="num">10</span>):
    obs, _ = eval_env.reset()
    done = <span class="num">False</span>
    ep_reward = <span class="num">0</span>
    <span class="kw">while</span> <span class="kw">not</span> done:
        action, _ = model.predict(obs, deterministic=<span class="num">True</span>)
        obs, reward, terminated, truncated, _ = eval_env.step(action)
        ep_reward += reward
        done = terminated <span class="kw">or</span> truncated
    total_reward += ep_reward
<span class="bi">print</span>(<span class="str">f"Average reward over 10 episodes: {total_reward / 10:.1f}"</span>)
</code></pre>
</div>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ æ‰‹å†™ vs SB3 å¯¹æ¯”æ€»ç»“</div>
  <table>
    <tr><th>ç‰¹æ€§</th><th>æ‰‹å†™ PPO</th><th>SB3 PPO</th></tr>
    <tr><td>ä»£ç é‡</td><td>~200 è¡Œ</td><td>~10 è¡Œ</td></tr>
    <tr><td>å¹¶è¡Œç¯å¢ƒ</td><td>éœ€æ‰‹å†™</td><td>å†…ç½® VecEnv</td></tr>
    <tr><td>è§‚æµ‹å½’ä¸€åŒ–</td><td>éœ€æ‰‹å†™</td><td>VecNormalize å°è£…</td></tr>
    <tr><td>æ—¥å¿—è®°å½•</td><td>éœ€æ‰‹å†™</td><td>å†…ç½® TensorBoard</td></tr>
    <tr><td>ä¿å­˜/åŠ è½½</td><td>éœ€æ‰‹å†™</td><td>model.save()/load()</td></tr>
    <tr><td>å­¦ä¹ ä»·å€¼</td><td>æ·±å…¥ç†è§£ç®—æ³•</td><td>å¿«é€Ÿå®éªŒ</td></tr>
    <tr><td>è°ƒè¯•éš¾åº¦</td><td>å®Œå…¨é€æ˜</td><td>é»‘ç®±ï¼ˆéœ€è¦é˜…è¯»æºç ï¼‰</td></tr>
  </table>
  <p><strong>å»ºè®®</strong>ï¼šå…ˆæ‰‹å†™ä¸€æ¬¡ PPO ç†è§£ç®—æ³•ï¼Œä¹‹åçš„å®éªŒç”¨ SB3ã€‚</p>
</div>


<!-- ============================================================ -->
<!-- CHAPTER 5: æœºå™¨äººæ§åˆ¶åº”ç”¨ -->
<!-- ============================================================ -->
<h2 id="ch5">ç¬¬äº”ç« ï¼šPPO åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„åº”ç”¨</h2>

<p>å‰å››ç« è®²è§£äº† PPO åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆå¦‚ CartPoleï¼‰ä¸­çš„åº”ç”¨ã€‚ä½†æœºå™¨äººæ§åˆ¶é€šå¸¸æ¶‰åŠ<strong>è¿ç»­åŠ¨ä½œç©ºé—´</strong>â€”â€”å…³èŠ‚è§’åº¦ã€åŠ›çŸ©ã€é€Ÿåº¦ç­‰éƒ½æ˜¯è¿ç»­å€¼ã€‚æœ¬ç« è®¨è®ºå¦‚ä½•å°† PPO åº”ç”¨äºè¿ç»­æ§åˆ¶ä»»åŠ¡ã€‚</p>

<h3 id="s5-1">5.1 è¿ç»­åŠ¨ä½œç©ºé—´ï¼šé«˜æ–¯ç­–ç•¥</h3>

<p>åœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­ï¼Œç­–ç•¥ç½‘ç»œä¸å†è¾“å‡ºç¦»æ•£æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œæ˜¯è¾“å‡ºä¸€ä¸ª<strong>é«˜æ–¯åˆ†å¸ƒ</strong>çš„å‚æ•°ï¼š</p>

$$\pi(a|s;\theta) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)$$

<p>å…¶ä¸­ï¼š</p>
<ul>
  <li>$\mu_\theta(s)$ï¼šå‡å€¼ç½‘ç»œï¼Œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œç»´åº¦çš„æœŸæœ›å€¼</li>
  <li>$\sigma_\theta(s)$ï¼šæ ‡å‡†å·®ï¼Œå¯ä»¥æ˜¯å¯å­¦ä¹ å‚æ•°æˆ–ç½‘ç»œè¾“å‡º</li>
</ul>

<h4>ä¸¤ç§æ ‡å‡†å·®å‚æ•°åŒ–æ–¹å¼</h4>

<div class="comparison-grid">
  <div class="comparison-card">
    <h4>çŠ¶æ€æ— å…³çš„ log_stdï¼ˆSB3 é»˜è®¤ï¼‰</h4>
    <p>$\sigma$ ä½œä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œä¸ä¾èµ–äºçŠ¶æ€ï¼š</p>
    <p><code>self.log_std = nn.Parameter(torch.zeros(act_dim))</code></p>
    <ul>
      <li>âœ… å®ç°ç®€å•ï¼Œè®­ç»ƒç¨³å®š</li>
      <li>âŒ æ— æ³•æ ¹æ®çŠ¶æ€è°ƒæ•´æ¢ç´¢ç¨‹åº¦</li>
    </ul>
  </div>
  <div class="comparison-card">
    <h4>çŠ¶æ€ä¾èµ–çš„ Ïƒ(s)</h4>
    <p>$\sigma$ ç”±ç½‘ç»œæ ¹æ®çŠ¶æ€è¾“å‡ºï¼š</p>
    <p><code>self.log_std_head = nn.Linear(hidden, act_dim)</code></p>
    <ul>
      <li>âœ… å¯ä»¥åœ¨ä¸åŒçŠ¶æ€ä½¿ç”¨ä¸åŒçš„æ¢ç´¢ç¨‹åº¦</li>
      <li>âŒ è®­ç»ƒæ›´ä¸ç¨³å®šï¼Œéœ€è¦ä»”ç»†è°ƒå‚</li>
    </ul>
  </div>
</div>

<h4>Log æ¦‚ç‡è®¡ç®—</h4>

<p>å¯¹äºé«˜æ–¯åˆ†å¸ƒï¼Œç»™å®šåŠ¨ä½œ $a$ï¼Œå…¶ log æ¦‚ç‡ä¸ºï¼š</p>

$$\log\pi(a|s) = -\frac{1}{2}\sum_{i=1}^{d}\left[\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2\log\sigma_i + \log(2\pi)\right]$$

<p>å…¶ä¸­ $d$ æ˜¯åŠ¨ä½œç»´åº¦ã€‚åœ¨ä»£ç ä¸­é€šå¸¸ç”¨ PyTorch çš„ <code>Normal</code> åˆ†å¸ƒç±»è‡ªåŠ¨è®¡ç®—ã€‚</p>

<div class="code-container">
  <div class="code-header">
    <span>continuous_policy.py â€” è¿ç»­åŠ¨ä½œé«˜æ–¯ç­–ç•¥</span>
    <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button>
  </div>
  <pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">from</span> torch.distributions <span class="kw">import</span> Normal

<span class="kw">class</span> <span class="cls">ContinuousActorCritic</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, obs_dim, act_dim, hidden=<span class="num">256</span>):
        <span class="bi">super</span>().__init__()

        <span class="cmt"># Actor: è¾“å‡ºåŠ¨ä½œå‡å€¼</span>
        <span class="sf">self</span>.actor_mean = nn.Sequential(
            nn.Linear(obs_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, act_dim),
        )
        <span class="cmt"># çŠ¶æ€æ— å…³çš„ log æ ‡å‡†å·®</span>
        <span class="sf">self</span>.actor_log_std = nn.Parameter(torch.zeros(act_dim))

        <span class="cmt"># Critic: è¾“å‡ºçŠ¶æ€ä»·å€¼</span>
        <span class="sf">self</span>.critic = nn.Sequential(
            nn.Linear(obs_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, <span class="num">1</span>),
        )

    <span class="kw">def</span> <span class="fn">get_action_and_value</span>(<span class="sf">self</span>, obs):
        mean = <span class="sf">self</span>.actor_mean(obs)
        std = <span class="sf">self</span>.actor_log_std.exp()
        dist = Normal(mean, std)

        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-<span class="num">1</span>)  <span class="cmt"># å„ç»´åº¦ç‹¬ç«‹ï¼Œæ±‚å’Œ</span>
        entropy = dist.entropy().sum(dim=-<span class="num">1</span>)
        value = <span class="sf">self</span>.critic(obs).squeeze(-<span class="num">1</span>)

        <span class="kw">return</span> action, log_prob, entropy, value

    <span class="kw">def</span> <span class="fn">evaluate</span>(<span class="sf">self</span>, obs, actions):
        mean = <span class="sf">self</span>.actor_mean(obs)
        std = <span class="sf">self</span>.actor_log_std.exp()
        dist = Normal(mean, std)

        log_prob = dist.log_prob(actions).sum(dim=-<span class="num">1</span>)
        entropy = dist.entropy().sum(dim=-<span class="num">1</span>)
        value = <span class="sf">self</span>.critic(obs).squeeze(-<span class="num">1</span>)

        <span class="kw">return</span> log_prob, entropy, value
</code></pre>
</div>

<h3 id="s5-2">5.2 åŠ¨ä½œè£å‰ªä¸ Squashing (tanh)</h3>

<p>å¤§å¤šæ•°ç‰©ç†ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´æ˜¯æœ‰ç•Œçš„ï¼Œä¾‹å¦‚ $a \in [-1, 1]$ã€‚é«˜æ–¯åˆ†å¸ƒçš„é‡‡æ ·å€¼ç†è®ºä¸Šå¯ä»¥æ˜¯ä»»æ„å®æ•°ï¼Œå› æ­¤éœ€è¦å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚</p>

<h4>æ–¹æ³• 1: ç¡¬è£å‰ª (Hard Clipping)</h4>

<p>ç›´æ¥å°†é‡‡æ ·åŠ¨ä½œè£å‰ªåˆ°åŠ¨ä½œç©ºé—´èŒƒå›´å†…ï¼š</p>

<div class="code-container">
  <div class="code-header"><span>hard_clipping.py</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code>action = dist.sample()
action = torch.clamp(action, -<span class="num">1.0</span>, <span class="num">1.0</span>)  <span class="cmt"># ç¡¬è£å‰ª</span>
</code></pre>
</div>

<p>é—®é¢˜ï¼šè£å‰ªæ”¹å˜äº†åˆ†å¸ƒçš„å½¢çŠ¶ï¼Œå¯¼è‡´ log æ¦‚ç‡è®¡ç®—ä¸å‡†ç¡®ã€‚</p>

<h4>æ–¹æ³• 2: Tanh Squashing (SAC é£æ ¼)</h4>

<p>ç”¨ tanh å‡½æ•°å°†æ— ç•Œçš„é«˜æ–¯é‡‡æ ·å‹ç¼©åˆ° $(-1, 1)$ï¼š</p>

$$a = \tanh(u), \quad u \sim \mathcal{N}(\mu, \sigma^2)$$

<p>éœ€è¦åº”ç”¨å˜é‡æ›¿æ¢å…¬å¼ä¿®æ­£ log æ¦‚ç‡ï¼š</p>

$$\log\pi(a|s) = \log\mathcal{N}(u|\mu,\sigma^2) - \sum_{i=1}^{d}\log(1 - \tanh^2(u_i))$$

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ PPO ä¸­å“ªç§æ–¹æ³•æ›´å¥½ï¼Ÿ</div>
  <p><strong>å¯¹äº PPO</strong>ï¼šé€šå¸¸ä½¿ç”¨<strong>ç¡¬è£å‰ª</strong>æˆ–<strong>ä¸è£å‰ª</strong>ï¼ˆè®©ç¯å¢ƒè‡ªå·±å¤„ç†ï¼‰ã€‚Tanh squashing æ›´å¸¸è§äº SACã€‚åŸå› æ˜¯ PPO çš„ clipping å·²ç»æä¾›äº†è¶³å¤Ÿçš„ç¨³å®šæ€§ï¼Œtanh squashing åè€Œå¢åŠ äº†å®ç°å¤æ‚åº¦ã€‚</p>
  <p><strong>SB3 çš„é»˜è®¤è¡Œä¸º</strong>ï¼šå¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼ŒSB3 çš„ PPO é‡‡æ ·åç›´æ¥å°†åŠ¨ä½œä¼ ç»™ç¯å¢ƒï¼ˆç”± <code>gym.spaces.Box</code> å¤„ç†è¾¹ç•Œï¼‰ã€‚</p>
</div>

<h3 id="s5-3">5.3 è§‚æµ‹å½’ä¸€åŒ– (Observation Normalization)</h3>

<p>åœ¨ç‰©ç†ä»¿çœŸç¯å¢ƒä¸­ï¼Œä¸åŒçš„è§‚æµ‹ç»´åº¦å¯èƒ½æœ‰å®Œå…¨ä¸åŒçš„é‡çº§å’ŒèŒƒå›´ã€‚ä¾‹å¦‚ï¼š</p>
<ul>
  <li>ä½ç½®ï¼š$[-10, 10]$ ç±³</li>
  <li>é€Ÿåº¦ï¼š$[-100, 100]$ ç±³/ç§’</li>
  <li>å…³èŠ‚è§’åº¦ï¼š$[-\pi, \pi]$ å¼§åº¦</li>
  <li>åŠ›/åŠ›çŸ©ï¼š$[-1000, 1000]$ ç‰›</li>
</ul>

<p>å¦‚æœä¸è¿›è¡Œå½’ä¸€åŒ–ï¼Œç½‘ç»œå¾ˆéš¾å­¦åˆ°æœ‰æ•ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚</p>

<h4>Running Mean/Std å½’ä¸€åŒ–</h4>

<p>SB3 æä¾›äº† <code>VecNormalize</code> å°è£…å™¨ï¼Œè‡ªåŠ¨ç»´æŠ¤è§‚æµ‹çš„è¿è¡Œå‡å€¼å’Œæ ‡å‡†å·®ï¼š</p>

$$\hat{s} = \frac{s - \mu_s}{\sigma_s + \epsilon}$$

<div class="code-container">
  <div class="code-header"><span>vec_normalize.py â€” SB3 è§‚æµ‹å½’ä¸€åŒ–</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code><span class="kw">from</span> stable_baselines3.common.vec_env <span class="kw">import</span> VecNormalize, DummyVecEnv

<span class="cmt"># åˆ›å»ºå¸¦å½’ä¸€åŒ–çš„ç¯å¢ƒ</span>
env = DummyVecEnv([<span class="kw">lambda</span>: gym.make(<span class="str">"HalfCheetah-v4"</span>)])
env = VecNormalize(
    env,
    norm_obs=<span class="num">True</span>,       <span class="cmt"># å½’ä¸€åŒ–è§‚æµ‹</span>
    norm_reward=<span class="num">True</span>,    <span class="cmt"># å½’ä¸€åŒ–å¥–åŠ±ï¼ˆå¯é€‰ï¼‰</span>
    clip_obs=<span class="num">10.0</span>,       <span class="cmt"># è£å‰ªå½’ä¸€åŒ–åçš„è§‚æµ‹</span>
    clip_reward=<span class="num">10.0</span>,    <span class="cmt"># è£å‰ªå½’ä¸€åŒ–åçš„å¥–åŠ±</span>
    gamma=<span class="num">0.99</span>,          <span class="cmt"># ç”¨äºå¥–åŠ±æŠ˜æ‰£å½’ä¸€åŒ–</span>
)

model = PPO(<span class="str">"MlpPolicy"</span>, env, verbose=<span class="num">1</span>)
model.learn(<span class="num">1_000_000</span>)

<span class="cmt"># ä¿å­˜æ—¶ä¹Ÿè¦ä¿å­˜å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼</span>
model.save(<span class="str">"ppo_halfcheetah"</span>)
env.save(<span class="str">"vec_normalize.pkl"</span>)
</code></pre>
</div>

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ å¸¸è§é”™è¯¯ï¼šè¯„ä¼°æ—¶å¿˜è®°åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡</div>
  <p>è®­ç»ƒæ—¶ä½¿ç”¨äº† <code>VecNormalize</code>ï¼Œè¯„ä¼°æ—¶å¿…é¡»åŠ è½½ç›¸åŒçš„å½’ä¸€åŒ–å‚æ•°ï¼Œå¦åˆ™ç½‘ç»œæ”¶åˆ°çš„è¾“å…¥å®Œå…¨ä¸åŒï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚</p>
</div>

<h3 id="s5-4">5.4 Reward Shaping æŠ€å·§</h3>

<p>Reward shaping æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æœ€é‡è¦ï¼ˆä¹Ÿæœ€å®¹æ˜“æç ¸ï¼‰çš„æŠ€å·§ä¹‹ä¸€ã€‚å¥½çš„å¥–åŠ±è®¾è®¡å¯ä»¥è®©ç®—æ³•å¿«é€Ÿå­¦åˆ°æœ‰æ•ˆç­–ç•¥ï¼Œå·®çš„å¥–åŠ±è®¾è®¡ä¼šå¯¼è‡´"reward hacking"â€”â€”æ™ºèƒ½ä½“æ‰¾åˆ°äº†æœ€å¤§åŒ–å¥–åŠ±ä½†ä¸ç¬¦åˆé¢„æœŸçš„"æ­ªé—¨é‚ªé“"ã€‚</p>

<h4>ç¨€ç–å¥–åŠ± vs å¯†é›†å¥–åŠ±</h4>

<table>
  <tr><th>ç±»å‹</th><th>ç¤ºä¾‹</th><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr>
  <tr>
    <td>ç¨€ç–å¥–åŠ±</td>
    <td>åªæœ‰æˆåŠŸæ¨ç®±å­åˆ°ç›®æ ‡æ—¶ +1</td>
    <td>å®šä¹‰ç®€å•ï¼Œä¸å¼•å…¥åè§</td>
    <td>æ™ºèƒ½ä½“å¾ˆéš¾å‘ç°æ­£å‘ä¿¡å·</td>
  </tr>
  <tr>
    <td>å¯†é›†å¥–åŠ±</td>
    <td>æ¯æ­¥æ ¹æ®è·ç¦»ç›®æ ‡çš„è¿œè¿‘ç»™å¥–åŠ±</td>
    <td>æ¢¯åº¦ä¿¡å·ä¸°å¯Œï¼Œå­¦ä¹ å¿«</td>
    <td>å¯èƒ½å¼•å…¥é”™è¯¯çš„å½’çº³åç½®</td>
  </tr>
</table>

<h4>å¸¸ç”¨çš„ Reward Shaping ç­–ç•¥</h4>

<ol>
  <li><strong>è·ç¦»å¥–åŠ±</strong>ï¼š$r_t = -\|x_t - x_{goal}\|_2$ï¼ˆè¶Šè¿‘è¶Šå¥½ï¼‰</li>
  <li><strong>é€Ÿåº¦å¥–åŠ±</strong>ï¼š$r_t = v_t \cdot \hat{d}$ï¼ˆå‘ç›®æ ‡æ–¹å‘ç§»åŠ¨å¥–åŠ±ï¼‰</li>
  <li><strong>èƒ½é‡æƒ©ç½š</strong>ï¼š$r_t = -c\|a_t\|^2$ï¼ˆé¼“åŠ±é«˜æ•ˆåŠ¨ä½œï¼‰</li>
  <li><strong>å­˜æ´»å¥–åŠ±</strong>ï¼šæ¯æ­¥ +1ï¼ˆé¼“åŠ±æ´»ç€ï¼‰</li>
  <li><strong>é‡Œç¨‹ç¢‘å¥–åŠ±</strong>ï¼šè¾¾åˆ°ä¸­é—´ç›®æ ‡ç»™é¢å¤–å¥–åŠ±</li>
</ol>

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ Potential-Based Reward Shaping</div>
  <p>Ng et al. (1999) è¯æ˜äº†ä¸€ç§ç‰¹æ®Šçš„ reward shaping æ–¹å¼ä¸ä¼šæ”¹å˜æœ€ä¼˜ç­–ç•¥ï¼š</p>
  $$r'(s, a, s') = r(s, a, s') + \gamma\Phi(s') - \Phi(s)$$
  <p>å…¶ä¸­ $\Phi(s)$ æ˜¯åŠ¿å‡½æ•°ã€‚è¿™ç§å½¢å¼çš„ shaping å¯ä»¥åŠ é€Ÿå­¦ä¹ ä½†ä¿è¯æ”¶æ•›åˆ°åŒä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ã€‚ç›´è§‰ä¸Šï¼Œ$\Phi(s)$ ç›¸å½“äºç»™ value function ä¸€ä¸ª"æç¤º"ã€‚</p>
</div>

<h3 id="s5-5">5.5 PhysRobot PushBox è®­ç»ƒåˆ†æ</h3>

<div class="callout callout-physrobot">
  <div class="callout-title">ğŸ¤– PhysRobot é¡¹ç›®å…³è”ï¼šPushBox ç¯å¢ƒ</div>
  <p>PushBox æ˜¯ PhysRobot é¡¹ç›®ä¸­çš„æ ¸å¿ƒæµ‹è¯•ç¯å¢ƒï¼šä¸€ä¸ªæœºæ¢°è‡‚éœ€è¦å°†æ¡Œé¢ä¸Šçš„ç®±å­æ¨åˆ°ç›®æ ‡ä½ç½®ã€‚</p>
</div>

<h4>V1 ç‰ˆæœ¬ï¼šç¨€ç–å¥–åŠ±ï¼ˆ6% æˆåŠŸç‡ï¼‰</h4>

<p>åˆå§‹ç‰ˆæœ¬ä½¿ç”¨ç®€å•çš„ç¨€ç–å¥–åŠ±ï¼š</p>

<div class="code-container">
  <div class="code-header"><span>pushbox_v1_reward.py â€” V1 ç¨€ç–å¥–åŠ±</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code><span class="kw">def</span> <span class="fn">compute_reward_v1</span>(box_pos, target_pos, threshold=<span class="num">0.05</span>):
    <span class="str">"""V1: ç¨€ç–å¥–åŠ± â€” åªæœ‰æˆåŠŸæ‰æœ‰å¥–åŠ±"""</span>
    dist = np.linalg.norm(box_pos - target_pos)
    <span class="kw">if</span> dist < threshold:
        <span class="kw">return</span> <span class="num">10.0</span>   <span class="cmt"># æˆåŠŸï¼</span>
    <span class="kw">return</span> -<span class="num">0.01</span>    <span class="cmt"># æ¯æ­¥å°æƒ©ç½š</span>
</code></pre>
</div>

<p><strong>é—®é¢˜åˆ†æ</strong>ï¼šPPO åœ¨ V1 å¥–åŠ±ä¸‹åªè¾¾åˆ° 6% çš„æˆåŠŸç‡ã€‚åŸå› ï¼š</p>
<ul>
  <li><strong>ä¿¡ç”¨åˆ†é…å›°éš¾</strong>ï¼šæ™ºèƒ½ä½“å¾ˆå°‘å¶ç„¶åœ°æŠŠç®±å­æ¨åˆ°ç›®æ ‡ï¼Œè·å¾—æ­£å‘ä¿¡å·çš„æ¦‚ç‡å¤ªä½</li>
  <li><strong>æ¢ç´¢ä¸è¶³</strong>ï¼šåœ¨ç¨€ç–å¥–åŠ±ä¸‹ï¼ŒPPO çš„æ ‡å‡†æ¢ç´¢ï¼ˆé«˜æ–¯å™ªå£°ï¼‰ä¸å¤Ÿ</li>
  <li><strong>å¥–åŠ±ä¿¡å·å¤ªå¼±</strong>ï¼šå¤§éƒ¨åˆ†æ—¶é—´æ­¥çš„å¥–åŠ±éƒ½æ˜¯ -0.01ï¼Œæ¢¯åº¦ä¿¡å·å¾®å¼±</li>
</ul>

<h4>V2 ç‰ˆæœ¬ï¼šå¯†é›†å¥–åŠ± + Shaping</h4>

<div class="code-container">
  <div class="code-header"><span>pushbox_v2_reward.py â€” V2 å¯†é›†å¥–åŠ±</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code><span class="kw">def</span> <span class="fn">compute_reward_v2</span>(
    gripper_pos, box_pos, target_pos,
    action, prev_box_dist, prev_gripper_dist
):
    <span class="str">"""V2: å¯†é›†å¥–åŠ± + potential-based shaping"""</span>

    <span class="cmt"># 1. æ¥è¿‘å¥–åŠ±ï¼šé¼“åŠ±å¤¹çˆªé è¿‘ç®±å­</span>
    gripper_dist = np.linalg.norm(gripper_pos - box_pos)
    r_approach = (prev_gripper_dist - gripper_dist) * <span class="num">5.0</span>

    <span class="cmt"># 2. æ¨åŠ¨å¥–åŠ±ï¼šé¼“åŠ±ç®±å­é è¿‘ç›®æ ‡</span>
    box_dist = np.linalg.norm(box_pos - target_pos)
    r_push = (prev_box_dist - box_dist) * <span class="num">10.0</span>

    <span class="cmt"># 3. è·ç¦»æƒ©ç½šï¼šå½“å‰è·ç¦»è¶Šè¿œæƒ©ç½šè¶Šå¤§</span>
    r_dist = -box_dist * <span class="num">0.5</span>

    <span class="cmt"># 4. åŠ¨ä½œèƒ½é‡æƒ©ç½šï¼šé¼“åŠ±é«˜æ•ˆåŠ¨ä½œ</span>
    r_energy = -np.sum(action ** <span class="num">2</span>) * <span class="num">0.01</span>

    <span class="cmt"># 5. æˆåŠŸå¥–åŠ±</span>
    r_success = <span class="num">50.0</span> <span class="kw">if</span> box_dist < <span class="num">0.05</span> <span class="kw">else</span> <span class="num">0.0</span>

    <span class="kw">return</span> r_approach + r_push + r_dist + r_energy + r_success
</code></pre>
</div>

<p><strong>V2 æ”¹è¿›æ•ˆæœ</strong>ï¼šå¯†é›†å¥–åŠ±ç‰ˆæœ¬çš„ PPO æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ï¼ŒåŸå› ï¼š</p>
<ul>
  <li>æ¯æ­¥éƒ½æœ‰éé›¶çš„æ¢¯åº¦ä¿¡å·</li>
  <li>åˆ†é˜¶æ®µå¼•å¯¼ï¼šå…ˆæ¥è¿‘ç®±å­ â†’ å†æ¨ç®±å­ â†’ æœ€ååˆ°è¾¾ç›®æ ‡</li>
  <li>Potential-based shaping ä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥</li>
</ul>

<div class="callout callout-physrobot">
  <div class="callout-title">ğŸ¤– ä¸ºä»€ä¹ˆ PPO åœ¨ PushBox ä¸Šä»æœ‰æŒ‘æˆ˜ï¼Ÿ</div>
  <p>å³ä½¿æœ‰å¯†é›†å¥–åŠ±ï¼Œæ ‡å‡† PPO åœ¨ PushBox ä¸Šä¹Ÿé¢ä¸´å›°éš¾ï¼š</p>
  <ol>
    <li><strong>é«˜ç»´è¿ç»­åŠ¨ä½œç©ºé—´</strong>ï¼š7-DoF æœºæ¢°è‡‚æœ‰ 7 ä¸ªè¿ç»­åŠ¨ä½œç»´åº¦ï¼Œæ¢ç´¢ç©ºé—´å·¨å¤§</li>
    <li><strong>æ¥è§¦åŠ¨åŠ›å­¦</strong>ï¼šæ¨ç®±å­æ¶‰åŠå¤æ‚çš„æ¥è§¦åŠ›å­¦ï¼Œç­–ç•¥éœ€è¦å­¦ä¹ ç²¾ç¡®çš„åŠ›æ§åˆ¶</li>
    <li><strong>é•¿æœŸè§„åˆ’</strong>ï¼šéœ€è¦å¤šæ­¥æœ‰æ„ä¹‰çš„åŠ¨ä½œåºåˆ—ï¼Œå¯¹ $\gamma$ å’Œ GAE $\lambda$ æ•æ„Ÿ</li>
    <li><strong>ç‰©ç†å…ˆéªŒç¼ºå¤±</strong>ï¼šæ ‡å‡† MLP ç­–ç•¥ä¸äº†è§£ç‰©ç†è§„å¾‹ï¼Œéœ€è¦å¤§é‡æ ·æœ¬æ¥éšå¼å­¦ä¹ </li>
  </ol>
  <p>è¿™æ­£æ˜¯ PhysRobot é¡¹ç›®å¼•å…¥<strong>ç‰©ç†å…ˆéªŒ</strong>å’Œ<strong>dual-stream æ–¹æ³•</strong>çš„åŠ¨æœºï¼ˆç¬¬å…­ç« è¯¦è¿°ï¼‰ã€‚</p>
</div>

<h3 id="s5-6">5.6 ä»£ç ï¼šè¿ç»­æ§åˆ¶ PPO å®Œæ•´ç¤ºä¾‹</h3>

<p>ä¸‹é¢æ˜¯åœ¨ MuJoCo HalfCheetah ç¯å¢ƒä¸­ä½¿ç”¨ SB3 PPO çš„å®Œæ•´ç¤ºä¾‹ï¼š</p>

<div class="code-container">
  <div class="code-header"><span>ppo_halfcheetah.py â€” è¿ç»­æ§åˆ¶ PPO å®Œæ•´æµç¨‹</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code><span class="kw">import</span> gymnasium <span class="kw">as</span> gym
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> stable_baselines3 <span class="kw">import</span> PPO
<span class="kw">from</span> stable_baselines3.common.vec_env <span class="kw">import</span> DummyVecEnv, VecNormalize
<span class="kw">from</span> stable_baselines3.common.callbacks <span class="kw">import</span> EvalCallback
<span class="kw">from</span> stable_baselines3.common.monitor <span class="kw">import</span> Monitor

<span class="cmt"># ============================================================</span>
<span class="cmt"># 1. ç¯å¢ƒè®¾ç½®</span>
<span class="cmt"># ============================================================</span>
<span class="kw">def</span> <span class="fn">make_env</span>(env_id, seed):
    <span class="kw">def</span> <span class="fn">_init</span>():
        env = gym.make(env_id)
        env = Monitor(env)
        env.reset(seed=seed)
        <span class="kw">return</span> env
    <span class="kw">return</span> _init

<span class="cmt"># 8 ä¸ªå¹¶è¡Œç¯å¢ƒ</span>
n_envs = <span class="num">8</span>
env = DummyVecEnv([make_env(<span class="str">"HalfCheetah-v4"</span>, seed=i) <span class="kw">for</span> i <span class="kw">in</span> <span class="bi">range</span>(n_envs)])
env = VecNormalize(env, norm_obs=<span class="num">True</span>, norm_reward=<span class="num">True</span>)

<span class="cmt"># ============================================================</span>
<span class="cmt"># 2. PPO é…ç½®ï¼ˆè¿ç»­æ§åˆ¶æ¨èå‚æ•°ï¼‰</span>
<span class="cmt"># ============================================================</span>
model = PPO(
    <span class="str">"MlpPolicy"</span>,
    env,
    learning_rate=<span class="num">3e-4</span>,
    n_steps=<span class="num">2048</span>,            <span class="cmt"># æ¯ä¸ªç¯å¢ƒ 2048 æ­¥</span>
    batch_size=<span class="num">64</span>,
    n_epochs=<span class="num">10</span>,
    gamma=<span class="num">0.99</span>,
    gae_lambda=<span class="num">0.95</span>,
    clip_range=<span class="num">0.2</span>,
    ent_coef=<span class="num">0.0</span>,            <span class="cmt"># è¿ç»­ç©ºé—´é€šå¸¸ä¸éœ€è¦é¢å¤–ç†µå¥–åŠ±</span>
    vf_coef=<span class="num">0.5</span>,
    max_grad_norm=<span class="num">0.5</span>,
    policy_kwargs=<span class="bi">dict</span>(
        net_arch=<span class="bi">dict</span>(
            pi=[<span class="num">256</span>, <span class="num">256</span>],    <span class="cmt"># Actor ç½‘ç»œç»“æ„</span>
            vf=[<span class="num">256</span>, <span class="num">256</span>]     <span class="cmt"># Critic ç½‘ç»œç»“æ„</span>
        ),
        activation_fn=nn.Tanh,  <span class="cmt"># è¿ç»­æ§åˆ¶æ¨è Tanh</span>
    ),
    verbose=<span class="num">1</span>,
    tensorboard_log=<span class="str">"./tb_logs/"</span>,
)

<span class="cmt"># ============================================================</span>
<span class="cmt"># 3. è®­ç»ƒ</span>
<span class="cmt"># ============================================================</span>
eval_callback = EvalCallback(
    VecNormalize(DummyVecEnv([make_env(<span class="str">"HalfCheetah-v4"</span>, seed=<span class="num">100</span>)]),
                 norm_obs=<span class="num">True</span>, norm_reward=<span class="num">False</span>, training=<span class="num">False</span>),
    best_model_save_path=<span class="str">"./best_model/"</span>,
    eval_freq=<span class="num">10000</span>,
    n_eval_episodes=<span class="num">5</span>,
)

model.learn(total_timesteps=<span class="num">1_000_000</span>, callback=eval_callback)

<span class="cmt"># ä¿å­˜</span>
model.save(<span class="str">"ppo_halfcheetah_1M"</span>)
env.save(<span class="str">"vec_normalize_halfcheetah.pkl"</span>)
</code></pre>
</div>


<!-- ============================================================ -->
<!-- CHAPTER 6: é«˜çº§ PPO å˜ä½“ -->
<!-- ============================================================ -->
<h2 id="ch6">ç¬¬å…­ç« ï¼šé«˜çº§ PPO å˜ä½“</h2>

<p>PPO çš„æˆåŠŸå‚¬ç”Ÿäº†è®¸å¤šå˜ä½“å’Œæ‰©å±•ï¼Œé’ˆå¯¹ä¸åŒçš„åº”ç”¨åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚æœ¬ç« ä»‹ç»æœ€é‡è¦çš„äº”ä¸ªæ–¹å‘ã€‚</p>

<h3 id="s6-1">6.1 PPO-Penaltyï¼ˆKL æƒ©ç½šç‰ˆæœ¬ï¼‰</h3>

<p>PPO è®ºæ–‡å®é™…ä¸Šæå‡ºäº†ä¸¤ä¸ªç‰ˆæœ¬ï¼šPPO-Clipï¼ˆç¬¬ä¸‰ç« ï¼‰å’Œ PPO-Penaltyã€‚PPO-Penalty ä½¿ç”¨ KL æ•£åº¦ä½œä¸ºæ­£åˆ™é¡¹è€Œéç¡¬çº¦æŸï¼š</p>

$$L^{KLPEN}(\theta) = \mathbb{E}_t\left[r_t(\theta)\hat{A}_t - \beta \cdot D_{KL}\left(\pi_{\theta_{old}}(\cdot|s_t) \| \pi_\theta(\cdot|s_t)\right)\right]$$

<p>å…¶ä¸­ $\beta$ æ˜¯è‡ªé€‚åº”è°ƒæ•´çš„ KL æƒ©ç½šç³»æ•°ï¼š</p>

<div class="algorithm-box">
  <div class="algo-title">ç®—æ³• 3: PPO-Penalty çš„è‡ªé€‚åº” Î²</div>
  <ol>
    <li>è®¡ç®—æ¯æ¬¡æ›´æ–°åçš„å¹³å‡ KL æ•£åº¦ $d = \hat{\mathbb{E}}_t[D_{KL}(\pi_{old} \| \pi)]$</li>
    <li>å¦‚æœ $d < d_{targ} / 1.5$ï¼š$\beta \leftarrow \beta / 2$ï¼ˆKL å¤ªå°ï¼Œæ”¾æ¾çº¦æŸï¼‰</li>
    <li>å¦‚æœ $d > d_{targ} \times 1.5$ï¼š$\beta \leftarrow \beta \times 2$ï¼ˆKL å¤ªå¤§ï¼ŒåŠ å¼ºçº¦æŸï¼‰</li>
    <li>å¦åˆ™ä¿æŒ $\beta$ ä¸å˜</li>
  </ol>
</div>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ PPO-Clip vs PPO-Penalty</div>
  <table>
    <tr><th>ç‰¹æ€§</th><th>PPO-Clip</th><th>PPO-Penalty</th></tr>
    <tr><td>å®ç°å¤æ‚åº¦</td><td>ä½</td><td>ä¸­ï¼ˆéœ€è¦è‡ªé€‚åº” Î²ï¼‰</td></tr>
    <tr><td>è¶…å‚æ•°</td><td>Îµï¼ˆé€šå¸¸ 0.2ï¼‰</td><td>Î²_init, d_targ</td></tr>
    <tr><td>æ€§èƒ½</td><td>é€šå¸¸æ›´å¥½</td><td>åœ¨æŸäº›ä»»åŠ¡ä¸Šæ›´ç¨³å®š</td></tr>
    <tr><td>ç†è®ºè”ç³»</td><td>å¯å‘å¼</td><td>æ›´æ¥è¿‘ TRPO</td></tr>
    <tr><td>ä½¿ç”¨é¢‘ç‡</td><td>â­â­â­â­â­ï¼ˆä¸»æµï¼‰</td><td>â­â­ï¼ˆè¾ƒå°‘ï¼‰</td></tr>
  </table>
</div>

<h3 id="s6-2">6.2 MAPPOï¼ˆå¤šæ™ºèƒ½ä½“ PPOï¼‰</h3>

<p>MAPPO (Multi-Agent PPO) å°† PPO æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œåœ¨ã€Šæ˜Ÿé™…äº‰éœ¸ã€‹ç­‰å¤æ‚åšå¼ˆä¸­å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚</p>

<h4>æ ¸å¿ƒæ€æƒ³ï¼šé›†ä¸­å¼è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ (CTDE)</h4>

<ul>
  <li><strong>è®­ç»ƒæ—¶</strong>ï¼šæ¯ä¸ªæ™ºèƒ½ä½“çš„ Critic å¯ä»¥çœ‹åˆ°å…¨å±€çŠ¶æ€ï¼ˆé›†ä¸­å¼ä»·å€¼å‡½æ•°ï¼‰</li>
  <li><strong>æ‰§è¡Œæ—¶</strong>ï¼šæ¯ä¸ªæ™ºèƒ½ä½“åªæ ¹æ®è‡ªå·±çš„å±€éƒ¨è§‚æµ‹åšå†³ç­–ï¼ˆåˆ†å¸ƒå¼ç­–ç•¥ï¼‰</li>
</ul>

<div class="callout callout-math">
  <div class="callout-title">ğŸ“ MAPPO çš„å½¢å¼åŒ–</div>
  <p>å¯¹äº $N$ ä¸ªæ™ºèƒ½ä½“ï¼š</p>
  <ul>
    <li>æ¯ä¸ªæ™ºèƒ½ä½“ $i$ æœ‰å±€éƒ¨è§‚æµ‹ $o_i$ å’Œç­–ç•¥ $\pi_i(a_i|o_i;\theta_i)$</li>
    <li>é›†ä¸­å¼ä»·å€¼å‡½æ•°ï¼š$V_i(s;\phi_i)$ï¼Œå…¶ä¸­ $s = (o_1, o_2, \ldots, o_N)$ æ˜¯å…¨å±€çŠ¶æ€</li>
    <li>æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ä½¿ç”¨ PPO æ›´æ–°è‡ªå·±çš„ç­–ç•¥</li>
  </ul>
  <p><strong>å‚æ•°å…±äº«</strong>ï¼šå½“æ™ºèƒ½ä½“æ˜¯åŒè´¨çš„ï¼ˆå¦‚å¤šä¸ªç›¸åŒçš„æœºå™¨äººï¼‰ï¼Œå¯ä»¥è®©æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«åŒä¸€å¥—ç­–ç•¥å‚æ•° $\theta$ï¼Œå¤§å¹…æé«˜æ ·æœ¬æ•ˆç‡ã€‚</p>
</div>

<h4>MAPPO çš„å…³é”®æŠ€å·§</h4>

<ol>
  <li><strong>å…¨å±€çŠ¶æ€</strong>ä½œä¸º Critic è¾“å…¥ï¼ˆè€Œéå•ä¸ªæ™ºèƒ½ä½“çš„è§‚æµ‹ï¼‰</li>
  <li><strong>å‚æ•°å…±äº«</strong>ï¼šåŒè´¨æ™ºèƒ½ä½“å…±äº«ç­–ç•¥ç½‘ç»œ</li>
  <li><strong>ä»·å€¼å½’ä¸€åŒ–</strong>ï¼šä½¿ç”¨ PopArt å¯¹ä»·å€¼ç›®æ ‡è¿›è¡Œå½’ä¸€åŒ–</li>
  <li><strong>æ›´å¤§çš„ batch_size</strong>ï¼šå¤šæ™ºèƒ½ä½“äº§ç”Ÿæ›´å¤šæ•°æ®ï¼Œæ”¯æŒæ›´å¤§çš„ batch</li>
</ol>

<h3 id="s6-3">6.3 PPO + RNN/Transformerï¼ˆåºåˆ—å†³ç­–ï¼‰</h3>

<p>æ ‡å‡† PPO ä½¿ç”¨ MLP ç­–ç•¥ç½‘ç»œï¼Œåªèƒ½åŸºäºå½“å‰è§‚æµ‹åšå†³ç­–ã€‚åœ¨<strong>éƒ¨åˆ†å¯è§‚æµ‹</strong> (POMDP) ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åˆ©ç”¨å†å²ä¿¡æ¯ã€‚</p>

<h4>PPO + LSTM</h4>

<p>å°† MLP æ›¿æ¢ä¸º LSTMï¼Œè®©ç­–ç•¥èƒ½å¤Ÿ"è®°ä½"è¿‡å»çš„è§‚æµ‹ï¼š</p>

<div class="code-container">
  <div class="code-header"><span>ppo_lstm.py â€” å¸¦è®°å¿†çš„ PPO</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code><span class="kw">class</span> <span class="cls">RecurrentActorCritic</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, obs_dim, act_dim, hidden=<span class="num">128</span>):
        <span class="bi">super</span>().__init__()
        <span class="sf">self</span>.feature = nn.Linear(obs_dim, hidden)
        <span class="sf">self</span>.lstm = nn.LSTM(hidden, hidden, batch_first=<span class="num">True</span>)
        <span class="sf">self</span>.actor = nn.Linear(hidden, act_dim)
        <span class="sf">self</span>.critic = nn.Linear(hidden, <span class="num">1</span>)

    <span class="kw">def</span> <span class="fn">forward</span>(<span class="sf">self</span>, obs_seq, hidden_state):
        <span class="str">"""
        obs_seq: [batch, seq_len, obs_dim]
        hidden_state: (h, c) LSTM éšçŠ¶æ€
        """</span>
        x = torch.relu(<span class="sf">self</span>.feature(obs_seq))
        lstm_out, new_hidden = <span class="sf">self</span>.lstm(x, hidden_state)
        policy = <span class="sf">self</span>.actor(lstm_out)
        value = <span class="sf">self</span>.critic(lstm_out)
        <span class="kw">return</span> policy, value, new_hidden

<span class="cmt"># SB3 ç›´æ¥æ”¯æŒ RecurrentPPO</span>
<span class="kw">from</span> sb3_contrib <span class="kw">import</span> RecurrentPPO
model = RecurrentPPO(<span class="str">"MlpLstmPolicy"</span>, env, verbose=<span class="num">1</span>)
</code></pre>
</div>

<h4>PPO + Transformer</h4>

<p>è¿‘å¹´æ¥ï¼ŒTransformer æ¶æ„ä¹Ÿè¢«å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼š</p>
<ul>
  <li><strong>Decision Transformer</strong>ï¼ˆChen et al., 2021ï¼‰ï¼šå°† RL è½¬åŒ–ä¸ºåºåˆ—å»ºæ¨¡é—®é¢˜</li>
  <li><strong>GTrXL</strong>ï¼ˆParisotto et al., 2020ï¼‰ï¼šç¨³å®šçš„ Transformer-XL ç”¨äº RL</li>
</ul>

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ PPO + åºåˆ—æ¨¡å‹çš„æ³¨æ„äº‹é¡¹</div>
  <ul>
    <li><strong>Rollout ç®¡ç†</strong>ï¼šéœ€è¦åœ¨ episode è¾¹ç•Œæ­£ç¡®é‡ç½®éšçŠ¶æ€</li>
    <li><strong>Mini-batch å¤„ç†</strong>ï¼šéœ€è¦æŒ‰åºåˆ—åˆ‡åˆ†æ•°æ®ï¼Œä¸èƒ½éšæœºæ‰“ä¹±æ—¶é—´æ­¥</li>
    <li><strong>è®¡ç®—æˆæœ¬</strong>ï¼šLSTM/Transformer å¢åŠ äº†æ¨ç†å’Œè®­ç»ƒçš„è®¡ç®—é‡</li>
    <li><strong>ä¸ä¸€å®šéœ€è¦</strong>ï¼šå¦‚æœç¯å¢ƒæ˜¯å®Œå…¨å¯è§‚æµ‹çš„ï¼ˆMDPï¼‰ï¼ŒMLP å°±å¤Ÿäº†</li>
  </ul>
</div>

<h3 id="s6-4">6.4 PPO + ç‰©ç†å…ˆéªŒ</h3>

<div class="callout callout-physrobot">
  <div class="callout-title">ğŸ¤– PhysRobot çš„ Dual-Stream æ–¹æ³•</div>
  <p>PhysRobot é¡¹ç›®æå‡ºäº†å°†ç‰©ç†å…ˆéªŒèå…¥ PPO çš„ <strong>dual-stream æ¶æ„</strong>ï¼š</p>
  <ul>
    <li><strong>ç‰©ç†æµ (Physics Stream)</strong>ï¼šåŸºäºç‰©ç†æ¨¡å‹ï¼ˆå¦‚åˆšä½“åŠ¨åŠ›å­¦ã€æ¥è§¦æ¨¡å‹ï¼‰çš„å…ˆéªŒç­–ç•¥ï¼Œæä¾›åŸºç¡€çš„ç‰©ç†åˆç†åŠ¨ä½œ</li>
    <li><strong>å­¦ä¹ æµ (Learning Stream)</strong>ï¼šæ ‡å‡†çš„ PPO ç­–ç•¥ç½‘ç»œï¼Œå­¦ä¹ æ®‹å·®ä¿®æ­£</li>
    <li><strong>èåˆ</strong>ï¼š$a = a_{physics} + \alpha \cdot a_{learned}$ï¼Œå…¶ä¸­ $\alpha$ ä» 0 é€æ¸å¢å¤§</li>
  </ul>
</div>

<h4>ç‰©ç†å…ˆéªŒçš„å‡ ç§èå…¥æ–¹å¼</h4>

<table>
  <tr><th>æ–¹å¼</th><th>æè¿°</th><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr>
  <tr>
    <td>æ®‹å·®ç­–ç•¥</td>
    <td>$a = a_{base} + \pi_\theta(s)$</td>
    <td>ç®€å•ï¼Œä¿ç•™ base ç­–ç•¥çš„ä¼˜åŠ¿</td>
    <td>base ç­–ç•¥éœ€è¦è¶³å¤Ÿå¥½</td>
  </tr>
  <tr>
    <td>è§‚æµ‹å¢å¼º</td>
    <td>å°†ç‰©ç†é‡ï¼ˆåŠ›ã€èƒ½é‡ç­‰ï¼‰åŠ å…¥è§‚æµ‹</td>
    <td>ä¸ä¿®æ”¹ç®—æ³•ï¼Œå³æ’å³ç”¨</td>
    <td>ä¾èµ–ç½‘ç»œæå–æœ‰ç”¨ä¿¡æ¯</td>
  </tr>
  <tr>
    <td>å¥–åŠ±å¡‘é€ </td>
    <td>ç‰©ç†é‡ï¼ˆå¦‚èƒ½é‡æ¶ˆè€—ï¼‰ä½œä¸ºå¥–åŠ±</td>
    <td>å¼•å¯¼ç‰©ç†åˆç†çš„è¡Œä¸º</td>
    <td>å¯èƒ½å¼•å…¥åå·®</td>
  </tr>
  <tr>
    <td>ç½‘ç»œæ¶æ„</td>
    <td>ç‰©ç†å¯¹ç§°æ€§/ç­‰å˜æ€§åµŒå…¥ç½‘ç»œ</td>
    <td>å¼ºå½’çº³åç½®ï¼Œæ ·æœ¬æ•ˆç‡é«˜</td>
    <td>è®¾è®¡å¤æ‚</td>
  </tr>
</table>

<h4>ç‰©ç†ä¿¡æ¯å¢å¼ºçš„è¿ç»­æ§åˆ¶ PPO</h4>

<div class="code-container">
  <div class="code-header"><span>physics_informed_ppo.py â€” ç‰©ç†å…ˆéªŒèåˆç¤ºä¾‹</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code><span class="kw">class</span> <span class="cls">PhysicsInformedActor</span>(nn.Module):
    <span class="str">"""
    èåˆç‰©ç†å…ˆéªŒçš„ Actor ç½‘ç»œ
    - physics_controller: åŸºäºç‰©ç†æ¨¡å‹çš„æ§åˆ¶å™¨
    - residual_net: å­¦ä¹ æ®‹å·®ä¿®æ­£
    """</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>, obs_dim, act_dim, physics_controller):
        <span class="bi">super</span>().__init__()
        <span class="sf">self</span>.physics = physics_controller  <span class="cmt"># é¢„å®šä¹‰çš„ç‰©ç†æ§åˆ¶å™¨</span>

        <span class="cmt"># æ®‹å·®ç½‘ç»œï¼šå­¦ä¹ ç‰©ç†æ¨¡å‹æœªæ•æ‰çš„éƒ¨åˆ†</span>
        <span class="sf">self</span>.residual = nn.Sequential(
            nn.Linear(obs_dim, <span class="num">256</span>), nn.ReLU(),
            nn.Linear(<span class="num">256</span>, <span class="num">256</span>), nn.ReLU(),
            nn.Linear(<span class="num">256</span>, act_dim), nn.Tanh(),
        )
        <span class="sf">self</span>.log_std = nn.Parameter(torch.zeros(act_dim))

        <span class="cmt"># æ··åˆç³»æ•°ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å¢å¤§ï¼‰</span>
        <span class="sf">self</span>.alpha = nn.Parameter(torch.tensor(<span class="num">0.1</span>))

    <span class="kw">def</span> <span class="fn">forward</span>(<span class="sf">self</span>, obs):
        <span class="cmt"># ç‰©ç†å…ˆéªŒåŠ¨ä½œ</span>
        a_physics = <span class="sf">self</span>.physics(obs)
        <span class="cmt"># å­¦ä¹ çš„æ®‹å·®</span>
        a_residual = <span class="sf">self</span>.residual(obs)
        <span class="cmt"># èåˆï¼šç‰©ç† + Î± * æ®‹å·®</span>
        mean = a_physics + <span class="sf">self</span>.alpha.clamp(<span class="num">0</span>, <span class="num">1</span>) * a_residual
        std = <span class="sf">self</span>.log_std.exp()
        <span class="kw">return</span> Normal(mean, std)
</code></pre>
</div>

<h3 id="s6-5">6.5 PPO + è¯¾ç¨‹å­¦ä¹  (Curriculum Learning)</h3>

<p>è¯¾ç¨‹å­¦ä¹ é€šè¿‡é€æ¸å¢åŠ ä»»åŠ¡éš¾åº¦æ¥å¸®åŠ© PPO å­¦ä¹ å¤æ‚ä»»åŠ¡ã€‚è¿™ç±»ä¼¼äºäººç±»å­¦ä¹ ï¼šå…ˆå­¦ç®€å•çš„ï¼Œå†å­¦éš¾çš„ã€‚</p>

<h4>å¸¸è§çš„è¯¾ç¨‹ç­–ç•¥</h4>

<ol>
  <li><strong>ç›®æ ‡è·ç¦»è¯¾ç¨‹</strong>ï¼šå…ˆè®©ç›®æ ‡ä½ç½®å¾ˆè¿‘ï¼Œé€æ¸å¢å¤§è·ç¦»</li>
  <li><strong>ç¯å¢ƒå¤æ‚åº¦è¯¾ç¨‹</strong>ï¼šå…ˆåœ¨ç®€å•ç¯å¢ƒè®­ç»ƒï¼Œé€æ¸å¢åŠ éšœç¢ç‰©</li>
  <li><strong>åŠ¨ä½œçº¦æŸè¯¾ç¨‹</strong>ï¼šå…ˆå…è®¸å¤§èŒƒå›´åŠ¨ä½œï¼Œé€æ¸æ”¶ç´§çº¦æŸ</li>
  <li><strong>è‡ªåŠ¨è¯¾ç¨‹ (PLR/ACCEL)</strong>ï¼šè‡ªåŠ¨é€‰æ‹©é€‚å½“éš¾åº¦çš„è®­ç»ƒå…³å¡</li>
</ol>

<div class="code-container">
  <div class="code-header"><span>curriculum_example.py â€” ç®€å•è¯¾ç¨‹å­¦ä¹ ç¤ºä¾‹</span><button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶</button></div>
  <pre><code><span class="kw">class</span> <span class="cls">CurriculumPushBoxEnv</span>(PushBoxEnv):
    <span class="str">"""å¸¦è¯¾ç¨‹å­¦ä¹ çš„ PushBox ç¯å¢ƒ"""</span>
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="sf">self</span>):
        <span class="bi">super</span>().__init__()
        <span class="sf">self</span>.difficulty = <span class="num">0.0</span>   <span class="cmt"># 0~1, é€æ¸å¢å¤§</span>
        <span class="sf">self</span>.success_history = []

    <span class="kw">def</span> <span class="fn">reset</span>(<span class="sf">self</span>):
        <span class="cmt"># æ ¹æ®éš¾åº¦è®¾ç½®ç›®æ ‡è·ç¦»</span>
        max_dist = <span class="num">0.1</span> + <span class="sf">self</span>.difficulty * <span class="num">0.4</span>  <span class="cmt"># 0.1m ~ 0.5m</span>
        <span class="sf">self</span>.target_pos = <span class="sf">self</span>.box_pos + np.random.uniform(
            -max_dist, max_dist, size=<span class="num">3</span>)
        <span class="kw">return</span> <span class="bi">super</span>().reset()

    <span class="kw">def</span> <span class="fn">update_curriculum</span>(<span class="sf">self</span>, success_rate):
        <span class="str">"""æ ¹æ®æœ€è¿‘çš„æˆåŠŸç‡è°ƒæ•´éš¾åº¦"""</span>
        <span class="kw">if</span> success_rate > <span class="num">0.7</span>:    <span class="cmt"># æˆåŠŸç‡ > 70% â†’ å¢åŠ éš¾åº¦</span>
            <span class="sf">self</span>.difficulty = <span class="bi">min</span>(<span class="num">1.0</span>, <span class="sf">self</span>.difficulty + <span class="num">0.1</span>)
        <span class="kw">elif</span> success_rate < <span class="num">0.3</span>:  <span class="cmt"># æˆåŠŸç‡ < 30% â†’ é™ä½éš¾åº¦</span>
            <span class="sf">self</span>.difficulty = <span class="bi">max</span>(<span class="num">0.0</span>, <span class="sf">self</span>.difficulty - <span class="num">0.05</span>)
</code></pre>
</div>


<!-- ============================================================ -->
<!-- CHAPTER 7: è¶…å‚æ•°è°ƒä¼˜ -->
<!-- ============================================================ -->
<h2 id="ch7">ç¬¬ä¸ƒç« ï¼šè¶…å‚æ•°è°ƒä¼˜æŒ‡å—</h2>

<p>PPO è¢«è®¤ä¸ºå¯¹è¶…å‚æ•°ç›¸å¯¹ä¸æ•æ„Ÿï¼Œä½†"ç›¸å¯¹ä¸æ•æ„Ÿ"ä¸ç­‰äº"ä¸éœ€è¦è°ƒ"ã€‚æœ¬ç« æä¾›ä¸€ä»½å®ç”¨çš„è¶…å‚æ•°è°ƒä¼˜æŒ‡å—ï¼Œå¸®åŠ©ä½ å¿«é€Ÿæ‰¾åˆ°åˆé€‚çš„å‚æ•°é…ç½®ã€‚</p>

<h3 id="s7-1">7.1 å­¦ä¹ ç‡ (Learning Rate)</h3>

<p>å­¦ä¹ ç‡æ˜¯æœ€é‡è¦çš„è¶…å‚æ•°ï¼Œç›´æ¥æ§åˆ¶ç­–ç•¥æ›´æ–°çš„æ­¥é•¿å¤§å°ã€‚</p>

<div class="callout callout-theorem">
  <div class="callout-title">ğŸ¯ ç»éªŒæ³•åˆ™</div>
  <p><strong>é»„é‡‘èŒƒå›´</strong>ï¼š$\text{lr} \in [1 \times 10^{-4}, \; 3 \times 10^{-4}]$</p>
  <p>å‡ ä¹æ‰€æœ‰ç¯å¢ƒéƒ½å¯ä»¥ä» $3 \times 10^{-4}$ å¼€å§‹å°è¯•ã€‚</p>
</div>

<table>
  <tr><th>å­¦ä¹ ç‡</th><th>è¡¨ç°</th><th>é€‚ç”¨åœºæ™¯</th></tr>
  <tr><td>$1 \times 10^{-5}$</td><td>å¤ªå°ï¼Œå­¦ä¹ ææ…¢</td><td>æå°‘ä½¿ç”¨</td></tr>
  <tr><td>$1 \times 10^{-4}$</td><td>ç¨³å®šä½†å¯èƒ½è¾ƒæ…¢</td><td>å¤æ‚è¿ç»­æ§åˆ¶</td></tr>
  <tr><td>$3 \times 10^{-4}$</td><td>â­ é»˜è®¤æ¨èå€¼</td><td>å¤§å¤šæ•°ä»»åŠ¡</td></tr>
  <tr><td>$1 \times 10^{-3}$</td><td>è¾ƒå¿«ä½†å¯èƒ½ä¸ç¨³å®š</td><td>ç®€å•ç¦»æ•£ä»»åŠ¡</td></tr>
  <tr><td>$1 \times 10^{-2}$</td><td>å¤ªå¤§ï¼Œè®­ç»ƒå‘æ•£</td><td>å‡ ä¹ä¸ä½¿ç”¨</td></tr>
</table>

<h4>è¯Šæ–­å­¦ä¹ ç‡é—®é¢˜</h4>
<ul>
  <li><strong>policy_loss æ³¢åŠ¨å‰§çƒˆ</strong>ï¼šå­¦ä¹ ç‡å¯èƒ½å¤ªå¤§</li>
  <li><strong>value_loss ä¸ä¸‹é™</strong>ï¼šå­¦ä¹ ç‡å¯èƒ½å¤ªå°</li>
  <li><strong>approx_kl æŒç»­ > 0.1</strong>ï¼šå­¦ä¹ ç‡å¤ªå¤§ï¼Œç­–ç•¥æ›´æ–°è¿‡æ¿€</li>
  <li><strong>entropy å¿«é€Ÿä¸‹é™åˆ° 0</strong>ï¼šå­¦ä¹ ç‡å¤ªå¤§å¯¼è‡´ç­–ç•¥è¿‡æ—©ç¡®å®šåŒ–</li>
</ul>

<h3 id="s7-2">7.2 Batch Size ä¸ N-steps çš„å…³ç³»</h3>

<p><code>n_steps</code> å’Œ <code>batch_size</code> çš„å…³ç³»ç»å¸¸è®©åˆå­¦è€…å›°æƒ‘ã€‚è®©æˆ‘ä»¬æ˜ç¡®å®ƒä»¬çš„å«ä¹‰å’Œå…³ç³»ï¼š</p>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ å‚æ•°å…³ç³»</div>
  <ul>
    <li><code>n_steps</code>ï¼šæ¯ä¸ªç¯å¢ƒæ”¶é›†çš„æ­¥æ•°ã€‚å¦‚æœæœ‰ $N$ ä¸ªå¹¶è¡Œç¯å¢ƒï¼Œæ€»æ•°æ®é‡ = <code>n_steps Ã— N</code></li>
    <li><code>batch_size</code>ï¼šæ¯ä¸ª mini-batch çš„æ ·æœ¬æ•°</li>
    <li><code>n_epochs</code>ï¼šå¯¹åŒä¸€æ‰¹æ•°æ®è®­ç»ƒçš„è½®æ•°</li>
    <li>æ¯æ¬¡æ›´æ–°çš„æ¢¯åº¦æ­¥æ•° = <code>n_epochs Ã— (n_steps Ã— N / batch_size)</code></li>
  </ul>
  <p>ä¾‹å¦‚ SB3 é»˜è®¤ï¼š<code>n_steps=2048, batch_size=64, n_epochs=10, n_envs=1</code>
  â†’ æ¯æ¬¡æ›´æ–° = $10 \times 2048/64 = 320$ æ­¥æ¢¯åº¦ä¸‹é™</p>
</div>

<table>
  <tr><th>å‚æ•°</th><th>å…¸å‹èŒƒå›´</th><th>å¢å¤§æ•ˆæœ</th><th>å‡å°æ•ˆæœ</th></tr>
  <tr>
    <td><code>n_steps</code></td>
    <td>128 ~ 4096</td>
    <td>æ›´ç¨³å®šçš„ GAE ä¼°è®¡ï¼Œæ›´å¥½çš„é•¿æœŸè§„åˆ’</td>
    <td>æ›´é¢‘ç¹çš„æ›´æ–°ï¼Œä½† GAE ä¼°è®¡å™ªå£°æ›´å¤§</td>
  </tr>
  <tr>
    <td><code>batch_size</code></td>
    <td>32 ~ 512</td>
    <td>æ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®ï¼Œè®­ç»ƒæ›´ç¨³å®š</td>
    <td>æ›´å¤šå™ªå£°ä½†å¯èƒ½é€ƒå‡ºå±€éƒ¨æœ€ä¼˜</td>
  </tr>
  <tr>
    <td><code>n_epochs</code></td>
    <td>3 ~ 30</td>
    <td>æ›´å……åˆ†åœ°åˆ©ç”¨æ•°æ®</td>
    <td>å‡å°‘è¿‡æ‹Ÿåˆæ—§æ•°æ®çš„é£é™©</td>
  </tr>
</table>

<h3 id="s7-3">7.3 Clip Range ($\epsilon$)</h3>

<p><code>clip_range</code>ï¼ˆå³ $\epsilon$ï¼‰æ§åˆ¶ç­–ç•¥æ¯æ¬¡æ›´æ–°å…è®¸å˜åŒ–çš„å¹…åº¦ã€‚</p>

<table>
  <tr><th>clip_range</th><th>è¡Œä¸º</th><th>é€‚ç”¨åœºæ™¯</th></tr>
  <tr><td>0.1</td><td>éå¸¸ä¿å®ˆçš„æ›´æ–°</td><td>è®­ç»ƒä¸ç¨³å®šæ—¶å°è¯•</td></tr>
  <tr><td>0.2</td><td>â­ é»˜è®¤æ¨èå€¼</td><td>å¤§å¤šæ•°ä»»åŠ¡</td></tr>
  <tr><td>0.3</td><td>è¾ƒæ¿€è¿›çš„æ›´æ–°</td><td>ç®€å•ä»»åŠ¡æˆ–éœ€è¦å¿«é€Ÿæ”¶æ•›æ—¶</td></tr>
  <tr><td>0.4+</td><td>æ¥è¿‘æ— çº¦æŸ</td><td>åŸºæœ¬ä¸ä½¿ç”¨</td></tr>
</table>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ åŠ¨æ€ Clip Range</div>
  <p>SB3 æ”¯æŒ clip_range éšè®­ç»ƒè¿›åº¦å˜åŒ–ï¼š</p>
  <p><code>clip_range = lambda progress: 0.2 * progress</code></p>
  <p>ä» 0.2 çº¿æ€§è¡°å‡åˆ° 0ã€‚ç›´è§‰ï¼šè®­ç»ƒåˆæœŸå…è®¸å¤§å¹…æ¢ç´¢ï¼ŒåæœŸç²¾ç»†è°ƒæ•´ã€‚ä½†è¿™ä¸æ˜¯æ ‡å‡†åšæ³•ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹å›ºå®š 0.2 å³å¯ã€‚</p>
</div>

<h3 id="s7-4">7.4 Gamma ($\gamma$) å’Œ GAE Lambda ($\lambda$)</h3>

<h4>æŠ˜æ‰£å› å­ $\gamma$</h4>

<p>$\gamma$ æ§åˆ¶å¯¹æœªæ¥å›æŠ¥çš„é‡è§†ç¨‹åº¦ï¼š</p>

<table>
  <tr><th>$\gamma$</th><th>æœ‰æ•ˆè§†é‡</th><th>è¡Œä¸º</th></tr>
  <tr><td>0.9</td><td>~10 æ­¥</td><td>çŸ­è§†ï¼Œé€‚åˆå¿«é€Ÿå¥–åŠ±åé¦ˆçš„ç¯å¢ƒ</td></tr>
  <tr><td>0.99</td><td>~100 æ­¥</td><td>â­ é»˜è®¤å€¼ï¼Œå¹³è¡¡è¿‘æœŸå’Œè¿œæœŸ</td></tr>
  <tr><td>0.999</td><td>~1000 æ­¥</td><td>é•¿æœŸè§„åˆ’ï¼Œé€‚åˆå¥–åŠ±å»¶è¿Ÿå¤§çš„ä»»åŠ¡</td></tr>
  <tr><td>0.9999</td><td>~10000 æ­¥</td><td>æé•¿æœŸè§„åˆ’ï¼Œä½†å€¼å‡½æ•°ä¼°è®¡å›°éš¾</td></tr>
</table>

<h4>GAE Lambda ($\lambda$)</h4>

<table>
  <tr><th>$\lambda$</th><th>åå·®</th><th>æ–¹å·®</th><th>è¯´æ˜</th></tr>
  <tr><td>0.0</td><td>é«˜</td><td>ä½</td><td>çº¯ 1-step TD</td></tr>
  <tr><td>0.9</td><td>ä¸­</td><td>ä¸­</td><td>åå‘çŸ­æœŸ TD</td></tr>
  <tr><td>0.95</td><td>ä½åä¸­</td><td>ä¸­åé«˜</td><td>â­ é»˜è®¤å€¼</td></tr>
  <tr><td>1.0</td><td>æ— ï¼ˆæœ€ä½ï¼‰</td><td>æœ€é«˜</td><td>çº¯ Monte Carlo</td></tr>
</table>

<div class="callout callout-warning">
  <div class="callout-title">âš ï¸ $\gamma$ å’Œ $\lambda$ çš„äº¤äº’æ•ˆæœ</div>
  <p>GAE ä¸­å®é™…çš„è¡°å‡å› å­æ˜¯ $\gamma\lambda$ã€‚è¿™æ„å‘³ç€ï¼š</p>
  <ul>
    <li>$\gamma = 0.99, \lambda = 0.95$ï¼šå®é™…è¡°å‡ $0.9405$ â†’ æœ‰æ•ˆè§†é‡çº¦ 17 æ­¥</li>
    <li>$\gamma = 0.99, \lambda = 0.99$ï¼šå®é™…è¡°å‡ $0.9801$ â†’ æœ‰æ•ˆè§†é‡çº¦ 50 æ­¥</li>
    <li>$\gamma = 0.999, \lambda = 0.95$ï¼šå®é™…è¡°å‡ $0.94905$ â†’ æœ‰æ•ˆè§†é‡çº¦ 20 æ­¥</li>
  </ul>
  <p>å¦‚æœä»»åŠ¡éœ€è¦é•¿æœŸè§„åˆ’ï¼Œéœ€è¦ $\gamma$ å’Œ $\lambda$ <strong>åŒæ—¶</strong>è¾ƒå¤§ã€‚åªå¢å¤§ $\gamma$ è€Œ $\lambda$ è¾ƒå°ï¼Œä¼˜åŠ¿ä¼°è®¡ä»ç„¶æ˜¯çŸ­è§†çš„ã€‚</p>
</div>

<h3 id="s7-5">7.5 Entropy ç³»æ•° ($c_2$)</h3>

<p>Entropy ç³»æ•°æ§åˆ¶æ¢ç´¢ç¨‹åº¦ã€‚å®ƒçš„æœ€ä¼˜å€¼é«˜åº¦ä¾èµ–äºä»»åŠ¡ï¼š</p>

<table>
  <tr><th>åœºæ™¯</th><th>æ¨è $c_2$</th><th>ç†ç”±</th></tr>
  <tr><td>CartPoleï¼ˆç®€å•ç¦»æ•£ï¼‰</td><td>0.0</td><td>ä»»åŠ¡ç®€å•ï¼Œä¸éœ€è¦é¢å¤–æ¢ç´¢</td></tr>
  <tr><td>Atari æ¸¸æˆ</td><td>0.01</td><td>æ ‡å‡†é»˜è®¤å€¼</td></tr>
  <tr><td>è¿ç»­æ§åˆ¶ï¼ˆMuJoCoï¼‰</td><td>0.0 ~ 0.001</td><td>é«˜æ–¯å™ªå£°å·²æä¾›æ¢ç´¢</td></tr>
  <tr><td>å¤æ‚æ¢ç´¢ï¼ˆç¨€ç–å¥–åŠ±ï¼‰</td><td>0.01 ~ 0.05</td><td>éœ€è¦æ›´å¤šéšæœºæ¢ç´¢</td></tr>
  <tr><td>å¤šæ™ºèƒ½ä½“åšå¼ˆ</td><td>0.01 ~ 0.03</td><td>é˜²æ­¢è¿‡æ—©æ”¶æ•›åˆ°ç¡®å®šæ€§ç­–ç•¥</td></tr>
</table>

<h4>ç›‘æ§ entropy çš„å˜åŒ–</h4>

<p>è®­ç»ƒè¿‡ç¨‹ä¸­çš„ entropy å˜åŒ–å¯ä»¥å¸®åŠ©è¯Šæ–­é—®é¢˜ï¼š</p>
<ul>
  <li><strong>Entropy ç¨³å®šä¸‹é™</strong>ï¼šæ­£å¸¸ï¼Œç­–ç•¥åœ¨é€æ¸å˜å¾—æ›´ç¡®å®š</li>
  <li><strong>Entropy æ€¥å‰§ä¸‹é™åˆ° 0</strong>ï¼šç­–ç•¥è¿‡æ—©åç¼©ï¼Œå¯èƒ½éœ€è¦å¢åŠ  $c_2$ æˆ–é™ä½å­¦ä¹ ç‡</li>
  <li><strong>Entropy ä¸ä¸‹é™</strong>ï¼šç­–ç•¥æ²¡æœ‰åœ¨å­¦ä¹ ï¼Œæ£€æŸ¥ reward signal æ˜¯å¦è¶³å¤Ÿ</li>
  <li><strong>Entropy æ³¢åŠ¨</strong>ï¼šè®­ç»ƒä¸ç¨³å®šï¼Œè€ƒè™‘å‡å°å­¦ä¹ ç‡æˆ– clip_range</li>
</ul>

<h3 id="s7-6">7.6 å¸¸è§ Failure Modes è¯Šæ–­</h3>

<p>å½“ PPO è®­ç»ƒä¸æˆåŠŸæ—¶ï¼Œä»¥ä¸‹æ˜¯æœ€å¸¸è§çš„é—®é¢˜åŠå…¶è¯Šæ–­æ–¹æ³•ï¼š</p>

<h4>Failure Mode 1: ç­–ç•¥å´©æºƒ (Policy Collapse)</h4>

<p><strong>ç—‡çŠ¶</strong>ï¼šreward çªç„¶å¤§å¹…ä¸‹é™ï¼Œä¸”æ— æ³•æ¢å¤</p>
<p><strong>è¯Šæ–­æŒ‡æ ‡</strong>ï¼šapprox_kl çªç„¶å˜å¤§ï¼ˆ> 0.1ï¼‰ï¼Œclip_fraction æ¥è¿‘ 1</p>
<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<ul>
  <li>é™ä½å­¦ä¹ ç‡</li>
  <li>å‡å° clip_range</li>
  <li>å¢åŠ  n_stepsï¼ˆæ›´ç¨³å®šçš„ advantage ä¼°è®¡ï¼‰</li>
  <li>å‡å°‘ n_epochs</li>
</ul>

<h4>Failure Mode 2: å­¦ä¹ åœæ» (Plateau)</h4>

<p><strong>ç—‡çŠ¶</strong>ï¼šreward è¾¾åˆ°æŸä¸ªæ°´å¹³åä¸å†æå‡</p>
<p><strong>è¯Šæ–­æŒ‡æ ‡</strong>ï¼šentropy å¾ˆä½ä½† reward æœªè¾¾æœ€ä¼˜</p>
<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<ul>
  <li>å¢åŠ  entropy ç³»æ•°ï¼Œé¼“åŠ±æ›´å¤šæ¢ç´¢</li>
  <li>ä½¿ç”¨æ›´å¤§çš„ç½‘ç»œ</li>
  <li>æ£€æŸ¥ reward shaping æ˜¯å¦æœ‰è¯¯å¯¼</li>
  <li>å¢åŠ è®­ç»ƒæ€»æ­¥æ•°</li>
</ul>

<h4>Failure Mode 3: é«˜æ–¹å·® (High Variance)</h4>

<p><strong>ç—‡çŠ¶</strong>ï¼šreward æ›²çº¿æ³¢åŠ¨å‰§çƒˆï¼Œå¤šæ¬¡å®éªŒç»“æœå·®å¼‚å¤§</p>
<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<ul>
  <li>å¢åŠ  n_steps å’Œ batch_size</li>
  <li>ä½¿ç”¨æ›´å¤šå¹¶è¡Œç¯å¢ƒ</li>
  <li>å¢åŠ  n_epochsï¼ˆæ›´å……åˆ†åˆ©ç”¨æ•°æ®ï¼‰</li>
  <li>ç¡®ä¿ advantage normalization å¼€å¯</li>
</ul>

<h4>Failure Mode 4: Value Function ä¸å‡†ç¡®</h4>

<p><strong>ç—‡çŠ¶</strong>ï¼švalue_loss å§‹ç»ˆå¾ˆå¤§ï¼Œexplained_variance æ¥è¿‘ 0 æˆ–ä¸ºè´Ÿ</p>
<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<ul>
  <li>å¢å¤§ Critic ç½‘ç»œå®¹é‡</li>
  <li>å¢åŠ  value_coefï¼ˆ$c_1$ï¼‰</li>
  <li>ä½¿ç”¨ç‹¬ç«‹çš„ Critic ç½‘ç»œ</li>
  <li>é™ä½ $\gamma$ï¼ˆå‡å° value function éœ€è¦é¢„æµ‹çš„èŒƒå›´ï¼‰</li>
</ul>

<h3 id="s7-7">7.7 å®æˆ˜ Checklist</h3>

<p>å½“ä½ å¼€å§‹ä¸€ä¸ªæ–°çš„ PPO è®­ç»ƒä»»åŠ¡æ—¶ï¼ŒæŒ‰ç…§ä»¥ä¸‹ checklist ä¾æ¬¡æ£€æŸ¥ï¼š</p>

<ul class="checklist">
  <li><strong>ç¯å¢ƒéªŒè¯</strong>ï¼šéšæœºç­–ç•¥èƒ½å¦æ­£å¸¸è¿è¡Œï¼Ÿè§‚æµ‹å’Œå¥–åŠ±èŒƒå›´æ˜¯å¦åˆç†ï¼Ÿ</li>
  <li><strong>è§‚æµ‹å½’ä¸€åŒ–</strong>ï¼šæ˜¯å¦ä½¿ç”¨äº† VecNormalize æˆ–æ‰‹åŠ¨å½’ä¸€åŒ–ï¼Ÿ</li>
  <li><strong>å¥–åŠ±èŒƒå›´</strong>ï¼šå¥–åŠ±å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆé€šå¸¸ [-10, 10]ï¼‰ï¼Ÿæ˜¯å¦éœ€è¦ reward normalizationï¼Ÿ</li>
  <li><strong>é»˜è®¤è¶…å‚æ•°å¯åŠ¨</strong>ï¼šå…ˆç”¨ SB3 é»˜è®¤å‚æ•°è·‘ä¸€æ¬¡åŸºçº¿</li>
  <li><strong>TensorBoard ç›‘æ§</strong>ï¼špolicy_loss, value_loss, entropy, approx_kl, clip_fraction éƒ½æ­£å¸¸ï¼Ÿ</li>
  <li><strong>å¤šä¸ªéšæœºç§å­</strong>ï¼šè‡³å°‘ 3-5 ä¸ªç§å­ï¼Œç¡®è®¤ç»“æœçš„ä¸€è‡´æ€§</li>
  <li><strong>å­¦ä¹ ç‡è°ƒæ•´</strong>ï¼šåŸºçº¿æ•ˆæœä¸å¥½æ—¶ï¼Œé¦–å…ˆå°è¯•è°ƒå­¦ä¹ ç‡</li>
  <li><strong>ç½‘ç»œå®¹é‡</strong>ï¼šä»»åŠ¡å¤æ‚æ—¶å¢å¤§éšè—å±‚ï¼ˆ128 â†’ 256 â†’ 512ï¼‰</li>
  <li><strong>n_steps è°ƒæ•´</strong>ï¼šå¦‚æœéœ€è¦é•¿æœŸè§„åˆ’ï¼Œå¢å¤§ n_steps</li>
  <li><strong>è®­ç»ƒæ—¶é—´</strong>ï¼šç¡®ä¿æ€»è®­ç»ƒæ­¥æ•°è¶³å¤Ÿï¼ˆè¿ç»­æ§åˆ¶é€šå¸¸éœ€è¦ 1M+ æ­¥ï¼‰</li>
</ul>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ SB3 æ¨èèµ·å§‹å‚æ•°</div>
  <p>å¯¹äºå¤§å¤šæ•°è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼š</p>
  <table>
    <tr><th>å‚æ•°</th><th>æ¨èå€¼</th></tr>
    <tr><td>learning_rate</td><td>3e-4</td></tr>
    <tr><td>n_steps</td><td>2048</td></tr>
    <tr><td>batch_size</td><td>64</td></tr>
    <tr><td>n_epochs</td><td>10</td></tr>
    <tr><td>gamma</td><td>0.99</td></tr>
    <tr><td>gae_lambda</td><td>0.95</td></tr>
    <tr><td>clip_range</td><td>0.2</td></tr>
    <tr><td>ent_coef</td><td>0.0</td></tr>
    <tr><td>vf_coef</td><td>0.5</td></tr>
    <tr><td>max_grad_norm</td><td>0.5</td></tr>
    <tr><td>net_arch</td><td>[256, 256]</td></tr>
    <tr><td>activation_fn</td><td>Tanh</td></tr>
  </table>
</div>


<!-- ============================================================ -->
<!-- APPENDIX A: PPO è®ºæ–‡è§£è¯» -->
<!-- ============================================================ -->
<h2 id="appA">é™„å½• Aï¼šPPO è®ºæ–‡é€æ®µè§£è¯»</h2>

<p>è®ºæ–‡å…¨ç§°ï¼š<em>"Proximal Policy Optimization Algorithms"</em> (Schulman, Wolski, Dhariwal, Radford, Klimov, 2017)</p>

<h3>Abstract è§£è¯»</h3>

<p>è®ºæ–‡å¼€ç¯‡æŒ‡å‡ºï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨æ•°æ®æ•ˆç‡å’Œé²æ£’æ€§æ–¹é¢éœ€è¦æ”¹è¿›ã€‚ä½œè€…æå‡ºäº†ä¸€æ—æ–°çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œåœ¨è¢«ä¿¡èµ–åŸŸçº¦æŸå’Œä¸€é˜¶ä¼˜åŒ–æ–¹æ³•ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯ä½¿ç”¨ <strong>clipped surrogate objective</strong>ï¼Œé€šè¿‡å¤šè½® mini-batch SGD æ¥è¿‘ä¼¼ trust region æ›´æ–°ã€‚</p>

<h3>Section 1: Introduction</h3>

<p>è®ºæ–‡åˆ—ä¸¾äº†å½“æ—¶ RL ç®—æ³•çš„å‡ ä¸ªç—›ç‚¹ï¼š</p>
<ul>
  <li>Q-learning (DQN)ï¼šéš¾ä»¥ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œä¸”ä½¿ç”¨å‡½æ•°é€¼è¿‘æ—¶ç¼ºä¹ç†è®ºä¿è¯</li>
  <li>Vanilla Policy Gradient (REINFORCE)ï¼šé«˜æ–¹å·®ï¼Œæ•°æ®åˆ©ç”¨ç‡ä½</li>
  <li>TRPOï¼šæ•ˆæœå¥½ä½†å®ç°å¤æ‚ï¼ˆéœ€è¦äºŒé˜¶æ–¹æ³•ï¼‰ï¼Œä¸”éš¾ä»¥ä¸å‚æ•°å…±äº«æˆ–è¾…åŠ©æŸå¤±ç»“åˆ</li>
</ul>

<p>PPO çš„ç›®æ ‡æ˜¯ï¼šä¿æŒ TRPO çš„ç¨³å®šæ€§ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€é˜¶ä¼˜åŒ–ï¼Œè®©å®ç°æ›´ç®€å•ã€é€šç”¨æ€§æ›´å¼ºã€‚</p>

<h3>Section 2: Background</h3>

<p>è®ºæ–‡å›é¡¾äº†ç­–ç•¥æ¢¯åº¦å’Œ TRPO çš„åŸºç¡€ã€‚å…³é”®å…¬å¼ï¼š</p>

$$L^{CPI}(\theta) = \hat{\mathbb{E}}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A}_t\right] = \hat{\mathbb{E}}_t\left[r_t(\theta)\hat{A}_t\right]$$

<p>æŒ‡å‡º $L^{CPI}$ å¦‚æœä¸åŠ çº¦æŸåœ°æœ€å¤§åŒ–ï¼Œå¯èƒ½å¯¼è‡´è¿‡å¤§çš„ç­–ç•¥æ›´æ–°ã€‚</p>

<h3>Section 3: Clipped Surrogate Objective</h3>

<p>è¿™æ˜¯è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€‚æå‡ºä¸¤ç§ PPO å˜ä½“ï¼š</p>

<h4>PPO-Clipï¼ˆSection 3.1ï¼‰</h4>

$$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t\left[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\right]$$

<p>è®ºæ–‡ä¸­çš„ Figure 1 å±•ç¤ºäº†ä¸åŒ $\hat{A}$ ç¬¦å·ä¸‹çš„ç›®æ ‡å‡½æ•°å½¢çŠ¶ã€‚$\min$ æ“ä½œå½¢æˆäº†ä¸€ä¸ªæ‚²è§‚ä¸‹ç•Œï¼ˆpessimistic boundï¼‰ã€‚</p>

<h4>PPO-Penaltyï¼ˆSection 3.2ï¼‰</h4>

<p>ä½¿ç”¨è‡ªé€‚åº” KL æƒ©ç½šç³»æ•°ã€‚è®ºæ–‡æŒ‡å‡ºåœ¨å®éªŒä¸­ PPO-Clip é€šå¸¸ä¼˜äº PPO-Penaltyã€‚</p>

<h3>Section 4: Algorithm</h3>

<p>è®ºæ–‡æè¿°äº†å®Œæ•´çš„ç®—æ³•æµç¨‹ï¼š</p>
<ol>
  <li>ä½¿ç”¨å¤šä¸ªå¹¶è¡Œ actor æ”¶é›†æ•°æ®</li>
  <li>åœ¨æ”¶é›†çš„æ•°æ®ä¸Šè¿è¡Œå¤šè½® epoch çš„ mini-batch SGD</li>
  <li>æ€»æŸå¤±åŒ…å«ç­–ç•¥ç›®æ ‡ã€ä»·å€¼å‡½æ•°æŸå¤±å’Œç†µå¥–åŠ±</li>
</ol>

<p>è®ºæ–‡è¿˜æåˆ°ä¸¤ç§æ¶æ„é€‰æ‹©ï¼š</p>
<ul>
  <li><strong>å…±äº«å‚æ•°</strong>ï¼šç­–ç•¥å’Œä»·å€¼å‡½æ•°å…±äº«ç‰¹å¾æå–å±‚ï¼Œéœ€è¦ç»„åˆæŸå¤±</li>
  <li><strong>ç‹¬ç«‹ç½‘ç»œ</strong>ï¼šç­–ç•¥å’Œä»·å€¼å‡½æ•°å®Œå…¨ç‹¬ç«‹</li>
</ul>

<h3>Section 5 & 6: Experiments</h3>

<p>è®ºæ–‡åœ¨ä»¥ä¸‹ç¯å¢ƒä¸­æµ‹è¯•äº† PPOï¼š</p>
<ul>
  <li><strong>è¿ç»­æ§åˆ¶</strong>ï¼ˆMuJoCoï¼‰ï¼šä¸ TRPOã€CEMã€A2C ç­‰å¯¹æ¯”</li>
  <li><strong>Atari æ¸¸æˆ</strong>ï¼šä¸ A2Cã€ACER å¯¹æ¯”</li>
</ul>

<p>å…³é”®å®éªŒç»“æœï¼š</p>
<ul>
  <li>PPO-Clip åœ¨å¤§å¤šæ•°ç¯å¢ƒä¸­ä¼˜äºæˆ–æŒå¹³å…¶ä»–æ–¹æ³•</li>
  <li>PPO å¯¹è¶…å‚æ•°ä¸å¤ªæ•æ„Ÿï¼ˆ$\epsilon$ åœ¨ 0.1~0.3 èŒƒå›´å†…æ•ˆæœç›¸è¿‘ï¼‰</li>
  <li>KL penalty ç‰ˆæœ¬é€šå¸¸ä¸å¦‚ clipping ç‰ˆæœ¬</li>
</ul>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ è®ºæ–‡ä¸­æœªè§£ç­”çš„é—®é¢˜</div>
  <ul>
    <li>PPO ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿclipping æä¾›äº†ä»€ä¹ˆç†è®ºä¿è¯ï¼Ÿï¼ˆè‡³ä»Šä»æ˜¯å¼€æ”¾é—®é¢˜ï¼‰</li>
    <li>æœ€ä¼˜çš„ $\epsilon$ æ˜¯å¦ä¸ç¯å¢ƒæœ‰å…³ï¼Ÿå¦‚ä½•è‡ªé€‚åº”é€‰æ‹©ï¼Ÿ</li>
    <li>PPO åœ¨å“ªäº›åœºæ™¯ä¸‹ä¼šå¤±è´¥ï¼Ÿï¼ˆåç»­å·¥ä½œæœ‰è®¨è®ºï¼‰</li>
  </ul>
</div>


<!-- ============================================================ -->
<!-- APPENDIX B: ç®—æ³•å¯¹æ¯” -->
<!-- ============================================================ -->
<h2 id="appB">é™„å½• Bï¼šPPO vs å…¶ä»–ç®—æ³•å¯¹æ¯”</h2>

<p>æœ¬é™„å½•å¯¹æ¯” PPO ä¸å…¶ä»–ä¸»æµæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¸®åŠ©ä½ åœ¨å®é™…é¡¹ç›®ä¸­é€‰æ‹©åˆé€‚çš„ç®—æ³•ã€‚</p>

<h3>B.1 ç®—æ³•å…¨é¢å¯¹æ¯”</h3>

<table>
  <tr>
    <th>ç‰¹æ€§</th>
    <th>PPO</th>
    <th>TRPO</th>
    <th>SAC</th>
    <th>TD3</th>
  </tr>
  <tr>
    <td>ç±»å‹</td>
    <td>On-policy</td>
    <td>On-policy</td>
    <td>Off-policy</td>
    <td>Off-policy</td>
  </tr>
  <tr>
    <td>åŠ¨ä½œç©ºé—´</td>
    <td>ç¦»æ•£ + è¿ç»­</td>
    <td>ç¦»æ•£ + è¿ç»­</td>
    <td>è¿ç»­</td>
    <td>è¿ç»­</td>
  </tr>
  <tr>
    <td>é‡‡æ ·æ•ˆç‡</td>
    <td>ä¸­</td>
    <td>ä¸­</td>
    <td>â­ é«˜</td>
    <td>â­ é«˜</td>
  </tr>
  <tr>
    <td>è®­ç»ƒç¨³å®šæ€§</td>
    <td>â­ é«˜</td>
    <td>â­ é«˜</td>
    <td>é«˜</td>
    <td>ä¸­é«˜</td>
  </tr>
  <tr>
    <td>å®ç°å¤æ‚åº¦</td>
    <td>â­ ä½</td>
    <td>é«˜ï¼ˆäºŒé˜¶æ–¹æ³•ï¼‰</td>
    <td>ä¸­</td>
    <td>ä¸­</td>
  </tr>
  <tr>
    <td>è¶…å‚æ•°æ•æ„Ÿåº¦</td>
    <td>â­ ä½</td>
    <td>ä½</td>
    <td>ä¸­</td>
    <td>ä¸­</td>
  </tr>
  <tr>
    <td>å¹¶è¡ŒåŒ–</td>
    <td>â­ ä¼˜ç§€</td>
    <td>ä¼˜ç§€</td>
    <td>æœ‰é™</td>
    <td>æœ‰é™</td>
  </tr>
  <tr>
    <td>Replay Buffer</td>
    <td>ä¸éœ€è¦</td>
    <td>ä¸éœ€è¦</td>
    <td>éœ€è¦</td>
    <td>éœ€è¦</td>
  </tr>
  <tr>
    <td>éšæœºç­–ç•¥</td>
    <td>âœ…</td>
    <td>âœ…</td>
    <td>âœ…ï¼ˆæœ€å¤§ç†µï¼‰</td>
    <td>âŒï¼ˆç¡®å®šæ€§ï¼‰</td>
  </tr>
  <tr>
    <td>å¤šæ™ºèƒ½ä½“æ‰©å±•</td>
    <td>â­ MAPPO</td>
    <td>æœ‰é™</td>
    <td>MASAC</td>
    <td>MATD3</td>
  </tr>
  <tr>
    <td>RLHF é€‚ç”¨</td>
    <td>â­ ChatGPT</td>
    <td>å¯ä»¥</td>
    <td>ç†è®ºä¸Šå¯ä»¥</td>
    <td>ä¸é€‚ç”¨</td>
  </tr>
</table>

<h3>B.2 ä½•æ—¶é€‰æ‹© PPOï¼Ÿ</h3>

<div class="callout callout-tip">
  <div class="callout-title">ğŸ’¡ ç®—æ³•é€‰æ‹©å†³ç­–æ ‘</div>
  <ol>
    <li><strong>ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Ÿ</strong> â†’ PPO æˆ– DQN</li>
    <li><strong>è¿ç»­åŠ¨ä½œ + éœ€è¦å¤§é‡å¹¶è¡Œï¼Ÿ</strong> â†’ PPOï¼ˆon-policy å¤©ç„¶æ”¯æŒå¹¶è¡Œï¼‰</li>
    <li><strong>è¿ç»­åŠ¨ä½œ + æ ·æœ¬çè´µï¼Ÿ</strong> â†’ SACï¼ˆoff-policy æ›´æ ·æœ¬é«˜æ•ˆï¼‰</li>
    <li><strong>éœ€è¦ç¡®å®šæ€§ç­–ç•¥ï¼Ÿ</strong> â†’ TD3</li>
    <li><strong>å¤šæ™ºèƒ½ä½“ï¼Ÿ</strong> â†’ MAPPO</li>
    <li><strong>RLHFï¼Ÿ</strong> â†’ PPO</li>
    <li><strong>ä¸ç¡®å®šé€‰ä»€ä¹ˆï¼Ÿ</strong> â†’ PPOï¼ˆæœ€å®‰å…¨çš„é»˜è®¤é€‰æ‹©ï¼‰</li>
  </ol>
</div>

<h3>B.3 PPO vs SAC æ·±åº¦å¯¹æ¯”</h3>

<p>PPO å’Œ SAC æ˜¯å½“å‰æœ€æµè¡Œçš„ä¸¤ä¸ªç®—æ³•ï¼Œå„æœ‰æ‰€é•¿ï¼š</p>

<div class="comparison-grid">
  <div class="comparison-card">
    <h4>PPO çš„ä¼˜åŠ¿</h4>
    <ul>
      <li>â­ å®ç°ç®€å•ï¼Œbug å°‘</li>
      <li>â­ å¤©ç„¶æ”¯æŒç¦»æ•£å’Œè¿ç»­åŠ¨ä½œ</li>
      <li>â­ å®¹æ˜“å¹¶è¡ŒåŒ–ï¼ˆGPU åˆ©ç”¨ç‡é«˜ï¼‰</li>
      <li>â­ å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ</li>
      <li>â­ å¤šæ™ºèƒ½ä½“æ‰©å±•æˆç†Ÿï¼ˆMAPPOï¼‰</li>
    </ul>
  </div>
  <div class="comparison-card">
    <h4>SAC çš„ä¼˜åŠ¿</h4>
    <ul>
      <li>â­ æ ·æœ¬æ•ˆç‡æ›´é«˜ï¼ˆoff-policy + replay bufferï¼‰</li>
      <li>â­ æœ€å¤§ç†µæ¡†æ¶å¤©ç„¶é¼“åŠ±æ¢ç´¢</li>
      <li>â­ è‡ªåŠ¨è°ƒèŠ‚æ¸©åº¦å‚æ•° Î±</li>
      <li>â­ å¯¹ reward scale ä¸æ•æ„Ÿ</li>
      <li>â­ åœ¨å¤æ‚è¿ç»­æ§åˆ¶ä¸­å¸¸æ›´ä¼˜</li>
    </ul>
  </div>
</div>

<h3>B.4 æ€§èƒ½åŸºå‡†å‚è€ƒ</h3>

<p>ä»¥ä¸‹æ˜¯ä¸åŒç®—æ³•åœ¨ MuJoCo ç¯å¢ƒä¸­çš„å…¸å‹æ€§èƒ½ï¼ˆ1M æ­¥è®­ç»ƒåï¼‰ï¼š</p>

<table>
  <tr><th>ç¯å¢ƒ</th><th>PPO</th><th>SAC</th><th>TD3</th></tr>
  <tr><td>HalfCheetah-v4</td><td>~5000</td><td>~11000</td><td>~9500</td></tr>
  <tr><td>Hopper-v4</td><td>~2500</td><td>~3200</td><td>~3000</td></tr>
  <tr><td>Walker2d-v4</td><td>~3500</td><td>~4500</td><td>~4000</td></tr>
  <tr><td>Ant-v4</td><td>~3000</td><td>~5500</td><td>~4500</td></tr>
</table>

<div class="callout callout-info">
  <div class="callout-title">ğŸ“˜ æ³¨æ„</div>
  <p>ä¸Šè¡¨æ•°å­—ä»…ä¾›å‚è€ƒï¼Œå®é™…æ€§èƒ½é«˜åº¦ä¾èµ–äºè¶…å‚æ•°ã€éšæœºç§å­å’Œè®­ç»ƒæ—¶é—´ã€‚SAC å’Œ TD3 åœ¨ç»™å®šæ›´å¤šè®­ç»ƒæ­¥æ•°åé€šå¸¸èƒ½è¾¾åˆ°æ›´é«˜åˆ†æ•°ï¼ˆå› ä¸º off-policy çš„æ ·æœ¬æ•ˆç‡ä¼˜åŠ¿ï¼‰ï¼Œä½† PPO åœ¨è®­ç»ƒç¨³å®šæ€§å’Œå®ç°ç®€æ´æ€§æ–¹é¢å§‹ç»ˆæœ‰ä¼˜åŠ¿ã€‚</p>
</div>


<!-- ============================================================ -->
<!-- REFERENCES -->
<!-- ============================================================ -->
<h2 id="references">å‚è€ƒæ–‡çŒ®</h2>

<ol>
  <li><strong>Schulman, J., et al.</strong> (2017). <em>Proximal Policy Optimization Algorithms.</em> arXiv:1707.06347. [PPO åŸè®ºæ–‡]</li>
  <li><strong>Schulman, J., et al.</strong> (2015). <em>Trust Region Policy Optimization.</em> ICML 2015. [TRPO]</li>
  <li><strong>Schulman, J., et al.</strong> (2015). <em>High-Dimensional Continuous Control Using Generalized Advantage Estimation.</em> arXiv:1506.02438. [GAE]</li>
  <li><strong>Williams, R. J.</strong> (1992). <em>Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.</em> Machine Learning. [REINFORCE]</li>
  <li><strong>Mnih, V., et al.</strong> (2016). <em>Asynchronous Methods for Deep Reinforcement Learning.</em> ICML 2016. [A3C/A2C]</li>
  <li><strong>Haarnoja, T., et al.</strong> (2018). <em>Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL with a Stochastic Actor.</em> ICML 2018. [SAC]</li>
  <li><strong>Fujimoto, S., et al.</strong> (2018). <em>Addressing Function Approximation Error in Actor-Critic Methods.</em> ICML 2018. [TD3]</li>
  <li><strong>Yu, C., et al.</strong> (2022). <em>The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games.</em> NeurIPS 2022. [MAPPO]</li>
  <li><strong>Andrychowicz, M., et al.</strong> (2021). <em>What Matters In On-Policy Reinforcement Learning?</em> arXiv:2006.05990. [PPO å®ç°ç»†èŠ‚]</li>
  <li><strong>Engstrom, L., et al.</strong> (2020). <em>Implementation Matters in Deep Policy Gradients.</em> ICLR 2020. [PPO code-level optimizations]</li>
  <li><strong>Raffin, A., et al.</strong> (2021). <em>Stable-Baselines3: Reliable Reinforcement Learning Implementations.</em> JMLR 2021. [SB3]</li>
  <li><strong>Ng, A., et al.</strong> (1999). <em>Policy Invariance Under Reward Transformations.</em> ICML 1999. [Reward Shaping]</li>
  <li><strong>Kakade, S. & Langford, J.</strong> (2002). <em>Approximately Optimal Approximate Reinforcement Learning.</em> ICML 2002. [CPI]</li>
  <li><strong>Sutton, R. S. & Barto, A. G.</strong> (2018). <em>Reinforcement Learning: An Introduction.</em> MIT Press. [RL æ•™ç§‘ä¹¦]</li>
</ol>

</div><!-- end main-content -->

<!-- ============================================================ -->
<!-- JavaScript -->
<!-- ============================================================ -->
<script>
// ===== Theme Toggle =====
function toggleTheme() {
  const html = document.documentElement;
  const current = html.getAttribute('data-theme');
  const next = current === 'dark' ? 'light' : 'dark';
  html.setAttribute('data-theme', next);
  localStorage.setItem('ppo-theme', next);
}

// Load saved theme
(function() {
  const saved = localStorage.getItem('ppo-theme');
  if (saved) {
    document.documentElement.setAttribute('data-theme', saved);
  } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
    document.documentElement.setAttribute('data-theme', 'dark');
  }
})();

// ===== Code Copy =====
function copyCode(btn) {
  const code = btn.closest('.code-container').querySelector('code');
  const text = code.innerText;
  navigator.clipboard.writeText(text).then(() => {
    const orig = btn.textContent;
    btn.textContent = 'âœ“ å·²å¤åˆ¶';
    btn.style.color = 'var(--thm-border)';
    setTimeout(() => {
      btn.textContent = orig;
      btn.style.color = '';
    }, 2000);
  });
}

// ===== Reading Progress Bar =====
window.addEventListener('scroll', function() {
  const scrollTop = window.scrollY;
  const docHeight = document.documentElement.scrollHeight - window.innerHeight;
  const progress = docHeight > 0 ? (scrollTop / docHeight) * 100 : 0;
  document.getElementById('progressBar').style.width = progress + '%';
});

// ===== Active Sidebar Link =====
(function() {
  const sections = document.querySelectorAll('h2[id], h3[id]');
  const navLinks = document.querySelectorAll('.sidebar nav a');

  function updateActiveLink() {
    let current = '';
    const scrollPos = window.scrollY + 100;

    sections.forEach(section => {
      if (section.offsetTop <= scrollPos) {
        current = section.id;
      }
    });

    navLinks.forEach(link => {
      link.classList.remove('active');
      if (link.getAttribute('href') === '#' + current) {
        link.classList.add('active');
        // Scroll sidebar to keep active link visible
        const sidebar = document.getElementById('sidebar');
        const linkRect = link.getBoundingClientRect();
        const sidebarRect = sidebar.getBoundingClientRect();
        if (linkRect.top < sidebarRect.top || linkRect.bottom > sidebarRect.bottom) {
          link.scrollIntoView({ block: 'center', behavior: 'smooth' });
        }
      }
    });
  }

  window.addEventListener('scroll', updateActiveLink);
  updateActiveLink();
})();

// ===== Mobile Sidebar Toggle =====
document.addEventListener('click', function(e) {
  const sidebar = document.getElementById('sidebar');
  const toggle = document.querySelector('.menu-toggle');
  if (sidebar.classList.contains('open') &&
      !sidebar.contains(e.target) &&
      !toggle.contains(e.target)) {
    sidebar.classList.remove('open');
  }
});

// ===== Close sidebar on nav click (mobile) =====
document.querySelectorAll('.sidebar nav a').forEach(link => {
  link.addEventListener('click', () => {
    if (window.innerWidth <= 900) {
      document.getElementById('sidebar').classList.remove('open');
    }
  });
});
</script>

</body>
</html>
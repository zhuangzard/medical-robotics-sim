<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>PyTorch æ ¸å¿ƒå­¦ä¹ æ•™ç¨‹ â€” ä»å¼ é‡åˆ°ç‰©ç†æ„ŸçŸ¥ç¥ç»ç½‘ç»œ</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
<style>
/* ========== CSS Variables ========== */
:root {
  --bg: #ffffff;
  --bg-secondary: #f7f8fa;
  --text: #1a1a2e;
  --text-secondary: #555;
  --accent: #4361ee;
  --accent-light: #e8edff;
  --border: #e0e0e0;
  --sidebar-bg: #f0f2f5;
  --sidebar-active: #4361ee;
  --code-bg: #f6f8fa;
  --code-text: #24292e;
  --def-bg: #e8f4fd;
  --def-border: #2196F3;
  --thm-bg: #e8f5e9;
  --thm-border: #4CAF50;
  --ex-bg: #fff8e1;
  --ex-border: #FF9800;
  --warn-bg: #ffebee;
  --warn-border: #f44336;
  --shadow: 0 2px 8px rgba(0,0,0,0.08);
  --radius: 8px;
  --sidebar-w: 280px;
  --font-cn: "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Noto Sans CJK SC", sans-serif;
  --font-mono: "SF Mono", "Fira Code", "JetBrains Mono", Consolas, monospace;
}

[data-theme="dark"] {
  --bg: #0d1117;
  --bg-secondary: #161b22;
  --text: #c9d1d9;
  --text-secondary: #8b949e;
  --accent: #58a6ff;
  --accent-light: #1c2d41;
  --border: #30363d;
  --sidebar-bg: #161b22;
  --sidebar-active: #58a6ff;
  --code-bg: #161b22;
  --code-text: #c9d1d9;
  --def-bg: #0d2137;
  --def-border: #58a6ff;
  --thm-bg: #0d2818;
  --thm-border: #3fb950;
  --ex-bg: #2a1f00;
  --ex-border: #d29922;
  --warn-bg: #2d0f0f;
  --warn-border: #f85149;
  --shadow: 0 2px 8px rgba(0,0,0,0.3);
}

/* ========== Reset & Base ========== */
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
html { scroll-behavior: smooth; scroll-padding-top: 80px; }
body {
  font-family: var(--font-cn);
  background: var(--bg);
  color: var(--text);
  line-height: 1.8;
  font-size: 16px;
  transition: background 0.3s, color 0.3s;
}

/* ========== Sidebar Navigation ========== */
.sidebar {
  position: fixed; top: 0; left: 0;
  width: var(--sidebar-w); height: 100vh;
  background: var(--sidebar-bg);
  border-right: 1px solid var(--border);
  overflow-y: auto; z-index: 100;
  padding: 20px 0;
  transition: transform 0.3s ease, background 0.3s;
}
.sidebar-header { padding: 10px 20px 20px; border-bottom: 1px solid var(--border); margin-bottom: 10px; }
.sidebar-header h2 { font-size: 18px; color: var(--accent); margin-bottom: 4px; }
.sidebar-header p { font-size: 12px; color: var(--text-secondary); }
.sidebar nav a {
  display: block; padding: 6px 20px;
  color: var(--text-secondary); text-decoration: none;
  font-size: 13px; border-left: 3px solid transparent;
  transition: all 0.2s;
}
.sidebar nav a:hover { color: var(--accent); background: var(--accent-light); }
.sidebar nav a.active { color: var(--sidebar-active); border-left-color: var(--sidebar-active); background: var(--accent-light); font-weight: 600; }
.sidebar nav .chapter-title {
  display: block; padding: 12px 20px 4px;
  font-size: 12px; font-weight: 700;
  text-transform: uppercase; letter-spacing: 0.05em;
  color: var(--text-secondary);
}

/* ========== Main Content ========== */
.main { margin-left: var(--sidebar-w); max-width: 900px; padding: 40px 60px 100px; }

/* ========== Top Bar ========== */
.top-bar {
  position: fixed; top: 0; left: var(--sidebar-w); right: 0;
  height: 50px; background: var(--bg);
  border-bottom: 1px solid var(--border);
  display: flex; align-items: center; justify-content: flex-end;
  padding: 0 30px; z-index: 99; gap: 12px;
  transition: background 0.3s;
}
.top-bar .progress-bar { flex: 1; height: 3px; background: var(--border); border-radius: 2px; overflow: hidden; }
.top-bar .progress-bar-fill { height: 100%; background: var(--accent); width: 0%; transition: width 0.2s; }
.theme-toggle {
  cursor: pointer; background: none; border: 1px solid var(--border);
  border-radius: 6px; padding: 4px 10px; font-size: 14px;
  color: var(--text); transition: all 0.2s;
}
.theme-toggle:hover { border-color: var(--accent); }
.menu-toggle {
  display: none; cursor: pointer; background: none;
  border: 1px solid var(--border); border-radius: 6px;
  padding: 4px 10px; font-size: 18px; color: var(--text);
}

/* ========== Typography ========== */
h1 { font-size: 2.2em; margin: 60px 0 20px; padding-bottom: 12px; border-bottom: 3px solid var(--accent); color: var(--accent); line-height: 1.3; }
h1:first-of-type { margin-top: 70px; }
h2 { font-size: 1.6em; margin: 50px 0 16px; padding-bottom: 8px; border-bottom: 2px solid var(--border); color: var(--text); }
h3 { font-size: 1.25em; margin: 30px 0 12px; color: var(--text); }
h4 { font-size: 1.05em; margin: 20px 0 8px; color: var(--text-secondary); }
p { margin: 12px 0; }
a { color: var(--accent); }
strong { color: var(--text); }
ul, ol { padding-left: 24px; margin: 8px 0; }
li { margin: 4px 0; }
hr { border: none; border-top: 1px solid var(--border); margin: 40px 0; }

/* ========== Callout Boxes ========== */
.box { padding: 16px 20px; border-radius: var(--radius); margin: 20px 0; border-left: 4px solid; position: relative; }
.box-title { font-weight: 700; margin-bottom: 8px; font-size: 0.95em; }
.def { background: var(--def-bg); border-color: var(--def-border); }
.def .box-title { color: var(--def-border); }
.def .box-title::before { content: "ğŸ“˜ "; }
.thm { background: var(--thm-bg); border-color: var(--thm-border); }
.thm .box-title { color: var(--thm-border); }
.thm .box-title::before { content: "ğŸ“— "; }
.example { background: var(--ex-bg); border-color: var(--ex-border); }
.example .box-title { color: var(--ex-border); }
.example .box-title::before { content: "ğŸ“™ "; }
.warn { background: var(--warn-bg); border-color: var(--warn-border); }
.warn .box-title { color: var(--warn-border); }
.warn .box-title::before { content: "ğŸ”´ "; }

/* ========== Code Blocks ========== */
pre {
  background: var(--code-bg); border: 1px solid var(--border);
  border-radius: var(--radius); padding: 16px 20px;
  overflow-x: auto; margin: 16px 0; font-size: 13.5px;
  line-height: 1.6; position: relative;
}
pre code { font-family: var(--font-mono); color: var(--code-text); background: none; padding: 0; font-size: inherit; }
code { font-family: var(--font-mono); background: var(--code-bg); padding: 2px 6px; border-radius: 4px; font-size: 0.9em; color: var(--accent); }
.copy-btn {
  position: absolute; top: 8px; right: 8px;
  background: var(--bg); border: 1px solid var(--border);
  border-radius: 4px; padding: 2px 8px; font-size: 12px;
  cursor: pointer; color: var(--text-secondary);
  opacity: 0; transition: opacity 0.2s;
}
pre:hover .copy-btn { opacity: 1; }
.copy-btn:hover { border-color: var(--accent); color: var(--accent); }

/* ========== Math Display ========== */
.katex-display { margin: 16px 0; overflow-x: auto; padding: 8px 0; }
.math-explain { background: var(--bg-secondary); border-radius: var(--radius); padding: 12px 16px; margin: 8px 0; font-size: 0.9em; color: var(--text-secondary); }
.math-explain strong { color: var(--accent); }

/* ========== Tables ========== */
table { width: 100%; border-collapse: collapse; margin: 16px 0; font-size: 0.95em; }
th, td { padding: 10px 14px; border: 1px solid var(--border); text-align: left; }
th { background: var(--bg-secondary); font-weight: 700; }

/* ========== SVG Diagrams ========== */
.diagram { text-align: center; margin: 24px 0; padding: 20px; background: var(--bg-secondary); border-radius: var(--radius); border: 1px solid var(--border); }
.diagram svg { max-width: 100%; height: auto; }

/* ========== Responsive ========== */
@media (max-width: 900px) {
  .sidebar { transform: translateX(-100%); }
  .sidebar.open { transform: translateX(0); }
  .main { margin-left: 0; padding: 20px 24px 60px; }
  .top-bar { left: 0; }
  .menu-toggle { display: block; }
  h1 { font-size: 1.6em; } h2 { font-size: 1.3em; }
}
@media (max-width: 600px) {
  body { font-size: 15px; }
  .main { padding: 16px 16px 60px; }
  pre { font-size: 12px; padding: 12px; }
}

/* ========== Syntax Highlighting ========== */
.kw { color: #d73a49; }
.fn { color: #6f42c1; }
.st { color: #032f62; }
.cm { color: #6a737d; font-style: italic; }
.nu { color: #005cc5; }
.op { color: #d73a49; }
.cl { color: #e36209; }
.dc { color: #22863a; }
.bi { color: #005cc5; }
[data-theme="dark"] .kw { color: #ff7b72; }
[data-theme="dark"] .fn { color: #d2a8ff; }
[data-theme="dark"] .st { color: #a5d6ff; }
[data-theme="dark"] .cm { color: #8b949e; }
[data-theme="dark"] .nu { color: #79c0ff; }
[data-theme="dark"] .op { color: #ff7b72; }
[data-theme="dark"] .cl { color: #ffa657; }
[data-theme="dark"] .dc { color: #7ee787; }
[data-theme="dark"] .bi { color: #79c0ff; }

/* ========== Shape Annotation ========== */
.shape { display: inline-block; background: var(--accent-light); color: var(--accent); padding: 1px 6px; border-radius: 4px; font-size: 0.85em; font-family: var(--font-mono); margin: 0 2px; }

/* ========== Flow Diagram ========== */
.flow { display: flex; align-items: center; flex-wrap: wrap; gap: 8px; margin: 16px 0; padding: 16px; background: var(--bg-secondary); border-radius: var(--radius); border: 1px solid var(--border); }
.flow-box { background: var(--accent-light); border: 2px solid var(--accent); border-radius: 8px; padding: 8px 14px; font-size: 0.85em; font-weight: 600; text-align: center; color: var(--accent); }
.flow-arrow { font-size: 1.2em; color: var(--text-secondary); }
.flow-box.highlight { background: var(--accent); color: #fff; }

/* ========== Tabs ========== */
.tab-group { margin: 16px 0; }
.tab-buttons { display: flex; gap: 0; border-bottom: 2px solid var(--border); }
.tab-btn { padding: 8px 16px; background: none; border: none; cursor: pointer; font-size: 14px; color: var(--text-secondary); border-bottom: 2px solid transparent; margin-bottom: -2px; transition: all 0.2s; font-family: var(--font-cn); }
.tab-btn.active { color: var(--accent); border-bottom-color: var(--accent); font-weight: 600; }
.tab-content { display: none; padding: 16px 0; }
.tab-content.active { display: block; }

/* ========== Print ========== */
@media print {
  .sidebar, .top-bar { display: none; }
  .main { margin: 0; max-width: 100%; }
}
</style>
</head>
<body>

<!-- ========== Sidebar ========== -->
<aside class="sidebar" id="sidebar">
  <div class="sidebar-header">
    <h2>ğŸ”¥ PyTorch æ•™ç¨‹</h2>
    <p>ä»å¼ é‡åˆ°ç‰©ç†æ„ŸçŸ¥ç¥ç»ç½‘ç»œ</p>
  </div>
  <nav id="nav">
    <span class="chapter-title">ç¬¬ä¸€ç«  Â· PyTorch åŸºç¡€</span>
    <a href="#ch1">1.0 å¼•è¨€</a>
    <a href="#s1-1">1.1 Tensor å¼ é‡</a>
    <a href="#s1-2">1.2 å¼ é‡è¿ç®—</a>
    <a href="#s1-3">1.3 ç´¢å¼•å’Œåˆ‡ç‰‡</a>
    <a href="#s1-4">1.4 å½¢çŠ¶å˜æ¢</a>
    <a href="#s1-5">1.5 GPU åŠ é€Ÿ</a>

    <span class="chapter-title">ç¬¬äºŒç«  Â· è‡ªåŠ¨å¾®åˆ†</span>
    <a href="#s2-1">2.1 è®¡ç®—å›¾</a>
    <a href="#s2-2">2.2 requires_grad</a>
    <a href="#s2-3">2.3 backward()</a>
    <a href="#s2-4">2.4 é“¾å¼æ³•åˆ™</a>
    <a href="#s2-5">2.5 æ¢¯åº¦ç´¯ç§¯å’Œæ¸…é›¶</a>
    <a href="#s2-6">2.6 detach() å’Œ no_grad()</a>
    <a href="#s2-7">2.7 æ‰‹åŠ¨ vs autograd</a>

    <span class="chapter-title">ç¬¬ä¸‰ç«  Â· ç¥ç»ç½‘ç»œ</span>
    <a href="#s3-1">3.1 nn.Module åŸºç±»</a>
    <a href="#s3-2">3.2 å¸¸ç”¨å±‚è¯¦è§£</a>
    <a href="#s3-3">3.3 å‚æ•°ç®¡ç†</a>
    <a href="#s3-4">3.4 æ¨¡å‹ä¿å­˜ä¸åŠ è½½</a>

    <span class="chapter-title">ç¬¬å››ç«  Â· æ ¸å¿ƒæ•°å­¦å…¬å¼</span>
    <a href="#s4-1">4.1 çº¿æ€§å˜æ¢</a>
    <a href="#s4-2">4.2 æ¿€æ´»å‡½æ•°</a>
    <a href="#s4-3">4.3 æŸå¤±å‡½æ•°</a>
    <a href="#s4-4">4.4 ä¼˜åŒ–å™¨</a>
    <a href="#s4-5">4.5 åå‘ä¼ æ’­</a>
    <a href="#s4-6">4.6 æ¦‚ç‡åˆ†å¸ƒ</a>

    <span class="chapter-title">ç¬¬äº”ç«  Â· é¡¹ç›®å®æˆ˜</span>
    <a href="#s5-1">5.1 ç­–ç•¥ç½‘ç»œ Actor</a>
    <a href="#s5-2">5.2 ä»·å€¼ç½‘ç»œ Critic</a>
    <a href="#s5-3">5.3 ç‰¹å¾æå–å™¨</a>
    <a href="#s5-4">5.4 å›¾ç½‘ç»œ GNN</a>
    <a href="#s5-5">5.5 æ‰‹å†™è®­ç»ƒå¾ªç¯</a>

    <span class="chapter-title">ç¬¬å…­ç«  Â· è°ƒè¯•ä¸å®è·µ</span>
    <a href="#s6-1">6.1 å¸¸è§é”™è¯¯</a>
    <a href="#s6-2">6.2 æ¢¯åº¦æ£€æŸ¥</a>
    <a href="#s6-3">6.3 TensorBoard</a>
    <a href="#s6-4">6.4 GPU å†…å­˜ç®¡ç†</a>
    <a href="#s6-5">6.5 æ€§èƒ½ä¼˜åŒ–</a>
    <a href="#s6-6">6.6 å¯å¤ç°æ€§</a>

    <span class="chapter-title">ç¬¬ä¸ƒç«  Â· é«˜çº§ä¸»é¢˜</span>
    <a href="#s7-1">7.1 è‡ªå®šä¹‰ autograd</a>
    <a href="#s7-2">7.2 æ¨¡å‹å¹¶è¡Œ</a>
    <a href="#s7-3">7.3 torch.compile</a>
    <a href="#s7-4">7.4 ONNX å¯¼å‡º</a>
    <a href="#s7-5">7.5 NumPy äº’æ“ä½œ</a>

    <span class="chapter-title">é™„å½•</span>
    <a href="#app-api">A. API é€ŸæŸ¥è¡¨</a>
    <a href="#app-numpy">B. NumPy å¯¹ç…§è¡¨</a>
    <a href="#app-formulas">C. å…¬å¼æ€»ç»“</a>
    <a href="#app-activations">D. æ¿€æ´»å‡½æ•°å›¾å½¢</a>
    <a href="#app-optimizers">E. ä¼˜åŒ–å™¨å¯¹æ¯”</a>
  </nav>
</aside>

<!-- ========== Top Bar ========== -->
<div class="top-bar">
  <button class="menu-toggle" onclick="document.getElementById('sidebar').classList.toggle('open')">â˜°</button>
  <div class="progress-bar"><div class="progress-bar-fill" id="progressFill"></div></div>
  <button class="theme-toggle" onclick="toggleTheme()">ğŸŒ“</button>
</div>

<!-- ========== Main Content ========== -->
<div class="main">

<!-- ================================================== -->
<!--                   ç¬¬ä¸€ç« ï¼šåŸºç¡€                       -->
<!-- ================================================== -->
<h1 id="ch1">ç¬¬ä¸€ç«  &nbsp; PyTorch åŸºç¡€</h1>

<p>PyTorch æ˜¯ Meta AI å¼€æºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥<strong>åŠ¨æ€è®¡ç®—å›¾</strong>å’Œ<strong>Pythonic API</strong>è‘—ç§°ã€‚å®ƒæ˜¯å½“å‰å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæœ€æµè¡Œçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ PushBox ç‰©ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ é¡¹ç›®çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚</p>

<div class="box def">
  <div class="box-title">ä¸ºä»€ä¹ˆé€‰æ‹© PyTorch</div>
  <ul>
    <li><strong>åŠ¨æ€è®¡ç®—å›¾</strong> â€” æ¯æ¬¡å‰å‘ä¼ æ’­æ„å»ºæ–°å›¾ï¼Œæ–¹ä¾¿è°ƒè¯•ï¼ˆä¸ TensorFlow 1.x çš„é™æ€å›¾ä¸åŒï¼‰</li>
    <li><strong>Pythonic</strong> â€” åƒå†™æ™®é€š Python ä¸€æ ·å†™æ·±åº¦å­¦ä¹ ä»£ç </li>
    <li><strong>å¼ºå¤§çš„ç”Ÿæ€</strong> â€” Stable-Baselines3 (RL)ã€PyTorch Geometric (GNN)ã€TorchVision ç­‰</li>
    <li><strong>GPU åŠ é€Ÿ</strong> â€” é€æ˜çš„ CUDA æ”¯æŒï¼Œä¸€è¡Œä»£ç åˆ‡æ¢è®¾å¤‡</li>
  </ul>
</div>

<p>æœ¬æ•™ç¨‹é¢å‘æƒ³ç”¨ PyTorch åš<strong>å¼ºåŒ–å­¦ä¹  (RL)</strong> å’Œ<strong>ç‰©ç†ä»¿çœŸ</strong>çš„è¯»è€…ã€‚æˆ‘ä»¬å°†ç”¨ PushBox é¡¹ç›®ä¸­çš„å®é™…ç½‘ç»œç»“æ„ä½œä¸ºè´¯ç©¿å…¨æ–‡çš„ä¾‹å­ã€‚</p>

<!-- ==================== 1.1 Tensor ==================== -->
<h2 id="s1-1">1.1 Tensor å¼ é‡ â€” PyTorch çš„åŸºæœ¬æ•°æ®ç»“æ„</h2>

<div class="box def">
  <div class="box-title">å®šä¹‰ï¼šTensorï¼ˆå¼ é‡ï¼‰</div>
  <p>Tensor æ˜¯ PyTorch ä¸­çš„å¤šç»´æ•°ç»„ï¼Œç±»ä¼¼ NumPy çš„ <code>ndarray</code>ï¼Œä½†å¢åŠ äº†ä¸¤ä¸ªå…³é”®èƒ½åŠ›ï¼š</p>
  <ol>
    <li><strong>GPU åŠ é€Ÿ</strong> â€” å¯ä»¥åœ¨ NVIDIA GPU ä¸Šè¿ç®—</li>
    <li><strong>è‡ªåŠ¨å¾®åˆ†</strong> â€” å¯ä»¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œï¼‰</li>
  </ol>
</div>

<h3>1.1.1 åˆ›å»º Tensor</h3>

<pre><code><span class="kw">import</span> torch

<span class="cm"># ===== ä» Python æ•°æ®åˆ›å»º =====</span>
a = torch.<span class="fn">tensor</span>([<span class="nu">1</span>, <span class="nu">2</span>, <span class="nu">3</span>])                    <span class="cm"># 1D tensor (å‘é‡)</span>
b = torch.<span class="fn">tensor</span>([[<span class="nu">1</span>, <span class="nu">2</span>], [<span class="nu">3</span>, <span class="nu">4</span>]])           <span class="cm"># 2D tensor (çŸ©é˜µ)</span>
c = torch.<span class="fn">tensor</span>([[[<span class="nu">1</span>, <span class="nu">2</span>], [<span class="nu">3</span>, <span class="nu">4</span>]]])        <span class="cm"># 3D tensor</span>

<span class="cm"># ===== å¸¸ç”¨æ„é€ å‡½æ•° =====</span>
zeros  = torch.<span class="fn">zeros</span>(<span class="nu">3</span>, <span class="nu">4</span>)                     <span class="cm"># å…¨é›¶ (3Ã—4)</span>
ones   = torch.<span class="fn">ones</span>(<span class="nu">2</span>, <span class="nu">3</span>)                      <span class="cm"># å…¨ä¸€ (2Ã—3)</span>
rand   = torch.<span class="fn">rand</span>(<span class="nu">5</span>, <span class="nu">5</span>)                      <span class="cm"># å‡åŒ€åˆ†å¸ƒ [0, 1)</span>
randn  = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">4</span>)                     <span class="cm"># æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0,1)</span>
eye    = torch.<span class="fn">eye</span>(<span class="nu">4</span>)                            <span class="cm"># å•ä½çŸ©é˜µ (4Ã—4)</span>
arange = torch.<span class="fn">arange</span>(<span class="nu">0</span>, <span class="nu">10</span>, <span class="nu">2</span>)                <span class="cm"># [0, 2, 4, 6, 8]</span>
linsp  = torch.<span class="fn">linspace</span>(<span class="nu">0</span>, <span class="nu">1</span>, <span class="nu">5</span>)               <span class="cm"># [0, 0.25, 0.5, 0.75, 1]</span>

<span class="cm"># ===== ä» NumPy åˆ›å»ºï¼ˆå…±äº«å†…å­˜ï¼ï¼‰=====</span>
<span class="kw">import</span> numpy <span class="kw">as</span> np
np_arr = np.<span class="fn">array</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>])
t = torch.<span class="fn">from_numpy</span>(np_arr)                    <span class="cm"># å…±äº«å†…å­˜ï¼Œä¿®æ”¹ä¸€ä¸ªå¦ä¸€ä¸ªä¹Ÿå˜</span>
t2 = torch.<span class="fn">tensor</span>(np_arr)                       <span class="cm"># æ‹·è´æ•°æ®ï¼Œä¸å…±äº«</span></code></pre>

<h3>1.1.2 Tensor çš„ä¸‰å¤§å±æ€§</h3>

<p>æ¯ä¸ª Tensor éƒ½æœ‰ä¸‰ä¸ªæ ¸å¿ƒå±æ€§ï¼š<strong>shape</strong>ï¼ˆå½¢çŠ¶ï¼‰ã€<strong>dtype</strong>ï¼ˆæ•°æ®ç±»å‹ï¼‰ã€<strong>device</strong>ï¼ˆè®¾å¤‡ï¼‰ã€‚</p>

<pre><code>x = torch.<span class="fn">randn</span>(<span class="nu">2</span>, <span class="nu">3</span>, <span class="nu">4</span>)

<span class="fn">print</span>(x.shape)     <span class="cm"># torch.Size([2, 3, 4])</span>
<span class="fn">print</span>(x.dtype)     <span class="cm"># torch.float32ï¼ˆé»˜è®¤ï¼‰</span>
<span class="fn">print</span>(x.device)    <span class="cm"># cpuï¼ˆé»˜è®¤ï¼‰</span>

<span class="cm"># æŒ‡å®š dtype å’Œ device</span>
x = torch.<span class="fn">zeros</span>(<span class="nu">3</span>, <span class="nu">4</span>, dtype=torch.float64)    <span class="cm"># 64ä½æµ®ç‚¹</span>
x = torch.<span class="fn">zeros</span>(<span class="nu">3</span>, <span class="nu">4</span>, dtype=torch.int32)      <span class="cm"># 32ä½æ•´æ•°</span>
x = torch.<span class="fn">zeros</span>(<span class="nu">3</span>, <span class="nu">4</span>, dtype=torch.bool)       <span class="cm"># å¸ƒå°”ç±»å‹</span></code></pre>

<div class="box example">
  <div class="box-title">æˆ‘ä»¬é¡¹ç›®ä¸­çš„ Tensor shape å®ä¾‹</div>
  <table>
    <tr><th>åç§°</th><th>Shape</th><th>å«ä¹‰</th></tr>
    <tr><td>è§‚å¯Ÿ obs</td><td><code>(batch, 16)</code></td><td>16ç»´è§‚å¯Ÿå‘é‡</td></tr>
    <tr><td>åŠ¨ä½œ action</td><td><code>(batch, 2)</code></td><td>2ç»´è¿ç»­åŠ¨ä½œ (fx, fy)</td></tr>
    <tr><td>ä»·å€¼ value</td><td><code>(batch, 1)</code></td><td>çŠ¶æ€ä»·å€¼æ ‡é‡</td></tr>
    <tr><td>ç²’å­ä½ç½® positions</td><td><code>(N, 3)</code></td><td>Nä¸ªç²’å­çš„3Dåæ ‡</td></tr>
    <tr><td>è¾¹ç´¢å¼• edge_index</td><td><code>(2, E)</code></td><td>Eæ¡è¾¹çš„æº/ç›®æ ‡èŠ‚ç‚¹</td></tr>
  </table>
</div>

<h3>1.1.3 å¸¸è§ dtype å‚è€ƒ</h3>

<table>
  <tr><th>PyTorch dtype</th><th>Python ç­‰ä»·</th><th>å¤§å°</th><th>ç”¨é€”</th></tr>
  <tr><td><code>torch.float32</code></td><td><code>float</code></td><td>4 bytes</td><td>é»˜è®¤ï¼Œç¥ç»ç½‘ç»œå‚æ•°/è®¡ç®—</td></tr>
  <tr><td><code>torch.float64</code></td><td><code>double</code></td><td>8 bytes</td><td>é«˜ç²¾åº¦ç‰©ç†ä»¿çœŸ</td></tr>
  <tr><td><code>torch.float16</code></td><td>â€”</td><td>2 bytes</td><td>æ··åˆç²¾åº¦è®­ç»ƒ</td></tr>
  <tr><td><code>torch.int64</code></td><td><code>long</code></td><td>8 bytes</td><td>ç´¢å¼•ã€æ ‡ç­¾</td></tr>
  <tr><td><code>torch.int32</code></td><td><code>int</code></td><td>4 bytes</td><td>ä¸€èˆ¬æ•´æ•°</td></tr>
  <tr><td><code>torch.bool</code></td><td><code>bool</code></td><td>1 byte</td><td>æ©ç ã€æ¡ä»¶</td></tr>
</table>

<!-- ==================== 1.2 å¼ é‡è¿ç®— ==================== -->
<h2 id="s1-2">1.2 å¼ é‡è¿ç®—</h2>

<h3>1.2.1 åŸºæœ¬ç®—æœ¯è¿ç®—</h3>

<pre><code>a = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>])
b = torch.<span class="fn">tensor</span>([<span class="nu">4.0</span>, <span class="nu">5.0</span>, <span class="nu">6.0</span>])

<span class="cm"># é€å…ƒç´ è¿ç®—ï¼ˆelement-wiseï¼‰</span>
c = a + b          <span class="cm"># tensor([5., 7., 9.])</span>
c = a - b          <span class="cm"># tensor([-3., -3., -3.])</span>
c = a * b          <span class="cm"># tensor([4., 10., 18.])  é€å…ƒç´ ä¹˜</span>
c = a / b          <span class="cm"># tensor([0.25, 0.40, 0.50])</span>
c = a ** <span class="nu">2</span>         <span class="cm"># tensor([1., 4., 9.])</span>

<span class="cm"># ä¹Ÿå¯ä»¥ç”¨å‡½æ•°å½¢å¼</span>
c = torch.<span class="fn">add</span>(a, b)
c = torch.<span class="fn">mul</span>(a, b)
c = torch.<span class="fn">div</span>(a, b)

<span class="cm"># åŸåœ°æ“ä½œï¼ˆåŠ ä¸‹åˆ’çº¿åç¼€ _ï¼‰</span>
a.<span class="fn">add_</span>(b)          <span class="cm"># a = a + bï¼Œç›´æ¥ä¿®æ”¹ a</span></code></pre>

<div class="box warn">
  <div class="box-title">æ³¨æ„ï¼šåŸåœ°æ“ä½œä¸ autograd</div>
  <p>åŸåœ°æ“ä½œï¼ˆ<code>add_</code>ã€<code>mul_</code> ç­‰ï¼‰å¯èƒ½ä¼šç ´åè®¡ç®—å›¾ï¼Œå¯¼è‡´æ¢¯åº¦è®¡ç®—é”™è¯¯ã€‚åœ¨éœ€è¦æ¢¯åº¦çš„å¼ é‡ä¸Šï¼Œ<strong>é¿å…ä½¿ç”¨åŸåœ°æ“ä½œ</strong>ã€‚</p>
</div>

<h3>1.2.2 çŸ©é˜µä¹˜æ³• â€” æœ€é‡è¦çš„è¿ç®—</h3>

<p>ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ“ä½œå°±æ˜¯çŸ©é˜µä¹˜æ³•ã€‚PyTorch æä¾›å¤šç§æ–¹å¼ï¼š</p>

<pre><code><span class="cm"># 2D çŸ©é˜µä¹˜æ³•</span>
A = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">4</span>)    <span class="cm"># (3, 4)</span>
B = torch.<span class="fn">randn</span>(<span class="nu">4</span>, <span class="nu">5</span>)    <span class="cm"># (4, 5)</span>

C = A <span class="op">@</span> B                <span class="cm"># (3, 5) â€” æ¨èå†™æ³•ï¼</span>
C = torch.<span class="fn">matmul</span>(A, B)   <span class="cm"># (3, 5) â€” ç­‰ä»·</span>
C = torch.<span class="fn">mm</span>(A, B)      <span class="cm"># (3, 5) â€” ä»…2D</span>

<span class="cm"># Batch çŸ©é˜µä¹˜æ³•ï¼ˆRL ä¸­å¸¸è§ï¼‰</span>
<span class="cm"># ä¾‹å¦‚ï¼šåŒæ—¶è®¡ç®—å¤šä¸ª obs çš„çº¿æ€§å˜æ¢</span>
W = torch.<span class="fn">randn</span>(<span class="nu">64</span>, <span class="nu">16</span>)     <span class="cm"># æƒé‡ (out_features, in_features)</span>
obs = torch.<span class="fn">randn</span>(<span class="nu">256</span>, <span class="nu">16</span>)  <span class="cm"># ä¸€æ‰¹ 256 ä¸ªè§‚å¯Ÿå‘é‡</span>

<span class="cm"># å…¨è¿æ¥å±‚æœ¬è´¨ä¸Šå°±æ˜¯è¿™ä¸ªæ“ä½œï¼</span>
output = obs <span class="op">@</span> W.T          <span class="cm"># (256, 64)  â† y = xW^T</span></code></pre>

<div class="box thm">
  <div class="box-title">çŸ©é˜µä¹˜æ³•ç»´åº¦è§„åˆ™</div>
  <p>$$\mathbf{A}_{(m \times k)} \times \mathbf{B}_{(k \times n)} = \mathbf{C}_{(m \times n)}$$</p>
  <p>ä¸­é—´ç»´åº¦ \(k\) å¿…é¡»åŒ¹é…ã€‚ç»“æœå– \(A\) çš„è¡Œæ•° \(m\) å’Œ \(B\) çš„åˆ—æ•° \(n\)ã€‚</p>
  <p>åœ¨æˆ‘ä»¬çš„ç½‘ç»œä¸­ï¼š<code>Linear(16, 64)</code> å†…éƒ¨å°±æ˜¯ <span class="shape">(batch, 16)</span> @ <span class="shape">(16, 64)</span> = <span class="shape">(batch, 64)</span></p>
</div>

<h3>1.2.3 å¹¿æ’­æœºåˆ¶ (Broadcasting)</h3>

<p>å½“ä¸¤ä¸ª Tensor å½¢çŠ¶ä¸å®Œå…¨ä¸€è‡´æ—¶ï¼ŒPyTorch ä¼šè‡ªåŠ¨æ‰©å±•è¾ƒå°çš„ Tensorï¼š</p>

<pre><code><span class="cm"># æ ‡é‡ + å‘é‡</span>
a = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>])    <span class="cm"># shape: (3,)</span>
b = <span class="nu">10.0</span>                                <span class="cm"># æ ‡é‡</span>
c = a + b                               <span class="cm"># tensor([11., 12., 13.])  è‡ªåŠ¨å¹¿æ’­</span>

<span class="cm"># å‘é‡ + çŸ©é˜µ</span>
A = torch.<span class="fn">randn</span>(<span class="nu">4</span>, <span class="nu">3</span>)                   <span class="cm"># (4, 3)</span>
bias = torch.<span class="fn">randn</span>(<span class="nu">3</span>)                   <span class="cm"># (3,)  â†’ è‡ªåŠ¨å¹¿æ’­ä¸º (4, 3)</span>
C = A + bias                             <span class="cm"># (4, 3)  æ¯è¡Œéƒ½åŠ  bias</span>

<span class="cm"># è¿™å°±æ˜¯ y = Wx + b ä¸­ b çš„å¹¿æ’­åŸç†ï¼</span>
<span class="cm"># W@x çš„ç»“æœæ˜¯ (batch, out)ï¼Œb æ˜¯ (out,)ï¼Œè‡ªåŠ¨å¹¿æ’­åˆ°æ¯ä¸ª batch</span>

<span class="cm"># å¹¿æ’­è§„åˆ™ï¼šä»å³å¾€å·¦å¯¹é½ï¼Œç»´åº¦è¦ä¹ˆç›¸åŒï¼Œè¦ä¹ˆå…¶ä¸­ä¸€ä¸ªæ˜¯1</span>
x = torch.<span class="fn">randn</span>(<span class="nu">4</span>, <span class="nu">1</span>)    <span class="cm"># (4, 1)</span>
y = torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">3</span>)    <span class="cm"># (1, 3)</span>
z = x + y                 <span class="cm"># (4, 3) â† ä¸¤ä¸ªç»´åº¦éƒ½è¢«å¹¿æ’­</span></code></pre>

<h3>1.2.4 å½’çº¦è¿ç®—</h3>

<pre><code>x = torch.<span class="fn">tensor</span>([[<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>],
                  [<span class="nu">4.0</span>, <span class="nu">5.0</span>, <span class="nu">6.0</span>]])    <span class="cm"># (2, 3)</span>

x.<span class="fn">sum</span>()           <span class="cm"># tensor(21.)  â€” æ‰€æœ‰å…ƒç´ æ±‚å’Œ</span>
x.<span class="fn">sum</span>(dim=<span class="nu">0</span>)     <span class="cm"># tensor([5., 7., 9.])  â€” æ²¿è¡Œæ±‚å’Œ â†’ (3,)</span>
x.<span class="fn">sum</span>(dim=<span class="nu">1</span>)     <span class="cm"># tensor([6., 15.])     â€” æ²¿åˆ—æ±‚å’Œ â†’ (2,)</span>
x.<span class="fn">mean</span>()          <span class="cm"># tensor(3.5)</span>
x.<span class="fn">mean</span>(dim=<span class="nu">1</span>)    <span class="cm"># tensor([2., 5.])</span>
x.<span class="fn">max</span>()           <span class="cm"># tensor(6.)</span>
x.<span class="fn">min</span>()           <span class="cm"># tensor(1.)</span>
x.<span class="fn">argmax</span>(dim=<span class="nu">1</span>)  <span class="cm"># tensor([2, 2])  â€” æ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•</span>
x.<span class="fn">std</span>()           <span class="cm"># æ ‡å‡†å·®</span>

<span class="cm"># åœ¨ RL ä¸­å¸¸ç”¨ï¼šè®¡ç®— batch çš„å¹³å‡ loss</span>
<span class="cm"># loss = losses.mean()  â† å°† (batch,) çš„ loss å‘é‡å˜æˆæ ‡é‡</span></code></pre>

<!-- ==================== 1.3 ç´¢å¼•å’Œåˆ‡ç‰‡ ==================== -->
<h2 id="s1-3">1.3 ç´¢å¼•å’Œåˆ‡ç‰‡</h2>

<h3>1.3.1 åŸºæœ¬ç´¢å¼•</h3>

<pre><code>x = torch.<span class="fn">randn</span>(<span class="nu">4</span>, <span class="nu">5</span>)

<span class="cm"># åŸºæœ¬ç´¢å¼•ï¼ˆä¸ NumPy ä¸€è‡´ï¼‰</span>
x[<span class="nu">0</span>]         <span class="cm"># ç¬¬0è¡Œ â†’ (5,)</span>
x[<span class="nu">0</span>, <span class="nu">2</span>]      <span class="cm"># ç¬¬0è¡Œç¬¬2åˆ— â†’ æ ‡é‡</span>
x[<span class="nu">1</span>:<span class="nu">3</span>]       <span class="cm"># ç¬¬1~2è¡Œ â†’ (2, 5)</span>
x[:, <span class="nu">2</span>]      <span class="cm"># æ‰€æœ‰è¡Œçš„ç¬¬2åˆ— â†’ (4,)</span>
x[:, <span class="nu">1</span>:<span class="nu">4</span>]    <span class="cm"># æ‰€æœ‰è¡Œçš„ç¬¬1~3åˆ— â†’ (4, 3)</span>

<span class="cm"># å¸ƒå°”ç´¢å¼•ï¼ˆæ©ç ï¼‰</span>
mask = x > <span class="nu">0</span>             <span class="cm"># å¸ƒå°”çŸ©é˜µ (4, 5)</span>
positive = x[mask]         <span class="cm"># æ‰€æœ‰æ­£å€¼ â†’ 1D tensor</span>

<span class="cm"># å®é™…åº”ç”¨ï¼šç­›é€‰ done çš„ episode</span>
dones = torch.<span class="fn">tensor</span>([<span class="kw">False</span>, <span class="kw">True</span>, <span class="kw">False</span>, <span class="kw">True</span>])
rewards = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>, <span class="nu">4.0</span>])
done_rewards = rewards[dones]  <span class="cm"># tensor([2., 4.])</span></code></pre>

<h3>1.3.2 é«˜çº§ç´¢å¼•ï¼šgather å’Œ scatter</h3>

<pre><code><span class="cm"># gather â€” æŒ‰ç´¢å¼•ä»å¼ é‡ä¸­"æ”¶é›†"å€¼</span>
<span class="cm"># å¸¸ç”¨äºï¼šæ ¹æ®åŠ¨ä½œç´¢å¼•å–Qå€¼</span>
q_values = torch.<span class="fn">tensor</span>([[<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>],    <span class="cm"># batch 0 çš„ Q(s,a) for 3 actions</span>
                         [<span class="nu">4.0</span>, <span class="nu">5.0</span>, <span class="nu">6.0</span>]])   <span class="cm"># batch 1 çš„ Q(s,a)</span>
actions = torch.<span class="fn">tensor</span>([[<span class="nu">2</span>], [<span class="nu">0</span>]])            <span class="cm"># batch 0 é€‰åŠ¨ä½œ2, batch 1 é€‰åŠ¨ä½œ0</span>

<span class="cm"># å–å‡ºå¯¹åº”åŠ¨ä½œçš„ Q å€¼</span>
selected_q = q_values.<span class="fn">gather</span>(<span class="nu">1</span>, actions)       <span class="cm"># tensor([[3.], [4.]])</span>
<span class="cm"># dim=1 è¡¨ç¤ºæ²¿åˆ—æ–¹å‘æ”¶é›†</span>

<span class="cm"># scatter â€” gather çš„åæ“ä½œï¼ŒæŒ‰ç´¢å¼•"åˆ†æ•£"å€¼</span>
<span class="cm"># å¸¸ç”¨äºï¼šæ„é€  one-hot ç¼–ç </span>
labels = torch.<span class="fn">tensor</span>([<span class="nu">0</span>, <span class="nu">2</span>, <span class="nu">1</span>])
one_hot = torch.<span class="fn">zeros</span>(<span class="nu">3</span>, <span class="nu">3</span>)
one_hot.<span class="fn">scatter_</span>(<span class="nu">1</span>, labels.<span class="fn">unsqueeze</span>(<span class="nu">1</span>), <span class="nu">1.0</span>)
<span class="cm"># tensor([[1, 0, 0],</span>
<span class="cm">#         [0, 0, 1],</span>
<span class="cm">#         [0, 1, 0]])</span></code></pre>

<!-- ==================== 1.4 å½¢çŠ¶å˜æ¢ ==================== -->
<h2 id="s1-4">1.4 å½¢çŠ¶å˜æ¢</h2>

<h3>1.4.1 reshape å’Œ view</h3>

<pre><code>x = torch.<span class="fn">arange</span>(<span class="nu">12</span>)       <span class="cm"># tensor([0, 1, ..., 11])  shape: (12,)</span>

<span class="cm"># reshape: æ”¹å˜å½¢çŠ¶ï¼ˆå¯èƒ½å¤åˆ¶æ•°æ®ï¼‰</span>
y = x.<span class="fn">reshape</span>(<span class="nu">3</span>, <span class="nu">4</span>)       <span class="cm"># (3, 4)</span>
y = x.<span class="fn">reshape</span>(<span class="nu">2</span>, <span class="nu">-1</span>)      <span class="cm"># (2, 6) â€” -1 è‡ªåŠ¨æ¨æ–­</span>

<span class="cm"># view: æ”¹å˜å½¢çŠ¶ï¼ˆè¦æ±‚å†…å­˜è¿ç»­ï¼Œä¸å¤åˆ¶æ•°æ®ï¼‰</span>
y = x.<span class="fn">view</span>(<span class="nu">3</span>, <span class="nu">4</span>)          <span class="cm"># (3, 4) â€” æ›´å¿«ï¼Œå…±äº«å†…å­˜</span>
y = x.<span class="fn">view</span>(<span class="nu">4</span>, <span class="nu">-1</span>)         <span class="cm"># (4, 3)</span>

<span class="cm"># å±•å¹³ä¸º1D</span>
y = x.<span class="fn">flatten</span>()            <span class="cm"># (12,)</span>
y = x.<span class="fn">view</span>(<span class="nu">-1</span>)             <span class="cm"># (12,)</span></code></pre>

<div class="box warn">
  <div class="box-title">view vs reshape</div>
  <p><code>view()</code> è¦æ±‚å¼ é‡å†…å­˜è¿ç»­ï¼ˆ<code>x.is_contiguous() == True</code>ï¼‰ï¼Œå¦‚æœä¸è¿ç»­ä¼šæŠ¥é”™ã€‚<code>reshape()</code> æ›´å®‰å…¨ï¼Œå†…å­˜ä¸è¿ç»­æ—¶ä¼šè‡ªåŠ¨å¤åˆ¶ã€‚å»ºè®®ï¼šå¦‚æœä¸ç¡®å®šï¼Œç”¨ <code>reshape</code>ï¼›è¿½æ±‚æ€§èƒ½ä¸”ç¡®å®šå†…å­˜è¿ç»­ï¼Œç”¨ <code>view</code>ã€‚</p>
</div>

<h3>1.4.2 permute â€” è°ƒæ¢ç»´åº¦é¡ºåº</h3>

<pre><code><span class="cm"># å¸¸ç”¨äºå›¾åƒæ•°æ®ï¼š(batch, H, W, C) â†’ (batch, C, H, W)</span>
img = torch.<span class="fn">randn</span>(<span class="nu">8</span>, <span class="nu">224</span>, <span class="nu">224</span>, <span class="nu">3</span>)   <span class="cm"># (B, H, W, C)</span>
img = img.<span class="fn">permute</span>(<span class="nu">0</span>, <span class="nu">3</span>, <span class="nu">1</span>, <span class="nu">2</span>)        <span class="cm"># (B, C, H, W) â€” PyTorch çš„æ ¼å¼</span>

<span class="cm"># transpose: äº¤æ¢ä¸¤ä¸ªç»´åº¦</span>
A = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">5</span>)
A_T = A.<span class="fn">T</span>              <span class="cm"># (5, 3) â€” è½¬ç½®å¿«æ·æ–¹å¼</span>
A_T = A.<span class="fn">transpose</span>(<span class="nu">0</span>, <span class="nu">1</span>) <span class="cm"># ç­‰ä»·</span></code></pre>

<h3>1.4.3 squeeze å’Œ unsqueeze â€” å¢å‡ç»´åº¦</h3>

<pre><code><span class="cm"># unsqueeze: æ·»åŠ ä¸€ä¸ªå¤§å°ä¸º1çš„ç»´åº¦</span>
x = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>])  <span class="cm"># shape: (3,)</span>
x = x.<span class="fn">unsqueeze</span>(<span class="nu">0</span>)                   <span class="cm"># shape: (1, 3) â€” æ·»åŠ  batch ç»´åº¦</span>
x = x.<span class="fn">unsqueeze</span>(<span class="nu">-1</span>)                  <span class="cm"># shape: (1, 3, 1)</span>

<span class="cm"># squeeze: å»é™¤å¤§å°ä¸º1çš„ç»´åº¦</span>
x = torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">3</span>, <span class="nu">1</span>)             <span class="cm"># (1, 3, 1)</span>
x = x.<span class="fn">squeeze</span>()                       <span class="cm"># (3,) â€” å»é™¤æ‰€æœ‰å¤§å°ä¸º1çš„ç»´åº¦</span>
x = x.<span class="fn">squeeze</span>(<span class="nu">0</span>)                     <span class="cm"># åªå»é™¤ç¬¬0ç»´</span>

<span class="cm"># åœ¨ RL ä¸­å¸¸ç”¨ï¼š</span>
<span class="cm"># å•ä¸ª obs ä¼ å…¥ç½‘ç»œéœ€è¦åŠ  batch ç»´åº¦</span>
obs = torch.<span class="fn">randn</span>(<span class="nu">16</span>)              <span class="cm"># (16,) å•ä¸ªè§‚å¯Ÿ</span>
obs_batch = obs.<span class="fn">unsqueeze</span>(<span class="nu">0</span>)       <span class="cm"># (1, 16) åŠ  batch ç»´åº¦</span>
value = model(obs_batch)             <span class="cm"># (1, 1)</span>
value = value.<span class="fn">squeeze</span>()             <span class="cm"># () æ ‡é‡</span></code></pre>

<h3>1.4.4 cat å’Œ stack â€” æ‹¼æ¥å¼ é‡</h3>

<pre><code><span class="cm"># cat: æ²¿å·²æœ‰ç»´åº¦æ‹¼æ¥</span>
a = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">4</span>)
b = torch.<span class="fn">randn</span>(<span class="nu">2</span>, <span class="nu">4</span>)
c = torch.<span class="fn">cat</span>([a, b], dim=<span class="nu">0</span>)    <span class="cm"># (5, 4) â€” æ²¿è¡Œæ‹¼æ¥</span>

<span class="cm"># stack: æ²¿æ–°ç»´åº¦å †å </span>
a = torch.<span class="fn">randn</span>(<span class="nu">3</span>)
b = torch.<span class="fn">randn</span>(<span class="nu">3</span>)
c = torch.<span class="fn">stack</span>([a, b], dim=<span class="nu">0</span>)  <span class="cm"># (2, 3) â€” æ–°å»ºä¸€ä¸ª batch ç»´åº¦</span>

<span class="cm"># åœ¨ EdgeFrame ä¸­çš„å®é™…åº”ç”¨ï¼š</span>
<span class="cm"># æ‹¼æ¥ä½ç§»å’Œé€Ÿåº¦ç‰¹å¾</span>
r_ij = positions[tgt_idx] - positions[src_idx]  <span class="cm"># (E, 3)</span>
r_norm = torch.<span class="fn">norm</span>(r_ij, dim=<span class="nu">1</span>, keepdim=<span class="kw">True</span>)  <span class="cm"># (E, 1)</span>
v_rel = velocities[tgt_idx] - velocities[src_idx]  <span class="cm"># (E, 3)</span>
v_norm = torch.<span class="fn">norm</span>(v_rel, dim=<span class="nu">1</span>, keepdim=<span class="kw">True</span>)  <span class="cm"># (E, 1)</span>
<span class="cm"># æ‹¼æ¥ä¸ºè¾¹ç‰¹å¾ (E, 8)</span>
edge_features = torch.<span class="fn">cat</span>([r_ij, r_norm, v_rel, v_norm], dim=<span class="nu">1</span>)</code></pre>

<!-- ==================== 1.5 GPU åŠ é€Ÿ ==================== -->
<h2 id="s1-5">1.5 GPU åŠ é€Ÿ</h2>

<pre><code><span class="cm"># æ£€æµ‹ GPU æ˜¯å¦å¯ç”¨</span>
<span class="fn">print</span>(torch.cuda.<span class="fn">is_available</span>())    <span class="cm"># True / False</span>
<span class="fn">print</span>(torch.cuda.<span class="fn">device_count</span>())    <span class="cm"># GPU æ•°é‡</span>
<span class="fn">print</span>(torch.cuda.<span class="fn">get_device_name</span>(<span class="nu">0</span>)) <span class="cm"># GPU åç§°</span>

<span class="cm"># è®¾å¤‡é€‰æ‹©çš„å¸¸è§æ¨¡å¼</span>
device = torch.<span class="fn">device</span>(<span class="st">'cuda'</span> <span class="kw">if</span> torch.cuda.<span class="fn">is_available</span>() <span class="kw">else</span> <span class="st">'cpu'</span>)

<span class="cm"># å°† Tensor ç§»åŠ¨åˆ° GPU</span>
x = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">4</span>)
x_gpu = x.<span class="fn">to</span>(device)           <span class="cm"># ç§»åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰</span>
x_gpu = x.<span class="fn">to</span>(<span class="st">'cuda'</span>)           <span class="cm"># æ˜ç¡®ç§»åˆ° GPU</span>
x_cpu = x_gpu.<span class="fn">cpu</span>()            <span class="cm"># ç§»å› CPU</span>

<span class="cm"># ç›´æ¥åœ¨ GPU ä¸Šåˆ›å»º</span>
x = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">4</span>, device=<span class="st">'cuda'</span>)

<span class="cm"># å°†æ¨¡å‹ç§»åˆ° GPU</span>
model = MyNetwork()
model = model.<span class="fn">to</span>(device)

<span class="cm"># âš ï¸ æ•°æ®å’Œæ¨¡å‹å¿…é¡»åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼</span>
<span class="cm"># output = model(x_cpu)  â† å¦‚æœæ¨¡å‹åœ¨ GPUï¼Œè¿™ä¼šæŠ¥é”™ï¼</span></code></pre>

<div class="box def">
  <div class="box-title">æ··åˆç²¾åº¦è®­ç»ƒ (AMP)</div>
  <p>æ··åˆç²¾åº¦ç”¨ <code>float16</code> åŠ é€Ÿè®¡ç®—ï¼Œç”¨ <code>float32</code> ä¿æŒæ¢¯åº¦ç²¾åº¦ï¼š</p>
<pre><code><span class="kw">from</span> torch.cuda.amp <span class="kw">import</span> autocast, GradScaler

scaler = <span class="fn">GradScaler</span>()

<span class="kw">for</span> batch <span class="kw">in</span> dataloader:
    optimizer.<span class="fn">zero_grad</span>()
    
    <span class="kw">with</span> <span class="fn">autocast</span>():          <span class="cm"># è‡ªåŠ¨é€‰æ‹© float16/32</span>
        output = model(batch)
        loss = criterion(output, target)
    
    scaler.<span class="fn">scale</span>(loss).<span class="fn">backward</span>()  <span class="cm"># ç¼©æ”¾ååå‘ä¼ æ’­</span>
    scaler.<span class="fn">step</span>(optimizer)         <span class="cm"># æ›´æ–°å‚æ•°</span>
    scaler.<span class="fn">update</span>()               <span class="cm"># æ›´æ–°ç¼©æ”¾å› å­</span></code></pre>
</div>

<!-- ================================================== -->
<!--                ç¬¬äºŒç« ï¼šè‡ªåŠ¨å¾®åˆ†                       -->
<!-- ================================================== -->
<h1 id="ch2">ç¬¬äºŒç«  &nbsp; è‡ªåŠ¨å¾®åˆ† (Autograd)</h1>

<p>è‡ªåŠ¨å¾®åˆ†æ˜¯ PyTorch çš„çµé­‚ã€‚å®ƒè®©æˆ‘ä»¬åªéœ€å†™<strong>å‰å‘ä¼ æ’­</strong>ï¼ˆè®¡ç®— lossï¼‰ï¼ŒPyTorch å°±èƒ½è‡ªåŠ¨è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦â€”â€”è¿™å°±æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„åŸºç¡€ã€‚</p>

<!-- ==================== 2.1 è®¡ç®—å›¾ ==================== -->
<h2 id="s2-1">2.1 è®¡ç®—å›¾ â€” å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ„å»º</h2>

<div class="box def">
  <div class="box-title">å®šä¹‰ï¼šè®¡ç®—å›¾ (Computational Graph)</div>
  <p>PyTorch åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶ï¼Œä¼šè‡ªåŠ¨è®°å½•æ‰€æœ‰å‚ä¸è®¡ç®—çš„æ“ä½œï¼Œæ„å»ºä¸€ä¸ª<strong>æœ‰å‘æ— ç¯å›¾ (DAG)</strong>ã€‚å›¾çš„èŠ‚ç‚¹æ˜¯å¼ é‡ï¼Œè¾¹æ˜¯è¿ç®—æ“ä½œã€‚åå‘ä¼ æ’­æ—¶ï¼ŒPyTorch æ²¿ç€è¿™ä¸ªå›¾ä» loss å›æº¯ï¼Œè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚</p>
</div>

<div class="diagram">
<svg width="600" height="200" viewBox="0 0 600 200">
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="currentColor"/>
    </marker>
  </defs>
  <!-- Nodes -->
  <rect x="10" y="75" width="60" height="40" rx="8" fill="#e8edff" stroke="#4361ee" stroke-width="2"/>
  <text x="40" y="100" text-anchor="middle" font-size="14" fill="#4361ee" font-weight="bold">x</text>
  
  <rect x="120" y="75" width="60" height="40" rx="8" fill="#e8edff" stroke="#4361ee" stroke-width="2"/>
  <text x="150" y="100" text-anchor="middle" font-size="14" fill="#4361ee" font-weight="bold">W</text>
  
  <circle cx="250" cy="95" r="20" fill="#e8f5e9" stroke="#4CAF50" stroke-width="2"/>
  <text x="250" y="100" text-anchor="middle" font-size="14" fill="#4CAF50" font-weight="bold">@</text>
  
  <rect x="300" y="75" width="60" height="40" rx="8" fill="#e8edff" stroke="#4361ee" stroke-width="2"/>
  <text x="330" y="100" text-anchor="middle" font-size="14" fill="#4361ee" font-weight="bold">b</text>
  
  <circle cx="410" cy="95" r="20" fill="#e8f5e9" stroke="#4CAF50" stroke-width="2"/>
  <text x="410" y="100" text-anchor="middle" font-size="14" fill="#4CAF50" font-weight="bold">+</text>
  
  <circle cx="490" cy="95" r="20" fill="#fff8e1" stroke="#FF9800" stroke-width="2"/>
  <text x="490" y="100" text-anchor="middle" font-size="12" fill="#FF9800" font-weight="bold">ReLU</text>
  
  <rect x="530" y="75" width="60" height="40" rx="8" fill="#ffebee" stroke="#f44336" stroke-width="2"/>
  <text x="560" y="100" text-anchor="middle" font-size="14" fill="#f44336" font-weight="bold">y</text>
  
  <!-- Arrows -->
  <line x1="70" y1="95" x2="228" y2="95" stroke="currentColor" stroke-width="1.5" marker-end="url(#arrowhead)"/>
  <line x1="180" y1="95" x2="228" y2="95" stroke="currentColor" stroke-width="1.5" marker-end="url(#arrowhead)"/>
  <line x1="270" y1="95" x2="388" y2="95" stroke="currentColor" stroke-width="1.5" marker-end="url(#arrowhead)"/>
  <line x1="360" y1="95" x2="388" y2="95" stroke="currentColor" stroke-width="1.5" marker-end="url(#arrowhead)"/>
  <line x1="430" y1="95" x2="468" y2="95" stroke="currentColor" stroke-width="1.5" marker-end="url(#arrowhead)"/>
  <line x1="510" y1="95" x2="528" y2="95" stroke="currentColor" stroke-width="1.5" marker-end="url(#arrowhead)"/>
  
  <!-- Labels -->
  <text x="250" y="55" text-anchor="middle" font-size="12" fill="currentColor">matmul</text>
  <text x="410" y="55" text-anchor="middle" font-size="12" fill="currentColor">add</text>
  <text x="40" y="65" text-anchor="middle" font-size="11" fill="currentColor">è¾“å…¥</text>
  <text x="150" y="65" text-anchor="middle" font-size="11" fill="currentColor">æƒé‡</text>
  <text x="330" y="65" text-anchor="middle" font-size="11" fill="currentColor">åç½®</text>
  <text x="560" y="65" text-anchor="middle" font-size="11" fill="currentColor">è¾“å‡º</text>
  
  <text x="300" y="175" text-anchor="middle" font-size="13" fill="currentColor">y = ReLU(W @ x + b) çš„è®¡ç®—å›¾</text>
</svg>
</div>

<!-- ==================== 2.2 requires_grad ==================== -->
<h2 id="s2-2">2.2 requires_grad â€” æŒ‡å®šå“ªäº›éœ€è¦æ¢¯åº¦</h2>

<pre><code><span class="cm"># requires_grad=True è¡¨ç¤ºè¿™ä¸ªå¼ é‡éœ€è¦è®¡ç®—æ¢¯åº¦</span>
x = torch.<span class="fn">tensor</span>([<span class="nu">2.0</span>, <span class="nu">3.0</span>], requires_grad=<span class="kw">True</span>)
<span class="fn">print</span>(x.requires_grad)  <span class="cm"># True</span>

<span class="cm"># nn.Module çš„å‚æ•°é»˜è®¤ requires_grad=True</span>
linear = nn.<span class="fn">Linear</span>(<span class="nu">16</span>, <span class="nu">64</span>)
<span class="fn">print</span>(linear.weight.requires_grad)  <span class="cm"># True</span>
<span class="fn">print</span>(linear.bias.requires_grad)    <span class="cm"># True</span>

<span class="cm"># æ™®é€š tensor é»˜è®¤ False</span>
data = torch.<span class="fn">randn</span>(<span class="nu">10</span>, <span class="nu">16</span>)
<span class="fn">print</span>(data.requires_grad)  <span class="cm"># False â€” è¾“å…¥æ•°æ®ä¸éœ€è¦æ¢¯åº¦</span>

<span class="cm"># å†»ç»“å‚æ•°ï¼ˆfine-tuning æ—¶å¸¸ç”¨ï¼‰</span>
<span class="kw">for</span> param <span class="kw">in</span> model.<span class="fn">parameters</span>():
    param.requires_grad = <span class="kw">False</span>  <span class="cm"># å†»ç»“æ‰€æœ‰å‚æ•°</span></code></pre>

<!-- ==================== 2.3 backward() ==================== -->
<h2 id="s2-3">2.3 backward() â€” åå‘ä¼ æ’­</h2>

<pre><code><span class="cm"># å®Œæ•´çš„å‰å‘ + åå‘ä¼ æ’­æµç¨‹</span>
x = torch.<span class="fn">tensor</span>([<span class="nu">2.0</span>], requires_grad=<span class="kw">True</span>)
W = torch.<span class="fn">tensor</span>([<span class="nu">3.0</span>], requires_grad=<span class="kw">True</span>)
b = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>], requires_grad=<span class="kw">True</span>)

<span class="cm"># å‰å‘ä¼ æ’­ï¼šy = Wx + b = 3*2 + 1 = 7</span>
y = W * x + b
<span class="fn">print</span>(y)  <span class="cm"># tensor([7.], grad_fn=&lt;AddBackward0&gt;)</span>

<span class="cm"># å®šä¹‰ lossï¼ˆç®€å•ç”¨ y æœ¬èº«ï¼‰</span>
loss = y.<span class="fn">sum</span>()

<span class="cm"># åå‘ä¼ æ’­ï¼šè®¡ç®— âˆ‚loss/âˆ‚W, âˆ‚loss/âˆ‚b, âˆ‚loss/âˆ‚x</span>
loss.<span class="fn">backward</span>()

<span class="cm"># æŸ¥çœ‹æ¢¯åº¦</span>
<span class="fn">print</span>(W.grad)  <span class="cm"># tensor([2.]) â† âˆ‚loss/âˆ‚W = x = 2</span>
<span class="fn">print</span>(b.grad)  <span class="cm"># tensor([1.]) â† âˆ‚loss/âˆ‚b = 1</span>
<span class="fn">print</span>(x.grad)  <span class="cm"># tensor([3.]) â† âˆ‚loss/âˆ‚x = W = 3</span></code></pre>

<div class="box thm">
  <div class="box-title">backward() çš„æ•°å­¦å«ä¹‰</div>
  <p>å¯¹äº \(y = Wx + b\)ï¼Œloss = \(y\)ï¼š</p>
  <p>$$\frac{\partial \text{loss}}{\partial W} = x, \quad \frac{\partial \text{loss}}{\partial b} = 1, \quad \frac{\partial \text{loss}}{\partial x} = W$$</p>
  <p><code>backward()</code> å°±æ˜¯è‡ªåŠ¨å®Œæˆäº†è¿™äº›åå¯¼æ•°çš„è®¡ç®—ã€‚</p>
</div>

<!-- ==================== 2.4 é“¾å¼æ³•åˆ™ ==================== -->
<h2 id="s2-4">2.4 é“¾å¼æ³•åˆ™ â€” è‡ªåŠ¨å¾®åˆ†çš„æ•°å­¦æ ¸å¿ƒ</h2>

<div class="box thm">
  <div class="box-title">é“¾å¼æ³•åˆ™ (Chain Rule)</div>
  <p>å¦‚æœ \(z = f(g(x))\)ï¼Œåˆ™ï¼š</p>
  <p>$$\frac{dz}{dx} = \frac{dz}{dg} \cdot \frac{dg}{dx}$$</p>
  <p>æ¨å¹¿åˆ°å¤šå±‚ï¼š</p>
  <p>$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial y_3} \cdot \frac{\partial y_3}{\partial y_2} \cdot \frac{\partial y_2}{\partial y_1} \cdot \frac{\partial y_1}{\partial W_1}$$</p>
  <p>PyTorch çš„ autograd å°±æ˜¯è‡ªåŠ¨åœ°ç”¨é“¾å¼æ³•åˆ™ä» loss ä¸€è·¯ä¹˜å›å»ã€‚</p>
</div>

<div class="box example">
  <div class="box-title">å¤šå±‚é“¾å¼æ³•åˆ™å®ä¾‹</div>
<pre><code><span class="cm"># y = Ïƒ(Wâ‚‚ Â· ReLU(Wâ‚ Â· x + bâ‚) + bâ‚‚)</span>
x = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>], requires_grad=<span class="kw">True</span>)

<span class="cm"># ç¬¬ä¸€å±‚</span>
W1 = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">2</span>, requires_grad=<span class="kw">True</span>)
b1 = torch.<span class="fn">randn</span>(<span class="nu">3</span>, requires_grad=<span class="kw">True</span>)
h1 = W1 <span class="op">@</span> x + b1              <span class="cm"># (3,)</span>
a1 = torch.<span class="fn">relu</span>(h1)            <span class="cm"># (3,)</span>

<span class="cm"># ç¬¬äºŒå±‚</span>
W2 = torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">3</span>, requires_grad=<span class="kw">True</span>)
b2 = torch.<span class="fn">randn</span>(<span class="nu">1</span>, requires_grad=<span class="kw">True</span>)
h2 = W2 <span class="op">@</span> a1 + b2              <span class="cm"># (1,)</span>
y = torch.<span class="fn">sigmoid</span>(h2)           <span class="cm"># (1,)</span>

<span class="cm"># loss = -log(y)  ï¼ˆäºŒå…ƒäº¤å‰ç†µçš„ä¸€åŠï¼‰</span>
loss = -torch.<span class="fn">log</span>(y)
loss.<span class="fn">backward</span>()

<span class="cm"># ç°åœ¨ W1.grad, W2.grad, b1.grad, b2.grad éƒ½æœ‰å€¼äº†</span>
<span class="cm"># PyTorch è‡ªåŠ¨åº”ç”¨äº†é“¾å¼æ³•åˆ™ï¼</span>
<span class="fn">print</span>(W1.grad.shape)  <span class="cm"># (3, 2) â€” ä¸ W1 ç›¸åŒ</span>
<span class="fn">print</span>(W2.grad.shape)  <span class="cm"># (1, 3) â€” ä¸ W2 ç›¸åŒ</span></code></pre>
</div>

<!-- ==================== 2.5 æ¢¯åº¦ç´¯ç§¯å’Œæ¸…é›¶ ==================== -->
<h2 id="s2-5">2.5 æ¢¯åº¦ç´¯ç§¯å’Œæ¸…é›¶</h2>

<div class="box warn">
  <div class="box-title">æ ¸å¿ƒæ³¨æ„ï¼šPyTorch æ¢¯åº¦ä¼šç´¯ç§¯ï¼</div>
  <p>æ¯æ¬¡è°ƒç”¨ <code>backward()</code>ï¼Œæ¢¯åº¦ä¼š<strong>ç´¯åŠ </strong>åˆ° <code>.grad</code> å±æ€§ä¸Šï¼Œè€Œä¸æ˜¯è¦†ç›–ã€‚å¦‚æœä¸æ‰‹åŠ¨æ¸…é›¶ï¼Œæ¢¯åº¦ä¼šè¶Šæ¥è¶Šå¤§ã€‚è¿™æ˜¯ PyTorch åˆå­¦è€…æœ€å¸¸çŠ¯çš„é”™è¯¯ä¹‹ä¸€ï¼</p>
</div>

<pre><code>x = torch.<span class="fn">tensor</span>([<span class="nu">2.0</span>], requires_grad=<span class="kw">True</span>)

<span class="cm"># ç¬¬ä¸€æ¬¡ backward</span>
y = x ** <span class="nu">2</span>
y.<span class="fn">backward</span>()
<span class="fn">print</span>(x.grad)  <span class="cm"># tensor([4.])  â† âˆ‚(xÂ²)/âˆ‚x = 2x = 4 âœ“</span>

<span class="cm"># ç¬¬äºŒæ¬¡ backwardï¼ˆä¸æ¸…é›¶ï¼ï¼‰</span>
y = x ** <span class="nu">2</span>
y.<span class="fn">backward</span>()
<span class="fn">print</span>(x.grad)  <span class="cm"># tensor([8.])  â† 4 + 4 = 8 âœ— é”™è¯¯ç´¯ç§¯ï¼</span>

<span class="cm"># æ­£ç¡®åšæ³•ï¼šæ¯æ¬¡ backward å‰æ¸…é›¶æ¢¯åº¦</span>
x.grad.<span class="fn">zero_</span>()    <span class="cm"># æ‰‹åŠ¨æ¸…é›¶</span>
y = x ** <span class="nu">2</span>
y.<span class="fn">backward</span>()
<span class="fn">print</span>(x.grad)  <span class="cm"># tensor([4.])  â† æ­£ç¡® âœ“</span>

<span class="cm"># åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œç”¨ optimizer.zero_grad()</span>
<span class="kw">for</span> batch <span class="kw">in</span> dataloader:
    optimizer.<span class="fn">zero_grad</span>()      <span class="cm"># â‘  æ¸…é›¶æ¢¯åº¦</span>
    output = model(batch)       <span class="cm"># â‘¡ å‰å‘ä¼ æ’­</span>
    loss = criterion(output)    <span class="cm"># â‘¢ è®¡ç®— loss</span>
    loss.<span class="fn">backward</span>()             <span class="cm"># â‘£ åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰</span>
    optimizer.<span class="fn">step</span>()            <span class="cm"># â‘¤ æ›´æ–°å‚æ•°</span></code></pre>

<div class="box example">
  <div class="box-title">æ¢¯åº¦ç´¯ç§¯çš„æ­£é¢ç”¨é€”</div>
  <p>æ¢¯åº¦ç´¯ç§¯ä¹Ÿæœ‰ç”¨ï¼å½“ GPU å†…å­˜ä¸å¤Ÿä¸€æ¬¡å¤„ç†å¤§ batch æ—¶ï¼Œå¯ä»¥åˆ†æˆå° batchï¼Œç´¯ç§¯æ¢¯åº¦åå†æ›´æ–°ï¼š</p>
<pre><code>accumulation_steps = <span class="nu">4</span>
optimizer.<span class="fn">zero_grad</span>()
<span class="kw">for</span> i, batch <span class="kw">in</span> <span class="fn">enumerate</span>(dataloader):
    loss = model(batch) / accumulation_steps  <span class="cm"># ç¼©æ”¾ loss</span>
    loss.<span class="fn">backward</span>()                            <span class="cm"># ç´¯ç§¯æ¢¯åº¦</span>
    <span class="kw">if</span> (i + <span class="nu">1</span>) % accumulation_steps == <span class="nu">0</span>:
        optimizer.<span class="fn">step</span>()       <span class="cm"># æ¯ 4 æ­¥æ›´æ–°ä¸€æ¬¡</span>
        optimizer.<span class="fn">zero_grad</span>() <span class="cm"># æ¸…é›¶é‡æ¥</span></code></pre>
</div>

<!-- ==================== 2.6 detach å’Œ no_grad ==================== -->
<h2 id="s2-6">2.6 detach() å’Œ no_grad() â€” ä½•æ—¶åœæ­¢æ¢¯åº¦è·Ÿè¸ª</h2>

<pre><code><span class="cm"># ===== torch.no_grad() â€” æ¨ç†æ—¶ç”¨ =====</span>
<span class="cm"># åœ¨ä¸éœ€è¦æ¢¯åº¦çš„æ“ä½œä¸­ï¼Œå…³é—­ autograd å¯ä»¥èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿ</span>
model.<span class="fn">eval</span>()  <span class="cm"># åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼</span>
<span class="kw">with</span> torch.<span class="fn">no_grad</span>():
    output = model(test_data)
    <span class="cm"># è¿™é‡Œä¸ä¼šæ„å»ºè®¡ç®—å›¾ï¼ŒèŠ‚çœå¤§é‡å†…å­˜</span>

<span class="cm"># åœ¨ PPO çš„æ•°æ®æ”¶é›†é˜¶æ®µï¼ˆrolloutï¼‰ï¼Œä¸éœ€è¦æ¢¯åº¦ï¼š</span>
<span class="kw">with</span> torch.<span class="fn">no_grad</span>():
    action_mean = actor(obs)
    value = critic(obs)
    <span class="cm"># åªéœ€è¦å€¼ï¼Œä¸éœ€è¦è®¡ç®—å›¾</span>

<span class="cm"># ===== .detach() â€” ä»è®¡ç®—å›¾ä¸­åˆ†ç¦» =====</span>
x = torch.<span class="fn">tensor</span>([<span class="nu">2.0</span>], requires_grad=<span class="kw">True</span>)
y = x ** <span class="nu">2</span>
z = y.<span class="fn">detach</span>()  <span class="cm"># z ä¸ y æ•°æ®ç›¸åŒï¼Œä½†ä¸åœ¨è®¡ç®—å›¾ä¸­</span>

<span class="fn">print</span>(z.requires_grad)  <span class="cm"># False</span>

<span class="cm"># å®é™…åº”ç”¨ï¼šPPO ä¸­æ—§ç­–ç•¥çš„ log_prob éœ€è¦ detach</span>
<span class="kw">with</span> torch.<span class="fn">no_grad</span>():
    old_log_prob = policy.<span class="fn">get_log_prob</span>(obs, action)
<span class="cm"># æˆ–è€…</span>
old_log_prob = policy.<span class="fn">get_log_prob</span>(obs, action).<span class="fn">detach</span>()</code></pre>

<div class="box def">
  <div class="box-title">no_grad vs detach å¯¹æ¯”</div>
  <table>
    <tr><th>ç‰¹æ€§</th><th><code>torch.no_grad()</code></th><th><code>.detach()</code></th></tr>
    <tr><td>ä½œç”¨èŒƒå›´</td><td>ä¸Šä¸‹æ–‡ç®¡ç†å™¨å†…æ‰€æœ‰æ“ä½œ</td><td>å•ä¸ªå¼ é‡</td></tr>
    <tr><td>ç”¨é€”</td><td>æ¨ç†ã€è¯„ä¼°</td><td>ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»æŸä¸ªå€¼</td></tr>
    <tr><td>å…¸å‹åœºæ™¯</td><td>rolloutã€evaluate</td><td>æ—§ç­–ç•¥ log_probã€target value</td></tr>
  </table>
</div>

<!-- ==================== 2.7 æ‰‹åŠ¨ vs autograd ==================== -->
<h2 id="s2-7">2.7 å®ä¾‹ï¼šæ‰‹åŠ¨è®¡ç®— vs autograd</h2>

<div class="box example">
  <div class="box-title">å¯¹æ¯”ï¼šæ‰‹åŠ¨æ¨å¯¼ vs autograd è‡ªåŠ¨è®¡ç®—</div>
  <p>è€ƒè™‘å‡½æ•° \(f(x) = 3x^2 + 2x + 1\)ï¼Œæˆ‘ä»¬å¯¹ \(x = 2\) æ±‚å¯¼ï¼š</p>
  <p>$$f'(x) = 6x + 2 \implies f'(2) = 14$$</p>
</div>

<div class="tab-group">
  <div class="tab-buttons">
    <button class="tab-btn active" onclick="switchTab(this, 'manual-calc')">æ‰‹åŠ¨è®¡ç®—</button>
    <button class="tab-btn" onclick="switchTab(this, 'autograd-calc')">autograd è®¡ç®—</button>
  </div>
  <div class="tab-content active" id="manual-calc">
<pre><code><span class="cm"># æ‰‹åŠ¨æ¨å¯¼ f'(x) = 6x + 2</span>
x_val = <span class="nu">2.0</span>
grad_manual = <span class="nu">6</span> * x_val + <span class="nu">2</span>
<span class="fn">print</span>(<span class="st">f"æ‰‹åŠ¨æ¢¯åº¦: {grad_manual}"</span>)  <span class="cm"># 14.0</span></code></pre>
  </div>
  <div class="tab-content" id="autograd-calc">
<pre><code><span class="cm"># autograd è‡ªåŠ¨è®¡ç®—</span>
x = torch.<span class="fn">tensor</span>(<span class="nu">2.0</span>, requires_grad=<span class="kw">True</span>)
f = <span class="nu">3</span> * x**<span class="nu">2</span> + <span class="nu">2</span> * x + <span class="nu">1</span>
f.<span class="fn">backward</span>()
<span class="fn">print</span>(<span class="st">f"autograd æ¢¯åº¦: {x.grad}"</span>)  <span class="cm"># tensor(14.)</span></code></pre>
  </div>
</div>

<p>å¯¹äºç®€å•å‡½æ•°ï¼Œæ‰‹åŠ¨æ¨å¯¼å¾ˆå®¹æ˜“ã€‚ä½†å½“ç½‘ç»œæœ‰å‡ ç™¾ä¸‡å‚æ•°ã€å‡ åå±‚æ—¶ï¼Œæ‰‹åŠ¨æ¨å¯¼å®Œå…¨ä¸å¯è¡Œâ€”â€”è¿™å°±æ˜¯ autograd çš„ä»·å€¼ã€‚</p>

<div class="box example">
  <div class="box-title">æ›´å¤æ‚çš„ä¾‹å­ï¼šå‘é‡å‡½æ•°çš„é›…å¯æ¯”</div>
<pre><code><span class="cm"># f(x) = Î£(Wx)Â²ï¼ŒW æ˜¯ 2Ã—3 çŸ©é˜µ</span>
W = torch.<span class="fn">randn</span>(<span class="nu">2</span>, <span class="nu">3</span>, requires_grad=<span class="kw">True</span>)
x = torch.<span class="fn">randn</span>(<span class="nu">3</span>)

y = (W <span class="op">@</span> x) ** <span class="nu">2</span>          <span class="cm"># (2,)</span>
loss = y.<span class="fn">sum</span>()              <span class="cm"># æ ‡é‡</span>
loss.<span class="fn">backward</span>()

<span class="cm"># W.grad çš„æ¯ä¸ªå…ƒç´ æ˜¯ âˆ‚loss/âˆ‚W_ij</span>
<span class="fn">print</span>(W.grad)               <span class="cm"># (2, 3) â€” ä¸ W å½¢çŠ¶ä¸€è‡´</span>
<span class="fn">print</span>(W.grad.shape)         <span class="cm"># torch.Size([2, 3])</span>

<span class="cm"># éªŒè¯ï¼šâˆ‚loss/âˆ‚W = 2 * diag(Wx) @ x^T</span>
Wx = W <span class="op">@</span> x
grad_manual = <span class="nu">2</span> * Wx.<span class="fn">unsqueeze</span>(<span class="nu">1</span>) * x.<span class="fn">unsqueeze</span>(<span class="nu">0</span>)
<span class="fn">print</span>(torch.<span class="fn">allclose</span>(W.grad, grad_manual))  <span class="cm"># True âœ“</span></code></pre>
</div>

<!-- ================================================== -->
<!--               ç¬¬ä¸‰ç« ï¼šç¥ç»ç½‘ç»œ                        -->
<!-- ================================================== -->
<h1 id="ch3">ç¬¬ä¸‰ç«  &nbsp; ç¥ç»ç½‘ç»œ (nn.Module)</h1>

<!-- ==================== 3.1 nn.Module ==================== -->
<h2 id="s3-1">3.1 nn.Module åŸºç±» â€” __init__ + forward</h2>

<div class="box def">
  <div class="box-title">å®šä¹‰ï¼šnn.Module</div>
  <p><code>nn.Module</code> æ˜¯ PyTorch ä¸­æ‰€æœ‰ç¥ç»ç½‘ç»œçš„åŸºç±»ã€‚æ‰€æœ‰è‡ªå®šä¹‰ç½‘ç»œéƒ½ç»§æ‰¿å®ƒã€‚ä½ åªéœ€å®šä¹‰ä¸¤ä¸ªæ–¹æ³•ï¼š</p>
  <ol>
    <li><code>__init__</code> â€” å®šä¹‰ç½‘ç»œå±‚ï¼ˆå‚æ•°åœ¨è¿™é‡Œåˆ›å»ºï¼‰</li>
    <li><code>forward</code> â€” å®šä¹‰å‰å‘ä¼ æ’­é€»è¾‘ï¼ˆæ•°æ®å¦‚ä½•æµè¿‡ç½‘ç»œï¼‰</li>
  </ol>
  <p>è°ƒç”¨æ¨¡å‹æ—¶ç”¨ <code>model(x)</code> è€Œä¸æ˜¯ <code>model.forward(x)</code>ï¼Œå› ä¸º <code>__call__</code> ä¼šé¢å¤–è°ƒç”¨ hooksã€‚</p>
</div>

<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn

<span class="kw">class</span> <span class="cl">SimpleNet</span>(nn.Module):
    <span class="st">"""æœ€ç®€å•çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ"""</span>
    
    <span class="kw">def</span> <span class="fn">__init__</span>(self, input_dim, hidden_dim, output_dim):
        <span class="fn">super</span>().<span class="fn">__init__</span>()  <span class="cm"># å¿…é¡»è°ƒç”¨ï¼</span>
        
        <span class="cm"># å®šä¹‰å±‚ï¼ˆåœ¨è¿™é‡Œåˆ›å»ºå‚æ•°ï¼‰</span>
        self.fc1 = nn.<span class="fn">Linear</span>(input_dim, hidden_dim)   <span class="cm"># ç¬¬ä¸€å±‚</span>
        self.fc2 = nn.<span class="fn">Linear</span>(hidden_dim, output_dim)  <span class="cm"># ç¬¬äºŒå±‚</span>
        self.relu = nn.<span class="fn">ReLU</span>()                         <span class="cm"># æ¿€æ´»å‡½æ•°</span>
    
    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        <span class="cm"># å®šä¹‰æ•°æ®æµå‘ï¼ˆå‰å‘ä¼ æ’­ï¼‰</span>
        x = self.fc1(x)      <span class="cm"># çº¿æ€§å˜æ¢: (batch, input) â†’ (batch, hidden)</span>
        x = self.relu(x)     <span class="cm"># æ¿€æ´»: (batch, hidden) â†’ (batch, hidden)</span>
        x = self.fc2(x)      <span class="cm"># çº¿æ€§å˜æ¢: (batch, hidden) â†’ (batch, output)</span>
        <span class="kw">return</span> x

<span class="cm"># ä½¿ç”¨</span>
model = <span class="fn">SimpleNet</span>(<span class="nu">16</span>, <span class="nu">64</span>, <span class="nu">2</span>)
obs = torch.<span class="fn">randn</span>(<span class="nu">32</span>, <span class="nu">16</span>)    <span class="cm"># batch=32, obs_dim=16</span>
action = model(obs)             <span class="cm"># (32, 2)</span>
<span class="fn">print</span>(action.shape)             <span class="cm"># torch.Size([32, 2])</span></code></pre>

<!-- ==================== 3.2 å¸¸ç”¨å±‚ ==================== -->
<h2 id="s3-2">3.2 å¸¸ç”¨å±‚è¯¦è§£</h2>

<h3>3.2.1 nn.Linear â€” å…¨è¿æ¥å±‚</h3>

<div class="box thm">
  <div class="box-title">nn.Linear çš„æ•°å­¦</div>
  <p>$$y = xW^T + b$$</p>
  <p>å…¶ä¸­ \(W \in \mathbb{R}^{out \times in}\)ï¼Œ\(b \in \mathbb{R}^{out}\)ã€‚</p>
  <p>å‚æ•°é‡ = \(\text{in} \times \text{out} + \text{out}\)ï¼ˆæƒé‡ + åç½®ï¼‰</p>
</div>

<pre><code>layer = nn.<span class="fn">Linear</span>(<span class="nu">16</span>, <span class="nu">64</span>)

<span class="cm"># å‚æ•°è¯¦æƒ…</span>
<span class="fn">print</span>(layer.weight.shape)   <span class="cm"># (64, 16) â€” æ³¨æ„æ˜¯ (out, in) è€Œä¸æ˜¯ (in, out)ï¼</span>
<span class="fn">print</span>(layer.bias.shape)     <span class="cm"># (64,)</span>
<span class="fn">print</span>(<span class="st">f"å‚æ•°é‡: {16*64 + 64} = {16*64 + 64}"</span>)  <span class="cm"># 1088</span>

<span class="cm"># å‰å‘ä¼ æ’­</span>
x = torch.<span class="fn">randn</span>(<span class="nu">32</span>, <span class="nu">16</span>)    <span class="cm"># (batch=32, in=16)</span>
y = layer(x)                 <span class="cm"># (32, 64)</span>

<span class="cm"># å†…éƒ¨åšçš„å°±æ˜¯ï¼š</span>
y_manual = x <span class="op">@</span> layer.weight.T + layer.bias  <span class="cm"># å®Œå…¨ç­‰ä»·</span></code></pre>

<div class="box example">
  <div class="box-title">æˆ‘ä»¬é¡¹ç›®ä¸­å„å±‚çš„å‚æ•°é‡</div>
  <table>
    <tr><th>å±‚</th><th>Input â†’ Output</th><th>å‚æ•°é‡</th></tr>
    <tr><td>Actor.fc1</td><td>16 â†’ 64</td><td>16Ã—64 + 64 = 1,088</td></tr>
    <tr><td>Actor.fc2</td><td>64 â†’ 64</td><td>64Ã—64 + 64 = 4,160</td></tr>
    <tr><td>Actor.fc3</td><td>64 â†’ 2</td><td>64Ã—2 + 2 = 130</td></tr>
    <tr><td><strong>Actor æ€»è®¡</strong></td><td></td><td><strong>5,378</strong></td></tr>
    <tr><td>Critic.fc1</td><td>16 â†’ 64</td><td>1,088</td></tr>
    <tr><td>Critic.fc2</td><td>64 â†’ 64</td><td>4,160</td></tr>
    <tr><td>Critic.fc3</td><td>64 â†’ 1</td><td>65</td></tr>
    <tr><td><strong>Critic æ€»è®¡</strong></td><td></td><td><strong>5,313</strong></td></tr>
  </table>
</div>

<h3>3.2.2 nn.Conv1d / Conv2d â€” å·ç§¯å±‚ï¼ˆç®€ä»‹ï¼‰</h3>

<pre><code><span class="cm"># Conv2d: å›¾åƒå¤„ç†å¸¸ç”¨</span>
conv = nn.<span class="fn">Conv2d</span>(
    in_channels=<span class="nu">3</span>,       <span class="cm"># è¾“å…¥é€šé“ï¼ˆå¦‚ RGB=3ï¼‰</span>
    out_channels=<span class="nu">16</span>,     <span class="cm"># è¾“å‡ºé€šé“ï¼ˆå·ç§¯æ ¸æ•°é‡ï¼‰</span>
    kernel_size=<span class="nu">3</span>,       <span class="cm"># å·ç§¯æ ¸å¤§å° 3Ã—3</span>
    padding=<span class="nu">1</span>,            <span class="cm"># å¡«å……</span>
)
img = torch.<span class="fn">randn</span>(<span class="nu">8</span>, <span class="nu">3</span>, <span class="nu">32</span>, <span class="nu">32</span>)   <span class="cm"># (batch, C, H, W)</span>
out = conv(img)                       <span class="cm"># (8, 16, 32, 32)</span>

<span class="cm"># Conv1d: æ—¶åºæ•°æ®</span>
conv1d = nn.<span class="fn">Conv1d</span>(in_channels=<span class="nu">4</span>, out_channels=<span class="nu">16</span>, kernel_size=<span class="nu">3</span>, padding=<span class="nu">1</span>)
seq = torch.<span class="fn">randn</span>(<span class="nu">8</span>, <span class="nu">4</span>, <span class="nu">100</span>)       <span class="cm"># (batch, channels, length)</span>
out = conv1d(seq)                      <span class="cm"># (8, 16, 100)</span>

<span class="cm"># åœ¨æˆ‘ä»¬é¡¹ç›®ä¸­ï¼Œä¸»è¦ç”¨å…¨è¿æ¥ç½‘ç»œï¼ˆMLPï¼‰ï¼Œå·ç§¯å±‚äº†è§£å³å¯</span></code></pre>

<h3>3.2.3 æ¿€æ´»å‡½æ•° â€” ReLU, Tanh, Sigmoid</h3>

<pre><code>x = torch.<span class="fn">linspace</span>(-<span class="nu">3</span>, <span class="nu">3</span>, <span class="nu">7</span>)  <span class="cm"># [-3, -2, -1, 0, 1, 2, 3]</span>

<span class="cm"># ReLU: max(0, x) â€” æœ€å¸¸ç”¨ï¼Œéšè—å±‚é»˜è®¤é€‰æ‹©</span>
relu = nn.<span class="fn">ReLU</span>()
<span class="fn">print</span>(relu(x))  <span class="cm"># [0, 0, 0, 0, 1, 2, 3]</span>

<span class="cm"># Tanh: è¾“å‡º [-1, 1] â€” Actor è¾“å‡ºå±‚å¸¸ç”¨</span>
tanh = nn.<span class="fn">Tanh</span>()
<span class="fn">print</span>(tanh(x))  <span class="cm"># [-0.995, -0.964, -0.762, 0, 0.762, 0.964, 0.995]</span>

<span class="cm"># Sigmoid: è¾“å‡º [0, 1] â€” æ¦‚ç‡è¾“å‡º</span>
sigmoid = nn.<span class="fn">Sigmoid</span>()
<span class="fn">print</span>(sigmoid(x))  <span class="cm"># [0.047, 0.119, 0.269, 0.5, 0.731, 0.881, 0.953]</span>

<span class="cm"># ä¹Ÿå¯ä»¥ç”¨å‡½æ•°å½¢å¼ï¼ˆä¸éœ€è¦å®ä¾‹åŒ–ï¼‰</span>
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F
y = F.<span class="fn">relu</span>(x)
y = torch.<span class="fn">tanh</span>(x)
y = torch.<span class="fn">sigmoid</span>(x)</code></pre>

<div class="box def">
  <div class="box-title">æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—</div>
  <table>
    <tr><th>å±‚</th><th>æ¨èæ¿€æ´»</th><th>åŸå› </th></tr>
    <tr><td>éšè—å±‚</td><td>ReLU</td><td>è®¡ç®—å¿«ã€æ¢¯åº¦ä¸æ˜“æ¶ˆå¤±</td></tr>
    <tr><td>Actor è¾“å‡º</td><td>Tanh</td><td>é™åˆ¶åŠ¨ä½œèŒƒå›´åˆ° [-1, 1]</td></tr>
    <tr><td>æ¦‚ç‡è¾“å‡º</td><td>Sigmoid</td><td>è¾“å‡º [0, 1] èŒƒå›´</td></tr>
    <tr><td>åˆ†ç±»è¾“å‡º</td><td>Softmax</td><td>è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼ˆå’Œä¸º1ï¼‰</td></tr>
    <tr><td>Critic è¾“å‡º</td><td>æ— </td><td>ä»·å€¼å¯ä»¥æ˜¯ä»»æ„å®æ•°</td></tr>
  </table>
</div>

<h3>3.2.4 å½’ä¸€åŒ–å±‚ â€” LayerNorm, BatchNorm</h3>

<pre><code><span class="cm"># LayerNorm â€” å¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å½’ä¸€åŒ–ï¼ˆRL ä¸­æ›´å¸¸ç”¨ï¼‰</span>
ln = nn.<span class="fn">LayerNorm</span>(<span class="nu">64</span>)
x = torch.<span class="fn">randn</span>(<span class="nu">32</span>, <span class="nu">64</span>)
y = ln(x)  <span class="cm"># æ¯ä¸ªæ ·æœ¬çš„ 64 ç»´ç‰¹å¾å½’ä¸€åŒ–ä¸ºå‡å€¼0ã€æ–¹å·®1</span>

<span class="cm"># BatchNorm â€” å¯¹æ•´ä¸ª batch å½’ä¸€åŒ–ï¼ˆCV ä¸­å¸¸ç”¨ï¼‰</span>
bn = nn.<span class="fn">BatchNorm1d</span>(<span class="nu">64</span>)
y = bn(x)  <span class="cm"># å¯¹ batch ç»´åº¦å½’ä¸€åŒ–</span>

<span class="cm"># åœ¨æˆ‘ä»¬çš„ EdgeFrame ä¸­ä½¿ç”¨äº† LayerNormï¼š</span>
<span class="cm"># self.edge_encoder = nn.Sequential(</span>
<span class="cm">#     nn.Linear(8, hidden_dim),</span>
<span class="cm">#     nn.LayerNorm(hidden_dim),  â† è¿™é‡Œ</span>
<span class="cm">#     nn.ReLU(),</span>
<span class="cm">#     nn.Linear(hidden_dim, hidden_dim),</span>
<span class="cm">#     nn.LayerNorm(hidden_dim),  â† å’Œè¿™é‡Œ</span>
<span class="cm">#     nn.ReLU(),</span>
<span class="cm"># )</span></code></pre>

<h3>3.2.5 nn.Sequential â€” å¿«é€Ÿæ­å»º</h3>

<pre><code><span class="cm"># Sequential æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰å±‚ï¼Œä¸éœ€è¦å†™ forward</span>
model = nn.<span class="fn">Sequential</span>(
    nn.<span class="fn">Linear</span>(<span class="nu">16</span>, <span class="nu">64</span>),
    nn.<span class="fn">ReLU</span>(),
    nn.<span class="fn">Linear</span>(<span class="nu">64</span>, <span class="nu">64</span>),
    nn.<span class="fn">ReLU</span>(),
    nn.<span class="fn">Linear</span>(<span class="nu">64</span>, <span class="nu">2</span>),
)

x = torch.<span class="fn">randn</span>(<span class="nu">32</span>, <span class="nu">16</span>)
output = model(x)  <span class="cm"># (32, 2)</span>

<span class="cm"># è¿™å°±æ˜¯æˆ‘ä»¬ Actor ç½‘ç»œçš„æ ¸å¿ƒç»“æ„ï¼</span>
<span class="cm"># ç­‰ä»·äºæ‰‹åŠ¨å†™ forwardï¼š</span>
<span class="cm"># x â†’ Linear(16,64) â†’ ReLU â†’ Linear(64,64) â†’ ReLU â†’ Linear(64,2)</span></code></pre>

<h3>3.2.6 nn.ModuleList â€” åŠ¨æ€å±‚åˆ—è¡¨</h3>

<pre><code><span class="cm"># ModuleList è®© PyTorch çŸ¥é“è¿™äº›å±‚å±äºæ¨¡å‹</span>
<span class="cm"># å¿…é¡»ç”¨ ModuleList è€Œä¸æ˜¯æ™®é€š listï¼</span>

<span class="kw">class</span> <span class="cl">DynamicNet</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, n_layers, hidden_dim):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        self.layers = nn.<span class="fn">ModuleList</span>([
            nn.<span class="fn">Linear</span>(hidden_dim, hidden_dim)
            <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layers)
        ])
    
    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        <span class="kw">for</span> layer <span class="kw">in</span> self.layers:
            x = F.<span class="fn">relu</span>(layer(x))
        <span class="kw">return</span> x

<span class="cm"># åœ¨ DynamicalGNN ä¸­çš„å®é™…ä½¿ç”¨ï¼š</span>
<span class="cm"># self.mp_layers = nn.ModuleList([</span>
<span class="cm">#     PhysicsMessagePassing(hidden_dim, edge_hidden_dim)</span>
<span class="cm">#     for _ in range(n_message_passing)  # 3 å±‚æ¶ˆæ¯ä¼ é€’</span>
<span class="cm"># ])</span></code></pre>

<div class="box warn">
  <div class="box-title">ModuleList vs æ™®é€š list</div>
  <p>å¦‚æœç”¨æ™®é€š Python list å­˜å‚¨å±‚ï¼Œ<code>model.parameters()</code> ä¸ä¼šåŒ…å«è¿™äº›å±‚çš„å‚æ•°ï¼æ¨¡å‹æ— æ³•è¢«æ­£ç¡®ä¼˜åŒ–ã€‚<strong>å¿…é¡»ç”¨ <code>nn.ModuleList</code>ã€‚</strong></p>
</div>

<!-- ==================== 3.3 å‚æ•°ç®¡ç† ==================== -->
<h2 id="s3-3">3.3 å‚æ•°ç®¡ç†</h2>

<pre><code>model = nn.<span class="fn">Sequential</span>(
    nn.<span class="fn">Linear</span>(<span class="nu">16</span>, <span class="nu">64</span>),
    nn.<span class="fn">ReLU</span>(),
    nn.<span class="fn">Linear</span>(<span class="nu">64</span>, <span class="nu">2</span>),
)

<span class="cm"># parameters() â€” æ‰€æœ‰å‚æ•°çš„è¿­ä»£å™¨</span>
total = <span class="fn">sum</span>(p.<span class="fn">numel</span>() <span class="kw">for</span> p <span class="kw">in</span> model.<span class="fn">parameters</span>())
<span class="fn">print</span>(<span class="st">f"æ€»å‚æ•°é‡: {total}"</span>)  <span class="cm"># 1088 + 130 = 1218</span>

<span class="cm"># named_parameters() â€” å¸¦åå­—çš„å‚æ•°è¿­ä»£å™¨</span>
<span class="kw">for</span> name, param <span class="kw">in</span> model.<span class="fn">named_parameters</span>():
    <span class="fn">print</span>(<span class="st">f"{name:20s} shape={str(param.shape):15s} numel={param.numel()}"</span>)
<span class="cm"># 0.weight              shape=torch.Size([64, 16]) numel=1024</span>
<span class="cm"># 0.bias                shape=torch.Size([64])     numel=64</span>
<span class="cm"># 2.weight              shape=torch.Size([2, 64])  numel=128</span>
<span class="cm"># 2.bias                shape=torch.Size([2])      numel=2</span>

<span class="cm"># state_dict() â€” æ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆå‚æ•°å â†’ tensorï¼‰</span>
state = model.<span class="fn">state_dict</span>()
<span class="fn">print</span>(state.keys())
<span class="cm"># odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])</span></code></pre>

<!-- ==================== 3.4 æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ ==================== -->
<h2 id="s3-4">3.4 æ¨¡å‹ä¿å­˜ä¸åŠ è½½</h2>

<pre><code><span class="cm"># ===== æ–¹æ³• 1ï¼šä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼ˆä¸æ¨èï¼‰=====</span>
torch.<span class="fn">save</span>(model, <span class="st">'model.pth'</span>)
model = torch.<span class="fn">load</span>(<span class="st">'model.pth'</span>)

<span class="cm"># ===== æ–¹æ³• 2ï¼šåªä¿å­˜å‚æ•°ï¼ˆæ¨èï¼ï¼‰=====</span>
<span class="cm"># ä¿å­˜</span>
torch.<span class="fn">save</span>(model.<span class="fn">state_dict</span>(), <span class="st">'model_weights.pth'</span>)

<span class="cm"># åŠ è½½</span>
model = <span class="fn">SimpleNet</span>(<span class="nu">16</span>, <span class="nu">64</span>, <span class="nu">2</span>)  <span class="cm"># å…ˆåˆ›å»ºç›¸åŒç»“æ„çš„æ¨¡å‹</span>
model.<span class="fn">load_state_dict</span>(torch.<span class="fn">load</span>(<span class="st">'model_weights.pth'</span>))

<span class="cm"># ===== æ–¹æ³• 3ï¼šä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼ˆæ¢å¤è®­ç»ƒï¼‰=====</span>
checkpoint = {
    <span class="st">'epoch'</span>: epoch,
    <span class="st">'model_state'</span>: model.<span class="fn">state_dict</span>(),
    <span class="st">'optimizer_state'</span>: optimizer.<span class="fn">state_dict</span>(),
    <span class="st">'loss'</span>: loss,
}
torch.<span class="fn">save</span>(checkpoint, <span class="st">'checkpoint.pth'</span>)

<span class="cm"># æ¢å¤</span>
ckpt = torch.<span class="fn">load</span>(<span class="st">'checkpoint.pth'</span>)
model.<span class="fn">load_state_dict</span>(ckpt[<span class="st">'model_state'</span>])
optimizer.<span class="fn">load_state_dict</span>(ckpt[<span class="st">'optimizer_state'</span>])

<span class="cm"># SB3 çš„æ¨¡å‹ä¿å­˜ï¼ˆæˆ‘ä»¬é¡¹ç›®ä¸­ç”¨çš„æ–¹å¼ï¼‰ï¼š</span>
<span class="cm"># model.save('./models/ppo/ppo_model')  â† ä¿å­˜</span>
<span class="cm"># model = PPO.load('./models/ppo/ppo_model')  â† åŠ è½½</span></code></pre>

<!-- ================================================== -->
<!--             ç¬¬å››ç« ï¼šæ ¸å¿ƒæ•°å­¦å…¬å¼                       -->
<!-- ================================================== -->
<h1 id="ch4">ç¬¬å››ç«  &nbsp; æ ¸å¿ƒæ•°å­¦å…¬å¼ï¼ˆå¿…é¡»æŒæ¡ï¼‰</h1>

<p>è¿™ä¸€ç« æ˜¯æ•´ä¸ªæ•™ç¨‹çš„<strong>æ•°å­¦æ ¸å¿ƒ</strong>ã€‚æŒæ¡è¿™äº›å…¬å¼ï¼Œä½ å°±èƒ½ç†è§£ PyTorch å†…éƒ¨åœ¨åšä»€ä¹ˆã€‚</p>

<!-- ==================== 4.1 çº¿æ€§å˜æ¢ ==================== -->
<h2 id="s4-1">4.1 çº¿æ€§å˜æ¢</h2>

<div class="box thm">
  <div class="box-title">çº¿æ€§å˜æ¢å…¬å¼</div>
  <p>$$\mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b}$$</p>
  <p>å…¶ä¸­ï¼š</p>
  <ul>
    <li>\(\mathbf{x} \in \mathbb{R}^n\) â€” è¾“å…¥å‘é‡</li>
    <li>\(\mathbf{W} \in \mathbb{R}^{m \times n}\) â€” æƒé‡çŸ©é˜µ</li>
    <li>\(\mathbf{b} \in \mathbb{R}^m\) â€” åç½®å‘é‡</li>
    <li>\(\mathbf{y} \in \mathbb{R}^m\) â€” è¾“å‡ºå‘é‡</li>
  </ul>
</div>

<div class="box thm">
  <div class="box-title">çº¿æ€§å±‚çš„æ¢¯åº¦</div>
  <p>è®¾æŸå¤±å‡½æ•°ä¸º \(L\)ï¼Œä¸Šæ¸¸æ¢¯åº¦ä¸º \(\boldsymbol{\delta} = \frac{\partial L}{\partial \mathbf{y}}\)ï¼Œåˆ™ï¼š</p>
  <p>$$\frac{\partial L}{\partial \mathbf{W}} = \boldsymbol{\delta} \cdot \mathbf{x}^T \qquad \text{(æƒé‡æ¢¯åº¦)}$$</p>
  <p>$$\frac{\partial L}{\partial \mathbf{b}} = \boldsymbol{\delta} \qquad \text{(åç½®æ¢¯åº¦)}$$</p>
  <p>$$\frac{\partial L}{\partial \mathbf{x}} = \mathbf{W}^T \cdot \boldsymbol{\delta} \qquad \text{(ä¼ åˆ°ä¸‹ä¸€å±‚çš„æ¢¯åº¦)}$$</p>
</div>

<div class="box example">
  <div class="box-title">å…·ä½“æ•°å€¼éªŒè¯</div>
<pre><code><span class="cm"># éªŒè¯çº¿æ€§å±‚æ¢¯åº¦å…¬å¼</span>
x = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>], requires_grad=<span class="kw">True</span>)
W = torch.<span class="fn">tensor</span>([[<span class="nu">3.0</span>, <span class="nu">4.0</span>], [<span class="nu">5.0</span>, <span class="nu">6.0</span>]], requires_grad=<span class="kw">True</span>)
b = torch.<span class="fn">tensor</span>([<span class="nu">0.1</span>, <span class="nu">0.2</span>], requires_grad=<span class="kw">True</span>)

y = W <span class="op">@</span> x + b       <span class="cm"># y = [3+8+0.1, 5+12+0.2] = [11.1, 17.2]</span>
loss = y.<span class="fn">sum</span>()        <span class="cm"># Î´ = [1, 1]ï¼ˆå› ä¸º sum çš„å¯¼æ•°æ˜¯å…¨1ï¼‰</span>
loss.<span class="fn">backward</span>()

<span class="cm"># éªŒè¯ âˆ‚L/âˆ‚W = Î´ Â· x^T</span>
delta = torch.<span class="fn">ones</span>(<span class="nu">2</span>)
expected_W_grad = delta.<span class="fn">unsqueeze</span>(<span class="nu">1</span>) <span class="op">@</span> x.<span class="fn">unsqueeze</span>(<span class="nu">0</span>)
<span class="fn">print</span>(torch.<span class="fn">allclose</span>(W.grad, expected_W_grad))  <span class="cm"># True âœ“</span>

<span class="cm"># éªŒè¯ âˆ‚L/âˆ‚b = Î´</span>
<span class="fn">print</span>(torch.<span class="fn">allclose</span>(b.grad, delta))  <span class="cm"># True âœ“</span>

<span class="cm"># éªŒè¯ âˆ‚L/âˆ‚x = W^T Â· Î´</span>
expected_x_grad = W.T <span class="op">@</span> delta
<span class="fn">print</span>(torch.<span class="fn">allclose</span>(x.grad, expected_x_grad))  <span class="cm"># True âœ“</span></code></pre>
</div>

<!-- ==================== 4.2 æ¿€æ´»å‡½æ•° ==================== -->
<h2 id="s4-2">4.2 æ¿€æ´»å‡½æ•°</h2>

<h3>4.2.1 ReLU</h3>

<div class="box thm">
  <div class="box-title">ReLU (Rectified Linear Unit)</div>
  <p>$$f(x) = \max(0, x) = \begin{cases} x & x > 0 \\ 0 & x \leq 0 \end{cases}$$</p>
  <p>å¯¼æ•°ï¼š</p>
  <p>$$f'(x) = \begin{cases} 1 & x > 0 \\ 0 & x < 0 \end{cases}$$</p>
  <p>ä¼˜ç‚¹ï¼šè®¡ç®—å¿«ã€ä¸ä¼šæ¢¯åº¦æ¶ˆå¤±ï¼ˆx>0æ—¶ï¼‰ã€‚ç¼ºç‚¹ï¼š"æ­»ç¥ç»å…ƒ"é—®é¢˜ï¼ˆx<0æ—¶æ¢¯åº¦æ°¸è¿œä¸º0ï¼‰ã€‚</p>
</div>

<h3>4.2.2 Tanh</h3>

<div class="box thm">
  <div class="box-title">Tanhï¼ˆåŒæ›²æ­£åˆ‡ï¼‰</div>
  <p>$$f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$</p>
  <p>å¯¼æ•°ï¼š</p>
  <p>$$f'(x) = 1 - \tanh^2(x) = 1 - f(x)^2$$</p>
  <p>è¾“å‡ºèŒƒå›´ \((-1, 1)\)ã€‚åœ¨ Actor ç½‘ç»œçš„è¾“å‡ºå±‚ç”¨äºå°†åŠ¨ä½œé™åˆ¶åœ¨å½’ä¸€åŒ–èŒƒå›´å†…ã€‚</p>
</div>

<h3>4.2.3 Sigmoid</h3>

<div class="box thm">
  <div class="box-title">Sigmoid</div>
  <p>$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</p>
  <p>å¯¼æ•°ï¼š</p>
  <p>$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$</p>
  <p>è¾“å‡ºèŒƒå›´ \((0, 1)\)ã€‚å¸¸ç”¨äºäºŒåˆ†ç±»æ¦‚ç‡è¾“å‡ºã€‚</p>
</div>

<h3>4.2.4 Softmax</h3>

<div class="box thm">
  <div class="box-title">Softmax â€” æ¦‚ç‡åˆ†å¸ƒè¾“å‡º</div>
  <p>$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}$$</p>
  <p>å°† K ä¸ªä»»æ„å®æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰è¾“å‡ºéè´Ÿä¸”å’Œä¸º1ï¼‰ã€‚ç”¨äºç¦»æ•£åŠ¨ä½œç©ºé—´çš„ç­–ç•¥ç½‘ç»œã€‚</p>
</div>

<pre><code><span class="cm"># Softmax ç¤ºä¾‹</span>
logits = torch.<span class="fn">tensor</span>([<span class="nu">2.0</span>, <span class="nu">1.0</span>, <span class="nu">0.1</span>])
probs = F.<span class="fn">softmax</span>(logits, dim=<span class="nu">0</span>)
<span class="fn">print</span>(probs)       <span class="cm"># tensor([0.659, 0.242, 0.099])</span>
<span class="fn">print</span>(probs.<span class="fn">sum</span>()) <span class="cm"># tensor(1.) â† å’Œä¸º1</span></code></pre>

<!-- ==================== 4.3 æŸå¤±å‡½æ•° ==================== -->
<h2 id="s4-3">4.3 æŸå¤±å‡½æ•°</h2>

<h3>4.3.1 MSE Loss â€” å‡æ–¹è¯¯å·®</h3>

<div class="box thm">
  <div class="box-title">MSE Loss (Mean Squared Error)</div>
  <p>$$L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$</p>
  <p>æ¢¯åº¦ï¼š\(\frac{\partial L}{\partial \hat{y}_i} = \frac{2}{n}(\hat{y}_i - y_i)\)</p>
  <p>ç”¨é€”ï¼šCritic ç½‘ç»œçš„ä»·å€¼é¢„æµ‹æŸå¤±ã€‚</p>
</div>

<pre><code><span class="cm"># MSE Loss</span>
criterion = nn.<span class="fn">MSELoss</span>()
predicted = torch.<span class="fn">tensor</span>([<span class="nu">2.5</span>, <span class="nu">0.0</span>, <span class="nu">2.1</span>])
target = torch.<span class="fn">tensor</span>([<span class="nu">3.0</span>, <span class="nu">-0.5</span>, <span class="nu">2.0</span>])

loss = criterion(predicted, target)
<span class="fn">print</span>(loss)  <span class="cm"># tensor(0.0867) = ((0.5)Â² + (0.5)Â² + (0.1)Â²) / 3</span>

<span class="cm"># åœ¨ PPO ä¸­ç”¨äºä»·å€¼å‡½æ•°æŸå¤±ï¼š</span>
<span class="cm"># value_loss = F.mse_loss(predicted_values, returns)</span></code></pre>

<h3>4.3.2 Cross-Entropy Loss â€” äº¤å‰ç†µ</h3>

<div class="box thm">
  <div class="box-title">Cross-Entropy Loss</div>
  <p>$$L = -\sum_{c=1}^{C} y_c \log(\hat{y}_c)$$</p>
  <p>å¯¹äºå•æ ‡ç­¾åˆ†ç±»ï¼ˆone-hot yï¼‰ï¼Œç®€åŒ–ä¸ºï¼š</p>
  <p>$$L = -\log(\hat{y}_{c^*})$$</p>
  <p>å…¶ä¸­ \(c^*\) æ˜¯çœŸå®ç±»åˆ«ã€‚ç”¨äºç¦»æ•£åŠ¨ä½œç©ºé—´çš„åˆ†ç±»ã€‚</p>
</div>

<pre><code><span class="cm"># Cross-Entropyï¼ˆè¾“å…¥æ˜¯ logitsï¼Œä¸æ˜¯ probabilitiesï¼ï¼‰</span>
criterion = nn.<span class="fn">CrossEntropyLoss</span>()
logits = torch.<span class="fn">tensor</span>([[<span class="nu">2.0</span>, <span class="nu">1.0</span>, <span class="nu">0.1</span>]])  <span class="cm"># (1, 3) ä¸‰ä¸ªç±»åˆ«çš„ logit</span>
target = torch.<span class="fn">tensor</span>([<span class="nu">0</span>])                    <span class="cm"># çœŸå®ç±»åˆ«æ˜¯ 0</span>

loss = criterion(logits, target)
<span class="cm"># = -log(softmax(2.0)) = -log(0.659) â‰ˆ 0.417</span></code></pre>

<h3>4.3.3 Huber Loss</h3>

<div class="box thm">
  <div class="box-title">Huber Loss â€” å¹³æ»‘ç‰ˆ L1</div>
  <p>$$L_\delta(a) = \begin{cases} \frac{1}{2}a^2 & |a| \leq \delta \\ \delta(|a| - \frac{1}{2}\delta) & |a| > \delta \end{cases}$$</p>
  <p>å½“è¯¯å·®å°æ—¶åƒ MSEï¼ˆå¹³æ»‘ï¼‰ï¼Œè¯¯å·®å¤§æ—¶åƒ L1ï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰ã€‚</p>
</div>

<pre><code>criterion = nn.<span class="fn">HuberLoss</span>(delta=<span class="nu">1.0</span>)
loss = criterion(predicted, target)</code></pre>

<h3>4.3.4 PPO Clip Loss</h3>

<div class="box thm">
  <div class="box-title">PPO-Clip æŸå¤±å‡½æ•°</div>
  <p>$$L^{CLIP}(\theta) = -\mathbb{E}_t\left[\min\left(r_t(\theta) \hat{A}_t,\ \text{clip}(r_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}_t\right)\right]$$</p>
  <p>å…¶ä¸­ï¼š</p>
  <ul>
    <li>\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\) â€” æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”</li>
    <li>\(\hat{A}_t\) â€” ä¼˜åŠ¿å‡½æ•°ä¼°è®¡ï¼ˆGAEï¼‰</li>
    <li>\(\varepsilon = 0.2\) â€” è£å‰ªèŒƒå›´</li>
  </ul>
  <p>è¯¦ç»†æ¨å¯¼è§ <a href="../PPO/index.html">PPO æ•™ç¨‹</a>ã€‚</p>
</div>

<pre><code><span class="cm"># PPO Clip Loss å®ç°</span>
<span class="kw">def</span> <span class="fn">ppo_clip_loss</span>(new_log_probs, old_log_probs, advantages, clip_eps=<span class="nu">0.2</span>):
    <span class="st">"""
    Args:
        new_log_probs: (batch,) æ–°ç­–ç•¥çš„ log Ï€(a|s)
        old_log_probs: (batch,) æ—§ç­–ç•¥çš„ log Ï€(a|s)  [detached]
        advantages:    (batch,) ä¼˜åŠ¿å‡½æ•° A(s,a)
        clip_eps:      è£å‰ªèŒƒå›´ Îµ
    """</span>
    <span class="cm"># æ¦‚ç‡æ¯” r = Ï€_new / Ï€_old = exp(log_new - log_old)</span>
    ratio = torch.<span class="fn">exp</span>(new_log_probs - old_log_probs)  <span class="cm"># (batch,)</span>
    
    <span class="cm"># è£å‰ªåçš„æ¯”ç‡</span>
    clipped_ratio = torch.<span class="fn">clamp</span>(ratio, <span class="nu">1</span> - clip_eps, <span class="nu">1</span> + clip_eps)
    
    <span class="cm"># å–ä¸¤è€…çš„è¾ƒå°å€¼ï¼ˆæ‚²è§‚ä¼°è®¡ï¼‰</span>
    loss = -torch.<span class="fn">min</span>(
        ratio * advantages,
        clipped_ratio * advantages
    ).<span class="fn">mean</span>()
    
    <span class="kw">return</span> loss</code></pre>

<!-- ==================== 4.4 ä¼˜åŒ–å™¨ ==================== -->
<h2 id="s4-4">4.4 ä¼˜åŒ–å™¨</h2>

<h3>4.4.1 SGD â€” éšæœºæ¢¯åº¦ä¸‹é™</h3>

<div class="box thm">
  <div class="box-title">SGD æ›´æ–°è§„åˆ™</div>
  <p>$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta L(\theta_t)$$</p>
  <p>å…¶ä¸­ \(\eta\) æ˜¯å­¦ä¹ ç‡ï¼Œ\(\nabla_\theta L\) æ˜¯æŸå¤±å¯¹å‚æ•°çš„æ¢¯åº¦ã€‚</p>
</div>

<pre><code>optimizer = torch.optim.<span class="fn">SGD</span>(model.<span class="fn">parameters</span>(), lr=<span class="nu">0.01</span>, momentum=<span class="nu">0.9</span>)</code></pre>

<h3>4.4.2 Adam â€” è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆæˆ‘ä»¬ç”¨çš„ï¼‰</h3>

<div class="box thm">
  <div class="box-title">Adam ä¼˜åŒ–å™¨</div>
  <p>Adam ç»´æŠ¤ä¸€é˜¶çŸ© (å‡å€¼) å’ŒäºŒé˜¶çŸ© (æ–¹å·®) çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼š</p>
  <p>$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \qquad \text{(ä¸€é˜¶çŸ©ä¼°è®¡)}$$</p>
  <p>$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \qquad \text{(äºŒé˜¶çŸ©ä¼°è®¡)}$$</p>
  <p>åå·®ä¿®æ­£ï¼š</p>
  <p>$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$</p>
  <p>å‚æ•°æ›´æ–°ï¼š</p>
  <p>$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$</p>
  <p>é»˜è®¤ï¼š\(\beta_1 = 0.9\), \(\beta_2 = 0.999\), \(\epsilon = 10^{-8}\)</p>
</div>

<pre><code><span class="cm"># Adam â€” æˆ‘ä»¬ PPO è®­ç»ƒä½¿ç”¨çš„ä¼˜åŒ–å™¨</span>
optimizer = torch.optim.<span class="fn">Adam</span>(
    model.<span class="fn">parameters</span>(),
    lr=<span class="nu">3e-4</span>,           <span class="cm"># PPO æ ‡å‡†å­¦ä¹ ç‡</span>
    betas=(<span class="nu">0.9</span>, <span class="nu">0.999</span>),  <span class="cm"># çŸ©ä¼°è®¡çš„è¡°å‡ç‡</span>
    eps=<span class="nu">1e-8</span>,           <span class="cm"># æ•°å€¼ç¨³å®šæ€§</span>
    weight_decay=<span class="nu">0</span>,     <span class="cm"># L2 æ­£åˆ™åŒ–ï¼ˆPPO é€šå¸¸ä¸ç”¨ï¼‰</span>
)

<span class="cm"># ä¸ºä»€ä¹ˆ Adam æ¯” SGD å¥½ï¼Ÿ</span>
<span class="cm"># 1. è‡ªé€‚åº”å­¦ä¹ ç‡ï¼šæ¯ä¸ªå‚æ•°æœ‰è‡ªå·±çš„å­¦ä¹ ç‡</span>
<span class="cm"># 2. åŠ¨é‡ï¼šåˆ©ç”¨å†å²æ¢¯åº¦å¹³æ»‘æ›´æ–°</span>
<span class="cm"># 3. ä¸éœ€è¦ä»”ç»†è°ƒ learning rate</span></code></pre>

<h3>4.4.3 å­¦ä¹ ç‡è°ƒåº¦</h3>

<pre><code><span class="kw">from</span> torch.optim.lr_scheduler <span class="kw">import</span> StepLR, CosineAnnealingLR, LambdaLR

<span class="cm"># StepLR: æ¯ step_size ä¸ª epochï¼Œlr *= gamma</span>
scheduler = <span class="fn">StepLR</span>(optimizer, step_size=<span class="nu">30</span>, gamma=<span class="nu">0.1</span>)

<span class="cm"># CosineAnnealing: ä½™å¼¦é€€ç«ï¼ˆå¹³æ»‘è¡°å‡ï¼‰</span>
scheduler = <span class="fn">CosineAnnealingLR</span>(optimizer, T_max=<span class="nu">100</span>, eta_min=<span class="nu">1e-6</span>)

<span class="cm"># Linear warmup + decayï¼ˆPPO å¸¸ç”¨æ¨¡å¼ï¼‰</span>
<span class="kw">def</span> <span class="fn">lr_lambda</span>(step):
    warmup_steps = <span class="nu">1000</span>
    total_steps = <span class="nu">200000</span>
    <span class="kw">if</span> step &lt; warmup_steps:
        <span class="kw">return</span> step / warmup_steps  <span class="cm"># çº¿æ€§å‡æ¸©</span>
    <span class="kw">return</span> max(<span class="nu">0.0</span>, <span class="nu">1.0</span> - (step - warmup_steps) / (total_steps - warmup_steps))

scheduler = <span class="fn">LambdaLR</span>(optimizer, lr_lambda=lr_lambda)

<span class="cm"># è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨</span>
<span class="kw">for</span> epoch <span class="kw">in</span> <span class="fn">range</span>(num_epochs):
    train_one_epoch()
    scheduler.<span class="fn">step</span>()  <span class="cm"># æ¯ä¸ª epoch æ›´æ–°å­¦ä¹ ç‡</span></code></pre>

<div class="box def">
  <div class="box-title">SGD vs Adam vs AdamW å¯¹æ¯”</div>
  <table>
    <tr><th>ä¼˜åŒ–å™¨</th><th>å­¦ä¹ ç‡</th><th>åŠ¨é‡</th><th>è‡ªé€‚åº”</th><th>é€‚ç”¨åœºæ™¯</th></tr>
    <tr><td>SGD</td><td>å›ºå®š</td><td>å¯é€‰</td><td>å¦</td><td>CVå¤§æ¨¡å‹å¾®è°ƒ</td></tr>
    <tr><td>Adam</td><td>3e-4</td><td>å†…ç½®</td><td>æ˜¯</td><td>RLã€NLP</td></tr>
    <tr><td>AdamW</td><td>3e-4</td><td>å†…ç½®</td><td>æ˜¯</td><td>Transformer</td></tr>
  </table>
</div>

<!-- ==================== 4.5 åå‘ä¼ æ’­ ==================== -->
<h2 id="s4-5">4.5 åå‘ä¼ æ’­</h2>

<h3>4.5.1 å®Œæ•´æ¨å¯¼ï¼šä»æŸå¤±åˆ°æ¯ä¸€å±‚çš„æ¢¯åº¦</h3>

<p>è€ƒè™‘ä¸€ä¸ªä¸¤å±‚ç½‘ç»œï¼š</p>

<p>$$\mathbf{z}_1 = \mathbf{W}_1\mathbf{x} + \mathbf{b}_1 \quad \rightarrow \quad \mathbf{a}_1 = \text{ReLU}(\mathbf{z}_1) \quad \rightarrow \quad \mathbf{z}_2 = \mathbf{W}_2\mathbf{a}_1 + \mathbf{b}_2 \quad \rightarrow \quad L = \frac{1}{2}\|\mathbf{z}_2 - \mathbf{y}\|^2$$</p>

<div class="box thm">
  <div class="box-title">åå‘ä¼ æ’­æ¨å¯¼</div>
  <p><strong>ç¬¬ä¸€æ­¥ï¼š</strong>è¾“å‡ºå±‚æ¢¯åº¦</p>
  <p>$$\boldsymbol{\delta}_2 = \frac{\partial L}{\partial \mathbf{z}_2} = \mathbf{z}_2 - \mathbf{y}$$</p>
  <p><strong>ç¬¬äºŒæ­¥ï¼š</strong>ç¬¬äºŒå±‚å‚æ•°æ¢¯åº¦</p>
  <p>$$\frac{\partial L}{\partial \mathbf{W}_2} = \boldsymbol{\delta}_2 \cdot \mathbf{a}_1^T, \quad \frac{\partial L}{\partial \mathbf{b}_2} = \boldsymbol{\delta}_2$$</p>
  <p><strong>ç¬¬ä¸‰æ­¥ï¼š</strong>ä¼ æ’­åˆ°éšè—å±‚ï¼ˆé“¾å¼æ³•åˆ™ï¼‰</p>
  <p>$$\boldsymbol{\delta}_1 = (\mathbf{W}_2^T \boldsymbol{\delta}_2) \odot \text{ReLU}'(\mathbf{z}_1)$$</p>
  <p>å…¶ä¸­ \(\odot\) æ˜¯é€å…ƒç´ ä¹˜æ³•ã€‚</p>
  <p><strong>ç¬¬å››æ­¥ï¼š</strong>ç¬¬ä¸€å±‚å‚æ•°æ¢¯åº¦</p>
  <p>$$\frac{\partial L}{\partial \mathbf{W}_1} = \boldsymbol{\delta}_1 \cdot \mathbf{x}^T, \quad \frac{\partial L}{\partial \mathbf{b}_1} = \boldsymbol{\delta}_1$$</p>
</div>

<pre><code><span class="cm"># æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼ˆéªŒè¯ autograd çš„æ­£ç¡®æ€§ï¼‰</span>
torch.<span class="fn">manual_seed</span>(<span class="nu">42</span>)
x = torch.<span class="fn">randn</span>(<span class="nu">2</span>)
y_true = torch.<span class="fn">randn</span>(<span class="nu">1</span>)

W1 = torch.<span class="fn">randn</span>(<span class="nu">3</span>, <span class="nu">2</span>, requires_grad=<span class="kw">True</span>)
b1 = torch.<span class="fn">randn</span>(<span class="nu">3</span>, requires_grad=<span class="kw">True</span>)
W2 = torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">3</span>, requires_grad=<span class="kw">True</span>)
b2 = torch.<span class="fn">randn</span>(<span class="nu">1</span>, requires_grad=<span class="kw">True</span>)

<span class="cm"># å‰å‘ä¼ æ’­</span>
z1 = W1 <span class="op">@</span> x + b1
a1 = torch.<span class="fn">relu</span>(z1)
z2 = W2 <span class="op">@</span> a1 + b2
loss = <span class="nu">0.5</span> * (z2 - y_true).<span class="fn">pow</span>(<span class="nu">2</span>).<span class="fn">sum</span>()

<span class="cm"># autograd åå‘ä¼ æ’­</span>
loss.<span class="fn">backward</span>()

<span class="cm"># æ‰‹åŠ¨åå‘ä¼ æ’­</span>
<span class="kw">with</span> torch.<span class="fn">no_grad</span>():
    delta2 = z2.<span class="fn">detach</span>() - y_true
    dW2_manual = delta2.<span class="fn">unsqueeze</span>(<span class="nu">1</span>) <span class="op">@</span> a1.<span class="fn">detach</span>().<span class="fn">unsqueeze</span>(<span class="nu">0</span>)
    db2_manual = delta2
    
    delta1 = (W2.<span class="fn">detach</span>().T <span class="op">@</span> delta2) * (z1.<span class="fn">detach</span>() > <span class="nu">0</span>).<span class="fn">float</span>()
    dW1_manual = delta1.<span class="fn">unsqueeze</span>(<span class="nu">1</span>) <span class="op">@</span> x.<span class="fn">unsqueeze</span>(<span class="nu">0</span>)
    db1_manual = delta1

<span class="cm"># å¯¹æ¯”</span>
<span class="fn">print</span>(<span class="st">"W2 æ¢¯åº¦ä¸€è‡´:"</span>, torch.<span class="fn">allclose</span>(W2.grad, dW2_manual, atol=<span class="nu">1e-6</span>))
<span class="fn">print</span>(<span class="st">"W1 æ¢¯åº¦ä¸€è‡´:"</span>, torch.<span class="fn">allclose</span>(W1.grad, dW1_manual, atol=<span class="nu">1e-6</span>))</code></pre>

<h3>4.5.2 æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸</h3>

<div class="box warn">
  <div class="box-title">æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradient)</div>
  <p><strong>åŸå› </strong>ï¼šå½“ä½¿ç”¨ Sigmoid/Tanh æ¿€æ´»æ—¶ï¼Œå¯¼æ•°æœ€å¤§å€¼åˆ†åˆ«ä¸º 0.25 å’Œ 1ã€‚ç»è¿‡å¤šå±‚è¿ä¹˜åï¼Œæ¢¯åº¦æŒ‡æ•°è¡°å‡ã€‚</p>
  <p>ä¾‹å¦‚ 10 å±‚ Sigmoid ç½‘ç»œï¼š\(0.25^{10} = 9.5 \times 10^{-7}\) â€” æ¢¯åº¦å‡ ä¹ä¸ºé›¶ã€‚</p>
  <p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
  <ul>
    <li>ä½¿ç”¨ ReLU æ¿€æ´»ï¼ˆx > 0 æ—¶æ¢¯åº¦ = 1ï¼‰</li>
    <li>ä½¿ç”¨ LayerNorm / BatchNorm</li>
    <li>ä½¿ç”¨æ®‹å·®è¿æ¥ (ResNet)</li>
    <li>åˆç†çš„æƒé‡åˆå§‹åŒ– (Xavier, Kaiming)</li>
  </ul>
</div>

<div class="box warn">
  <div class="box-title">æ¢¯åº¦çˆ†ç‚¸ (Exploding Gradient)</div>
  <p><strong>åŸå› </strong>ï¼šæƒé‡çŸ©é˜µçš„æœ€å¤§å¥‡å¼‚å€¼ > 1 æ—¶ï¼Œå¤šå±‚è¿ä¹˜å¯¼è‡´æ¢¯åº¦æŒ‡æ•°å¢é•¿ã€‚</p>
  <p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼šæ¢¯åº¦è£å‰ª (Gradient Clipping)</p>
</div>

<h3>4.5.3 æ¢¯åº¦è£å‰ª</h3>

<pre><code><span class="cm"># æ¢¯åº¦è£å‰ª â€” é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸</span>
<span class="cm"># åœ¨ backward() ä¹‹åã€step() ä¹‹å‰</span>

<span class="cm"># æ–¹æ³• 1: èŒƒæ•°è£å‰ªï¼ˆæ›´å¸¸ç”¨ï¼‰</span>
torch.nn.utils.<span class="fn">clip_grad_norm_</span>(
    model.<span class="fn">parameters</span>(),
    max_norm=<span class="nu">0.5</span>,     <span class="cm"># æˆ‘ä»¬ PPO é¡¹ç›®ç”¨çš„å€¼</span>
)
<span class="cm"># å¦‚æœæ‰€æœ‰å‚æ•°æ¢¯åº¦çš„ L2 èŒƒæ•° > 0.5ï¼Œç­‰æ¯”ä¾‹ç¼©å°</span>

<span class="cm"># æ–¹æ³• 2: å€¼è£å‰ª</span>
torch.nn.utils.<span class="fn">clip_grad_value_</span>(
    model.<span class="fn">parameters</span>(),
    clip_value=<span class="nu">1.0</span>,   <span class="cm"># æ¯ä¸ªæ¢¯åº¦å…ƒç´ é™åˆ¶åœ¨ [-1, 1]</span>
)

<span class="cm"># å®Œæ•´è®­ç»ƒæ­¥éª¤</span>
optimizer.<span class="fn">zero_grad</span>()
loss.<span class="fn">backward</span>()
torch.nn.utils.<span class="fn">clip_grad_norm_</span>(model.<span class="fn">parameters</span>(), <span class="nu">0.5</span>)  <span class="cm"># â† åœ¨è¿™é‡Œ</span>
optimizer.<span class="fn">step</span>()</code></pre>

<!-- ==================== 4.6 æ¦‚ç‡åˆ†å¸ƒ ==================== -->
<h2 id="s4-6">4.6 æ¦‚ç‡åˆ†å¸ƒ</h2>

<h3>4.6.1 Normal åˆ†å¸ƒ â€” è¿ç»­åŠ¨ä½œç©ºé—´</h3>

<div class="box thm">
  <div class="box-title">æ­£æ€åˆ†å¸ƒ (Gaussian)</div>
  <p>$$\pi(a|s) = \mathcal{N}(\mu_\theta(s),\ \sigma_\theta^2(s))$$</p>
  <p>æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š</p>
  <p>$$p(a) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(a - \mu)^2}{2\sigma^2}\right)$$</p>
  <p>å¯¹æ•°æ¦‚ç‡ï¼š</p>
  <p>$$\log p(a) = -\frac{(a-\mu)^2}{2\sigma^2} - \log \sigma - \frac{1}{2}\log(2\pi)$$</p>
  <p>ç†µï¼š</p>
  <p>$$H = \frac{1}{2}\log(2\pi e \sigma^2) = \frac{1}{2}(1 + \log(2\pi) + 2\log\sigma)$$</p>
</div>

<pre><code><span class="kw">from</span> torch.distributions <span class="kw">import</span> Normal

<span class="cm"># ç­–ç•¥ç½‘ç»œè¾“å‡ºå‡å€¼å’Œæ ‡å‡†å·®</span>
mu = torch.<span class="fn">tensor</span>([<span class="nu">0.5</span>, <span class="nu">-0.3</span>])    <span class="cm"># Actor çš„è¾“å‡ºï¼ˆ2ç»´åŠ¨ä½œï¼‰</span>
sigma = torch.<span class="fn">tensor</span>([<span class="nu">0.1</span>, <span class="nu">0.2</span>])  <span class="cm"># å¯å­¦ä¹ çš„æ ‡å‡†å·®</span>

<span class="cm"># åˆ›å»ºåˆ†å¸ƒ</span>
dist = <span class="fn">Normal</span>(mu, sigma)

<span class="cm"># é‡‡æ ·åŠ¨ä½œ</span>
action = dist.<span class="fn">sample</span>()              <span class="cm"># æ™®é€šé‡‡æ ·ï¼ˆä¸å¯å¾®ï¼‰</span>
action = dist.<span class="fn">rsample</span>()             <span class="cm"># é‡å‚æ•°åŒ–é‡‡æ ·ï¼ˆå¯å¾®ï¼ç”¨äºæ¢¯åº¦è®¡ç®—ï¼‰</span>

<span class="cm"># è®¡ç®— log æ¦‚ç‡</span>
log_prob = dist.<span class="fn">log_prob</span>(action)    <span class="cm"># (2,) â€” æ¯ä¸ªç»´åº¦çš„ log_prob</span>
total_log_prob = log_prob.<span class="fn">sum</span>()      <span class="cm"># æ ‡é‡ â€” è”åˆæ¦‚ç‡</span>

<span class="cm"># è®¡ç®—ç†µï¼ˆç”¨äºç†µæ­£åˆ™åŒ–ï¼‰</span>
entropy = dist.<span class="fn">entropy</span>()            <span class="cm"># (2,)</span>
total_entropy = entropy.<span class="fn">sum</span>()        <span class="cm"># æ ‡é‡</span></code></pre>

<h3>4.6.2 Categorical åˆ†å¸ƒ â€” ç¦»æ•£åŠ¨ä½œç©ºé—´</h3>

<pre><code><span class="kw">from</span> torch.distributions <span class="kw">import</span> Categorical

<span class="cm"># ç¦»æ•£åŠ¨ä½œçš„æ¦‚ç‡ï¼ˆå¦‚ä¸Šä¸‹å·¦å³4ä¸ªæ–¹å‘ï¼‰</span>
logits = torch.<span class="fn">tensor</span>([<span class="nu">2.0</span>, <span class="nu">1.0</span>, <span class="nu">0.5</span>, <span class="nu">0.1</span>])

dist = <span class="fn">Categorical</span>(logits=logits)   <span class="cm"># è‡ªåŠ¨ softmax</span>
action = dist.<span class="fn">sample</span>()               <span class="cm"># è¿”å›åŠ¨ä½œç´¢å¼• (0/1/2/3)</span>
log_prob = dist.<span class="fn">log_prob</span>(action)     <span class="cm"># è¯¥åŠ¨ä½œçš„ log æ¦‚ç‡</span>
entropy = dist.<span class="fn">entropy</span>()             <span class="cm"># ç†µ</span></code></pre>

<h3>4.6.3 é‡å‚æ•°åŒ–æŠ€å·§ (Reparameterization Trick)</h3>

<div class="box def">
  <div class="box-title">ä¸ºä»€ä¹ˆéœ€è¦é‡å‚æ•°åŒ–ï¼Ÿ</div>
  <p>ä» \(\mathcal{N}(\mu, \sigma^2)\) é‡‡æ ·çš„æ“ä½œæœ¬èº«<strong>ä¸å¯å¾®</strong>ï¼ˆå› ä¸ºéšæœºæ€§ï¼‰ã€‚ä½†æˆ‘ä»¬éœ€è¦è®¡ç®— \(\frac{\partial}{\partial \mu}\) å’Œ \(\frac{\partial}{\partial \sigma}\)ã€‚</p>
  <p>é‡å‚æ•°åŒ–æŠ€å·§æŠŠé‡‡æ ·å†™æˆç¡®å®šæ€§å‡½æ•° + å¤–éƒ¨å™ªå£°ï¼š</p>
  <p>$$a = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)$$</p>
  <p>è¿™æ ·æ¢¯åº¦å¯ä»¥é€šè¿‡ \(\mu\) å’Œ \(\sigma\) åå‘ä¼ æ’­ï¼Œ\(\epsilon\) æ˜¯å¸¸æ•°ã€‚</p>
  <p><code>dist.rsample()</code> å°±æ˜¯ç”¨è¿™ä¸ªæŠ€å·§å®ç°çš„ã€‚</p>
</div>

<pre><code><span class="cm"># rsample vs sample</span>
mu = torch.<span class="fn">tensor</span>([<span class="nu">0.0</span>], requires_grad=<span class="kw">True</span>)
sigma = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>], requires_grad=<span class="kw">True</span>)
dist = <span class="fn">Normal</span>(mu, sigma)

<span class="cm"># sample() â€” ä¸èƒ½åå‘ä¼ æ’­åˆ° mu/sigma</span>
a1 = dist.<span class="fn">sample</span>()
<span class="cm"># a1.backward()  â† ä¼šæŠ¥é”™ï¼</span>

<span class="cm"># rsample() â€” å¯ä»¥åå‘ä¼ æ’­</span>
a2 = dist.<span class="fn">rsample</span>()     <span class="cm"># a2 = mu + sigma * Îµ</span>
a2.<span class="fn">backward</span>()
<span class="fn">print</span>(mu.grad)          <span class="cm"># tensor([1.]) â† âˆ‚a/âˆ‚Î¼ = 1 âœ“</span></code></pre>

<!-- ================================================== -->
<!--              ç¬¬äº”ç« ï¼šé¡¹ç›®å®æˆ˜                         -->
<!-- ================================================== -->
<h1 id="ch5">ç¬¬äº”ç«  &nbsp; åœ¨æˆ‘ä»¬é¡¹ç›®ä¸­çš„åº”ç”¨</h1>

<p>æœ¬ç« å°† PyTorch çŸ¥è¯†åº”ç”¨åˆ°æˆ‘ä»¬çš„ PushBox ç‰©ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ é¡¹ç›®ä¸­ã€‚æ‰€æœ‰ä»£ç éƒ½æ¥è‡ªé¡¹ç›®çš„å®é™…æ–‡ä»¶ã€‚</p>

<!-- ==================== 5.1 Actor ==================== -->
<h2 id="s5-1">5.1 ç­–ç•¥ç½‘ç»œ (Actor)</h2>

<div class="flow">
  <div class="flow-box">obs<br><span class="shape">(batch, 16)</span></div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">Linear(16, 64)</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box highlight">ReLU</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">Linear(64, 64)</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box highlight">ReLU</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">Linear(64, 2)</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box highlight">Tanh</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">action<br><span class="shape">(batch, 2)</span></div>
</div>

<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">from</span> torch.distributions <span class="kw">import</span> Normal

<span class="kw">class</span> <span class="cl">ActorNetwork</span>(nn.Module):
    <span class="st">"""
    ç­–ç•¥ç½‘ç»œ (Actor)
    
    è¾“å…¥: obs (batch, 16) â€” è§‚å¯Ÿå‘é‡
        [box_pos(3), box_quat(4), box_vel(3), gripper_pos(3), goal_pos(3)]
    
    è¾“å‡º: action (batch, 2) â€” è¿ç»­åŠ¨ä½œ (force_x, force_y)
          èŒƒå›´ [-10, 10] N
    """</span>
    
    <span class="kw">def</span> <span class="fn">__init__</span>(self, obs_dim=<span class="nu">16</span>, action_dim=<span class="nu">2</span>, hidden_dim=<span class="nu">64</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        
        <span class="cm"># ========== ç½‘ç»œç»“æ„ ==========</span>
        <span class="cm"># å…±äº«ç‰¹å¾æå–å™¨</span>
        self.feature_net = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(obs_dim, hidden_dim),   <span class="cm"># 16 â†’ 64, å‚æ•°: 16*64+64 = 1088</span>
            nn.<span class="fn">ReLU</span>(),                         <span class="cm"># max(0, x)</span>
            nn.<span class="fn">Linear</span>(hidden_dim, hidden_dim), <span class="cm"># 64 â†’ 64, å‚æ•°: 64*64+64 = 4160</span>
            nn.<span class="fn">ReLU</span>(),                         <span class="cm"># max(0, x)</span>
        )
        
        <span class="cm"># å‡å€¼è¾“å‡ºå¤´</span>
        self.mean_head = nn.<span class="fn">Linear</span>(hidden_dim, action_dim)  <span class="cm"># 64 â†’ 2, å‚æ•°: 64*2+2 = 130</span>
        
        <span class="cm"># å¯å­¦ä¹ çš„ log æ ‡å‡†å·®</span>
        self.log_std = nn.<span class="fn">Parameter</span>(torch.<span class="fn">zeros</span>(action_dim))  <span class="cm"># 2 ä¸ªå‚æ•°</span>
        
        <span class="cm"># åŠ¨ä½œèŒƒå›´ç¼©æ”¾</span>
        self.action_scale = <span class="nu">10.0</span>  <span class="cm"># Tanh è¾“å‡º [-1,1] Ã— 10 = [-10, 10]</span>
    
    <span class="kw">def</span> <span class="fn">forward</span>(self, obs):
        <span class="st">"""
        å‰å‘ä¼ æ’­
        
        Args:
            obs: (batch, 16) è§‚å¯Ÿå‘é‡
            
        Returns:
            action_mean: (batch, 2) åŠ¨ä½œå‡å€¼
            action_std:  (batch, 2) åŠ¨ä½œæ ‡å‡†å·®
        """</span>
        <span class="cm"># ç‰¹å¾æå–</span>
        features = self.feature_net(obs)    <span class="cm"># (batch, 16) â†’ (batch, 64)</span>
        
        <span class="cm"># è¾“å‡ºå‡å€¼ï¼ˆTanh é™åˆ¶åˆ° [-1, 1]ï¼Œå†ä¹˜ä»¥ç¼©æ”¾å› å­ï¼‰</span>
        action_mean = torch.<span class="fn">tanh</span>(self.mean_head(features)) * self.action_scale
        <span class="cm"># (batch, 64) â†’ Linear â†’ (batch, 2) â†’ Tanh â†’ [-1,1] â†’ Ã— 10 â†’ [-10, 10]</span>
        
        <span class="cm"># æ ‡å‡†å·®ï¼ˆä» log_std å‚æ•°ä¸­è·å–ï¼Œä¿è¯ä¸ºæ­£ï¼‰</span>
        action_std = self.log_std.<span class="fn">exp</span>().<span class="fn">expand_as</span>(action_mean)
        <span class="cm"># exp(log_std) ä¿è¯ std > 0</span>
        
        <span class="kw">return</span> action_mean, action_std
    
    <span class="kw">def</span> <span class="fn">get_action</span>(self, obs):
        <span class="st">"""é‡‡æ ·åŠ¨ä½œå¹¶è¿”å› log_prob"""</span>
        mean, std = self.<span class="fn">forward</span>(obs)
        dist = <span class="fn">Normal</span>(mean, std)
        
        action = dist.<span class="fn">rsample</span>()                <span class="cm"># é‡å‚æ•°åŒ–é‡‡æ ·</span>
        log_prob = dist.<span class="fn">log_prob</span>(action).<span class="fn">sum</span>(-<span class="nu">1</span>)  <span class="cm"># (batch,)</span>
        entropy = dist.<span class="fn">entropy</span>().<span class="fn">sum</span>(-<span class="nu">1</span>)          <span class="cm"># (batch,)</span>
        
        <span class="kw">return</span> action, log_prob, entropy

<span class="cm"># ä½¿ç”¨ç¤ºä¾‹</span>
actor = <span class="fn">ActorNetwork</span>()
obs = torch.<span class="fn">randn</span>(<span class="nu">32</span>, <span class="nu">16</span>)   <span class="cm"># batch=32</span>
action, log_prob, entropy = actor.<span class="fn">get_action</span>(obs)

<span class="fn">print</span>(<span class="st">f"action shape: {action.shape}"</span>)     <span class="cm"># (32, 2)</span>
<span class="fn">print</span>(<span class="st">f"log_prob shape: {log_prob.shape}"</span>) <span class="cm"># (32,)</span>
<span class="fn">print</span>(<span class="st">f"action range: [{action.min():.1f}, {action.max():.1f}]"</span>)
<span class="fn">print</span>(<span class="st">f"æ€»å‚æ•°é‡: {sum(p.numel() for p in actor.parameters())}"</span>)  <span class="cm"># 5380</span></code></pre>

<!-- ==================== 5.2 Critic ==================== -->
<h2 id="s5-2">5.2 ä»·å€¼ç½‘ç»œ (Critic)</h2>

<div class="flow">
  <div class="flow-box">obs<br><span class="shape">(batch, 16)</span></div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">Linear(16, 64)</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box highlight">ReLU</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">Linear(64, 64)</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box highlight">ReLU</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">Linear(64, 1)</div>
  <div class="flow-arrow">â†’</div>
  <div class="flow-box">value<br><span class="shape">(batch, 1)</span></div>
</div>

<pre><code><span class="kw">class</span> <span class="cl">CriticNetwork</span>(nn.Module):
    <span class="st">"""
    ä»·å€¼ç½‘ç»œ (Critic)
    
    è¾“å…¥: obs (batch, 16)
    è¾“å‡º: value (batch, 1) â€” çŠ¶æ€ä»·å€¼ V(s) çš„ä¼°è®¡
    
    æ³¨æ„ï¼šè¾“å‡ºå±‚æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼ä»·å€¼å¯ä»¥æ˜¯ä»»æ„å®æ•°ã€‚
    """</span>
    
    <span class="kw">def</span> <span class="fn">__init__</span>(self, obs_dim=<span class="nu">16</span>, hidden_dim=<span class="nu">64</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        
        self.value_net = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(obs_dim, hidden_dim),     <span class="cm"># 16 â†’ 64</span>
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden_dim, hidden_dim),   <span class="cm"># 64 â†’ 64</span>
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden_dim, <span class="nu">1</span>),            <span class="cm"># 64 â†’ 1ï¼ˆæ— æ¿€æ´»ï¼ï¼‰</span>
        )
    
    <span class="kw">def</span> <span class="fn">forward</span>(self, obs):
        <span class="st">"""
        Args:
            obs: (batch, 16)
        Returns:
            value: (batch, 1) â€” V(s) ä¼°è®¡
        """</span>
        <span class="kw">return</span> self.value_net(obs)

<span class="cm"># ä½¿ç”¨</span>
critic = <span class="fn">CriticNetwork</span>()
value = critic(obs)    <span class="cm"># (32, 1)</span>
<span class="fn">print</span>(value.shape)     <span class="cm"># torch.Size([32, 1])</span>
<span class="fn">print</span>(<span class="st">f"å‚æ•°é‡: {sum(p.numel() for p in critic.parameters())}"</span>)  <span class="cm"># 5313</span></code></pre>

<!-- ==================== 5.3 ç‰¹å¾æå–å™¨ ==================== -->
<h2 id="s5-3">5.3 è‡ªå®šä¹‰ç‰¹å¾æå–å™¨ (BaseFeaturesExtractor)</h2>

<div class="box def">
  <div class="box-title">SB3 ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾æå–å™¨ï¼Ÿ</div>
  <p>Stable-Baselines3 çš„ MLP ç­–ç•¥é»˜è®¤ç”¨è‡ªå·±çš„å…¨è¿æ¥ç½‘ç»œå¤„ç†è§‚å¯Ÿã€‚ä½†å¦‚æœä½ æƒ³è‡ªå®šä¹‰ç½‘ç»œç»“æ„ï¼ˆæ¯”å¦‚åŠ å…¥ç‰©ç†å…ˆéªŒï¼‰ï¼Œéœ€è¦å†™ä¸€ä¸ª <code>BaseFeaturesExtractor</code>ã€‚å®ƒæ¥æ”¶åŸå§‹è§‚å¯Ÿï¼Œè¾“å‡ºå›ºå®šç»´åº¦çš„ç‰¹å¾å‘é‡ï¼Œç„¶å SB3 çš„ç­–ç•¥/ä»·å€¼å¤´æ¥åœ¨åé¢ã€‚</p>
</div>

<pre><code><span class="kw">from</span> stable_baselines3.common.torch_layers <span class="kw">import</span> BaseFeaturesExtractor
<span class="kw">import</span> gymnasium <span class="kw">as</span> gym

<span class="kw">class</span> <span class="cl">PhysRobotFeaturesExtractor</span>(BaseFeaturesExtractor):
    <span class="st">"""
    ç‰©ç†æ„ŸçŸ¥ç‰¹å¾æå–å™¨
    
    å°†åŸå§‹ obs åˆ†è§£ä¸ºç‰©ç†é‡ï¼Œåˆ†åˆ«ç¼–ç åæ‹¼æ¥ï¼š
    - ä½ç½®ç‰¹å¾: box_pos, gripper_pos, goal_pos
    - åŠ¨åŠ›å­¦ç‰¹å¾: box_vel, box_quat
    """</span>
    
    <span class="kw">def</span> <span class="fn">__init__</span>(self, observation_space: gym.spaces.Box, features_dim: <span class="bi">int</span> = <span class="nu">64</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>(observation_space, features_dim)
        
        obs_dim = observation_space.shape[<span class="nu">0</span>]  <span class="cm"># 16</span>
        
        <span class="cm"># ä½ç½®ç¼–ç å™¨ï¼šå¤„ç†ç©ºé—´ä½ç½®ä¿¡æ¯</span>
        self.pos_encoder = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(<span class="nu">9</span>, <span class="nu">32</span>),   <span class="cm"># 3 positions Ã— 3 = 9</span>
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(<span class="nu">32</span>, <span class="nu">32</span>),
            nn.<span class="fn">ReLU</span>(),
        )
        
        <span class="cm"># åŠ¨åŠ›å­¦ç¼–ç å™¨ï¼šå¤„ç†é€Ÿåº¦å’Œå§¿æ€</span>
        self.dyn_encoder = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(<span class="nu">7</span>, <span class="nu">32</span>),   <span class="cm"># quat(4) + vel(3) = 7</span>
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(<span class="nu">32</span>, <span class="nu">32</span>),
            nn.<span class="fn">ReLU</span>(),
        )
        
        <span class="cm"># èåˆå±‚</span>
        self.fusion = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(<span class="nu">64</span>, features_dim),  <span class="cm"># 32 + 32 = 64 â†’ features_dim</span>
            nn.<span class="fn">ReLU</span>(),
        )
    
    <span class="kw">def</span> <span class="fn">forward</span>(self, observations: torch.Tensor) -> torch.Tensor:
        <span class="st">"""
        Args:
            observations: (batch, 16) åŸå§‹è§‚å¯Ÿ
                [box_pos(3) | box_quat(4) | box_vel(3) | gripper_pos(3) | goal_pos(3)]
        
        Returns:
            features: (batch, features_dim) æå–çš„ç‰¹å¾
        """</span>
        <span class="cm"># åˆ†è§£è§‚å¯Ÿå‘é‡</span>
        box_pos = observations[:, <span class="nu">0</span>:<span class="nu">3</span>]      <span class="cm"># (batch, 3)</span>
        box_quat = observations[:, <span class="nu">3</span>:<span class="nu">7</span>]     <span class="cm"># (batch, 4)</span>
        box_vel = observations[:, <span class="nu">7</span>:<span class="nu">10</span>]     <span class="cm"># (batch, 3)</span>
        gripper_pos = observations[:, <span class="nu">10</span>:<span class="nu">13</span>] <span class="cm"># (batch, 3)</span>
        goal_pos = observations[:, <span class="nu">13</span>:<span class="nu">16</span>]   <span class="cm"># (batch, 3)</span>
        
        <span class="cm"># ç¼–ç ä½ç½®ç‰¹å¾</span>
        pos_features = torch.<span class="fn">cat</span>([box_pos, gripper_pos, goal_pos], dim=-<span class="nu">1</span>)  <span class="cm"># (batch, 9)</span>
        pos_encoded = self.pos_encoder(pos_features)  <span class="cm"># (batch, 32)</span>
        
        <span class="cm"># ç¼–ç åŠ¨åŠ›å­¦ç‰¹å¾</span>
        dyn_features = torch.<span class="fn">cat</span>([box_quat, box_vel], dim=-<span class="nu">1</span>)  <span class="cm"># (batch, 7)</span>
        dyn_encoded = self.dyn_encoder(dyn_features)  <span class="cm"># (batch, 32)</span>
        
        <span class="cm"># èåˆ</span>
        combined = torch.<span class="fn">cat</span>([pos_encoded, dyn_encoded], dim=-<span class="nu">1</span>)  <span class="cm"># (batch, 64)</span>
        features = self.fusion(combined)  <span class="cm"># (batch, features_dim)</span>
        
        <span class="kw">return</span> features

<span class="cm"># åœ¨ SB3 ä¸­ä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾æå–å™¨</span>
<span class="cm"># policy_kwargs = dict(</span>
<span class="cm">#     features_extractor_class=PhysRobotFeaturesExtractor,</span>
<span class="cm">#     features_extractor_kwargs=dict(features_dim=64),</span>
<span class="cm"># )</span>
<span class="cm"># model = PPO("MlpPolicy", env, policy_kwargs=policy_kwargs)</span></code></pre>

<!-- ==================== 5.4 å›¾ç½‘ç»œ ==================== -->
<h2 id="s5-4">5.4 å›¾ç½‘ç»œä¸­çš„ PyTorch</h2>

<div class="box def">
  <div class="box-title">å›¾ç¥ç»ç½‘ç»œ (GNN) åœ¨æˆ‘ä»¬é¡¹ç›®ä¸­çš„è§’è‰²</div>
  <p>æˆ‘ä»¬çš„ DynamicalGNN å°†ç‰©ç†ç³»ç»Ÿå»ºæ¨¡ä¸ºå›¾ï¼šèŠ‚ç‚¹æ˜¯ç‰©ç†å®ä½“ï¼ˆboxã€gripperï¼‰ï¼Œè¾¹è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚GNN é€šè¿‡æ¶ˆæ¯ä¼ é€’å­¦ä¹ ç‰©ç†åŠ¨åŠ›å­¦ã€‚</p>
</div>

<h3>5.4.1 torch_geometric çš„ Data å’Œ Batch</h3>

<pre><code><span class="kw">from</span> torch_geometric.data <span class="kw">import</span> Data, Batch

<span class="cm"># å•ä¸ªå›¾çš„æ•°æ®ç»“æ„</span>
graph = <span class="fn">Data</span>(
    x=torch.<span class="fn">randn</span>(<span class="nu">4</span>, <span class="nu">6</span>),          <span class="cm"># èŠ‚ç‚¹ç‰¹å¾ (4 nodes, 6 features)</span>
    edge_index=torch.<span class="fn">tensor</span>([      <span class="cm"># è¾¹è¿æ¥ (2, E)</span>
        [<span class="nu">0</span>, <span class="nu">1</span>, <span class="nu">2</span>, <span class="nu">3</span>, <span class="nu">0</span>, <span class="nu">1</span>],        <span class="cm"># æºèŠ‚ç‚¹</span>
        [<span class="nu">1</span>, <span class="nu">0</span>, <span class="nu">3</span>, <span class="nu">2</span>, <span class="nu">2</span>, <span class="nu">3</span>],        <span class="cm"># ç›®æ ‡èŠ‚ç‚¹</span>
    ]),
    edge_attr=torch.<span class="fn">randn</span>(<span class="nu">6</span>, <span class="nu">8</span>),   <span class="cm"># è¾¹ç‰¹å¾ (6 edges, 8 features)</span>
)

<span class="fn">print</span>(graph.num_nodes)    <span class="cm"># 4</span>
<span class="fn">print</span>(graph.num_edges)    <span class="cm"># 6</span>

<span class="cm"># æ‰¹å¤„ç†å¤šä¸ªå›¾</span>
graphs = [<span class="fn">Data</span>(x=torch.<span class="fn">randn</span>(n, <span class="nu">6</span>), 
               edge_index=torch.<span class="fn">randint</span>(<span class="nu">0</span>, n, (<span class="nu">2</span>, n*<span class="nu">2</span>)))
          <span class="kw">for</span> n <span class="kw">in</span> [<span class="nu">3</span>, <span class="nu">4</span>, <span class="nu">5</span>]]

batch = Batch.<span class="fn">from_data_list</span>(graphs)
<span class="fn">print</span>(batch.x.shape)        <span class="cm"># (12, 6) â€” 3+4+5=12 ä¸ªèŠ‚ç‚¹</span>
<span class="fn">print</span>(batch.batch)          <span class="cm"># tensor([0,0,0, 1,1,1,1, 2,2,2,2,2]) â€” å›¾å½’å±</span></code></pre>

<h3>5.4.2 MessagePassing åŸºç±»</h3>

<pre><code><span class="kw">from</span> torch_geometric.nn <span class="kw">import</span> MessagePassing

<span class="kw">class</span> <span class="cl">PhysicsMessagePassing</span>(MessagePassing):
    <span class="st">"""
    ç‰©ç†çº¦æŸçš„æ¶ˆæ¯ä¼ é€’
    
    æ¥è‡ªæˆ‘ä»¬çš„ dynamical_gnn.pyï¼š
    Message: m_ij = Ï†(e_ij, h_i, h_j)
    Update:  h_i' = Ïˆ(h_i, Î£_j m_ij)
    """</span>
    
    <span class="kw">def</span> <span class="fn">__init__</span>(self, hidden_dim, edge_dim):
        <span class="fn">super</span>().<span class="fn">__init__</span>(aggr=<span class="st">'add'</span>)  <span class="cm"># æ±‚å’Œèšåˆï¼ˆä¿æŒå¯åŠ æ€§ç‰©ç†é‡ï¼‰</span>
        
        <span class="cm"># æ¶ˆæ¯ç½‘ç»œ Ï†</span>
        self.message_net = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(edge_dim + <span class="nu">2</span> * hidden_dim, hidden_dim),
            nn.<span class="fn">LayerNorm</span>(hidden_dim),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden_dim, hidden_dim),
        )
        
        <span class="cm"># æ›´æ–°ç½‘ç»œ Ïˆï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰</span>
        self.update_net = nn.<span class="fn">Sequential</span>(
            nn.<span class="fn">Linear</span>(<span class="nu">2</span> * hidden_dim, hidden_dim),
            nn.<span class="fn">LayerNorm</span>(hidden_dim),
            nn.<span class="fn">ReLU</span>(),
            nn.<span class="fn">Linear</span>(hidden_dim, hidden_dim),
        )
    
    <span class="kw">def</span> <span class="fn">forward</span>(self, x, edge_index, edge_attr):
        <span class="cm"># propagate ä¼šè‡ªåŠ¨è°ƒç”¨ message â†’ aggregate â†’ update</span>
        <span class="kw">return</span> self.<span class="fn">propagate</span>(edge_index, x=x, edge_attr=edge_attr)
    
    <span class="kw">def</span> <span class="fn">message</span>(self, x_i, x_j, edge_attr):
        <span class="st">"""
        æ„é€ æ¶ˆæ¯ m_ij
        
        x_i: ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾ (E, hidden)  â€” è‡ªåŠ¨ç´¢å¼•
        x_j: æºèŠ‚ç‚¹ç‰¹å¾   (E, hidden)  â€” è‡ªåŠ¨ç´¢å¼•
        edge_attr: è¾¹ç‰¹å¾  (E, edge_dim)
        """</span>
        msg_input = torch.<span class="fn">cat</span>([edge_attr, x_i, x_j], dim=-<span class="nu">1</span>)
        <span class="kw">return</span> self.message_net(msg_input)  <span class="cm"># (E, hidden)</span>
    
    <span class="kw">def</span> <span class="fn">update</span>(self, aggr_out, x):
        <span class="st">"""
        æ›´æ–°èŠ‚ç‚¹ç‰¹å¾
        
        aggr_out: èšåˆåçš„æ¶ˆæ¯ Î£_j m_ij  (N, hidden)
        x: åŸå§‹èŠ‚ç‚¹ç‰¹å¾ (N, hidden)
        """</span>
        update_input = torch.<span class="fn">cat</span>([x, aggr_out], dim=-<span class="nu">1</span>)
        <span class="kw">return</span> self.update_net(update_input) + x  <span class="cm"># æ®‹å·®è¿æ¥ï¼</span></code></pre>

<h3>5.4.3 scatter_add èšåˆæ“ä½œ</h3>

<pre><code><span class="kw">from</span> torch_geometric.utils <span class="kw">import</span> scatter

<span class="cm"># scatter_add: å°†æ¶ˆæ¯æŒ‰ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•èšåˆ</span>
messages = torch.<span class="fn">randn</span>(<span class="nu">12</span>, <span class="nu">64</span>)    <span class="cm"># 12 æ¡æ¶ˆæ¯</span>
target_idx = torch.<span class="fn">tensor</span>([<span class="nu">0</span>,<span class="nu">0</span>,<span class="nu">0</span>, <span class="nu">1</span>,<span class="nu">1</span>, <span class="nu">2</span>,<span class="nu">2</span>,<span class="nu">2</span>,<span class="nu">2</span>, <span class="nu">3</span>,<span class="nu">3</span>,<span class="nu">3</span>])  <span class="cm"># 4 ä¸ªç›®æ ‡èŠ‚ç‚¹</span>

<span class="cm"># å°†æ¶ˆæ¯æŒ‰ç›®æ ‡èŠ‚ç‚¹æ±‚å’Œ</span>
aggregated = <span class="fn">scatter</span>(messages, target_idx, dim=<span class="nu">0</span>, reduce=<span class="st">'sum'</span>)
<span class="fn">print</span>(aggregated.shape)  <span class="cm"># (4, 64) â€” 4 ä¸ªèŠ‚ç‚¹çš„èšåˆæ¶ˆæ¯</span>

<span class="cm"># ç­‰ä»·äºï¼š</span>
<span class="cm"># aggregated[i] = sum(messages[j] for j where target_idx[j] == i)</span></code></pre>

<!-- ==================== 5.5 è®­ç»ƒå¾ªç¯ ==================== -->
<h2 id="s5-5">5.5 æ‰‹å†™è®­ç»ƒå¾ªç¯ï¼ˆå®Œæ•´ PPOï¼‰</h2>

<div class="box example">
  <div class="box-title">å®Œæ•´çš„ PPO è®­ç»ƒå¾ªç¯ï¼ˆä¸ç”¨ SB3ï¼‰</div>
  <p>ä¸‹é¢å±•ç¤ºå®Œæ•´çš„æ‰‹å†™ PPO è®­ç»ƒå¾ªç¯ï¼Œæ¯ä¸€æ­¥éƒ½æ ‡æ³¨äº† tensor shape å˜åŒ–ã€‚</p>
</div>

<pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F
<span class="kw">from</span> torch.distributions <span class="kw">import</span> Normal
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ========== è¶…å‚æ•° ==========</span>
GAMMA = <span class="nu">0.99</span>          <span class="cm"># æŠ˜æ‰£å› å­</span>
GAE_LAMBDA = <span class="nu">0.95</span>     <span class="cm"># GAE Î»</span>
CLIP_EPS = <span class="nu">0.2</span>        <span class="cm"># PPO è£å‰ªèŒƒå›´</span>
LR = <span class="nu">3e-4</span>             <span class="cm"># å­¦ä¹ ç‡</span>
N_STEPS = <span class="nu">2048</span>        <span class="cm"># æ¯æ¬¡æ”¶é›†çš„æ­¥æ•°</span>
BATCH_SIZE = <span class="nu">64</span>       <span class="cm"># mini-batch å¤§å°</span>
N_EPOCHS = <span class="nu">10</span>         <span class="cm"># æ¯æ¬¡æ›´æ–°çš„ epoch æ•°</span>
VF_COEF = <span class="nu">0.5</span>        <span class="cm"># ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°</span>
ENT_COEF = <span class="nu">0.0</span>       <span class="cm"># ç†µæ­£åˆ™åŒ–ç³»æ•°</span>
MAX_GRAD_NORM = <span class="nu">0.5</span>  <span class="cm"># æ¢¯åº¦è£å‰ª</span>

<span class="cm"># ========== åˆ›å»ºç½‘ç»œå’Œä¼˜åŒ–å™¨ ==========</span>
actor = <span class="fn">ActorNetwork</span>(obs_dim=<span class="nu">16</span>, action_dim=<span class="nu">2</span>)
critic = <span class="fn">CriticNetwork</span>(obs_dim=<span class="nu">16</span>)
optimizer = torch.optim.<span class="fn">Adam</span>(
    <span class="fn">list</span>(actor.<span class="fn">parameters</span>()) + <span class="fn">list</span>(critic.<span class="fn">parameters</span>()),
    lr=LR
)

<span class="cm"># ========== ç¬¬ä¸€æ­¥ï¼šæ•°æ®æ”¶é›† (Rollout) ==========</span>
<span class="kw">def</span> <span class="fn">collect_rollout</span>(env, actor, critic, n_steps):
    <span class="st">"""æ”¶é›† n_steps æ­¥çš„ç»éªŒæ•°æ®"""</span>
    
    <span class="cm"># å­˜å‚¨ç¼“å†²åŒº</span>
    obs_buf      = torch.<span class="fn">zeros</span>(n_steps, <span class="nu">16</span>)    <span class="cm"># (T, 16)</span>
    actions_buf  = torch.<span class="fn">zeros</span>(n_steps, <span class="nu">2</span>)     <span class="cm"># (T, 2)</span>
    rewards_buf  = torch.<span class="fn">zeros</span>(n_steps)        <span class="cm"># (T,)</span>
    dones_buf    = torch.<span class="fn">zeros</span>(n_steps)        <span class="cm"># (T,)</span>
    values_buf   = torch.<span class="fn">zeros</span>(n_steps)        <span class="cm"># (T,)</span>
    log_probs_buf = torch.<span class="fn">zeros</span>(n_steps)       <span class="cm"># (T,)</span>
    
    obs = torch.<span class="fn">tensor</span>(env.<span class="fn">reset</span>()[<span class="nu">0</span>], dtype=torch.float32)  <span class="cm"># (16,)</span>
    
    <span class="kw">with</span> torch.<span class="fn">no_grad</span>():  <span class="cm"># æ•°æ®æ”¶é›†ä¸éœ€è¦æ¢¯åº¦ï¼</span>
        <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(n_steps):
            <span class="cm"># Actor é€‰æ‹©åŠ¨ä½œ</span>
            mean, std = actor(obs.<span class="fn">unsqueeze</span>(<span class="nu">0</span>))  <span class="cm"># (1, 16) â†’ mean: (1, 2), std: (1, 2)</span>
            dist = <span class="fn">Normal</span>(mean, std)
            action = dist.<span class="fn">sample</span>()              <span class="cm"># (1, 2)</span>
            log_prob = dist.<span class="fn">log_prob</span>(action).<span class="fn">sum</span>(-<span class="nu">1</span>)  <span class="cm"># (1,)</span>
            
            <span class="cm"># Critic ä¼°è®¡ä»·å€¼</span>
            value = critic(obs.<span class="fn">unsqueeze</span>(<span class="nu">0</span>))    <span class="cm"># (1, 1)</span>
            
            <span class="cm"># ä¸ç¯å¢ƒäº¤äº’</span>
            action_np = action.<span class="fn">squeeze</span>(<span class="nu">0</span>).<span class="fn">numpy</span>()  <span class="cm"># (2,) numpy</span>
            next_obs, reward, done, truncated, info = env.<span class="fn">step</span>(action_np)
            
            <span class="cm"># å­˜å‚¨</span>
            obs_buf[t] = obs
            actions_buf[t] = action.<span class="fn">squeeze</span>(<span class="nu">0</span>)   <span class="cm"># (2,)</span>
            rewards_buf[t] = reward
            dones_buf[t] = <span class="fn">float</span>(done)
            values_buf[t] = value.<span class="fn">squeeze</span>()       <span class="cm"># æ ‡é‡</span>
            log_probs_buf[t] = log_prob.<span class="fn">squeeze</span>()  <span class="cm"># æ ‡é‡</span>
            
            obs = torch.<span class="fn">tensor</span>(next_obs, dtype=torch.float32)
            <span class="kw">if</span> done:
                obs = torch.<span class="fn">tensor</span>(env.<span class="fn">reset</span>()[<span class="nu">0</span>], dtype=torch.float32)
    
    <span class="kw">return</span> obs_buf, actions_buf, rewards_buf, dones_buf, values_buf, log_probs_buf

<span class="cm"># ========== ç¬¬äºŒæ­¥ï¼šè®¡ç®—ä¼˜åŠ¿ (GAE) ==========</span>
<span class="kw">def</span> <span class="fn">compute_gae</span>(rewards, values, dones, gamma=GAMMA, lam=GAE_LAMBDA):
    <span class="st">"""
    å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (Generalized Advantage Estimation)
    
    Args:
        rewards: (T,) æ¯æ­¥å¥–åŠ±
        values:  (T,) æ¯æ­¥ä»·å€¼ä¼°è®¡
        dones:   (T,) æ˜¯å¦ç»“æŸ
        
    Returns:
        advantages: (T,) ä¼˜åŠ¿ä¼°è®¡
        returns:    (T,) å›æŠ¥ï¼ˆç”¨äºä»·å€¼å‡½æ•°è®­ç»ƒï¼‰
    """</span>
    T = <span class="fn">len</span>(rewards)
    advantages = torch.<span class="fn">zeros</span>(T)    <span class="cm"># (T,)</span>
    last_gae = <span class="nu">0.0</span>
    
    <span class="cm"># ä»åå¾€å‰è®¡ç®—ï¼ˆåŠ¨æ€è§„åˆ’ï¼‰</span>
    <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">reversed</span>(<span class="fn">range</span>(T)):
        <span class="kw">if</span> t == T - <span class="nu">1</span>:
            next_value = <span class="nu">0.0</span>
        <span class="kw">else</span>:
            next_value = values[t + <span class="nu">1</span>]
        
        <span class="cm"># TD è¯¯å·®: Î´ = r + Î³V(s') - V(s)</span>
        delta = rewards[t] + gamma * next_value * (<span class="nu">1</span> - dones[t]) - values[t]
        
        <span class="cm"># GAE: A_t = Î£ (Î³Î»)^l Î´_{t+l}</span>
        last_gae = delta + gamma * lam * (<span class="nu">1</span> - dones[t]) * last_gae
        advantages[t] = last_gae
    
    returns = advantages + values  <span class="cm"># (T,) å›æŠ¥ = ä¼˜åŠ¿ + ä»·å€¼</span>
    <span class="kw">return</span> advantages, returns

<span class="cm"># ========== ç¬¬ä¸‰æ­¥ï¼šPPO æ›´æ–° ==========</span>
<span class="kw">def</span> <span class="fn">ppo_update</span>(actor, critic, optimizer, obs, actions, old_log_probs, advantages, returns):
    <span class="st">"""
    PPO mini-batch æ›´æ–°
    
    Shape å˜åŒ–ï¼š
        obs:           (T, 16)
        actions:       (T, 2)
        old_log_probs: (T,)
        advantages:    (T,)
        returns:       (T,)
    """</span>
    T = obs.shape[<span class="nu">0</span>]
    
    <span class="cm"># å½’ä¸€åŒ–ä¼˜åŠ¿</span>
    advantages = (advantages - advantages.<span class="fn">mean</span>()) / (advantages.<span class="fn">std</span>() + <span class="nu">1e-8</span>)
    
    <span class="kw">for</span> epoch <span class="kw">in</span> <span class="fn">range</span>(N_EPOCHS):
        <span class="cm"># éšæœºæ‰“ä¹±ç´¢å¼•ï¼Œåˆ†æˆ mini-batch</span>
        indices = torch.<span class="fn">randperm</span>(T)
        
        <span class="kw">for</span> start <span class="kw">in</span> <span class="fn">range</span>(<span class="nu">0</span>, T, BATCH_SIZE):
            end = start + BATCH_SIZE
            batch_idx = indices[start:end]  <span class="cm"># (batch_size,)</span>
            
            <span class="cm"># å–å‡º mini-batch æ•°æ®</span>
            mb_obs = obs[batch_idx]                   <span class="cm"># (64, 16)</span>
            mb_actions = actions[batch_idx]            <span class="cm"># (64, 2)</span>
            mb_old_log_probs = old_log_probs[batch_idx]  <span class="cm"># (64,)</span>
            mb_advantages = advantages[batch_idx]     <span class="cm"># (64,)</span>
            mb_returns = returns[batch_idx]            <span class="cm"># (64,)</span>
            
            <span class="cm"># â‘  æ–°ç­–ç•¥çš„ log_prob</span>
            mean, std = actor(mb_obs)                 <span class="cm"># mean: (64, 2), std: (64, 2)</span>
            dist = <span class="fn">Normal</span>(mean, std)
            new_log_probs = dist.<span class="fn">log_prob</span>(mb_actions).<span class="fn">sum</span>(-<span class="nu">1</span>)  <span class="cm"># (64,)</span>
            entropy = dist.<span class="fn">entropy</span>().<span class="fn">sum</span>(-<span class="nu">1</span>).<span class="fn">mean</span>()             <span class="cm"># æ ‡é‡</span>
            
            <span class="cm"># â‘¡ æ¦‚ç‡æ¯”</span>
            ratio = torch.<span class="fn">exp</span>(new_log_probs - mb_old_log_probs)  <span class="cm"># (64,)</span>
            
            <span class="cm"># â‘¢ PPO-Clip ç­–ç•¥æŸå¤±</span>
            surr1 = ratio * mb_advantages                                <span class="cm"># (64,)</span>
            surr2 = torch.<span class="fn">clamp</span>(ratio, <span class="nu">1</span>-CLIP_EPS, <span class="nu">1</span>+CLIP_EPS) * mb_advantages  <span class="cm"># (64,)</span>
            policy_loss = -torch.<span class="fn">min</span>(surr1, surr2).<span class="fn">mean</span>()          <span class="cm"># æ ‡é‡</span>
            
            <span class="cm"># â‘£ ä»·å€¼å‡½æ•°æŸå¤±</span>
            values = critic(mb_obs).<span class="fn">squeeze</span>(-<span class="nu">1</span>)  <span class="cm"># (64, 1) â†’ (64,)</span>
            value_loss = F.<span class="fn">mse_loss</span>(values, mb_returns)  <span class="cm"># æ ‡é‡</span>
            
            <span class="cm"># â‘¤ æ€»æŸå¤±</span>
            loss = policy_loss + VF_COEF * value_loss - ENT_COEF * entropy
            
            <span class="cm"># â‘¥ æ¢¯åº¦æ›´æ–°</span>
            optimizer.<span class="fn">zero_grad</span>()      <span class="cm"># æ¸…é›¶æ¢¯åº¦</span>
            loss.<span class="fn">backward</span>()             <span class="cm"># åå‘ä¼ æ’­</span>
            torch.nn.utils.<span class="fn">clip_grad_norm_</span>(  <span class="cm"># æ¢¯åº¦è£å‰ª</span>
                <span class="fn">list</span>(actor.<span class="fn">parameters</span>()) + <span class="fn">list</span>(critic.<span class="fn">parameters</span>()),
                MAX_GRAD_NORM
            )
            optimizer.<span class="fn">step</span>()            <span class="cm"># æ›´æ–°å‚æ•°</span>

<span class="cm"># ========== å®Œæ•´è®­ç»ƒå¾ªç¯ ==========</span>
<span class="cm"># for iteration in range(1000):</span>
<span class="cm">#     # 1. æ”¶é›†æ•°æ®</span>
<span class="cm">#     obs, actions, rewards, dones, values, log_probs = collect_rollout(</span>
<span class="cm">#         env, actor, critic, N_STEPS)</span>
<span class="cm">#     </span>
<span class="cm">#     # 2. è®¡ç®—ä¼˜åŠ¿</span>
<span class="cm">#     advantages, returns = compute_gae(rewards, values, dones)</span>
<span class="cm">#     </span>
<span class="cm">#     # 3. PPO æ›´æ–°</span>
<span class="cm">#     ppo_update(actor, critic, optimizer, obs, actions, log_probs, advantages, returns)</span></code></pre>

<!-- ================================================== -->
<!--            ç¬¬å…­ç« ï¼šè°ƒè¯•å’Œæœ€ä½³å®è·µ                      -->
<!-- ================================================== -->
<h1 id="ch6">ç¬¬å…­ç«  &nbsp; è°ƒè¯•å’Œæœ€ä½³å®è·µ</h1>

<!-- ==================== 6.1 å¸¸è§é”™è¯¯ ==================== -->
<h2 id="s6-1">6.1 å¸¸è§é”™è¯¯</h2>

<h3>6.1.1 Shape Mismatch</h3>

<div class="box warn">
  <div class="box-title">RuntimeError: mat1 and mat2 shapes cannot be multiplied</div>
<pre><code><span class="cm"># é”™è¯¯ç¤ºä¾‹</span>
linear = nn.<span class="fn">Linear</span>(<span class="nu">16</span>, <span class="nu">64</span>)
x = torch.<span class="fn">randn</span>(<span class="nu">32</span>)    <span class="cm"># (32,) â† é”™ï¼åº”è¯¥æ˜¯ (batch, 16)</span>
<span class="cm"># linear(x)  â†’ æŠ¥é”™ï¼</span>

<span class="cm"># ä¿®æ­£</span>
x = torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">16</span>)  <span class="cm"># (1, 16) âœ“</span>
y = linear(x)            <span class="cm"># (1, 64) âœ“</span>

<span class="cm"># è°ƒè¯•æŠ€å·§ï¼šåœ¨æ¯ä¸€æ­¥æ‰“å° shape</span>
<span class="fn">print</span>(<span class="st">f"x: {x.shape}"</span>)  <span class="cm"># å…»æˆä¹ æƒ¯ï¼</span></code></pre>
</div>

<h3>6.1.2 Device Mismatch</h3>

<div class="box warn">
  <div class="box-title">RuntimeError: Expected all tensors to be on the same device</div>
<pre><code><span class="cm"># é”™è¯¯ç¤ºä¾‹</span>
model = model.<span class="fn">to</span>(<span class="st">'cuda'</span>)
x = torch.<span class="fn">randn</span>(<span class="nu">32</span>, <span class="nu">16</span>)  <span class="cm"># åœ¨ CPU ä¸Š</span>
<span class="cm"># model(x)  â†’ æŠ¥é”™ï¼æ¨¡å‹åœ¨ GPUï¼Œæ•°æ®åœ¨ CPU</span>

<span class="cm"># ä¿®æ­£ï¼šç¡®ä¿æ•°æ®å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡</span>
x = x.<span class="fn">to</span>(<span class="st">'cuda'</span>)
y = model(x)  <span class="cm"># âœ“</span>

<span class="cm"># æœ€ä½³å®è·µï¼šå®šä¹‰å…¨å±€ device</span>
device = torch.<span class="fn">device</span>(<span class="st">'cuda'</span> <span class="kw">if</span> torch.cuda.<span class="fn">is_available</span>() <span class="kw">else</span> <span class="st">'cpu'</span>)
model = model.<span class="fn">to</span>(device)
x = x.<span class="fn">to</span>(device)</code></pre>
</div>

<h3>6.1.3 NaN å€¼</h3>

<div class="box warn">
  <div class="box-title">NaN è°ƒè¯•</div>
<pre><code><span class="cm"># æ£€æµ‹ NaN</span>
<span class="kw">if</span> torch.<span class="fn">isnan</span>(loss):
    <span class="fn">print</span>(<span class="st">"âš ï¸ NaN detected!"</span>)
    <span class="cm"># æ£€æŸ¥æ¯ä¸€å±‚çš„è¾“å‡º</span>
    <span class="kw">for</span> name, param <span class="kw">in</span> model.<span class="fn">named_parameters</span>():
        <span class="kw">if</span> torch.<span class="fn">isnan</span>(param).<span class="fn">any</span>():
            <span class="fn">print</span>(<span class="st">f"NaN in {name}"</span>)
        <span class="kw">if</span> param.grad <span class="kw">is not</span> <span class="kw">None</span> <span class="kw">and</span> torch.<span class="fn">isnan</span>(param.grad).<span class="fn">any</span>():
            <span class="fn">print</span>(<span class="st">f"NaN grad in {name}"</span>)

<span class="cm"># å¸¸è§åŸå› </span>
<span class="cm"># 1. log(0) â€” åŠ  epsilon: log(x + 1e-8)</span>
<span class="cm"># 2. é™¤ä»¥ 0 â€” åŠ  epsilon: x / (y + 1e-8)</span>
<span class="cm"># 3. å­¦ä¹ ç‡å¤ªå¤§ â€” å‡å° lr</span>
<span class="cm"># 4. æ¢¯åº¦çˆ†ç‚¸ â€” åŠ æ¢¯åº¦è£å‰ª</span>

<span class="cm"># å¯ç”¨å¼‚å¸¸æ£€æµ‹æ¨¡å¼</span>
torch.<span class="fn">autograd</span>.<span class="fn">set_detect_anomaly</span>(<span class="kw">True</span>)
<span class="cm"># ç°åœ¨ backward() ä¼šæŒ‡å‡ºå“ªä¸ªæ“ä½œäº§ç”Ÿäº† NaN</span></code></pre>
</div>

<!-- ==================== 6.2 æ¢¯åº¦æ£€æŸ¥ ==================== -->
<h2 id="s6-2">6.2 æ¢¯åº¦æ£€æŸ¥</h2>

<pre><code><span class="cm"># æ‰“å°æ¢¯åº¦ç»Ÿè®¡</span>
<span class="kw">def</span> <span class="fn">print_grad_stats</span>(model):
    <span class="kw">for</span> name, param <span class="kw">in</span> model.<span class="fn">named_parameters</span>():
        <span class="kw">if</span> param.grad <span class="kw">is not</span> <span class="kw">None</span>:
            grad = param.grad
            <span class="fn">print</span>(<span class="st">f"{name:30s} "
                  f"grad_mean={grad.mean():.2e}  "
                  f"grad_std={grad.std():.2e}  "
                  f"grad_max={grad.abs().max():.2e}"</span>)
        <span class="kw">else</span>:
            <span class="fn">print</span>(<span class="st">f"{name:30s} grad=None"</span>)

<span class="cm"># åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨</span>
loss.<span class="fn">backward</span>()
<span class="fn">print_grad_stats</span>(model)

<span class="cm"># æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±ï¼šå¦‚æœæ¢¯åº¦ â‰ˆ 0ï¼ˆå¦‚ 1e-10ï¼‰ï¼Œè¯´æ˜æ¢¯åº¦æ¶ˆå¤±äº†</span>
<span class="cm"># æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸ï¼šå¦‚æœæ¢¯åº¦å¾ˆå¤§ï¼ˆå¦‚ 1e+6ï¼‰ï¼Œè¯´æ˜æ¢¯åº¦çˆ†ç‚¸äº†</span>

<span class="cm"># æ¢¯åº¦ç›´æ–¹å›¾ï¼ˆé…åˆ TensorBoardï¼‰</span>
<span class="kw">from</span> torch.utils.tensorboard <span class="kw">import</span> SummaryWriter
writer = <span class="fn">SummaryWriter</span>()
<span class="kw">for</span> name, param <span class="kw">in</span> model.<span class="fn">named_parameters</span>():
    <span class="kw">if</span> param.grad <span class="kw">is not</span> <span class="kw">None</span>:
        writer.<span class="fn">add_histogram</span>(<span class="st">f'grad/{name}'</span>, param.grad, step)</code></pre>

<!-- ==================== 6.3 TensorBoard ==================== -->
<h2 id="s6-3">6.3 TensorBoard å¯è§†åŒ–</h2>

<pre><code><span class="kw">from</span> torch.utils.tensorboard <span class="kw">import</span> SummaryWriter

<span class="cm"># åˆ›å»º writer</span>
writer = <span class="fn">SummaryWriter</span>(log_dir=<span class="st">'./logs/ppo'</span>)

<span class="cm"># è®°å½•æ ‡é‡</span>
writer.<span class="fn">add_scalar</span>(<span class="st">'loss/policy'</span>, policy_loss, step)
writer.<span class="fn">add_scalar</span>(<span class="st">'loss/value'</span>, value_loss, step)
writer.<span class="fn">add_scalar</span>(<span class="st">'reward/mean'</span>, mean_reward, step)

<span class="cm"># è®°å½•å¤šä¸ªæ ‡é‡</span>
writer.<span class="fn">add_scalars</span>(<span class="st">'losses'</span>, {
    <span class="st">'policy'</span>: policy_loss,
    <span class="st">'value'</span>: value_loss,
}, step)

<span class="cm"># è®°å½•æ¨¡å‹å›¾</span>
writer.<span class="fn">add_graph</span>(model, torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">16</span>))

writer.<span class="fn">close</span>()

<span class="cm"># æŸ¥çœ‹ï¼štensorboard --logdir=./logs/ppo</span>
<span class="cm"># ç„¶åæµè§ˆå™¨æ‰“å¼€ http://localhost:6006</span>

<span class="cm"># SB3 è‡ªå¸¦ TensorBoard æ”¯æŒï¼š</span>
<span class="cm"># model = PPO("MlpPolicy", env, tensorboard_log="./logs/ppo")</span></code></pre>

<!-- ==================== 6.4 GPU å†…å­˜ç®¡ç† ==================== -->
<h2 id="s6-4">6.4 GPU å†…å­˜ç®¡ç†</h2>

<pre><code><span class="cm"># æŸ¥çœ‹ GPU å†…å­˜ä½¿ç”¨</span>
<span class="fn">print</span>(torch.cuda.<span class="fn">memory_allocated</span>() / <span class="nu">1e9</span>, <span class="st">"GB"</span>)  <span class="cm"># å·²åˆ†é…</span>
<span class="fn">print</span>(torch.cuda.<span class="fn">memory_reserved</span>() / <span class="nu">1e9</span>, <span class="st">"GB"</span>)   <span class="cm"># å·²é¢„ç•™</span>
<span class="fn">print</span>(torch.cuda.<span class="fn">max_memory_allocated</span>() / <span class="nu">1e9</span>, <span class="st">"GB"</span>)  <span class="cm"># å³°å€¼</span>

<span class="cm"># æ‰‹åŠ¨é‡Šæ”¾å†…å­˜</span>
torch.cuda.<span class="fn">empty_cache</span>()

<span class="cm"># å‡å°‘å†…å­˜çš„æŠ€å·§</span>
<span class="cm"># 1. å‡å° batch_size</span>
<span class="cm"># 2. ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)</span>
<span class="cm"># 3. æ¢¯åº¦ç´¯ç§¯ä»£æ›¿å¤§ batch</span>
<span class="cm"># 4. åŠæ—¶ del ä¸éœ€è¦çš„ tensor</span>
<span class="cm"># 5. ç”¨ torch.no_grad() åšæ¨ç†</span>

<span class="cm"># æ£€æŸ¥å“ªä¸ªæ“ä½œæœ€å å†…å­˜</span>
torch.cuda.<span class="fn">reset_peak_memory_stats</span>()
<span class="cm"># ... ä½ çš„æ“ä½œ ...</span>
<span class="fn">print</span>(<span class="st">f"å³°å€¼: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB"</span>)</code></pre>

<!-- ==================== 6.5 æ€§èƒ½ä¼˜åŒ– ==================== -->
<h2 id="s6-5">6.5 æ€§èƒ½ä¼˜åŒ–</h2>

<pre><code><span class="cm"># â‘  DataLoader ä¼˜åŒ–</span>
<span class="kw">from</span> torch.utils.data <span class="kw">import</span> DataLoader

loader = <span class="fn">DataLoader</span>(
    dataset,
    batch_size=<span class="nu">64</span>,
    shuffle=<span class="kw">True</span>,
    num_workers=<span class="nu">4</span>,     <span class="cm"># å¤šè¿›ç¨‹æ•°æ®åŠ è½½</span>
    pin_memory=<span class="kw">True</span>,   <span class="cm"># å›ºå®šå†…å­˜ï¼ŒåŠ é€Ÿ CPUâ†’GPU ä¼ è¾“</span>
    drop_last=<span class="kw">True</span>,    <span class="cm"># ä¸¢å¼ƒä¸å®Œæ•´çš„æœ€åä¸€æ‰¹</span>
)

<span class="cm"># â‘¡ torch.compileï¼ˆPyTorch 2.0+ï¼‰</span>
model = torch.<span class="fn">compile</span>(model)  <span class="cm"># JIT ç¼–è¯‘ï¼ŒåŠ é€Ÿ 10-30%</span>

<span class="cm"># â‘¢ æ··åˆç²¾åº¦</span>
<span class="cm"># è§ 1.5 èŠ‚</span>

<span class="cm"># â‘£ é¿å… CPU-GPU é¢‘ç¹æ•°æ®ä¼ è¾“</span>
<span class="cm"># å°½é‡åœ¨åŒä¸€è®¾å¤‡ä¸Šå®Œæˆæ‰€æœ‰è®¡ç®—</span>
<span class="cm"># ä¸è¦åœ¨å¾ªç¯ä¸­åå¤ .cpu() å’Œ .cuda()</span></code></pre>

<!-- ==================== 6.6 å¯å¤ç°æ€§ ==================== -->
<h2 id="s6-6">6.6 å¯å¤ç°æ€§</h2>

<pre><code><span class="kw">import</span> random
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">import</span> torch

<span class="kw">def</span> <span class="fn">set_seed</span>(seed=<span class="nu">42</span>):
    <span class="st">"""è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°"""</span>
    random.<span class="fn">seed</span>(seed)
    np.random.<span class="fn">seed</span>(seed)
    torch.<span class="fn">manual_seed</span>(seed)
    torch.cuda.<span class="fn">manual_seed_all</span>(seed)
    
    <span class="cm"># ç¡®ä¿ CUDA ç®—æ³•çš„ç¡®å®šæ€§</span>
    torch.backends.cudnn.deterministic = <span class="kw">True</span>
    torch.backends.cudnn.benchmark = <span class="kw">False</span>

<span class="fn">set_seed</span>(<span class="nu">42</span>)
<span class="cm"># ç°åœ¨ä½ çš„å®éªŒç»“æœåº”è¯¥æ˜¯å¯å¤ç°çš„</span></code></pre>

<!-- ================================================== -->
<!--              ç¬¬ä¸ƒç« ï¼šé«˜çº§ä¸»é¢˜                         -->
<!-- ================================================== -->
<h1 id="ch7">ç¬¬ä¸ƒç«  &nbsp; é«˜çº§ä¸»é¢˜</h1>

<!-- ==================== 7.1 è‡ªå®šä¹‰ autograd ==================== -->
<h2 id="s7-1">7.1 è‡ªå®šä¹‰ autograd Function</h2>

<pre><code><span class="kw">class</span> <span class="cl">MySigmoid</span>(torch.autograd.Function):
    <span class="st">"""æ‰‹å†™ Sigmoid çš„å‰å‘å’Œåå‘ä¼ æ’­"""</span>
    
    <span class="dc">@staticmethod</span>
    <span class="kw">def</span> <span class="fn">forward</span>(ctx, x):
        <span class="st">"""å‰å‘ä¼ æ’­ï¼šÏƒ(x) = 1 / (1 + exp(-x))"""</span>
        result = <span class="nu">1</span> / (<span class="nu">1</span> + torch.<span class="fn">exp</span>(-x))
        ctx.<span class="fn">save_for_backward</span>(result)  <span class="cm"># ä¿å­˜ç»“æœç”¨äºåå‘ä¼ æ’­</span>
        <span class="kw">return</span> result
    
    <span class="dc">@staticmethod</span>
    <span class="kw">def</span> <span class="fn">backward</span>(ctx, grad_output):
        <span class="st">"""åå‘ä¼ æ’­ï¼šÏƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))"""</span>
        result, = ctx.saved_tensors
        grad_input = grad_output * result * (<span class="nu">1</span> - result)
        <span class="kw">return</span> grad_input

<span class="cm"># ä½¿ç”¨</span>
my_sigmoid = MySigmoid.<span class="fn">apply</span>
x = torch.<span class="fn">randn</span>(<span class="nu">5</span>, requires_grad=<span class="kw">True</span>)
y = <span class="fn">my_sigmoid</span>(x)
y.<span class="fn">sum</span>().<span class="fn">backward</span>()

<span class="cm"># éªŒè¯ä¸ torch.sigmoid ä¸€è‡´</span>
<span class="fn">print</span>(torch.<span class="fn">allclose</span>(y, torch.<span class="fn">sigmoid</span>(x)))  <span class="cm"># True</span></code></pre>

<div class="box example">
  <div class="box-title">å®é™…ç”¨ä¾‹ï¼šç‰©ç†çº¦æŸçš„è‡ªå®šä¹‰æ¢¯åº¦</div>
  <p>åœ¨ç‰©ç†ä»¿çœŸä¸­ï¼Œæœ‰æ—¶éœ€è¦è‡ªå®šä¹‰æ¢¯åº¦æ¥å¼ºåˆ¶æ»¡è¶³çº¦æŸï¼ˆå¦‚èƒ½é‡å®ˆæ’ï¼‰ã€‚è‡ªå®šä¹‰ autograd Function å…è®¸ä½ å®Œå…¨æ§åˆ¶å‰å‘å’Œåå‘ä¼ æ’­çš„è¡Œä¸ºã€‚</p>
</div>

<!-- ==================== 7.2 æ¨¡å‹å¹¶è¡Œ ==================== -->
<h2 id="s7-2">7.2 æ¨¡å‹å¹¶è¡Œ</h2>

<pre><code><span class="cm"># æ•°æ®å¹¶è¡Œï¼ˆDataParallelï¼‰â€” æœ€ç®€å•</span>
model = nn.<span class="fn">DataParallel</span>(model)  <span class="cm"># è‡ªåŠ¨åˆ†é…åˆ°å¤š GPU</span>

<span class="cm"># åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDistributedDataParallelï¼‰â€” æ›´é«˜æ•ˆ</span>
<span class="kw">from</span> torch.nn.parallel <span class="kw">import</span> DistributedDataParallel <span class="kw">as</span> DDP
model = <span class="fn">DDP</span>(model, device_ids=[local_rank])

<span class="cm"># æ‰‹åŠ¨æ¨¡å‹å¹¶è¡Œï¼ˆå¤§æ¨¡å‹åˆ†å‰²åˆ°ä¸åŒ GPUï¼‰</span>
<span class="kw">class</span> <span class="cl">SplitModel</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        self.part1 = nn.<span class="fn">Linear</span>(<span class="nu">1000</span>, <span class="nu">1000</span>).<span class="fn">to</span>(<span class="st">'cuda:0'</span>)
        self.part2 = nn.<span class="fn">Linear</span>(<span class="nu">1000</span>, <span class="nu">1000</span>).<span class="fn">to</span>(<span class="st">'cuda:1'</span>)
    
    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        x = self.part1(x.<span class="fn">to</span>(<span class="st">'cuda:0'</span>))
        x = self.part2(x.<span class="fn">to</span>(<span class="st">'cuda:1'</span>))
        <span class="kw">return</span> x</code></pre>

<!-- ==================== 7.3 torch.compile ==================== -->
<h2 id="s7-3">7.3 torch.compile â€” 2.0+ ç¼–è¯‘åŠ é€Ÿ</h2>

<pre><code><span class="cm"># torch.compile å°† Python æ¨¡å‹ç¼–è¯‘ä¸ºä¼˜åŒ–çš„å†…æ ¸</span>
<span class="cm"># é€šå¸¸åŠ é€Ÿ 10-30%ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç </span>

model = <span class="fn">ActorNetwork</span>()
compiled_model = torch.<span class="fn">compile</span>(model)  <span class="cm"># å°±è¿™ä¸€è¡Œï¼</span>

<span class="cm"># ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šæ…¢ï¼ˆç¼–è¯‘ï¼‰ï¼Œä¹‹åä¼šå¿«</span>
output = compiled_model(torch.<span class="fn">randn</span>(<span class="nu">32</span>, <span class="nu">16</span>))

<span class="cm"># ç¼–è¯‘æ¨¡å¼é€‰æ‹©</span>
compiled = torch.<span class="fn">compile</span>(model, mode=<span class="st">'default'</span>)     <span class="cm"># å¹³è¡¡</span>
compiled = torch.<span class="fn">compile</span>(model, mode=<span class="st">'reduce-overhead'</span>)  <span class="cm"># å‡å°‘å¼€é”€</span>
compiled = torch.<span class="fn">compile</span>(model, mode=<span class="st">'max-autotune'</span>)    <span class="cm"># æœ€å¤§ä¼˜åŒ–ï¼ˆç¼–è¯‘æ›´æ…¢ï¼‰</span></code></pre>

<!-- ==================== 7.4 ONNX ==================== -->
<h2 id="s7-4">7.4 ONNX å¯¼å‡º</h2>

<pre><code><span class="cm"># å°† PyTorch æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼</span>
<span class="cm"># å¯ä»¥åœ¨å…¶ä»–æ¡†æ¶ï¼ˆTensorRT, TensorFlowï¼‰ä¸­è¿è¡Œ</span>

model = <span class="fn">ActorNetwork</span>()
model.<span class="fn">eval</span>()

dummy_input = torch.<span class="fn">randn</span>(<span class="nu">1</span>, <span class="nu">16</span>)

torch.onnx.<span class="fn">export</span>(
    model,
    dummy_input,
    <span class="st">"actor.onnx"</span>,
    input_names=[<span class="st">'observation'</span>],
    output_names=[<span class="st">'action_mean'</span>, <span class="st">'action_std'</span>],
    dynamic_axes={<span class="st">'observation'</span>: {<span class="nu">0</span>: <span class="st">'batch_size'</span>}},
)

<span class="cm"># éªŒè¯ ONNX æ¨¡å‹</span>
<span class="kw">import</span> onnx
model_onnx = onnx.<span class="fn">load</span>(<span class="st">"actor.onnx"</span>)
onnx.checker.<span class="fn">check_model</span>(model_onnx)</code></pre>

<!-- ==================== 7.5 NumPy äº’æ“ä½œ ==================== -->
<h2 id="s7-5">7.5 ä¸ NumPy çš„äº’æ“ä½œ</h2>

<pre><code><span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># ===== Tensor â†’ NumPy =====</span>
t = torch.<span class="fn">tensor</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>])
n = t.<span class="fn">numpy</span>()            <span class="cm"># å…±äº«å†…å­˜ï¼</span>
n = t.<span class="fn">detach</span>().<span class="fn">numpy</span>()   <span class="cm"># å¦‚æœæœ‰ requires_gradï¼Œå…ˆ detach</span>
n = t.<span class="fn">cpu</span>().<span class="fn">numpy</span>()       <span class="cm"># å¦‚æœåœ¨ GPU ä¸Šï¼Œå…ˆç§»åˆ° CPU</span>
n = t.<span class="fn">detach</span>().<span class="fn">cpu</span>().<span class="fn">numpy</span>()  <span class="cm"># æœ€å®‰å…¨çš„å†™æ³•</span>

<span class="cm"># ===== NumPy â†’ Tensor =====</span>
n = np.<span class="fn">array</span>([<span class="nu">1.0</span>, <span class="nu">2.0</span>, <span class="nu">3.0</span>])
t = torch.<span class="fn">from_numpy</span>(n)          <span class="cm"># å…±äº«å†…å­˜</span>
t = torch.<span class="fn">tensor</span>(n)              <span class="cm"># å¤åˆ¶æ•°æ®</span>
t = torch.<span class="fn">as_tensor</span>(n)           <span class="cm"># å°½é‡å…±äº«</span>

<span class="cm"># åœ¨ RL ä¸­å¸¸è§çš„æ¨¡å¼ï¼š</span>
<span class="cm"># env è¿”å› numpy â†’ è½¬ä¸º tensor â†’ é€å…¥æ¨¡å‹ â†’ è¾“å‡ºè½¬ numpy â†’ é€å› env</span>
obs_np = env.<span class="fn">reset</span>()[<span class="nu">0</span>]                            <span class="cm"># numpy (16,)</span>
obs_t = torch.<span class="fn">tensor</span>(obs_np, dtype=torch.float32)  <span class="cm"># tensor (16,)</span>
obs_t = obs_t.<span class="fn">unsqueeze</span>(<span class="nu">0</span>)                          <span class="cm"># tensor (1, 16)</span>
action_t = actor(obs_t)                              <span class="cm"># tensor (1, 2)</span>
action_np = action_t.<span class="fn">squeeze</span>(<span class="nu">0</span>).<span class="fn">detach</span>().<span class="fn">numpy</span>()     <span class="cm"># numpy (2,)</span>
obs_np, reward, done, _, info = env.<span class="fn">step</span>(action_np)</code></pre>

<!-- ================================================== -->
<!--                     é™„å½•                            -->
<!-- ================================================== -->
<h1 id="appendix">é™„å½•</h1>

<!-- ==================== é™„å½• A: API é€ŸæŸ¥è¡¨ ==================== -->
<h2 id="app-api">é™„å½• Aï¼šPyTorch å¸¸ç”¨ API é€ŸæŸ¥è¡¨</h2>

<table>
  <tr><th>ç±»åˆ«</th><th>API</th><th>è¯´æ˜</th></tr>
  <tr><td rowspan="7">åˆ›å»º Tensor</td><td><code>torch.tensor(data)</code></td><td>ä»æ•°æ®åˆ›å»º</td></tr>
  <tr><td><code>torch.zeros(shape)</code></td><td>å…¨é›¶</td></tr>
  <tr><td><code>torch.ones(shape)</code></td><td>å…¨ä¸€</td></tr>
  <tr><td><code>torch.randn(shape)</code></td><td>æ ‡å‡†æ­£æ€</td></tr>
  <tr><td><code>torch.rand(shape)</code></td><td>å‡åŒ€ [0,1)</td></tr>
  <tr><td><code>torch.eye(n)</code></td><td>å•ä½çŸ©é˜µ</td></tr>
  <tr><td><code>torch.arange(start, end, step)</code></td><td>ç­‰å·®åºåˆ—</td></tr>
  <tr><td rowspan="5">å½¢çŠ¶æ“ä½œ</td><td><code>x.reshape(shape)</code></td><td>æ”¹å˜å½¢çŠ¶ï¼ˆå®‰å…¨ï¼‰</td></tr>
  <tr><td><code>x.view(shape)</code></td><td>æ”¹å˜å½¢çŠ¶ï¼ˆè¦æ±‚è¿ç»­ï¼‰</td></tr>
  <tr><td><code>x.permute(dims)</code></td><td>è°ƒæ¢ç»´åº¦é¡ºåº</td></tr>
  <tr><td><code>x.squeeze(dim)</code></td><td>å»é™¤å¤§å°ä¸º1çš„ç»´åº¦</td></tr>
  <tr><td><code>x.unsqueeze(dim)</code></td><td>æ·»åŠ å¤§å°ä¸º1çš„ç»´åº¦</td></tr>
  <tr><td rowspan="4">æ‹¼æ¥</td><td><code>torch.cat(tensors, dim)</code></td><td>æ²¿å·²æœ‰ç»´åº¦æ‹¼æ¥</td></tr>
  <tr><td><code>torch.stack(tensors, dim)</code></td><td>æ²¿æ–°ç»´åº¦å †å </td></tr>
  <tr><td><code>torch.split(x, size, dim)</code></td><td>åˆ†å‰²</td></tr>
  <tr><td><code>torch.chunk(x, n, dim)</code></td><td>ç­‰åˆ†</td></tr>
  <tr><td rowspan="4">æ•°å­¦è¿ç®—</td><td><code>x @ y</code> / <code>torch.matmul</code></td><td>çŸ©é˜µä¹˜æ³•</td></tr>
  <tr><td><code>x * y</code></td><td>é€å…ƒç´ ä¹˜</td></tr>
  <tr><td><code>x.sum(dim)</code></td><td>æ±‚å’Œ</td></tr>
  <tr><td><code>x.mean(dim)</code></td><td>æ±‚å‡å€¼</td></tr>
  <tr><td rowspan="4">è®¾å¤‡</td><td><code>x.to(device)</code></td><td>ç§»åŠ¨åˆ°è®¾å¤‡</td></tr>
  <tr><td><code>x.cuda()</code></td><td>ç§»åŠ¨åˆ° GPU</td></tr>
  <tr><td><code>x.cpu()</code></td><td>ç§»åŠ¨åˆ° CPU</td></tr>
  <tr><td><code>x.device</code></td><td>æŸ¥çœ‹è®¾å¤‡</td></tr>
  <tr><td rowspan="4">æ¢¯åº¦</td><td><code>x.requires_grad_()</code></td><td>å¯ç”¨æ¢¯åº¦è·Ÿè¸ª</td></tr>
  <tr><td><code>x.backward()</code></td><td>åå‘ä¼ æ’­</td></tr>
  <tr><td><code>x.grad</code></td><td>æŸ¥çœ‹æ¢¯åº¦</td></tr>
  <tr><td><code>x.detach()</code></td><td>ä»è®¡ç®—å›¾åˆ†ç¦»</td></tr>
</table>

<!-- ==================== é™„å½• B: NumPy å¯¹ç…§ ==================== -->
<h2 id="app-numpy">é™„å½• Bï¼šTensor æ“ä½œå¯¹ç…§è¡¨ (NumPy vs PyTorch)</h2>

<table>
  <tr><th>æ“ä½œ</th><th>NumPy</th><th>PyTorch</th></tr>
  <tr><td>åˆ›å»ºæ•°ç»„</td><td><code>np.array([1,2,3])</code></td><td><code>torch.tensor([1,2,3])</code></td></tr>
  <tr><td>å…¨é›¶</td><td><code>np.zeros((3,4))</code></td><td><code>torch.zeros(3,4)</code></td></tr>
  <tr><td>éšæœº</td><td><code>np.random.randn(3,4)</code></td><td><code>torch.randn(3,4)</code></td></tr>
  <tr><td>å½¢çŠ¶</td><td><code>x.shape</code></td><td><code>x.shape</code></td></tr>
  <tr><td>ç±»å‹</td><td><code>x.dtype</code></td><td><code>x.dtype</code></td></tr>
  <tr><td>è½¬ç½®</td><td><code>x.T</code></td><td><code>x.T</code></td></tr>
  <tr><td>æ”¹å½¢</td><td><code>x.reshape(3,4)</code></td><td><code>x.reshape(3,4)</code></td></tr>
  <tr><td>æ‹¼æ¥</td><td><code>np.concatenate</code></td><td><code>torch.cat</code></td></tr>
  <tr><td>å †å </td><td><code>np.stack</code></td><td><code>torch.stack</code></td></tr>
  <tr><td>çŸ©é˜µä¹˜</td><td><code>np.dot / @</code></td><td><code>torch.matmul / @</code></td></tr>
  <tr><td>å…ƒç´ ä¹˜</td><td><code>x * y</code></td><td><code>x * y</code></td></tr>
  <tr><td>æ±‚å’Œ</td><td><code>x.sum(axis=0)</code></td><td><code>x.sum(dim=0)</code></td></tr>
  <tr><td>æœ€å¤§å€¼</td><td><code>x.max(axis=0)</code></td><td><code>x.max(dim=0)</code></td></tr>
  <tr><td>åˆ‡ç‰‡</td><td><code>x[1:3, :]</code></td><td><code>x[1:3, :]</code></td></tr>
  <tr><td>å¸ƒå°”ç´¢å¼•</td><td><code>x[x > 0]</code></td><td><code>x[x > 0]</code></td></tr>
  <tr><td>äº’è½¬</td><td><code>torch.from_numpy(x)</code></td><td><code>x.numpy()</code></td></tr>
</table>

<!-- ==================== é™„å½• C: å…¬å¼æ€»ç»“ ==================== -->
<h2 id="app-formulas">é™„å½• Cï¼šæ ¸å¿ƒå…¬å¼æ€»ç»“</h2>

<div class="box thm">
  <div class="box-title">ä¸€é¡µçº¸å…¬å¼é€ŸæŸ¥</div>

  <h4>çº¿æ€§å˜æ¢</h4>
  <p>$$y = Wx + b, \quad \frac{\partial L}{\partial W} = \delta \cdot x^T, \quad \frac{\partial L}{\partial x} = W^T \delta$$</p>

  <h4>æ¿€æ´»å‡½æ•°</h4>
  <p>$$\text{ReLU}(x) = \max(0, x), \quad \text{ReLU}'(x) = \mathbb{1}_{x>0}$$</p>
  <p>$$\sigma(x) = \frac{1}{1+e^{-x}}, \quad \sigma'(x) = \sigma(1-\sigma)$$</p>
  <p>$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}, \quad \tanh'(x) = 1 - \tanh^2(x)$$</p>

  <h4>æŸå¤±å‡½æ•°</h4>
  <p>$$\text{MSE} = \frac{1}{n}\sum(y - \hat{y})^2, \quad \text{CE} = -\sum y \log \hat{y}$$</p>
  <p>$$L^{CLIP} = -\mathbb{E}\left[\min(r\hat{A},\ \text{clip}(r, 1\pm\varepsilon)\hat{A})\right]$$</p>

  <h4>Adam ä¼˜åŒ–å™¨</h4>
  <p>$$m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t, \quad v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$$</p>
  <p>$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t$$</p>

  <h4>æ¦‚ç‡åˆ†å¸ƒ</h4>
  <p>$$\log \mathcal{N}(a|\mu,\sigma^2) = -\frac{(a-\mu)^2}{2\sigma^2} - \log\sigma - \frac{1}{2}\log(2\pi)$$</p>
  <p>$$H[\mathcal{N}] = \frac{1}{2}\log(2\pi e\sigma^2)$$</p>

  <h4>GAE</h4>
  <p>$$\hat{A}_t = \sum_{l=0}^{\infty}(\gamma\lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$</p>
</div>

<!-- ==================== é™„å½• D: æ¿€æ´»å‡½æ•°å›¾å½¢ ==================== -->
<h2 id="app-activations">é™„å½• Dï¼šæ¿€æ´»å‡½æ•°å›¾å½¢å¯¹æ¯”</h2>

<div class="diagram">
<svg width="700" height="300" viewBox="0 0 700 300">
  <!-- Axes -->
  <line x1="50" y1="150" x2="650" y2="150" stroke="currentColor" stroke-width="1" opacity="0.3"/>
  <line x1="350" y1="20" x2="350" y2="280" stroke="currentColor" stroke-width="1" opacity="0.3"/>
  
  <!-- Grid lines -->
  <line x1="50" y1="50" x2="650" y2="50" stroke="currentColor" stroke-width="0.5" opacity="0.1" stroke-dasharray="4"/>
  <line x1="50" y1="100" x2="650" y2="100" stroke="currentColor" stroke-width="0.5" opacity="0.1" stroke-dasharray="4"/>
  <line x1="50" y1="200" x2="650" y2="200" stroke="currentColor" stroke-width="0.5" opacity="0.1" stroke-dasharray="4"/>
  <line x1="50" y1="250" x2="650" y2="250" stroke="currentColor" stroke-width="0.5" opacity="0.1" stroke-dasharray="4"/>
  
  <!-- ReLU (blue) -->
  <polyline points="50,150 150,150 200,150 250,150 300,150 350,150 400,120 450,90 500,60 550,30 600,0" fill="none" stroke="#4361ee" stroke-width="2.5"/>
  
  <!-- Sigmoid (red) -->
  <polyline points="50,245 100,240 150,230 200,215 250,195 300,170 350,150 400,130 450,105 500,85 550,70 600,60 650,55" fill="none" stroke="#f44336" stroke-width="2.5"/>
  
  <!-- Tanh (green) -->
  <polyline points="50,245 100,242 150,235 200,220 250,200 300,175 350,150 400,125 450,100 500,80 550,65 600,58 650,55" fill="none" stroke="#4CAF50" stroke-width="2.5"/>
  
  <!-- Labels -->
  <text x="610" y="25" fill="#4361ee" font-size="14" font-weight="bold">ReLU</text>
  <text x="610" y="50" fill="#f44336" font-size="14" font-weight="bold">Sigmoid</text>
  <text x="610" y="75" fill="#4CAF50" font-size="14" font-weight="bold">Tanh</text>
  
  <!-- Axis labels -->
  <text x="640" y="165" fill="currentColor" font-size="12">x</text>
  <text x="355" y="18" fill="currentColor" font-size="12">f(x)</text>
  <text x="345" y="165" fill="currentColor" font-size="11">0</text>
  <text x="355" y="55" fill="currentColor" font-size="11">1</text>
  <text x="355" y="255" fill="currentColor" font-size="11">-1</text>
</svg>
</div>

<table>
  <tr><th>æ¿€æ´»å‡½æ•°</th><th>å…¬å¼</th><th>è¾“å‡ºèŒƒå›´</th><th>æ¢¯åº¦èŒƒå›´</th><th>ä¸»è¦ç”¨é€”</th></tr>
  <tr><td>ReLU</td><td>\(\max(0, x)\)</td><td>\([0, +\infty)\)</td><td>\(\{0, 1\}\)</td><td>éšè—å±‚ï¼ˆé»˜è®¤ï¼‰</td></tr>
  <tr><td>LeakyReLU</td><td>\(\max(\alpha x, x)\)</td><td>\((-\infty, +\infty)\)</td><td>\(\{\alpha, 1\}\)</td><td>é¿å…æ­»ç¥ç»å…ƒ</td></tr>
  <tr><td>Sigmoid</td><td>\(\frac{1}{1+e^{-x}}\)</td><td>\((0, 1)\)</td><td>\((0, 0.25]\)</td><td>æ¦‚ç‡è¾“å‡º</td></tr>
  <tr><td>Tanh</td><td>\(\frac{e^x-e^{-x}}{e^x+e^{-x}}\)</td><td>\((-1, 1)\)</td><td>\((0, 1]\)</td><td>Actor è¾“å‡º</td></tr>
  <tr><td>Softmax</td><td>\(\frac{e^{x_i}}{\sum e^{x_j}}\)</td><td>\((0, 1), \sum=1\)</td><td>â€”</td><td>åˆ†ç±»</td></tr>
  <tr><td>GELU</td><td>\(x \cdot \Phi(x)\)</td><td>\(\approx(-0.17, +\infty)\)</td><td>å¹³æ»‘</td><td>Transformer</td></tr>
</table>

<!-- ==================== é™„å½• E: ä¼˜åŒ–å™¨å¯¹æ¯” ==================== -->
<h2 id="app-optimizers">é™„å½• Eï¼šä¼˜åŒ–å™¨å¯¹æ¯”è¡¨</h2>

<table>
  <tr><th>ä¼˜åŒ–å™¨</th><th>å­¦ä¹ ç‡</th><th>å…³é”®ç‰¹æ€§</th><th>é€‚ç”¨åœºæ™¯</th><th>ä»£ç </th></tr>
  <tr>
    <td><strong>SGD</strong></td>
    <td>0.01-0.1</td>
    <td>ç®€å•ï¼Œå¯+åŠ¨é‡</td>
    <td>CV (ResNetç­‰)</td>
    <td><code>SGD(params, lr=0.01, momentum=0.9)</code></td>
  </tr>
  <tr>
    <td><strong>Adam</strong></td>
    <td>1e-4 ~ 3e-4</td>
    <td>è‡ªé€‚åº”å­¦ä¹ ç‡</td>
    <td>RL, NLP, é€šç”¨</td>
    <td><code>Adam(params, lr=3e-4)</code></td>
  </tr>
  <tr>
    <td><strong>AdamW</strong></td>
    <td>1e-4 ~ 3e-4</td>
    <td>ä¿®æ­£æƒé‡è¡°å‡</td>
    <td>Transformer</td>
    <td><code>AdamW(params, lr=3e-4, weight_decay=0.01)</code></td>
  </tr>
  <tr>
    <td><strong>RMSprop</strong></td>
    <td>1e-3</td>
    <td>è‡ªé€‚åº”ï¼Œæ— åŠ¨é‡ä¿®æ­£</td>
    <td>RNN</td>
    <td><code>RMSprop(params, lr=1e-3)</code></td>
  </tr>
  <tr>
    <td><strong>LBFGS</strong></td>
    <td>1.0</td>
    <td>äºŒé˜¶æ–¹æ³•ï¼Œå…¨ batch</td>
    <td>å°è§„æ¨¡ä¼˜åŒ–</td>
    <td><code>LBFGS(params)</code></td>
  </tr>
</table>

<hr>

<p style="text-align: center; color: var(--text-secondary); font-size: 0.9em; padding: 40px 0;">
  ğŸ”¥ PyTorch æ ¸å¿ƒå­¦ä¹ æ•™ç¨‹ â€” PushBox ç‰©ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ é¡¹ç›®<br>
  æœ€åæ›´æ–°ï¼š2026-02-06 &nbsp;|&nbsp; ç›¸å…³æ•™ç¨‹ï¼š<a href="../PPO/index.html">PPO æ•™ç¨‹</a> &nbsp;|&nbsp; <a href="../GNS/index.html">GNS æ•™ç¨‹</a>
</p>

</div><!-- .main -->

<!-- ========== JavaScript ========== -->
<script>
// Theme toggle
function toggleTheme() {
  const body = document.body;
  const isDark = body.getAttribute('data-theme') === 'dark';
  body.setAttribute('data-theme', isDark ? '' : 'dark');
  localStorage.setItem('theme', isDark ? 'light' : 'dark');
}

// Load saved theme
(function() {
  const saved = localStorage.getItem('theme');
  if (saved === 'dark') document.body.setAttribute('data-theme', 'dark');
})();

// Tab switching
function switchTab(btn, tabId) {
  const group = btn.closest('.tab-group');
  group.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));
  group.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
  btn.classList.add('active');
  document.getElementById(tabId).classList.add('active');
}

// Active sidebar link
const observer = new IntersectionObserver(entries => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      const id = entry.target.id;
      document.querySelectorAll('#nav a').forEach(a => {
        a.classList.toggle('active', a.getAttribute('href') === '#' + id);
      });
    }
  });
}, { rootMargin: '-80px 0px -70% 0px' });

document.querySelectorAll('h1[id], h2[id]').forEach(el => observer.observe(el));

// Reading progress
window.addEventListener('scroll', () => {
  const scrollTop = window.scrollY;
  const docHeight = document.documentElement.scrollHeight - window.innerHeight;
  const progress = (scrollTop / docHeight) * 100;
  const fill = document.getElementById('progressFill');
  if (fill) fill.style.width = progress + '%';
});

// Copy button for code blocks
document.querySelectorAll('pre').forEach(pre => {
  const btn = document.createElement('button');
  btn.className = 'copy-btn';
  btn.textContent = 'ğŸ“‹';
  btn.onclick = () => {
    const code = pre.querySelector('code') || pre;
    navigator.clipboard.writeText(code.textContent).then(() => {
      btn.textContent = 'âœ“';
      setTimeout(() => btn.textContent = 'ğŸ“‹', 2000);
    });
  };
  pre.appendChild(btn);
});

// KaTeX auto-render
document.addEventListener('DOMContentLoaded', () => {
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '\\[', right: '\\]', display: true },
        { left: '\\(', right: '\\)', display: false },
        { left: '$', right: '$', display: false },
      ],
      throwOnError: false,
    });
  }
});
</script>
</body>
</html>
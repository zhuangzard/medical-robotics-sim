<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 1: Introduction | GDL å­¦ä¹ æŒ‡å—</title>
  <link rel="stylesheet" href="../assets/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Noto+Serif+SC:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]})"></script>
</head>
<body>
  <div class="progress-bar"></div>

  <header class="header">
    <div class="header-title"><a href="../index.html">ğŸ“ GDL å­¦ä¹ æŒ‡å—</a></div>
    <div class="header-nav">
      <a href="../index.html">ç›®å½•</a>
      <a href="../chapter2/index.html">ä¸‹ä¸€ç«  â†’</a>
      <button class="theme-toggle" onclick="toggleTheme()">ğŸŒ™</button>
    </div>
  </header>

  <button class="sidebar-toggle" onclick="toggleSidebar()">â˜°</button>

  <nav class="sidebar">
    <h3>Chapter 1</h3>
    <a href="#overview">æ¦‚è¿°</a>
    <a href="#why-gdl">ä¸ºä»€ä¹ˆéœ€è¦ GDLï¼Ÿ</a>
    <a href="#revolution" class="sub">æ·±åº¦å­¦ä¹ é©å‘½</a>
    <a href="#beyond-euclidean" class="sub">è¶…è¶Šæ¬§å‡ é‡Œå¾—</a>
    <a href="#symmetry-principle" class="sub">å¯¹ç§°æ€§åŸåˆ™</a>
    <a href="#two-principles">ä¸¤ä¸ªæ ¸å¿ƒç®—æ³•åŸåˆ™</a>
    <a href="#representation-learning" class="sub">è¡¨å¾å­¦ä¹ </a>
    <a href="#gradient-descent" class="sub">æ¢¯åº¦ä¸‹é™</a>
    <a href="#curse-overview" class="sub">ç»´åº¦ç¾éš¾é¢„è§ˆ</a>
    <a href="#5g-framework">5G ç»Ÿä¸€æ¡†æ¶</a>
    <a href="#grids" class="sub">Grids (ç½‘æ ¼)</a>
    <a href="#groups" class="sub">Groups (ç¾¤)</a>
    <a href="#graphs" class="sub">Graphs (å›¾)</a>
    <a href="#geodesics" class="sub">Geodesics (æµ‹åœ°çº¿)</a>
    <a href="#gauges" class="sub">Gauges (è§„èŒƒ)</a>
    <a href="#erlangen">Erlangen Program</a>
    <a href="#erlangen-history" class="sub">å†å²èƒŒæ™¯</a>
    <a href="#erlangen-ml" class="sub">å¯¹æœºå™¨å­¦ä¹ çš„å¯ç¤º</a>
    <a href="#architectures">æ¶æ„æ™¯è§‚</a>
    <a href="#cnn-intro" class="sub">CNN å®¶æ—</a>
    <a href="#rnn-intro" class="sub">RNN å®¶æ—</a>
    <a href="#gnn-intro" class="sub">GNN å®¶æ—</a>
    <a href="#transformer-intro" class="sub">Transformer</a>
    <a href="#scope">æœ¬ä¹¦èŒƒå›´ä¸éèŒƒå›´</a>
    <a href="#roadmap">é˜…è¯»è·¯çº¿å›¾</a>
    <a href="#roadmap-ch2" class="sub">Ch2: é«˜ç»´å­¦ä¹ </a>
    <a href="#roadmap-ch3" class="sub">Ch3: å‡ ä½•å…ˆéªŒ</a>
    <a href="#roadmap-ch4" class="sub">Ch4: å‡ ä½•åŸŸ</a>
    <a href="#roadmap-ch5" class="sub">Ch5: GDL æ¨¡å‹</a>
    <a href="#roadmap-ch6" class="sub">Ch6: åº”ç”¨</a>
    <a href="#roadmap-ch7" class="sub">Ch7: å†å²è§†è§’</a>
    <a href="#physrobot">ä¸ PhysRobot çš„å…³è”</a>
    <a href="#code-setup">ä»£ç ç¯å¢ƒå‡†å¤‡</a>
    <a href="#exercises">ç»ƒä¹ é¢˜</a>
    <h3>å¯¼èˆª</h3>
    <a href="../index.html">ğŸ“š æ€»ç›®å½•</a>
    <a href="../chapter2/index.html">â†’ Ch.2 é«˜ç»´å­¦ä¹ </a>
  </nav>

  <main class="main">
    <h1>Chapter 1: Introduction<br><span style="font-size:0.6em;color:var(--text-secondary)">å¼•è¨€ â€” å‡ ä½•æ·±åº¦å­¦ä¹ çš„ç»Ÿä¸€è§†è§’</span></h1>

    <div class="callout callout-info" id="overview">
      <h4>æœ¬ç« æ¦‚è¿°</h4>
      <p>æœ¬ç« æ˜¯å…¨ä¹¦çš„<strong>èµ·ç‚¹</strong>ï¼Œå»ºç«‹äº†å‡ ä½•æ·±åº¦å­¦ä¹ ï¼ˆGeometric Deep Learning, GDLï¼‰çš„å®è§‚è§†è§’ã€‚æˆ‘ä»¬å°†å›ç­”ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š</p>
      <ul>
        <li><strong>ä¸ºä»€ä¹ˆ</strong>å‡ ä½•æ·±åº¦å­¦ä¹ å¦‚æ­¤é‡è¦ï¼Ÿ</li>
        <li><strong>ä»€ä¹ˆæ˜¯</strong>æ·±åº¦å­¦ä¹ ä¸­çš„"å‡ ä½•ç»Ÿä¸€"ï¼Ÿ</li>
        <li><strong>5G æ¡†æ¶</strong>å¦‚ä½•ç»Ÿä¸€ CNNã€RNNã€GNN å’Œ Transformerï¼Ÿ</li>
        <li>Erlangen Program çš„ç²¾ç¥å¦‚ä½•æŒ‡å¯¼æ¶æ„è®¾è®¡ï¼Ÿ</li>
        <li><strong>é˜…è¯»è·¯çº¿å›¾</strong>ï¼šæœ¬ä¹¦å„ç« å¦‚ä½•ä¸²è”ï¼Ÿ</li>
      </ul>
      <p><strong>é¢„è®¡é˜…è¯»æ—¶é—´</strong>ï¼š1.5 å°æ—¶ &nbsp;|&nbsp; <strong>å…ˆä¿®çŸ¥è¯†</strong>ï¼šåŸºç¡€çº¿æ€§ä»£æ•°ã€å¾®ç§¯åˆ†ã€Python</p>
    </div>

    <!-- ========= ä¸ºä»€ä¹ˆéœ€è¦ GDL ========= -->
    <h2 id="why-gdl">ä¸ºä»€ä¹ˆéœ€è¦å‡ ä½•æ·±åº¦å­¦ä¹ ï¼Ÿ<br><span style="font-size:0.7em;color:var(--text-secondary)">Why Geometric Deep Learning?</span></h2>

    <h3 id="revolution">æ·±åº¦å­¦ä¹ é©å‘½</h3>

    <div class="bilingual">
      <div class="zh">
        <p>è¿‡å»åå¹´è§è¯äº†æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„<strong>å®éªŒæ€§é©å‘½</strong>ï¼Œå…¶æ ‡å¿—æ˜¯æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å´›èµ·ã€‚è®¸å¤šæ­¤å‰è¢«è®¤ä¸ºä¸å¯èƒ½çš„é«˜ç»´å­¦ä¹ ä»»åŠ¡â€”â€”å¦‚<strong>è®¡ç®—æœºè§†è§‰</strong>ã€<strong>å›´æ£‹åšå¼ˆ</strong>æˆ–<strong>è›‹ç™½è´¨æŠ˜å </strong>â€”â€”å®é™…ä¸Šåœ¨é€‚å½“çš„è®¡ç®—è§„æ¨¡ä¸‹æ˜¯å¯è¡Œçš„ã€‚</p>
      </div>
      <div class="en">
        The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach â€” such as computer vision, playing Go, or protein folding â€” are in fact feasible with appropriate computational scale.
      </div>
    </div>

    <p>è®©æˆ‘ä»¬ç”¨ä¸€ç»„æ•°å­—æ¥æ„Ÿå—è¿™åœºé©å‘½çš„è§„æ¨¡ï¼š</p>

    <table>
      <thead>
        <tr><th>é‡Œç¨‹ç¢‘</th><th>å¹´ä»½</th><th>æ„ä¹‰</th></tr>
      </thead>
      <tbody>
        <tr><td>AlexNet (ImageNet)</td><td>2012</td><td>CNN åœ¨å›¾åƒè¯†åˆ«ä¸Šé¦–æ¬¡å¤§å¹…è¶…è¶Šä¼ ç»Ÿæ–¹æ³•ï¼Œå¼€å¯æ·±åº¦å­¦ä¹ æ—¶ä»£</td></tr>
        <tr><td>AlphaGo</td><td>2016</td><td>CNN + MCTS åœ¨å›´æ£‹ä¸Šå‡»è´¥ä¸–ç•Œå† å†›æä¸–ä¹­</td></tr>
        <tr><td>Transformer / Attention is All You Need</td><td>2017</td><td>è‡ªæ³¨æ„åŠ›æœºåˆ¶ç»Ÿä¸€ NLPï¼Œå¼€å¯å¤§æ¨¡å‹æ—¶ä»£</td></tr>
        <tr><td>AlphaFold 2</td><td>2020</td><td>ç­‰å˜ GNN è§£å†³è›‹ç™½è´¨ç»“æ„é¢„æµ‹é—®é¢˜</td></tr>
        <tr><td>GPT-3</td><td>2020</td><td>1750 äº¿å‚æ•°è¯­è¨€æ¨¡å‹å±•ç¤º few-shot å­¦ä¹ èƒ½åŠ›</td></tr>
        <tr><td>GNS (ç²’å­ä»¿çœŸ)</td><td>2020</td><td>GNN å®ç°é«˜ç²¾åº¦ç‰©ç†ä»¿çœŸï¼Œè¶…è¶Šä¼ ç»Ÿæ•°å€¼æ–¹æ³•</td></tr>
      </tbody>
    </table>

    <p>ä½†è¿™åœºé©å‘½èƒŒåæœ‰ä¸€ä¸ª<strong>æ·±åˆ»çš„ç»Ÿä¸€ä¸»é¢˜</strong>ï¼šæ‰€æœ‰è¿™äº›æˆåŠŸéƒ½æºäº<strong>å¯¹æ•°æ®åº•å±‚å‡ ä½•ç»“æ„çš„æ­£ç¡®åˆ©ç”¨</strong>ã€‚è¿™æ­£æ˜¯å‡ ä½•æ·±åº¦å­¦ä¹ è¦æ­ç¤ºçš„æ ¸å¿ƒæ´å¯Ÿã€‚</p>

    <h3 id="beyond-euclidean">è¶…è¶Šæ¬§å‡ é‡Œå¾—ï¼šéç»“æ„åŒ–æ•°æ®çš„å´›èµ·</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æˆåŠŸä¸»è¦é›†ä¸­åœ¨<strong>æ¬§å‡ é‡Œå¾—åŸŸ</strong>çš„æ•°æ®ä¸Šï¼šå›¾åƒï¼ˆ2D ç½‘æ ¼ï¼‰ã€è¯­éŸ³ï¼ˆ1D åºåˆ—ï¼‰ã€æ–‡æœ¬ï¼ˆ1D åºåˆ—ï¼‰ã€‚ä½†ç°å®ä¸–ç•Œçš„å¤§é‡æ•°æ®å¤©ç„¶å…·æœ‰<strong>éæ¬§å‡ é‡Œå¾—</strong>ç»“æ„ï¼š</p>
        <ul>
          <li><strong>ç¤¾äº¤ç½‘ç»œ</strong> â€” ç”¨æˆ·å’Œå…³ç³»æ„æˆå›¾ç»“æ„</li>
          <li><strong>åˆ†å­</strong> â€” åŸå­å’ŒåŒ–å­¦é”®æ„æˆå›¾ï¼›3D ç»“æ„åœ¨æ—‹è½¬ä¸‹ä¸å˜</li>
          <li><strong>è›‹ç™½è´¨</strong> â€” æ°¨åŸºé…¸é“¾æŠ˜å æˆå¤æ‚çš„ 3D è¡¨é¢</li>
          <li><strong>äº¤é€šç½‘ç»œ</strong> â€” é“è·¯å’Œäº¤å‰å£æ„æˆç©ºé—´å›¾</li>
          <li><strong>ç²’å­ç³»ç»Ÿ</strong> â€” ç‰©ç†ç²’å­ä¹‹é—´çš„ç›¸äº’ä½œç”¨æ„æˆåŠ¨æ€å›¾</li>
          <li><strong>åŒ»ç–—æ•°æ®</strong> â€” æ‚£è€…å…³ç³»ç½‘ç»œã€è„‘åŠŸèƒ½è¿æ¥å›¾</li>
        </ul>
        <p>å¯¹è¿™äº›æ•°æ®ï¼Œä¼ ç»Ÿçš„ CNN æˆ– RNN æ— æ³•ç›´æ¥é€‚ç”¨ã€‚æˆ‘ä»¬éœ€è¦ä¸€ç§<strong>æ›´é€šç”¨çš„ç†è®ºæ¡†æ¶</strong>ã€‚</p>
      </div>
      <div class="en">
        While traditional deep learning succeeded on Euclidean-structured data (images, speech, text), the real world is full of non-Euclidean data: social networks, molecules, proteins, road networks, particle systems, medical data, and more. We need a more general theoretical framework.
      </div>
    </div>

    <div class="callout callout-project">
      <h4>PhysRobot é¡¹ç›®ä¸­çš„éæ¬§å‡ é‡Œå¾—æ•°æ®</h4>
      <p>æˆ‘ä»¬çš„åŒ»ç–—æœºå™¨äººç‰©ç†ä»¿çœŸé¡¹ç›®æ­£æ˜¯è¿™ç§éæ¬§å‡ é‡Œå¾—æ•°æ®çš„å…¸å‹æ¡ˆä¾‹ï¼š</p>
      <ul>
        <li><strong>è½¯ç»„ç»‡</strong>æ¨¡å‹ä¸ºä¸‰è§’ç½‘æ ¼ï¼ˆç¦»æ•£æµå½¢ï¼‰</li>
        <li><strong>ç²’å­ç³»ç»Ÿ</strong>ç”±åŠ¨æ€å›¾è¡¨ç¤ºï¼ˆk-NN å›¾éšä»¿çœŸæ¼”åŒ–ï¼‰</li>
        <li><strong>æ‰‹æœ¯å™¨æ¢°</strong>åœ¨ SE(3) ç©ºé—´ä¸­è¿åŠ¨ï¼ˆæ—‹è½¬ + å¹³ç§»ï¼‰</li>
        <li>ç‰©ç†å®šå¾‹å…·æœ‰<strong>æ—‹è½¬ä¸å˜æ€§</strong>å’Œ<strong>å¹³ç§»ä¸å˜æ€§</strong></li>
      </ul>
      <p>GDL æä¾›äº†æ­£ç¡®åˆ©ç”¨è¿™äº›å¯¹ç§°æ€§çš„ç†è®ºåŸºç¡€ã€‚</p>
    </div>

    <h3 id="symmetry-principle">å¯¹ç§°æ€§ï¼šç‰©ç†å­¦çš„æŒ‡å¯¼åŸåˆ™</h3>

    <div class="bilingual">
      <div class="zh">
        <p>åˆ©ç”¨å¤§å‹ç³»ç»Ÿçš„<strong>å·²çŸ¥å¯¹ç§°æ€§</strong>æ˜¯å¯¹æŠ—ç»´åº¦ç¾éš¾çš„ç»å…¸è€Œå¼ºå¤§çš„æ–¹æ³•ï¼Œä¹Ÿæ˜¯å¤§å¤šæ•°ç‰©ç†ç†è®ºçš„åŸºç¡€ã€‚æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¹Ÿä¸ä¾‹å¤–â€”â€”ä»æ—©æœŸå¼€å§‹ï¼Œç ”ç©¶è€…å°±è°ƒæ•´ç¥ç»ç½‘ç»œæ¥åˆ©ç”¨ç‰©ç†æµ‹é‡äº§ç”Ÿçš„ä½ç»´å‡ ä½•ç»“æ„ï¼š</p>
        <ul>
          <li><strong>å›¾åƒ</strong>ä¸­çš„ç½‘æ ¼ç»“æ„ â†’ <strong>å¹³ç§»å¯¹ç§°æ€§</strong> â†’ CNN</li>
          <li><strong>æ—¶é—´åºåˆ—</strong>ä¸­çš„åºåˆ—ç»“æ„ â†’ <strong>æ—¶é—´ä¸å˜æ€§</strong> â†’ RNN</li>
          <li><strong>åˆ†å­</strong>ä¸­çš„ä½ç½®å’ŒåŠ¨é‡ â†’ <strong>æ—‹è½¬å¯¹ç§°æ€§</strong> â†’ ç­‰å˜ç½‘ç»œ</li>
        </ul>
        <p>æœ¬ä¹¦çš„æ ¸å¿ƒè®ºç‚¹æ˜¯ï¼šè¿™äº›çœ‹ä¼¼ä¸åŒçš„æ¶æ„å®é™…ä¸Šæ˜¯<strong>åŒä¸€ä¸ªåº•å±‚åŸåˆ™â€”â€”å‡ ä½•æ­£åˆ™æ€§</strong>çš„è‡ªç„¶å®ä¾‹ã€‚</p>
      </div>
      <div class="en">
        Exploiting the known symmetries of a large system is a powerful and classical remedy against the curse of dimensionality, and forms the basis of most physical theories. Deep learning systems are no exception. Throughout our exposition, we will describe these models as natural instances of the same underlying principle of geometric regularity.
      </div>
    </div>

    <div class="math-block">
      $$\text{å¯¹ç§°æ€§} \xrightarrow{\text{çº¦æŸ}} \text{å‡½æ•°ç©ºé—´} \xrightarrow{\text{å‡å°‘}} \text{éœ€è¦çš„æ ·æœ¬æ•°} \xrightarrow{\text{æå‡}} \text{æ³›åŒ–æ€§èƒ½}$$
      <div class="math-explain">
        æ ¸å¿ƒé€»è¾‘é“¾ï¼šå¯¹ç§°æ€§çº¦æŸäº†å¯èƒ½çš„å‡½æ•°ç©ºé—´ï¼ˆå‡è®¾ç±»ï¼‰ï¼Œä½¿å¾—å­¦ä¹ ç®—æ³•ä¸éœ€è¦ä»æ‰€æœ‰å¯èƒ½çš„å‡½æ•°ä¸­æœç´¢ï¼Œè€Œåªéœ€åœ¨ä¸€ä¸ª<strong>æ›´å°ä½†æ›´æ­£ç¡®</strong>çš„å‡½æ•°ç±»ä¸­æœç´¢ã€‚è¿™ç›´æ¥å¯¼è‡´æ›´å¥½çš„<strong>æ ·æœ¬æ•ˆç‡</strong>å’Œ<strong>æ³›åŒ–æ€§èƒ½</strong>ã€‚
      </div>
    </div>

    <p>è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç›´è§‰æ€§çš„ä¾‹å­æ¥ç†è§£ï¼š</p>

    <pre><code># å¯¹ç§°æ€§å¦‚ä½•å‡å°‘å‚æ•°é‡çš„ç›´è§‰
import numpy as np

# ä¸åˆ©ç”¨å¯¹ç§°æ€§ï¼šå…¨è¿æ¥å±‚
# è¾“å…¥: 32x32 å›¾åƒ = 1024 ç»´
# è¾“å‡º: 1024 ç»´
fc_params = 1024 * 1024  # = 1,048,576 å‚æ•°

# åˆ©ç”¨å¹³ç§»å¯¹ç§°æ€§ï¼šå·ç§¯å±‚
# 3x3 å·ç§¯æ ¸ï¼Œå…±äº«æƒé‡
conv_params = 3 * 3  # = 9 å‚æ•°ï¼ˆå‡å°‘äº† ~100,000 å€ï¼ï¼‰

print(f"å…¨è¿æ¥å±‚å‚æ•°: {fc_params:,}")
print(f"å·ç§¯å±‚å‚æ•°: {conv_params}")
print(f"å‡å°‘å€æ•°: {fc_params / conv_params:.0f}x")

# ä½†å·ç§¯å±‚å¹¶æ²¡æœ‰æŸå¤±"æœ‰ç”¨çš„"è¡¨è¾¾èƒ½åŠ›
# å› ä¸ºå›¾åƒçš„ç»Ÿè®¡è§„å¾‹ç¡®å®å…·æœ‰å¹³ç§»ä¸å˜æ€§
# è¿™å°±æ˜¯"å¯¹ç§°æ€§ â†’ æ›´å¥½çš„å½’çº³åç½®"çš„æ ¸å¿ƒæ€æƒ³</code></pre>

    <!-- ========= ä¸¤ä¸ªæ ¸å¿ƒåŸåˆ™ ========= -->
    <h2 id="two-principles">ä¸¤ä¸ªæ ¸å¿ƒç®—æ³•åŸåˆ™<br><span style="font-size:0.7em;color:var(--text-secondary)">Two Algorithmic Principles</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>æ·±åº¦å­¦ä¹ çš„æœ¬è´¨å»ºç«‹åœ¨ä¸¤ä¸ªç®€å•çš„ç®—æ³•åŸåˆ™ä¹‹ä¸Šï¼š</p>
      </div>
      <div class="en">
        The essence of deep learning is built from two simple algorithmic principles.
      </div>
    </div>

    <h3 id="representation-learning">åŸåˆ™ä¸€ï¼šè¡¨å¾å­¦ä¹  (Representation Learning)</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">è¡¨å¾å­¦ä¹ </span>ï¼ˆæˆ–<span class="term">ç‰¹å¾å­¦ä¹ </span>ï¼‰æ˜¯æŒ‡é€šè¿‡è‡ªé€‚åº”çš„ã€é€šå¸¸æ˜¯å±‚çº§åŒ–çš„ç‰¹å¾æ¥æ•è·æ¯ä¸ªä»»åŠ¡çš„é€‚å½“æ­£åˆ™æ€§æ¦‚å¿µã€‚ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸­éœ€è¦äººå·¥è®¾è®¡ç‰¹å¾ä¸åŒï¼Œæ·±åº¦å­¦ä¹ <strong>è‡ªåŠ¨å­¦ä¹ </strong>æ•°æ®è¡¨å¾ã€‚</p>
      </div>
      <div class="en">
        Representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task.
      </div>
    </div>

    <div class="math-block">
      $$x \xrightarrow{f_1} h^{(1)} \xrightarrow{f_2} h^{(2)} \xrightarrow{\cdots} h^{(L)} \xrightarrow{g} \hat{y}$$
      <div class="math-explain">
        <strong>å±‚çº§è¡¨å¾</strong>ï¼šè¾“å…¥ $x$ ç»è¿‡å¤šå±‚éçº¿æ€§å˜æ¢ $f_1, f_2, \ldots$ï¼Œé€æ­¥æå–è¶Šæ¥è¶ŠæŠ½è±¡çš„ç‰¹å¾ $h^{(k)}$ï¼Œæœ€ç»ˆé€šè¿‡è¾“å‡ºå±‚ $g$ åšå‡ºé¢„æµ‹ $\hat{y}$ã€‚ä¾‹å¦‚åœ¨å›¾åƒè¯†åˆ«ä¸­ï¼š$h^{(1)}$ æ•è·è¾¹ç¼˜ï¼Œ$h^{(2)}$ æ•è·çº¹ç†/å½¢çŠ¶ï¼Œ$h^{(3)}$ æ•è·å¯¹è±¡éƒ¨ä»¶ï¼Œ$h^{(L)}$ æ•è·å®Œæ•´å¯¹è±¡ã€‚
      </div>
    </div>

    <pre><code># å±‚çº§ç‰¹å¾å­¦ä¹ çš„ç›´è§‰
import torch
import torch.nn as nn

class HierarchicalFeatureLearner(nn.Module):
    """å±‚çº§è¡¨å¾å­¦ä¹ çš„ç®€å•ç¤ºä¾‹"""
    def __init__(self):
        super().__init__()
        # æ¯ä¸€å±‚å­¦ä¹ è¶Šæ¥è¶ŠæŠ½è±¡çš„ç‰¹å¾
        self.layer1 = nn.Conv2d(3, 32, 3, padding=1)    # ä½çº§: è¾¹ç¼˜ã€é¢œè‰²
        self.layer2 = nn.Conv2d(32, 64, 3, padding=1)   # ä¸­çº§: çº¹ç†ã€å½¢çŠ¶
        self.layer3 = nn.Conv2d(64, 128, 3, padding=1)  # é«˜çº§: å¯¹è±¡éƒ¨ä»¶
        self.classifier = nn.Linear(128, 10)             # åˆ†ç±»: å®Œæ•´å¯¹è±¡
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        h1 = self.relu(self.layer1(x))   # h^(1): è¾¹ç¼˜ç‰¹å¾
        h2 = self.relu(self.layer2(h1))  # h^(2): çº¹ç†ç‰¹å¾
        h3 = self.relu(self.layer3(h2))  # h^(3): å¯¹è±¡ç‰¹å¾
        pooled = self.pool(h3).flatten(1)
        return self.classifier(pooled)

# å…³é”®æ´å¯Ÿ: è¿™äº›ç‰¹å¾ä¸æ˜¯äººå·¥è®¾è®¡çš„ï¼Œ
# è€Œæ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™è‡ªåŠ¨å­¦ä¹ çš„ï¼</code></pre>

    <h3 id="gradient-descent">åŸåˆ™äºŒï¼šå±€éƒ¨æ¢¯åº¦ä¸‹é™ (Backpropagation)</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ç¬¬äºŒä¸ªåŸåˆ™æ˜¯é€šè¿‡<span class="term">å±€éƒ¨æ¢¯åº¦ä¸‹é™</span>è¿›è¡Œå­¦ä¹ ï¼Œé€šå¸¸ä»¥<span class="term">åå‘ä¼ æ’­</span>ï¼ˆbackpropagationï¼‰çš„å½¢å¼å®ç°ã€‚ç»™å®šæŸå¤±å‡½æ•° $\mathcal{L}(\theta)$ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—æ¢¯åº¦ $\nabla_\theta \mathcal{L}$ å¹¶æ²¿è´Ÿæ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°æ¥æœ€å°åŒ–æŸå¤±ã€‚</p>
      </div>
      <div class="en">
        Learning by local gradient-descent, typically implemented as backpropagation.
      </div>
    </div>

    <div class="math-block">
      $$\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta \mathcal{L}(\theta^{(t)})$$
      <div class="math-explain">
        <strong>æ¢¯åº¦ä¸‹é™</strong>ï¼š$\theta$ æ˜¯æ¨¡å‹å‚æ•°ï¼Œ$\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\mathcal{L}$ æ˜¯æŸå¤±å‡½æ•°ã€‚åå‘ä¼ æ’­é€šè¿‡é“¾å¼æ³•åˆ™é«˜æ•ˆè®¡ç®— $\nabla_\theta \mathcal{L}$ã€‚è¿™ä¸¤ä¸ªåŸåˆ™ï¼ˆè¡¨å¾å­¦ä¹  + æ¢¯åº¦ä¸‹é™ï¼‰åœ¨å„ç§æ¶æ„ä¸­æ˜¯<strong>é€šç”¨çš„</strong>â€”â€”æ— è®ºæ˜¯ CNNã€RNNã€GNN è¿˜æ˜¯ Transformerï¼Œéƒ½ä½¿ç”¨åŒæ ·çš„å­¦ä¹ æœºåˆ¶ã€‚
      </div>
    </div>

    <h3 id="curse-overview">ç»´åº¦ç¾éš¾é¢„è§ˆ</h3>

    <div class="bilingual">
      <div class="zh">
        <p>è™½ç„¶åœ¨é«˜ç»´ç©ºé—´ä¸­å­¦ä¹ é€šç”¨å‡½æ•°æ˜¯ä¸€ä¸ª<strong>è¢«è¯…å’’çš„</strong>ä¼°è®¡é—®é¢˜ï¼ˆéœ€è¦æŒ‡æ•°çº§çš„æ ·æœ¬ï¼‰ï¼Œä½†å¤§å¤šæ•°å®é™…ä»»åŠ¡å¹¶ä¸æ˜¯é€šç”¨çš„ã€‚å®ƒä»¬æ¥è‡ªç‰©ç†ä¸–ç•Œåº•å±‚çš„<strong>ä½ç»´æ€§</strong>å’Œ<strong>ç»“æ„æ€§</strong>ï¼Œå¸¦æœ‰æœ¬è´¨çš„é¢„å®šä¹‰æ­£åˆ™æ€§ã€‚</p>
        <p>æœ¬ä¹¦çš„æ ¸å¿ƒç›®æ ‡å°±æ˜¯ï¼šé€šè¿‡<strong>ç»Ÿä¸€çš„å‡ ä½•åŸåˆ™</strong>æš´éœ²è¿™äº›æ­£åˆ™æ€§ï¼Œå¹¶å°†å®ƒä»¬åº”ç”¨äºå¹¿æ³›çš„åº”ç”¨é¢†åŸŸã€‚</p>
      </div>
      <div class="en">
        While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.
      </div>
    </div>

    <div class="callout callout-warning">
      <h4>ç»´åº¦ç¾éš¾çš„ç›´è§‰</h4>
      <p>åœ¨ $d$ ç»´ç©ºé—´ä¸­ï¼Œå¦‚æœæˆ‘ä»¬åœ¨æ¯ä¸ªç»´åº¦ä¸Šå– $m$ ä¸ªæ ·æœ¬ç‚¹ï¼Œæ€»å…±éœ€è¦ $m^d$ ä¸ªæ ·æœ¬ã€‚å¯¹äº $d = 100$ï¼ˆä¸€å¼  10Ã—10 çš„ç°åº¦å›¾ï¼‰ï¼Œå³ä½¿æ¯ç»´åªå– 2 ä¸ªç‚¹ï¼Œä¹Ÿéœ€è¦ $2^{100} \approx 10^{30}$ ä¸ªæ ·æœ¬â€”â€”è¿œè¶…å®‡å®™ä¸­çš„åŸå­æ•°ï¼ˆ$\sim 10^{80}$...ç­‰ç­‰ï¼Œè™½ç„¶æ²¡è¶…è¿‡ï¼Œä½†è¿™ä»…ä»…æ˜¯ 10Ã—10 çš„å›¾åƒï¼å¯¹äºç°å®çš„ 224Ã—224 RGB å›¾åƒï¼Œç»´åº¦æ˜¯ $224 \times 224 \times 3 = 150{,}528$ï¼‰ã€‚</p>
      <p><strong>è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å½’çº³åç½®</strong>â€”â€”åˆ©ç”¨æ•°æ®çš„ç»“æ„æ¥çº¦æŸæœç´¢ç©ºé—´ã€‚è¯¦è§ <a href="../chapter2/index.html">Chapter 2</a>ã€‚</p>
    </div>

    <!-- ========= 5G æ¡†æ¶ ========= -->
    <h2 id="5g-framework">5G ç»Ÿä¸€æ¡†æ¶<br><span style="font-size:0.7em;color:var(--text-secondary)">The 5G Framework: Grids, Groups, Graphs, Geodesics, Gauges</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>æœ¬ä¹¦æå‡ºçš„<strong>"å‡ ä½•ç»Ÿä¸€"</strong>æ–¹æ¡ˆâ€”â€”ä»¥ Erlangen Program çš„ç²¾ç¥â€”â€”æœ‰åŒé‡ç›®çš„ï¼šä¸€æ–¹é¢ï¼Œå®ƒæä¾›äº†ä¸€ä¸ª<strong>å…±åŒçš„æ•°å­¦æ¡†æ¶</strong>æ¥ç ”ç©¶æœ€æˆåŠŸçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼›å¦ä¸€æ–¹é¢ï¼Œå®ƒæä¾›äº†ä¸€ä¸ª<strong>å»ºè®¾æ€§ç¨‹åº</strong>æ¥å°†å…ˆéªŒç‰©ç†çŸ¥è¯†èå…¥ç¥ç»æ¶æ„ï¼Œå¹¶ä¸ºè®¾è®¡æœªæ¥çš„æ–°æ¶æ„æä¾›åŸåˆ™æ€§æ–¹æ³•ã€‚</p>
        <p>æˆ‘ä»¬å°†è¿™ä¸ªæ¡†æ¶ç§°ä¸º <strong>5G</strong>ï¼šäº”ä¸ªä»¥ G å¼€å¤´çš„å‡ ä½•åŸŸï¼Œè¦†ç›–äº†å‡ ä¹æ‰€æœ‰çš„æ•°æ®ç»“æ„ã€‚</p>
      </div>
      <div class="en">
        Such a 'geometric unification' endeavour in the spirit of the Erlangen Program serves a dual purpose: providing a common mathematical framework to study the most successful architectures, and giving a constructive procedure to incorporate prior physical knowledge into neural architectures.
      </div>
    </div>

    <div class="figure">
      <img src="../assets/misc_p5_img0.png" alt="5G Framework overview">
      <figcaption>5G æ¡†æ¶æ¦‚è§ˆï¼šäº”ç§å‡ ä½•åŸŸåŠå…¶å¯¹åº”çš„å¯¹ç§°æ€§å’Œç¥ç»ç½‘ç»œæ¶æ„ã€‚ä»å·¦åˆ°å³ï¼Œå¯¹ç§°æ€§ä»å¤§åˆ°å°ï¼Œç»“æ„ä»é€šç”¨åˆ°ç‰¹æ®Šã€‚</figcaption>
    </div>

    <h3 id="grids">ğŸ”² Grids â€” ç½‘æ ¼</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">ç½‘æ ¼</span>æ˜¯æœ€ç®€å•ä¹Ÿæ˜¯æœ€æˆåŠŸçš„å‡ ä½•åŸŸã€‚å›¾åƒæ˜¯ 2D ç½‘æ ¼ä¸Šçš„ä¿¡å·ï¼Œè¯­éŸ³å’Œæ–‡æœ¬æ˜¯ 1D ç½‘æ ¼ä¸Šçš„ä¿¡å·ã€‚ç½‘æ ¼çš„å…³é”®å¯¹ç§°æ€§æ˜¯<strong>å¹³ç§»å¯¹ç§°æ€§</strong>ï¼ˆtranslation symmetryï¼‰ã€‚</p>
      </div>
      <div class="en">
        Grids are the simplest and most successful geometric domain. Images are signals on 2D grids, speech/text on 1D grids. The key symmetry is translation.
      </div>
    </div>

    <div class="math-block">
      $$\Omega = \mathbb{Z}^d, \quad G = (\mathbb{Z}^d, +), \quad \text{å¯¹åº”æ¶æ„: CNN}$$
      <div class="math-explain">
        <strong>åŸŸ</strong>ï¼š$d$ ç»´æ•´æ•°ç½‘æ ¼ã€‚<strong>å¯¹ç§°ç¾¤</strong>ï¼šæ•´æ•°å¹³ç§»ç¾¤ã€‚<strong>ç­‰å˜æ“ä½œ</strong>ï¼šç¦»æ•£å·ç§¯ã€‚CNN çš„æˆåŠŸæ­£æ˜¯å› ä¸ºå®ƒ<strong>æ­£ç¡®åˆ©ç”¨</strong>äº†å›¾åƒåŸŸçš„å¹³ç§»å¯¹ç§°æ€§ã€‚å·ç§¯æ ¸åœ¨æ‰€æœ‰ä½ç½®å…±äº«æƒé‡ï¼ˆæƒé‡å…±äº« = å¹³ç§»ç­‰å˜æ€§ï¼‰ã€‚
      </div>
    </div>

    <pre><code># ç½‘æ ¼åŸŸ: å›¾åƒå·ç§¯
import torch
import torch.nn.functional as F

# 3x3 å·ç§¯æ ¸ (æƒé‡å…±äº« = å¹³ç§»ç­‰å˜æ€§)
kernel = torch.tensor([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1]
], dtype=torch.float).view(1, 1, 3, 3)

# åŒä¸€ä¸ªæ ¸åº”ç”¨äºå›¾åƒçš„æ¯ä¸ªä½ç½®
# è¿™å°±æ˜¯ "å¹³ç§»ç­‰å˜æ€§" â€” å¦‚æœè¾“å…¥å¹³ç§»ï¼Œè¾“å‡ºä¹Ÿå¹³ç§»
image = torch.randn(1, 1, 28, 28)  # MNIST-like
output = F.conv2d(image, kernel, padding=1)
print(f"è¾“å…¥å½¢çŠ¶: {image.shape}, è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"å·ç§¯æ ¸å‚æ•°: {kernel.numel()} (vs å…¨è¿æ¥: {28*28*28*28:,})")</code></pre>

    <h3 id="groups">ğŸ‘¥ Groups â€” ç¾¤</h3>

    <div class="bilingual">
      <div class="zh">
        <p>å½“åŸŸä¸ä»…æœ‰å¹³ç§»å¯¹ç§°æ€§ï¼Œè¿˜æœ‰<strong>æ—‹è½¬</strong>ã€<strong>åå°„</strong>ç­‰æ›´ä¸°å¯Œçš„å¯¹ç§°æ€§æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘<span class="term">ç¾¤</span>åŸŸä¸Šçš„ä¿¡å·å¤„ç†ã€‚å…¸å‹ä¾‹å­åŒ…æ‹¬çƒé¢ä¸Šçš„ä¿¡å·ï¼ˆå¤©æ–‡å­¦ã€æ°”è±¡å­¦ï¼‰å’Œåˆ†å­ç³»ç»Ÿã€‚</p>
      </div>
      <div class="en">
        When the domain has richer symmetries beyond translation (rotation, reflection, etc.), we need signal processing on group domains.
      </div>
    </div>

    <div class="math-block">
      $$\Omega = G \text{ (ç¾¤æœ¬èº«)}, \quad \text{å¯¹ç§°ç¾¤: } G \text{ è‡ªèº«çš„å·¦/å³ä½œç”¨}$$
      $$\text{å¯¹åº”æ¶æ„: Group Equivariant CNN, Spherical CNN}$$
      <div class="math-explain">
        åœ¨ç¾¤åŸŸä¸Šï¼Œä¿¡å·å®šä¹‰ä¸º $f : G \to \mathbb{R}^c$ã€‚ç¾¤å·ç§¯å®šä¹‰ä¸ºï¼š$(f \star \psi)(g) = \int_G f(g'^{-1}g)\psi(g')dg'$ã€‚è¿™ä¿è¯äº†å¯¹ç¾¤ä½œç”¨çš„ç­‰å˜æ€§ã€‚ä¾‹å¦‚ï¼ŒSO(3)-ç­‰å˜çš„ç½‘ç»œåœ¨åˆ†å­æ€§è´¨é¢„æµ‹ä¸­è‡³å…³é‡è¦ã€‚
      </div>
    </div>

    <h3 id="graphs">ğŸ”— Graphs â€” å›¾</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">å›¾</span>å¯èƒ½æ˜¯æœ€é€šç”¨çš„éæ¬§å‡ é‡Œå¾—æ•°æ®ç»“æ„ã€‚ç¤¾äº¤ç½‘ç»œã€åˆ†å­ã€çŸ¥è¯†å›¾è°±ã€äº¤é€šç½‘ç»œâ€”â€”å®ƒä»¬éƒ½æ˜¯å›¾ã€‚å›¾çš„å…³é”®å¯¹ç§°æ€§æ˜¯<strong>èŠ‚ç‚¹ç½®æ¢å¯¹ç§°æ€§</strong>ï¼šå›¾çš„æ€§è´¨ä¸åº”è¯¥ä¾èµ–äºèŠ‚ç‚¹çš„ç¼–å·é¡ºåºã€‚</p>
      </div>
      <div class="en">
        Graphs are perhaps the most general non-Euclidean data structure. The key symmetry is node permutation: graph properties should not depend on the ordering of nodes.
      </div>
    </div>

    <div class="math-block">
      $$\Omega = (\mathcal{V}, \mathcal{E}), \quad G = \Sigma_n, \quad \text{å¯¹åº”æ¶æ„: GNN (æ¶ˆæ¯ä¼ é€’)}$$
      <div class="math-explain">
        <strong>åŸŸ</strong>ï¼šå›¾ $(\mathcal{V}, \mathcal{E})$ï¼ŒèŠ‚ç‚¹é›† $\mathcal{V}$ï¼Œè¾¹é›† $\mathcal{E}$ã€‚<strong>å¯¹ç§°ç¾¤</strong>ï¼šèŠ‚ç‚¹ç½®æ¢ç¾¤ $\Sigma_n$ã€‚<strong>ç­‰å˜æ“ä½œ</strong>ï¼šæ¶ˆæ¯ä¼ é€’ï¼ˆMessage Passingï¼‰ã€‚GNN é€šè¿‡é‚»åŸŸèšåˆï¼ˆ$h_i \leftarrow \text{AGG}(\{h_j : j \in \mathcal{N}(i)\})$ï¼‰å®ç°ç½®æ¢ç­‰å˜æ€§ã€‚
      </div>
    </div>

    <pre><code># å›¾åŸŸ: æ¶ˆæ¯ä¼ é€’ GNN
import torch

def message_passing(node_features, edge_index, message_fn, update_fn):
    """
    åŸºæœ¬çš„æ¶ˆæ¯ä¼ é€’æ“ä½œ
    - ç½®æ¢ç­‰å˜: é‡æ–°æ’åˆ—èŠ‚ç‚¹ä¸æ”¹å˜è®¡ç®—ç»“æœ
    """
    num_nodes = node_features.size(0)
    messages = torch.zeros_like(node_features)

    for src, dst in edge_index.t():
        # ä»é‚»å±… src å‘èŠ‚ç‚¹ dst å‘é€æ¶ˆæ¯
        msg = message_fn(node_features[src], node_features[dst])
        messages[dst] += msg  # èšåˆ (æ±‚å’Œæ˜¯ç½®æ¢ä¸å˜çš„!)

    # æ›´æ–°èŠ‚ç‚¹ç‰¹å¾
    new_features = update_fn(node_features, messages)
    return new_features

# æ¶ˆæ¯ä¼ é€’æ˜¯ç½®æ¢ç­‰å˜çš„:
# å¦‚æœæˆ‘ä»¬æ‰“ä¹±èŠ‚ç‚¹é¡ºåº, ç»“æœåªæ˜¯ç›¸åº”åœ°æ‰“ä¹±
# è€Œä¸ä¼šæ”¹å˜æ¯ä¸ªèŠ‚ç‚¹çš„å®é™…è®¡ç®—ç»“æœ</code></pre>

    <h3 id="geodesics">ğŸ“ Geodesics â€” æµ‹åœ°çº¿</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">æµ‹åœ°çº¿</span>ï¼ˆGeodesicsï¼‰æŒ‡çš„æ˜¯<strong>æµå½¢</strong>ï¼ˆmanifoldï¼‰å’Œ<strong>ç½‘æ ¼</strong>ï¼ˆmeshï¼‰ä¸Šçš„<strong>å†…è•´åº¦é‡</strong>å’Œ<strong>ç­‰è·å¯¹ç§°æ€§</strong>ã€‚å…¸å‹ä¾‹å­æ˜¯è›‹ç™½è´¨è¡¨é¢ã€äººä½“å½¢çŠ¶ç­‰ï¼Œå…¶ä¸­å½¢çŠ¶çš„å†…è•´å‡ ä½•ï¼ˆå¦‚æ›²ç‡ã€æµ‹åœ°è·ç¦»ï¼‰æ¯”å¤–åœ¨åµŒå…¥æ›´é‡è¦ã€‚</p>
      </div>
      <div class="en">
        Geodesics refer to intrinsic metrics and isometric symmetries on manifolds and meshes. Shape analysis (e.g., protein surfaces, body shapes) relies on intrinsic geometry rather than extrinsic embedding.
      </div>
    </div>

    <div class="math-block">
      $$\Omega = \mathcal{M} \text{ (æµå½¢)}, \quad G = \text{Iso}(\mathcal{M}), \quad \text{å¯¹åº”æ¶æ„: Mesh CNN, Geodesic CNN}$$
      <div class="math-explain">
        åœ¨æµå½¢ä¸Šï¼Œ"è·ç¦»" ç”±æµ‹åœ°çº¿ï¼ˆè¡¨é¢ä¸Šçš„æœ€çŸ­è·¯å¾„ï¼‰å®šä¹‰ï¼Œè€Œä¸æ˜¯ç›´çº¿è·ç¦»ã€‚ç­‰è·å¯¹ç§°ç¾¤ $\text{Iso}(\mathcal{M})$ ä¿æŒæµ‹åœ°è·ç¦»ä¸å˜ã€‚ä¾‹å¦‚ï¼Œå½“äººä½“å¼¯æ›²æ‰‹è‡‚æ—¶ï¼Œçš®è‚¤è¡¨é¢ä¸Šä¸¤ç‚¹çš„æµ‹åœ°è·ç¦»ä¸å˜ï¼ˆç­‰è·å˜æ¢ï¼‰ï¼Œä½†æ¬§å‡ é‡Œå¾—è·ç¦»å˜äº†ã€‚
      </div>
    </div>

    <h3 id="gauges">ğŸ”¬ Gauges â€” è§„èŒƒ</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">è§„èŒƒ</span>ï¼ˆGaugeï¼‰æ˜¯æœ€ç²¾ç»†çš„å‡ ä½•ç»“æ„ï¼Œæ¶‰åŠæµå½¢ä¸Šçš„<strong>å‚è€ƒæ¡†æ¶</strong>ï¼ˆframeï¼‰å’Œ<strong>çº¤ç»´ä¸›</strong>ï¼ˆfiber bundleï¼‰ã€‚åœ¨æµå½¢ä¸Šå®šä¹‰å·ç§¯éœ€è¦é€‰æ‹©å±€éƒ¨å‚è€ƒæ–¹å‘â€”â€”è¿™ä¸ªé€‰æ‹©å°±æ˜¯"è§„èŒƒ"ã€‚è§„èŒƒç­‰å˜æ„å‘³ç€è®¡ç®—ç»“æœä¸ä¾èµ–äºè¿™ä¸ªä»»æ„é€‰æ‹©ã€‚</p>
      </div>
      <div class="en">
        Gauges involve reference frames on manifolds and fiber bundles. Defining convolution on manifolds requires choosing local reference directions â€” this choice is a "gauge". Gauge equivariance means computations are independent of this arbitrary choice.
      </div>
    </div>

    <div class="math-block">
      $$\Omega = (\mathcal{M}, \text{gauge}), \quad G = \text{Structure group of the bundle}$$
      $$\text{å¯¹åº”æ¶æ„: Gauge Equivariant CNN}$$
      <div class="math-explain">
        è§„èŒƒç­‰å˜ CNN æ˜¯æœ€é€šç”¨çš„å‡ ä½• CNN å½¢å¼ã€‚å®ƒåœ¨æµå½¢çš„æ¯ä¸ªç‚¹ä¸Šå®šä¹‰å±€éƒ¨æ»¤æ³¢å™¨ï¼Œå¹¶ç¡®ä¿ç»“æœä¸ä¾èµ–äºå±€éƒ¨åæ ‡ç³»çš„é€‰æ‹©ã€‚è¿™ä¸ç‰©ç†å­¦ä¸­çš„è§„èŒƒç†è®ºï¼ˆå¦‚ç”µç£å­¦ä¸­çš„ U(1) è§„èŒƒä¸å˜æ€§ï¼‰æœ‰æ·±åˆ»çš„ç±»æ¯”ã€‚
      </div>
    </div>

    <div class="callout callout-info">
      <h4>5G ä¹‹é—´çš„å…³ç³»</h4>
      <table>
        <thead>
          <tr><th>åŸŸ</th><th>å¯¹ç§°æ€§</th><th>å…¸å‹æ¶æ„</th><th>åº”ç”¨ä¾‹å­</th></tr>
        </thead>
        <tbody>
          <tr><td>é›†åˆ (Sets)</td><td>$\Sigma_n$ (ç½®æ¢)</td><td>DeepSets, Transformer</td><td>ç‚¹äº‘ã€NLP</td></tr>
          <tr><td>ç½‘æ ¼ (Grids)</td><td>$\mathbb{Z}^d$ (å¹³ç§»)</td><td>CNN, RNN</td><td>å›¾åƒã€è¯­éŸ³</td></tr>
          <tr><td>ç¾¤ (Groups)</td><td>$G$ (ç¾¤ä½œç”¨)</td><td>Group CNN, Spherical CNN</td><td>å¤©æ–‡å­¦ã€åˆ†å­</td></tr>
          <tr><td>å›¾ (Graphs)</td><td>$\Sigma_n$ (èŠ‚ç‚¹ç½®æ¢)</td><td>GCN, GAT, MPNN</td><td>ç¤¾äº¤ç½‘ç»œã€åˆ†å­</td></tr>
          <tr><td>æµå½¢ (Manifolds)</td><td>Iso($\mathcal{M}$) / Gauge</td><td>Mesh CNN, Gauge CNN</td><td>è›‹ç™½è´¨è¡¨é¢ã€3Då½¢çŠ¶</td></tr>
        </tbody>
      </table>
      <p>è¿™äº›åŸŸå½¢æˆä¸€ä¸ª<strong>å±‚çº§ç»“æ„</strong>ï¼šé›†åˆ âŠƒ å›¾ âŠƒ ç½‘æ ¼ï¼›æµå½¢ âŠƒ ç½‘æ ¼ã€‚5G æ¡†æ¶ç”¨<strong>å¯¹ç§°æ€§</strong>è¿™ä¸€ç»Ÿä¸€è¯­è¨€æè¿°æ‰€æœ‰è¿™äº›åŸŸã€‚</p>
    </div>

    <!-- ========= Erlangen Program ========= -->
    <h2 id="erlangen">Erlangen Program çš„ç²¾ç¥<br><span style="font-size:0.7em;color:var(--text-secondary)">The Spirit of the Erlangen Programme</span></h2>

    <h3 id="erlangen-history">å†å²èƒŒæ™¯</h3>

    <div class="bilingual">
      <div class="zh">
        <p>1872 å¹´ï¼Œå¹´ä»… 23 å²çš„æ•°å­¦å®¶ <strong>Felix Klein</strong> åœ¨å¾·å›½ Erlangen å¤§å­¦çš„å°±èŒæ¼”è®²ä¸­æå‡ºäº†è‘—åçš„ <span class="term">Erlangen Program</span>ï¼ˆErlangen çº²é¢†ï¼‰ã€‚ä»–çš„é©å‘½æ€§æƒ³æ³•æ˜¯ï¼šä¸åŒçš„<strong>å‡ ä½•å­¦</strong>å¯ä»¥é€šè¿‡å…¶<strong>å¯¹ç§°ç¾¤</strong>æ¥åˆ†ç±»å’Œç»Ÿä¸€ã€‚</p>
        <ul>
          <li><strong>æ¬§å‡ é‡Œå¾—å‡ ä½•</strong> = åœ¨æ¬§å‡ é‡Œå¾—ç¾¤ $E(d)$ ä¸‹ä¸å˜çš„æ€§è´¨</li>
          <li><strong>ä»¿å°„å‡ ä½•</strong> = åœ¨ä»¿å°„ç¾¤ä¸‹ä¸å˜çš„æ€§è´¨</li>
          <li><strong>å°„å½±å‡ ä½•</strong> = åœ¨å°„å½±ç¾¤ä¸‹ä¸å˜çš„æ€§è´¨</li>
          <li><strong>æ‹“æ‰‘å­¦</strong> = åœ¨åŒèƒšç¾¤ä¸‹ä¸å˜çš„æ€§è´¨</li>
        </ul>
        <p>Klein çš„æ ¸å¿ƒæ´å¯Ÿï¼š<strong>å‡ ä½•å°±æ˜¯ç ”ç©¶åœ¨æŸä¸ªå˜æ¢ç¾¤ä¸‹ä¸å˜çš„æ€§è´¨</strong>ã€‚</p>
      </div>
      <div class="en">
        In 1872, the 23-year-old mathematician Felix Klein proclaimed group theory to be the organising principle of geometry in his famous Erlangen Programme. His key insight: geometry is the study of properties invariant under a transformation group.
      </div>
    </div>

    <h3 id="erlangen-ml">å¯¹æœºå™¨å­¦ä¹ çš„å¯ç¤º</h3>

    <div class="bilingual">
      <div class="zh">
        <p>æœ¬ä¹¦å°† Erlangen Program çš„ç²¾ç¥åº”ç”¨äºæ·±åº¦å­¦ä¹ ï¼š<strong>ä¸åŒçš„æ·±åº¦å­¦ä¹ æ¶æ„å¯ä»¥é€šè¿‡å…¶åˆ©ç”¨çš„å¯¹ç§°ç¾¤æ¥åˆ†ç±»å’Œç»Ÿä¸€</strong>ã€‚</p>
        <ul>
          <li><strong>CNN</strong> = åˆ©ç”¨å¹³ç§»å¯¹ç§°ç¾¤ $(\mathbb{Z}^d, +)$ çš„ç½‘ç»œ</li>
          <li><strong>GNN</strong> = åˆ©ç”¨ç½®æ¢å¯¹ç§°ç¾¤ $\Sigma_n$ çš„ç½‘ç»œ</li>
          <li><strong>Transformer</strong> = åœ¨å®Œå…¨å›¾ä¸Šåˆ©ç”¨ç½®æ¢å¯¹ç§°æ€§çš„ç½‘ç»œ</li>
          <li><strong>Spherical CNN</strong> = åˆ©ç”¨æ—‹è½¬å¯¹ç§°ç¾¤ $SO(3)$ çš„ç½‘ç»œ</li>
        </ul>
        <p>è¿™ä¸ä»…æ˜¯äº‹åçš„"ç†è§£"ï¼Œæ›´æ˜¯<strong>å‰ç»æ€§çš„è®¾è®¡åŸåˆ™</strong>ï¼šç»™å®šä¸€ä¸ªæ–°é—®é¢˜ï¼Œç¡®å®šå…¶æ•°æ®çš„å¯¹ç§°æ€§ï¼Œç„¶åæ„å»ºç­‰å˜æ¶æ„ã€‚</p>
      </div>
      <div class="en">
        We apply the Erlangen Program spirit to deep learning: different architectures can be classified and unified by the symmetry group they exploit. This is not just retrospective understanding, but a prospective design principle.
      </div>
    </div>

    <div class="math-block">
      $$\text{Erlangen for ML: } \quad \fbox{é—®é¢˜} \xrightarrow{\text{è¯†åˆ«}} \fbox{å¯¹ç§°ç¾¤ } G \xrightarrow{\text{æ„å»º}} \fbox{G-ç­‰å˜æ¶æ„}$$
      <div class="math-explain">
        è¿™æ˜¯ GDL çš„<strong>å»ºè®¾æ€§ç¨‹åº</strong>ï¼š(1) åˆ†æé—®é¢˜çš„æ•°æ®åŸŸ $\Omega$ï¼›(2) ç¡®å®šç›¸å…³çš„å¯¹ç§°ç¾¤ $G$ï¼›(3) æ„å»º $G$-ç­‰å˜çš„ç¥ç»ç½‘ç»œå±‚ï¼›(4) é€šè¿‡å±€éƒ¨æ± åŒ–å’Œå…¨å±€æ± åŒ–å¾—åˆ° $G$-ä¸å˜çš„è¾“å‡ºã€‚
      </div>
    </div>

    <!-- ========= æ¶æ„æ™¯è§‚ ========= -->
    <h2 id="architectures">æ¶æ„æ™¯è§‚<br><span style="font-size:0.7em;color:var(--text-secondary)">The Landscape of Architectures</span></h2>

    <h3 id="cnn-intro">CNN å®¶æ— â€” ç½‘æ ¼ä¸Šçš„ç­‰å˜ç½‘ç»œ</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">å·ç§¯ç¥ç»ç½‘ç»œ</span>ï¼ˆCNNï¼‰æ˜¯æ·±åº¦å­¦ä¹ æœ€æ—©ä¹Ÿæ˜¯æœ€æˆåŠŸçš„æ¶æ„ä¹‹ä¸€ã€‚ä» GDL çš„è§†è§’çœ‹ï¼ŒCNN æ˜¯åœ¨<strong>$d$ ç»´ç½‘æ ¼</strong>ä¸Šåˆ©ç”¨<strong>å¹³ç§»å¯¹ç§°æ€§</strong>çš„ç­‰å˜ç½‘ç»œã€‚å…¶æ ¸å¿ƒæ“ä½œâ€”â€”å·ç§¯â€”â€”æ­£æ˜¯å”¯ä¸€çš„å¹³ç§»ç­‰å˜çº¿æ€§æ“ä½œã€‚</p>
        <ul>
          <li><strong>1D CNN</strong>ï¼šç”¨äºåºåˆ—æ•°æ®ï¼ˆWaveNet è¯­éŸ³åˆæˆã€æ—¶é—´åºåˆ—ï¼‰</li>
          <li><strong>2D CNN</strong>ï¼šç”¨äºå›¾åƒï¼ˆAlexNet â†’ VGG â†’ ResNet â†’ EfficientNetï¼‰</li>
          <li><strong>3D CNN</strong>ï¼šç”¨äºè§†é¢‘å’Œä½“ç§¯æ•°æ®ï¼ˆåŒ»å­¦ CT/MRIï¼‰</li>
        </ul>
      </div>
      <div class="en">
        CNNs are equivariant networks on d-dimensional grids exploiting translation symmetry. Convolution is the unique translation-equivariant linear operation.
      </div>
    </div>

    <h3 id="rnn-intro">RNN å®¶æ— â€” åºåˆ—ä¸Šçš„å¤„ç†</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">å¾ªç¯ç¥ç»ç½‘ç»œ</span>ï¼ˆRNNï¼‰å¤„ç†å¯å˜é•¿åº¦çš„åºåˆ—æ•°æ®ã€‚ä» GDL è§†è§’çœ‹ï¼ŒRNN å¯ä»¥è§†ä¸ºåœ¨ 1D ç½‘æ ¼ä¸Šåˆ©ç”¨äº†<strong>å› æœç»“æ„</strong>ï¼ˆåªçœ‹è¿‡å»ï¼Œä¸çœ‹æœªæ¥ï¼‰çš„æ¶æ„ã€‚LSTM å’Œ GRU é€šè¿‡é—¨æ§æœºåˆ¶è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚</p>
      </div>
      <div class="en">
        RNNs process variable-length sequences. From GDL perspective, they exploit causal structure on 1D grids. LSTM and GRU solve vanishing gradient via gating.
      </div>
    </div>

    <h3 id="gnn-intro">GNN å®¶æ— â€” å›¾ä¸Šçš„æ¶ˆæ¯ä¼ é€’</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">å›¾ç¥ç»ç½‘ç»œ</span>ï¼ˆGNNï¼‰æ˜¯ GDL çš„æ ¸å¿ƒæˆå‘˜ï¼Œå¤„ç†å›¾ç»“æ„æ•°æ®ã€‚å…¶åŸºæœ¬æ“ä½œæ˜¯<span class="term">æ¶ˆæ¯ä¼ é€’</span>ï¼ˆmessage passingï¼‰ï¼šæ¯ä¸ªèŠ‚ç‚¹èšåˆå…¶é‚»å±…çš„ä¿¡æ¯æ¥æ›´æ–°è‡ªèº«è¡¨ç¤ºã€‚è¿™ä¸ªæ“ä½œæ˜¯<strong>ç½®æ¢ç­‰å˜</strong>çš„ã€‚</p>
        <ul>
          <li><strong>GCN</strong>ï¼ˆKipf & Welling, 2016ï¼‰ï¼šè°±æ–¹æ³•çš„ç®€åŒ–</li>
          <li><strong>GAT</strong>ï¼ˆVeliÄkoviÄ‡ et al., 2018ï¼‰ï¼šæ³¨æ„åŠ›åŠ æƒçš„æ¶ˆæ¯ä¼ é€’</li>
          <li><strong>MPNN</strong>ï¼ˆGilmer et al., 2017ï¼‰ï¼šç»Ÿä¸€æ¶ˆæ¯ä¼ é€’æ¡†æ¶</li>
          <li><strong>GNS</strong>ï¼ˆSanchez-Gonzalez et al., 2020ï¼‰ï¼šç”¨äºç‰©ç†ä»¿çœŸ</li>
        </ul>
      </div>
      <div class="en">
        GNNs are the core of GDL, processing graph-structured data via message passing â€” a permutation-equivariant operation.
      </div>
    </div>

    <h3 id="transformer-intro">Transformer â€” é›†åˆä¸Šçš„å…¨å±€æ³¨æ„åŠ›</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">Transformer</span> ä» GDL è§†è§’å¯ä»¥ç†è§£ä¸ºåœ¨<strong>å®Œå…¨å›¾</strong>ä¸Šçš„ GNNâ€”â€”æ¯ä¸ªå…ƒç´ éƒ½å…³æ³¨æ‰€æœ‰å…¶ä»–å…ƒç´ ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§åœ¨<strong>é›†åˆ</strong>ï¼ˆæ— å›ºå®šæ‹“æ‰‘ç»“æ„çš„åŸŸï¼‰ä¸Šçš„ç½®æ¢ç­‰å˜æ“ä½œã€‚</p>
        <p>Transformer çš„ä½ç½®ç¼–ç æ‰“ç ´äº†çº¯ç½®æ¢å¯¹ç§°æ€§ï¼Œå¼•å…¥äº†åºåˆ—/ç©ºé—´ä½ç½®ä¿¡æ¯ã€‚è¿™æ˜¯ä¸€ä¸ª"å‡å°‘å¯¹ç§°æ€§"çš„ä¾‹å­â€”â€”å½“æ•°æ®ç¡®å®æœ‰åºåˆ—ç»“æ„æ—¶ï¼Œæˆ‘ä»¬<strong>ä¸åº”è¯¥</strong>ä¿æŒå®Œå…¨çš„ç½®æ¢ä¸å˜æ€§ã€‚</p>
      </div>
      <div class="en">
        From a GDL perspective, Transformers can be understood as GNNs on a complete graph â€” each element attends to all others. Self-attention is a permutation-equivariant operation on sets.
      </div>
    </div>

    <pre><code># GDL è§†è§’ä¸‹çš„è‡ªæ³¨æ„åŠ› = å®Œå…¨å›¾ä¸Šçš„æ¶ˆæ¯ä¼ é€’
import torch
import torch.nn.functional as F
import math

def self_attention_as_gnn(Q, K, V):
    """
    è‡ªæ³¨æ„åŠ› â‰¡ å®Œå…¨å›¾ä¸Šçš„æ³¨æ„åŠ›åŠ æƒæ¶ˆæ¯ä¼ é€’
    Q, K, V: [batch, seq_len, d_k]
    """
    d_k = Q.size(-1)

    # è®¡ç®—æ³¨æ„åŠ›æƒé‡ (å®Œå…¨å›¾çš„è¾¹æƒé‡)
    # æ¯ä¸ªèŠ‚ç‚¹å…³æ³¨æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹
    attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    attention_weights = F.softmax(attention_weights, dim=-1)

    # èšåˆæ¶ˆæ¯ (åŠ æƒæ±‚å’Œ = æ³¨æ„åŠ›æ¶ˆæ¯ä¼ é€’)
    output = torch.matmul(attention_weights, V)
    return output

# å…³é”®æ´å¯Ÿ:
# - Transformer = GNN on complete graph
# - å¦‚æœé™åˆ¶æ³¨æ„åŠ›åªåœ¨å±€éƒ¨é‚»å±…: å¾—åˆ°ç±»ä¼¼ GAT çš„ä¸œè¥¿
# - å¦‚æœæ³¨æ„åŠ›æƒé‡åªä¾èµ–äºè·ç¦»: å¾—åˆ°ç±»ä¼¼ CNN çš„ä¸œè¥¿
# - è¿™å°±æ˜¯ GDL ç»Ÿä¸€è§†è§’çš„åŠ›é‡!</code></pre>

    <!-- ========= æœ¬ä¹¦èŒƒå›´ ========= -->
    <h2 id="scope">æœ¬ä¹¦çš„èŒƒå›´ä¸éèŒƒå›´<br><span style="font-size:0.7em;color:var(--text-secondary)">Scope and Non-Scope</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ¬ä¹¦å…³æ³¨çš„æ˜¯<strong>è¡¨å¾å­¦ä¹ æ¶æ„</strong>åŠå…¶ä¸­çš„<strong>æ•°æ®å¯¹ç§°æ€§åˆ©ç”¨</strong>ã€‚ä»¥ä¸‹å†…å®¹<strong>ä¸æ˜¯</strong>æœ¬ä¹¦çš„æ ¸å¿ƒå…³æ³¨ç‚¹ï¼ˆè™½ç„¶æˆ‘ä»¬è®¤ä¸ºå‡ ä½•åŸåˆ™åœ¨è¿™äº›é¢†åŸŸåŒæ ·é‡è¦ï¼‰ï¼š</p>
      </div>
      <div class="en">
        Our work concerns representation learning architectures and exploiting the symmetries of data therein. The following exciting pipelines are not our central focus.
      </div>
    </div>

    <table>
      <thead>
        <tr><th>ä¸»é¢˜</th><th>å…³ç³»</th><th>ä»£è¡¨æ€§å·¥ä½œ</th></tr>
      </thead>
      <tbody>
        <tr><td>è‡ªç›‘ç£å­¦ä¹ </td><td>ä½¿ç”¨ GDL æ¶æ„ä½œä¸ºç¼–ç å™¨</td><td>BERT, GPT, SimCLR</td></tr>
        <tr><td>ç”Ÿæˆæ¨¡å‹</td><td>VAE, GAN, Flow çš„ç¼–ç å™¨/è§£ç å™¨å¯ä»¥æ˜¯ GDL æ¶æ„</td><td>VAE, GAN, Normalizing Flows</td></tr>
        <tr><td>å¼ºåŒ–å­¦ä¹ </td><td>ç­–ç•¥/ä»·å€¼ç½‘ç»œå¯ä»¥ä½¿ç”¨ GDL æ¶æ„</td><td>DQN, PPO, AlphaGo</td></tr>
        <tr><td>ä¼˜åŒ–æŠ€æœ¯</td><td>Adam, Dropout, BatchNorm ç­‰æ˜¯é€šç”¨è®­ç»ƒæŠ€å·§</td><td>Adam, Dropout, BN</td></tr>
        <tr><td>äº’ä¿¡æ¯æœ€å¤§åŒ–</td><td>DGI ç­‰å°† GNN ä¸è‡ªç›‘ç£ç»“åˆ</td><td>DGI, InfoNCE</td></tr>
      </tbody>
    </table>

    <!-- ========= é˜…è¯»è·¯çº¿å›¾ ========= -->
    <h2 id="roadmap">é˜…è¯»è·¯çº¿å›¾<br><span style="font-size:0.7em;color:var(--text-secondary)">Reading Roadmap</span></h2>

    <div class="callout callout-info">
      <h4>æœ¬ä¹¦ç»“æ„</h4>
      <p>å…¨ä¹¦ç”± 7 ç« ç»„æˆï¼Œå½¢æˆä¸€ä¸ª<strong>ä»ç†è®ºåˆ°åº”ç”¨</strong>çš„å®Œæ•´é“¾æ¡ã€‚</p>
    </div>

    <h3 id="roadmap-ch2">Chapter 2: é«˜ç»´å­¦ä¹  (Learning in High Dimensions)</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>æ ¸å¿ƒé—®é¢˜</strong>ï¼šä¸ºä»€ä¹ˆé€šç”¨é«˜ç»´å‡½æ•°å­¦ä¹ æ˜¯ä¸å¯èƒ½çš„ï¼Ÿ</p>
        <ul>
          <li>ç»´åº¦ç¾éš¾çš„æ•°å­¦è®ºè¯</li>
          <li>å½’çº³åç½®çš„æ¦‚å¿µ</li>
          <li>å‡½æ•°æ­£åˆ™æ€§ä¸å¤æ‚åº¦åº¦é‡</li>
          <li>ä¸‡èƒ½é€¼è¿‘å®šç†åŠå…¶å±€é™æ€§</li>
        </ul>
        <p><strong>å…³é”®æ”¶è·</strong>ï¼šç†è§£ä¸ºä»€ä¹ˆæˆ‘ä»¬<em>å¿…é¡»</em>åˆ©ç”¨æ•°æ®ç»“æ„â€”â€”è¿™ä¸æ˜¯å¯é€‰çš„ä¼˜åŒ–ï¼Œè€Œæ˜¯<em>å¿…è¦çš„</em>ã€‚</p>
      </div>
    </div>

    <h3 id="roadmap-ch3">Chapter 3: å‡ ä½•å…ˆéªŒ (Geometric Priors)</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>æ ¸å¿ƒæ¦‚å¿µ</strong>ï¼šå¯¹ç§°æ€§ã€ä¸å˜æ€§ã€ç­‰å˜æ€§</p>
        <ul>
          <li>ç¾¤è®ºåŸºç¡€</li>
          <li>G-ä¸å˜æ€§å’Œ G-ç­‰å˜æ€§</li>
          <li>å½¢å˜ç¨³å®šæ€§</li>
          <li>å°ºåº¦åˆ†ç¦»</li>
          <li><strong>GDL è“å›¾</strong>â€”â€”ç»Ÿä¸€æ‰€æœ‰æ¶æ„çš„æŠ½è±¡æ¡†æ¶</li>
        </ul>
        <p><strong>å…³é”®æ”¶è·</strong>ï¼šæŒæ¡ GDL çš„æ ¸å¿ƒæ•°å­¦è¯­è¨€ï¼Œç†è§£"å¯¹ç§°æ€§ â†’ çº¦æŸ â†’ æ›´å¥½çš„å­¦ä¹ "è¿™æ¡é€»è¾‘é“¾ã€‚</p>
      </div>
    </div>

    <h3 id="roadmap-ch4">Chapter 4: å‡ ä½•åŸŸ (Geometric Domains)</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>æ ¸å¿ƒå†…å®¹</strong>ï¼š5G çš„è¯¦ç»†æ•°å­¦æè¿°</p>
        <ul>
          <li>é›†åˆã€ç½‘æ ¼ã€ç¾¤ã€å›¾ã€æµå½¢â€”â€”æ¯ä¸ªåŸŸçš„å®šä¹‰å’Œæ€§è´¨</li>
          <li>æ¯ä¸ªåŸŸä¸Šçš„ä¿¡å·ç©ºé—´</li>
          <li>æ¯ä¸ªåŸŸçš„å¯¹ç§°ç¾¤</li>
          <li>Fourier åˆ†æçš„æ¨å¹¿</li>
        </ul>
        <p><strong>å…³é”®æ”¶è·</strong>ï¼šå…·ä½“ç†è§£æ¯ç§æ•°æ®ç»“æ„çš„æ•°å­¦æœ¬è´¨ã€‚</p>
      </div>
    </div>

    <h3 id="roadmap-ch5">Chapter 5: GDL æ¨¡å‹ (Geometric Deep Learning Models)</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>æ ¸å¿ƒå†…å®¹</strong>ï¼šå…·ä½“æ¶æ„çš„æ¨å¯¼å’Œåˆ†æ</p>
        <ul>
          <li>CNN ä» GDL è“å›¾çš„æ¨å¯¼</li>
          <li>GNN çš„ä¸‰ç§é£æ ¼ï¼ˆå·ç§¯å‹ã€æ³¨æ„åŠ›å‹ã€æ¶ˆæ¯ä¼ é€’å‹ï¼‰</li>
          <li>Transformer çš„å‡ ä½•ç†è§£</li>
          <li>ç­‰å˜ç½‘ç»œï¼ˆEGNN, TFN, PaiNNï¼‰</li>
        </ul>
        <p><strong>å…³é”®æ”¶è·</strong>ï¼šèƒ½å¤Ÿä»ç¬¬ä¸€æ€§åŸç†<em>æ¨å¯¼</em>å‡ºå„ç§æ¶æ„ï¼Œè€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒã€‚</p>
      </div>
    </div>

    <h3 id="roadmap-ch6">Chapter 6: åº”ç”¨ (Problems and Applications)</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>æ ¸å¿ƒå†…å®¹</strong>ï¼šGDL åœ¨å®é™…é¢†åŸŸçš„åº”ç”¨</p>
        <ul>
          <li>è®¡ç®—æœºè§†è§‰ã€è¯ç‰©å‘ç°ã€ç²’å­ç‰©ç†</li>
          <li>è›‹ç™½è´¨ç»“æ„é¢„æµ‹ï¼ˆAlphaFoldï¼‰</li>
          <li>æ¨èç³»ç»Ÿã€ç¤¾äº¤ç½‘ç»œ</li>
          <li>äº¤é€šé¢„æµ‹ã€æ¸¸æˆ AI</li>
          <li><strong>åŒ»ç–—æœºå™¨äºº</strong>â€”â€”ä¸æˆ‘ä»¬é¡¹ç›®çš„ç›´æ¥å…³è”</li>
        </ul>
        <p><strong>å…³é”®æ”¶è·</strong>ï¼šçœ‹åˆ°ç†è®ºå¦‚ä½•è½åœ°ï¼Œç†è§£æ¯ä¸ªåº”ç”¨ä¸­çš„å¯¹ç§°æ€§é€‰æ‹©ã€‚</p>
      </div>
    </div>

    <h3 id="roadmap-ch7">Chapter 7: å†å²è§†è§’ (Historic Perspective)</h3>
    <div class="bilingual">
      <div class="zh">
        <p><strong>æ ¸å¿ƒå†…å®¹</strong>ï¼šGDL çš„æ€æƒ³æ¸Šæº</p>
        <ul>
          <li>ä» Erlangen Program åˆ° Noether å®šç†</li>
          <li>CNN çš„å‘æ˜å²</li>
          <li>GNN çš„ç‹¬ç«‹èµ·æºï¼ˆAI vs è®¡ç®—åŒ–å­¦ï¼‰</li>
          <li>è°±æ–¹æ³•å’Œè°ƒå’Œåˆ†æ</li>
          <li>WL å›¾åŒæ„æµ‹è¯•ä¸ GNN è¡¨è¾¾åŠ›</li>
        </ul>
        <p><strong>å…³é”®æ”¶è·</strong>ï¼šç†è§£æ€æƒ³çš„æ¼”è¿›è„‰ç»œï¼Œå‘ç°ä¸åŒé¢†åŸŸä¹‹é—´çš„æ„å¤–è”ç³»ã€‚</p>
      </div>
    </div>

    <!-- ========= PhysRobot å…³è” ========= -->
    <h2 id="physrobot">ä¸ PhysRobot é¡¹ç›®çš„å…³è”<br><span style="font-size:0.7em;color:var(--text-secondary)">Connection to PhysRobot</span></h2>

    <div class="callout callout-project">
      <h4>ä¸ºä»€ä¹ˆ GDL å¯¹åŒ»ç–—æœºå™¨äººä»¿çœŸå¦‚æ­¤é‡è¦ï¼Ÿ</h4>
      <p>æˆ‘ä»¬çš„ PhysRobot é¡¹ç›®ï¼ˆåŒ»ç–—æœºå™¨äººç‰©ç†ä»¿çœŸï¼‰æ˜¯ GDL åŸåˆ™çš„<strong>å®Œç¾åº”ç”¨åœºæ™¯</strong>ã€‚ä»¥ä¸‹æ˜¯æ ¸å¿ƒè¿æ¥ç‚¹ï¼š</p>
    </div>

    <table>
      <thead>
        <tr><th>PhysRobot ç»„ä»¶</th><th>å‡ ä½•åŸŸ</th><th>å¯¹ç§°ç¾¤</th><th>GDL æ¶æ„</th></tr>
      </thead>
      <tbody>
        <tr><td>ç²’å­ç³»ç»Ÿä»¿çœŸ</td><td>åŠ¨æ€å›¾</td><td>SE(3) Ã— $\Sigma_n$</td><td>GNS (Graph Network Simulator)</td></tr>
        <tr><td>è½¯ç»„ç»‡å˜å½¢</td><td>ç½‘æ ¼/æµå½¢</td><td>SE(3)</td><td>MeshGraphNet</td></tr>
        <tr><td>åˆšä½“åŠ¨åŠ›å­¦</td><td>SE(3) ç¾¤</td><td>SE(3)</td><td>ç­‰å˜æ¶ˆæ¯ä¼ é€’</td></tr>
        <tr><td>å™¨æ¢°-ç»„ç»‡äº¤äº’</td><td>å¼‚æ„å›¾</td><td>SE(3)</td><td>å¼‚æ„ GNN</td></tr>
        <tr><td>åŠ›åœºé¢„æµ‹</td><td>ç²’å­å›¾</td><td>SE(3) (ç­‰å˜)</td><td>EGNN / PaiNN</td></tr>
        <tr><td>èƒ½é‡é¢„æµ‹</td><td>ç²’å­å›¾</td><td>SE(3) (ä¸å˜)</td><td>SchNet / DimeNet</td></tr>
      </tbody>
    </table>

    <pre><code># PhysRobot ä¸­ GDL åŸåˆ™çš„ä½“ç°
"""
GNS (Graph Network Simulator) æ˜¯ä¸€ä¸ªå®Œç¾çš„ GDL å®ä¾‹:

1. åŸŸ: åŠ¨æ€å›¾ (ç²’å­ = èŠ‚ç‚¹, ç›¸äº’ä½œç”¨ = è¾¹)
2. å¯¹ç§°ç¾¤: 
   - SE(3): ç‰©ç†å®šå¾‹ä¸ä¾èµ–äºåæ ‡ç³»é€‰æ‹©
   - Î£_n: ç²’å­æ²¡æœ‰å›ºæœ‰æ’åº
3. ç­‰å˜æ“ä½œ: æ¶ˆæ¯ä¼ é€’ (ç½®æ¢ç­‰å˜)
4. å…³é”®è®¾è®¡:
   - ç”¨ç›¸å¯¹ä½ç½® (r_ij = r_j - r_i) ä½œä¸ºè¾¹ç‰¹å¾ â†’ å¹³ç§»ä¸å˜
   - ç”¨è·ç¦» |r_ij| ä½œä¸ºæ ‡é‡ç‰¹å¾ â†’ æ—‹è½¬ä¸å˜
   - é¢„æµ‹åŠ é€Ÿåº¦è€Œéä½ç½® â†’ ä¿æŒç‰©ç†å› æœæ€§
"""

class GNS_GDL_Blueprint:
    """
    GNS åœ¨ GDL è“å›¾ä¸­çš„ä½ç½®:
    
    Signal:     x = (particle_type, position, velocity)
    Domain:     Î© = dynamic graph (k-NN in 3D space)
    Symmetry:   G = SE(3) Ã— Î£_n
    Equivariant: message passing layers
    Invariant:  aggregation for global properties
    Coarsening: not used (single-scale)
    Output:     acceleration (SE(3)-equivariant vector)
    """
    pass</code></pre>

    <!-- ========= ä»£ç ç¯å¢ƒ ========= -->
    <h2 id="code-setup">ä»£ç ç¯å¢ƒå‡†å¤‡<br><span style="font-size:0.7em;color:var(--text-secondary)">Setting Up Your Environment</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>æœ¬æ•™ç¨‹ä¸­çš„ä»£ç ç¤ºä¾‹ä½¿ç”¨ Python å’Œ PyTorch ç”Ÿæ€ç³»ç»Ÿã€‚ä»¥ä¸‹æ˜¯æ¨èçš„ç¯å¢ƒé…ç½®ï¼š</p>
      </div>
    </div>

    <pre><code># åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n gdl python=3.10
conda activate gdl

# æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio
pip install torch-geometric          # PyG: GNN æ¡†æ¶
pip install torch-scatter torch-sparse torch-cluster

# å¯è§†åŒ–å’Œæ•°å€¼è®¡ç®—
pip install numpy scipy matplotlib networkx
pip install plotly                   # 3D å¯è§†åŒ–

# å¯é€‰: ç­‰å˜ç½‘ç»œ
pip install e3nn                     # E(3)-ç­‰å˜ç½‘ç»œæ¡†æ¶

# éªŒè¯å®‰è£…
python -c "
import torch
import torch_geometric
print(f'PyTorch: {torch.__version__}')
print(f'PyG: {torch_geometric.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'MPS available: {torch.backends.mps.is_available()}')  # Apple Silicon
"</code></pre>

    <pre><code># éªŒè¯ GDL æ¦‚å¿µçš„å¿«é€Ÿè„šæœ¬
import torch
import torch_geometric
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data

# åˆ›å»ºä¸€ä¸ªç®€å•çš„å›¾: 5 ä¸ªèŠ‚ç‚¹ï¼Œ6 æ¡è¾¹
edge_index = torch.tensor([
    [0, 1, 1, 2, 3, 4],
    [1, 0, 2, 1, 4, 3]
], dtype=torch.long)

# èŠ‚ç‚¹ç‰¹å¾: æ¯ä¸ªèŠ‚ç‚¹ 3 ç»´
x = torch.randn(5, 3)

# åˆ›å»º PyG Data å¯¹è±¡
data = Data(x=x, edge_index=edge_index)
print(f"å›¾: {data.num_nodes} èŠ‚ç‚¹, {data.num_edges} è¾¹")
print(f"èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {data.x.shape}")

# GCN å±‚ (ç½®æ¢ç­‰å˜!)
conv = GCNConv(3, 16)
out = conv(data.x, data.edge_index)
print(f"GCN è¾“å‡º: {out.shape}")

# éªŒè¯ç½®æ¢ç­‰å˜æ€§
perm = torch.randperm(5)
x_perm = x[perm]
edge_index_perm = perm[edge_index]  # ç®€åŒ–, å®é™…éœ€è¦æ›´ç»†è‡´çš„å¤„ç†
# ç†è®ºä¸Š: conv(x_perm, edge_perm) == conv(x, edge)[perm]
print("âœ… GDL ç¯å¢ƒé…ç½®å®Œæˆ!")</code></pre>

    <!-- ========= ç»ƒä¹ é¢˜ ========= -->
    <div class="exercises" id="exercises">
      <h3>ç»ƒä¹ é¢˜ Exercises</h3>
      <ol>
        <li><strong>å¯¹ç§°æ€§è¯†åˆ«</strong>ï¼šå¯¹ä»¥ä¸‹æ•°æ®ç±»å‹ï¼Œè¯†åˆ«å…¶è‡ªç„¶çš„å‡ ä½•åŸŸå’Œå¯¹ç§°ç¾¤ï¼š
          (a) ä¸€ç»„ç‚¹äº‘ä¸­çš„ 3D ç‰©ä½“  
          (b) åŒ–å­¦åˆ†å­  
          (c) ç¤¾äº¤åª’ä½“ä¸Šç”¨æˆ·ä¹‹é—´çš„å…³ç³»  
          (d) å…¨çƒå¤©æ°”æ•°æ®ï¼ˆå®šä¹‰åœ¨åœ°çƒè¡¨é¢ä¸Šï¼‰  
          (e) æ‰‹æœ¯è§†é¢‘ä¸­å™¨æ¢°ä¸ç»„ç»‡çš„äº¤äº’  
        </li>
        <li><strong>å‚æ•°é‡æ¯”è¾ƒ</strong>ï¼šè®¡ç®—ä»¥ä¸‹æ¶æ„å¯¹ $32 \times 32$ ç°åº¦å›¾åƒçš„å‚æ•°é‡ï¼š
          (a) å…¨è¿æ¥å±‚ï¼ˆè¾“å‡º 1024 ç»´ï¼‰  
          (b) $3 \times 3$ å·ç§¯å±‚ï¼ˆ64 ä¸ªæ»¤æ³¢å™¨ï¼‰  
          (c) è®¨è®ºå‚æ•°é‡å·®å¼‚çš„åŸå› ï¼ˆæç¤ºï¼šå¯¹ç§°æ€§ï¼ï¼‰
        </li>
        <li><strong>Erlangen æ€è€ƒ</strong>ï¼šå¦‚æœä¸€ä¸ªæœºå™¨å­¦ä¹ é—®é¢˜çš„æ•°æ®å…·æœ‰ SE(3)ï¼ˆæ—‹è½¬+å¹³ç§»ï¼‰å¯¹ç§°æ€§ï¼Œä½†ä½ ä½¿ç”¨äº†ä¸€ä¸ªä¸å…·æœ‰ SE(3) ç­‰å˜æ€§çš„ç½‘ç»œï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿä»æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–çš„è§’åº¦è®¨è®ºã€‚</li>
        <li><strong>5G åˆ†ç±»</strong>ï¼šå°†ä»¥ä¸‹å…·ä½“é—®é¢˜æ˜ å°„åˆ° 5G æ¡†æ¶ä¸­æœ€åˆé€‚çš„åŸŸï¼š
          (a) å›¾åƒè¯­ä¹‰åˆ†å‰²  
          (b) è›‹ç™½è´¨-è›‹ç™½è´¨å¯¹æ¥  
          (c) äº¤é€šæµé‡é¢„æµ‹  
          (d) çƒé¢ä¸Šçš„æ°”å€™æ¨¡å¼è¯†åˆ«  
          (e) ç²’å­ç‰©ç†ä¸­çš„ jet åˆ†ç±»
        </li>
        <li><strong>PhysRobot æ€è€ƒ</strong>ï¼šåœ¨åŒ»ç–—æœºå™¨äººä»¿çœŸä¸­ï¼Œå½“æ‰‹æœ¯å™¨æ¢°åˆºå…¥è½¯ç»„ç»‡æ—¶ï¼š
          (a) æè¿°ç³»ç»Ÿçš„å‡ ä½•åŸŸ  
          (b) ç³»ç»Ÿæœ‰å“ªäº›å¯¹ç§°æ€§ï¼Ÿ  
          (c) æœ‰å“ªäº›å¯¹ç§°æ€§è¢«<em>æ‰“ç ´</em>äº†ï¼ˆä¾‹å¦‚é‡åŠ›æ–¹å‘ï¼‰ï¼Ÿ  
          (d) å¦‚ä½•åœ¨ç½‘ç»œè®¾è®¡ä¸­å¤„ç†è¿™ç§"éƒ¨åˆ†å¯¹ç§°æ€§"ï¼Ÿ
        </li>
        <li><strong>ç¼–ç¨‹ç»ƒä¹ </strong>ï¼šä½¿ç”¨ PyTorch Geometric åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¶ˆæ¯ä¼ é€’ GNNï¼Œåº”ç”¨äº Karate Club æ•°æ®é›†ã€‚
          (a) åŠ è½½æ•°æ®å¹¶å¯è§†åŒ–å›¾ç»“æ„  
          (b) å®ç° 2 å±‚ GCN  
          (c) éªŒè¯è¾“å‡ºçš„ç»´åº¦å’Œå¯è§†åŒ–èŠ‚ç‚¹åµŒå…¥ï¼ˆä½¿ç”¨ t-SNEï¼‰
        </li>
        <li><strong>æ€è€ƒé¢˜</strong>ï¼šTransformer ä¸­çš„ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰ä» GDL çš„è§’åº¦æ„å‘³ç€ä»€ä¹ˆï¼Ÿå®ƒå¢åŠ äº†è¿˜æ˜¯å‡å°‘äº†å¯¹ç§°æ€§ï¼Ÿä¸ºä»€ä¹ˆæœ‰æ—¶å€™éœ€è¦"å‡å°‘å¯¹ç§°æ€§"ï¼Ÿ</li>
      </ol>
    </div>

    <!-- ========= ç« èŠ‚å¯¼èˆª ========= -->
    <div class="chapter-nav">
      <a href="../index.html">ğŸ“š æ€»ç›®å½•</a>
      <a href="../chapter2/index.html">Chapter 2: é«˜ç»´å­¦ä¹  â†’</a>
    </div>
  </main>

  <script src="../assets/script.js"></script>
</body>
</html>

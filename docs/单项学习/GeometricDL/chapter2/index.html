<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 2: Learning in High Dimensions | GDL å­¦ä¹ æŒ‡å—</title>
  <link rel="stylesheet" href="../assets/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Noto+Serif+SC:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]})"></script>
</head>
<body>
  <div class="progress-bar"></div>

  <header class="header">
    <div class="header-title"><a href="../index.html">ğŸ“ GDL å­¦ä¹ æŒ‡å—</a></div>
    <div class="header-nav">
      <a href="../chapter1/index.html">â† ä¸Šä¸€ç« </a>
      <a href="../index.html">ç›®å½•</a>
      <a href="../chapter3/index.html">ä¸‹ä¸€ç«  â†’</a>
      <button class="theme-toggle" onclick="toggleTheme()">ğŸŒ™</button>
    </div>
  </header>

  <button class="sidebar-toggle" onclick="toggleSidebar()">â˜°</button>

  <nav class="sidebar">
    <h3>Chapter 2</h3>
    <a href="#overview">æ¦‚è¿°</a>
    <a href="#setup">å­¦ä¹ é—®é¢˜çš„å½¢å¼åŒ–</a>
    <a href="#supervised" class="sub">ç›‘ç£å­¦ä¹ è®¾ç½®</a>
    <a href="#risk" class="sub">ç»éªŒé£é™© vs æ³›åŒ–é£é™©</a>
    <a href="#interpolation" class="sub">æ’å€¼ä½“åˆ¶</a>
    <a href="#sec2-1">2.1 å½’çº³åç½®ä¸å‡½æ•°æ­£åˆ™æ€§</a>
    <a href="#universal-approx" class="sub">ä¸‡èƒ½é€¼è¿‘å®šç†</a>
    <a href="#complexity-measure" class="sub">å¤æ‚åº¦åº¦é‡</a>
    <a href="#regularization" class="sub">æ­£åˆ™åŒ–</a>
    <a href="#implicit-reg" class="sub">éšå¼æ­£åˆ™åŒ–</a>
    <a href="#sec2-2">2.2 ç»´åº¦ç¾éš¾</a>
    <a href="#lipschitz" class="sub">Lipschitz ç±»çš„ç¾éš¾</a>
    <a href="#sobolev" class="sub">Sobolev ç±»çš„ç¾éš¾</a>
    <a href="#volume-concentration" class="sub">ä½“ç§¯é›†ä¸­ç°è±¡</a>
    <a href="#geometric-intuition" class="sub">å‡ ä½•ç›´è§‰</a>
    <a href="#breaking-curse">2.3 æ‰“ç ´ç»´åº¦ç¾éš¾</a>
    <a href="#fc-nets" class="sub">å…¨è¿æ¥ç½‘ç»œçš„æ–¹æ³•</a>
    <a href="#low-dim-proj" class="sub">ä½ç»´æŠ•å½±å‡è®¾</a>
    <a href="#limitations" class="sub">å±€é™æ€§</a>
    <a href="#geometric-priors">2.4 èµ°å‘å‡ ä½•å…ˆéªŒ</a>
    <a href="#structure-exploitation" class="sub">åˆ©ç”¨ç»“æ„</a>
    <a href="#preview-ch3" class="sub">Chapter 3 é¢„è§ˆ</a>
    <a href="#code-examples">ä»£ç ç¤ºä¾‹</a>
    <a href="#curse-demo" class="sub">ç»´åº¦ç¾éš¾æ¼”ç¤º</a>
    <a href="#mlp-demo" class="sub">MLP ä¸‡èƒ½é€¼è¿‘</a>
    <a href="#bias-demo" class="sub">å½’çº³åç½®å®éªŒ</a>
    <a href="#physrobot">PhysRobot å…³è”</a>
    <a href="#exercises">ç»ƒä¹ é¢˜</a>
    <h3>å¯¼èˆª</h3>
    <a href="../index.html">ğŸ“š æ€»ç›®å½•</a>
    <a href="../chapter1/index.html">â† Ch.1 å¼•è¨€</a>
    <a href="../chapter3/index.html">â†’ Ch.3 å‡ ä½•å…ˆéªŒ</a>
  </nav>

  <main class="main">
    <h1>Chapter 2: Learning in High Dimensions<br><span style="font-size:0.6em;color:var(--text-secondary)">é«˜ç»´å­¦ä¹  â€” ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å‡ ä½•å…ˆéªŒ</span></h1>

    <div class="callout callout-info" id="overview">
      <h4>æœ¬ç« æ¦‚è¿°</h4>
      <p>æœ¬ç« æ˜¯å…¨ä¹¦çš„<strong>åŠ¨æœºç« </strong>â€”â€”å›ç­”ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼š<em>ä¸ºä»€ä¹ˆ</em>æˆ‘ä»¬éœ€è¦å‡ ä½•å…ˆéªŒï¼Ÿé€šè¿‡ä¸¥æ ¼çš„æ•°å­¦åˆ†æï¼Œæˆ‘ä»¬å°†çœ‹åˆ°ï¼š</p>
      <ul>
        <li>åœ¨é«˜ç»´ç©ºé—´ä¸­å­¦ä¹ <strong>é€šç”¨å‡½æ•°</strong>éœ€è¦æŒ‡æ•°çº§çš„æ ·æœ¬â€”â€”<strong>ç»´åº¦ç¾éš¾</strong></li>
        <li><strong>ä¸‡èƒ½é€¼è¿‘å®šç†</strong>ä¿è¯äº†è¡¨è¾¾èƒ½åŠ›ï¼Œä½†<em>ä¸</em>ä¿è¯å­¦ä¹ æ•ˆç‡</li>
        <li><strong>å½’çº³åç½®</strong>ï¼ˆé€šè¿‡æ­£åˆ™æ€§å‡è®¾ï¼‰æ˜¯å…‹æœç»´åº¦ç¾éš¾çš„å”¯ä¸€é€”å¾„</li>
        <li>å…¨è¿æ¥ç½‘ç»œå¯ä»¥é€šè¿‡ç¨€ç–æ­£åˆ™åŒ–éƒ¨åˆ†ç¼“è§£ç¾éš¾ï¼Œä½†å‡è®¾å¤ªå¼º</li>
        <li>å”¯ä¸€ç°å®çš„å‡ºè·¯ï¼šåˆ©ç”¨æ•°æ®çš„<strong>å‡ ä½•ç»“æ„</strong>â€”â€”å¼•å‡º Chapter 3</li>
      </ul>
      <p><strong>é¢„è®¡é˜…è¯»æ—¶é—´</strong>ï¼š2 å°æ—¶ &nbsp;|&nbsp; <strong>å…ˆä¿®çŸ¥è¯†</strong>ï¼šæ¦‚ç‡è®ºåŸºç¡€ã€å‡½æ•°åˆ†æå…¥é—¨</p>
    </div>

    <!-- ========= å­¦ä¹ é—®é¢˜çš„å½¢å¼åŒ– ========= -->
    <h2 id="setup">å­¦ä¹ é—®é¢˜çš„å½¢å¼åŒ–<br><span style="font-size:0.7em;color:var(--text-secondary)">Formalisation of the Learning Problem</span></h2>

    <h3 id="supervised">ç›‘ç£å­¦ä¹ è®¾ç½®</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ç›‘ç£æœºå™¨å­¦ä¹ ï¼Œåœ¨æœ€ç®€å•çš„å½¢å¼åŒ–ä¸­ï¼Œè€ƒè™‘ä»åº•å±‚æ•°æ®åˆ†å¸ƒ $P$ ä¸­ç‹¬ç«‹åŒåˆ†å¸ƒæŠ½å–çš„ $N$ ä¸ªè§‚æµ‹å€¼çš„é›†åˆ $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ï¼Œå…¶ä¸­ $P$ å®šä¹‰åœ¨ $\mathcal{X} \times \mathcal{Y}$ ä¸Šã€‚</p>
        <p>è¿™ä¸ªè®¾ç½®çš„<strong>å®šä¹‰æ€§ç‰¹å¾</strong>æ˜¯ $\mathcal{X}$ æ˜¯ä¸€ä¸ª<strong>é«˜ç»´ç©ºé—´</strong>ï¼šé€šå¸¸å‡è®¾ $\mathcal{X} = \mathbb{R}^d$ï¼Œç»´åº¦ $d$ å¾ˆå¤§ã€‚</p>
      </div>
      <div class="en">
        Supervised machine learning considers a set of $N$ observations $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution $P$ defined over $\mathcal{X} \times \mathcal{Y}$. The defining feature is that $\mathcal{X}$ is a high-dimensional space.
      </div>
    </div>

    <div class="math-block">
      $$\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N \sim P(\mathcal{X} \times \mathcal{Y}), \quad \mathcal{X} = \mathbb{R}^d, \; d \gg 1$$
      <div class="math-explain">
        <strong>å…³é”®è¦ç´ </strong>ï¼š
        <br>â€¢ $\mathcal{X} = \mathbb{R}^d$ â€” è¾“å…¥ç©ºé—´ï¼ˆé«˜ç»´ï¼‰ã€‚å¯¹äº MNIST å›¾åƒï¼š$d = 28 \times 28 = 784$ã€‚å¯¹äº ImageNetï¼š$d = 224 \times 224 \times 3 = 150{,}528$ã€‚
        <br>â€¢ $\mathcal{Y}$ â€” æ ‡ç­¾ç©ºé—´ã€‚åˆ†ç±»ï¼š$\mathcal{Y} = \{1, \ldots, K\}$ï¼›å›å½’ï¼š$\mathcal{Y} = \mathbb{R}$ã€‚
        <br>â€¢ $P$ â€” æ•°æ®çš„è”åˆåˆ†å¸ƒï¼ˆæœªçŸ¥ï¼‰ã€‚
        <br>â€¢ $N$ â€” è®­ç»ƒæ ·æœ¬æ•°ã€‚
      </div>
    </div>

    <p>è¿›ä¸€æ­¥å‡è®¾æ ‡ç­¾ç”±æœªçŸ¥å‡½æ•° $f$ ç”Ÿæˆï¼š$y_i = f(x_i)$ã€‚å­¦ä¹ é—®é¢˜å½’ç»“ä¸ºä½¿ç”¨å‚æ•°åŒ–å‡½æ•°ç±» $\mathcal{F} = \{f_\theta : \theta \in \Theta\}$ æ¥ä¼°è®¡å‡½æ•° $f$ã€‚</p>

    <div class="math-block">
      $$\text{ç›®æ ‡: æ‰¾åˆ° } \tilde{f} \in \mathcal{F} \text{ ä½¿å¾— } \tilde{f} \approx f$$
      <div class="math-explain">
        ç¥ç»ç½‘ç»œæ˜¯è¿™ç§å‚æ•°åŒ–å‡½æ•°ç±»çš„ä¸€ç§å¸¸è§å®ç°ï¼Œå…¶ä¸­ $\theta \in \Theta$ å¯¹åº”ç½‘ç»œæƒé‡ã€‚åœ¨ç†æƒ³åŒ–è®¾ç½®ä¸­ï¼Œæ ‡ç­¾æ²¡æœ‰å™ªå£°ï¼Œç°ä»£æ·±åº¦å­¦ä¹ ç³»ç»Ÿé€šå¸¸åœ¨æ‰€è°“çš„<strong>æ’å€¼ä½“åˆ¶</strong>ä¸­è¿è¡Œã€‚
      </div>
    </div>

    <h3 id="risk">ç»éªŒé£é™© vs æ³›åŒ–é£é™©</h3>

    <div class="bilingual">
      <div class="zh">
        <p>å­¦ä¹ ç®—æ³•çš„æ€§èƒ½é€šè¿‡åœ¨<strong>æ–°æ ·æœ¬</strong>ä¸Šçš„é¢„æœŸæ€§èƒ½æ¥è¡¡é‡ï¼Œä½¿ç”¨æŸä¸ªæŸå¤±å‡½æ•° $L(\cdot, \cdot)$ï¼š</p>
      </div>
      <div class="en">
        Performance is measured in terms of expected performance on new samples drawn from $P$, using some loss $L(\cdot, \cdot)$.
      </div>
    </div>

    <div class="math-block">
      $$R(\tilde{f}) := \mathbb{E}_{P}\left[L(\tilde{f}(x), f(x))\right]$$
      <div class="math-explain">
        <strong>æ³›åŒ–é£é™©</strong>ï¼ˆgeneralization riskï¼‰$R(\tilde{f})$ è¡¡é‡æ¨¡å‹åœ¨<strong>æœªè§è¿‡çš„æ•°æ®</strong>ä¸Šçš„è¡¨ç°ã€‚å¸¸ç”¨æŸå¤±åŒ…æ‹¬ï¼š
        <br>â€¢ å‡æ–¹è¯¯å·®ï¼š$L(y, y') = \frac{1}{2}|y - y'|^2$
        <br>â€¢ äº¤å‰ç†µæŸå¤±ï¼ˆç”¨äºåˆ†ç±»ï¼‰
        <br>å…³é”®é—®é¢˜ï¼šæˆ‘ä»¬åªèƒ½è®¡ç®—<strong>ç»éªŒé£é™©</strong>ï¼ˆåœ¨è®­ç»ƒé›†ä¸Šï¼‰ï¼Œå¦‚ä½•ä¿è¯æ³›åŒ–é£é™©ä¹Ÿä½ï¼Ÿ
      </div>
    </div>

    <div class="math-block">
      $$\hat{R}(\tilde{f}) = \frac{1}{N}\sum_{i=1}^N L(\tilde{f}(x_i), f(x_i)) \quad \text{vs} \quad R(\tilde{f}) = \mathbb{E}_P[L(\tilde{f}(x), f(x))]$$
      <div class="math-explain">
        <strong>å·¦è¾¹</strong>ï¼šç»éªŒé£é™©ï¼ˆtraining lossï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä¼˜åŒ–ã€‚<strong>å³è¾¹</strong>ï¼šæ³›åŒ–é£é™©ï¼ˆtest lossï¼‰ï¼Œæˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„ã€‚ä»ç»éªŒé£é™©åˆ°æ³›åŒ–é£é™©çš„æ¡¥æ¢éœ€è¦<strong>ç»Ÿè®¡å­¦ä¹ ç†è®º</strong>ï¼ˆé›†ä¸­ä¸ç­‰å¼ã€Rademacher å¤æ‚åº¦ç­‰ï¼‰â€”â€”è¿™è¶…å‡ºäº†æœ¬ç« èŒƒå›´ï¼Œä½†æ ¸å¿ƒç»“è®ºæ˜¯ï¼šå‡½æ•°ç±» $\mathcal{F}$ è¶Šå¤§ï¼ˆè¶Š"å¤æ‚"ï¼‰ï¼Œç»éªŒé£é™©å’Œæ³›åŒ–é£é™©ä¹‹é—´çš„å·®è·è¶Šå¤§ã€‚
      </div>
    </div>

    <h3 id="interpolation">æ’å€¼ä½“åˆ¶</h3>

    <div class="bilingual">
      <div class="zh">
        <p>åœ¨ç†æƒ³åŒ–è®¾ç½®ä¸­ï¼ˆæ— æ ‡ç­¾å™ªå£°ï¼‰ï¼Œç°ä»£æ·±åº¦å­¦ä¹ ç³»ç»Ÿé€šå¸¸åœ¨<strong>æ’å€¼ä½“åˆ¶</strong>ä¸­è¿è¡Œï¼šä¼°è®¡å‡½æ•° $\tilde{f} \in \mathcal{F}$ æ»¡è¶³ $\tilde{f}(x_i) = f(x_i)$ å¯¹æ‰€æœ‰ $i = 1, \ldots, N$ã€‚å³æ¨¡å‹å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚</p>
        <p>ä½†è¿™å¼•å‡ºä¸€ä¸ªå…³é”®é—®é¢˜ï¼šé€šå¸¸å­˜åœ¨<strong>æ— ç©·å¤šä¸ª</strong>æ’å€¼å‡½æ•°â€”â€”æˆ‘ä»¬å¦‚ä½•é€‰æ‹©"æ­£ç¡®çš„"é‚£ä¸ªï¼Ÿ</p>
      </div>
      <div class="en">
        Modern deep learning systems typically operate in the interpolating regime, where $\tilde{f}(x_i) = f(x_i)$ for all $i$. But there are infinitely many interpolating functions â€” how do we choose?
      </div>
    </div>

    <!-- ========= 2.1 å½’çº³åç½® ========= -->
    <h2 id="sec2-1">2.1 å½’çº³åç½®ä¸å‡½æ•°æ­£åˆ™æ€§<br><span style="font-size:0.7em;color:var(--text-secondary)">Inductive Bias via Function Regularity</span></h2>

    <h3 id="universal-approx">ä¸‡èƒ½é€¼è¿‘å®šç† (Universal Approximation)</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ç°ä»£æœºå™¨å­¦ä¹ ä½¿ç”¨å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œè¿™æ¿€åŠ±äº†è®¾è®¡<strong>å¯Œæœ‰è¡¨è¾¾åŠ›çš„å‡½æ•°ç±»</strong> $\mathcal{F}$ã€‚ç¥ç»ç½‘ç»œå³ä½¿æœ€ç®€å•çš„æ¶æ„ä¹Ÿèƒ½äº§ç”Ÿ<strong>ç¨ å¯†</strong>çš„å‡½æ•°ç±»â€”â€”è¿™å°±æ˜¯å„ç§<strong>ä¸‡èƒ½é€¼è¿‘å®šç†</strong>çš„å†…å®¹ã€‚</p>
      </div>
      <div class="en">
        Modern machine learning operates with large, high-quality datasets, motivating rich function classes $\mathcal{F}$. Even simple neural network architectures yield dense function classes â€” the subject of Universal Approximation Theorems.
      </div>
    </div>

    <div class="figure">
      <img src="../assets/ch2_p11_img0.png" alt="Figure 1: MLP Universal Approximation">
      <figcaption>å›¾ 1ï¼šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆRosenblatt, 1958ï¼‰ï¼Œæœ€ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ˜¯ä¸‡èƒ½é€¼è¿‘å™¨ï¼šä»…éœ€ä¸€ä¸ªéšè—å±‚ï¼Œå°±å¯ä»¥è¡¨ç¤ºé˜¶è·ƒå‡½æ•°çš„ç»„åˆï¼Œä»è€Œä»¥ä»»æ„ç²¾åº¦é€¼è¿‘ä»»ä½•è¿ç»­å‡½æ•°ã€‚</figcaption>
    </div>

    <div class="blueprint">
      <h4>ä¸‡èƒ½é€¼è¿‘å®šç† Universal Approximation Theorem</h4>
      <p>è€ƒè™‘ä¸¤å±‚æ„ŸçŸ¥æœº $f(x) = c^\top \text{sign}(Ax + b)$ã€‚å¯¹äº<strong>ä»»ä½•</strong>ç´§é›†ä¸Šçš„è¿ç»­å‡½æ•° $g$ï¼Œå¯¹äº<strong>ä»»ä½•</strong> $\epsilon > 0$ï¼Œå­˜åœ¨è¶³å¤Ÿå¤§çš„éšè—å±‚ä½¿å¾— $\|f - g\|_\infty < \epsilon$ã€‚</p>
      <p>æ¢è¨€ä¹‹ï¼Œä¸¤å±‚æ„ŸçŸ¥æœºçš„å‡½æ•°ç±»åœ¨è¿ç»­å‡½æ•°ç©ºé—´ä¸­æ˜¯<strong>ç¨ å¯†çš„</strong>ã€‚</p>
      <p><strong>å†å²</strong>ï¼šç”± Cybenko (1989), Hornik (1991), Barron (1993), Leshno et al. (1993) ç­‰äººåœ¨ 1990s ç‹¬ç«‹è¯æ˜å’Œæ¨å¹¿ã€‚</p>
    </div>

    <div class="callout callout-warning">
      <h4>ä¸‡èƒ½é€¼è¿‘ â‰  å¯å­¦ä¹ ï¼</h4>
      <p>ä¸‡èƒ½é€¼è¿‘å®šç†è¯´çš„æ˜¯<strong>å­˜åœ¨æ€§</strong>â€”â€”å­˜åœ¨ä¸€ç»„æƒé‡ä½¿å¾—ç½‘ç»œèƒ½é€¼è¿‘ç›®æ ‡å‡½æ•°ã€‚ä½†å®ƒ<strong>ä¸</strong>å‘Šè¯‰æˆ‘ä»¬ï¼š</p>
      <ul>
        <li>éœ€è¦<strong>å¤šå¤§</strong>çš„ç½‘ç»œï¼ˆéšè—å±‚å®½åº¦å¯èƒ½éœ€è¦æŒ‡æ•°çº§ï¼‰</li>
        <li>éœ€è¦<strong>å¤šå°‘æ•°æ®</strong>æ‰èƒ½æ‰¾åˆ°è¿™ç»„æƒé‡</li>
        <li>æ¢¯åº¦ä¸‹é™èƒ½å¦<strong>é«˜æ•ˆåœ°</strong>æ‰¾åˆ°è¿™ç»„æƒé‡</li>
      </ul>
      <p>è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸‡èƒ½é€¼è¿‘<strong>ä¸</strong>æ„å‘³ç€æ²¡æœ‰å½’çº³åç½®â€”â€”æ°æ°ç›¸åï¼</p>
    </div>

    <pre><code># ä¸‡èƒ½é€¼è¿‘çš„ç›´è§‰ï¼šé˜¶è·ƒå‡½æ•°çš„ç»„åˆ
import numpy as np
import matplotlib.pyplot as plt

def step_approximation(x, centers, heights, steepness=50):
    """ç”¨ sigmoid (å…‰æ»‘é˜¶è·ƒ) çš„ç»„åˆé€¼è¿‘ä»»æ„å‡½æ•°"""
    result = np.zeros_like(x)
    for c, h in zip(centers, heights):
        result += h * (1 / (1 + np.exp(-steepness * (x - c))))
    return result

# ç›®æ ‡å‡½æ•°: sin(2Ï€x) åœ¨ [0, 1] ä¸Š
x = np.linspace(0, 1, 1000)
target = np.sin(2 * np.pi * x)

# ç”¨ä¸åŒæ•°é‡çš„"é˜¶è·ƒ"æ¥é€¼è¿‘
for n_steps in [5, 10, 50]:
    centers = np.linspace(0, 1, n_steps)
    # ç®€å•åœ°ç”¨ç›®æ ‡å‡½æ•°çš„å·®åˆ†ä½œä¸ºé«˜åº¦
    heights = np.diff(np.sin(2 * np.pi * centers), prepend=0)
    approx = step_approximation(x, centers, heights)
    error = np.mean((approx - target) ** 2)
    print(f"{n_steps:3d} steps â†’ MSE = {error:.6f}")

# è¾“å‡º:
#   5 steps â†’ MSE = 0.089421
#  10 steps â†’ MSE = 0.023156
#  50 steps â†’ MSE = 0.000987
# â†’ éšç€é˜¶è·ƒæ•°å¢åŠ ï¼Œé€¼è¿‘è¶Šæ¥è¶Šå¥½ (ä¸‡èƒ½é€¼è¿‘!)</code></pre>

    <h3 id="complexity-measure">å¤æ‚åº¦åº¦é‡ä¸å½’çº³åç½®</h3>

    <div class="bilingual">
      <div class="zh">
        <p>æ—¢ç„¶ä¸‡èƒ½é€¼è¿‘ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„å…¨éƒ¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹å¼æ¥<strong>åå¥½æŸäº›å‡½æ•°èƒœè¿‡å…¶ä»–å‡½æ•°</strong>ã€‚ç»™å®šä¸€ä¸ªå…·æœ‰ä¸‡èƒ½é€¼è¿‘èƒ½åŠ›çš„å‡è®¾ç©ºé—´ $\mathcal{F}$ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª<span class="term">å¤æ‚åº¦åº¦é‡</span> $c : \mathcal{F} \to \mathbb{R}^+$ å¹¶é‡æ–°å®šä¹‰æ’å€¼é—®é¢˜ï¼š</p>
      </div>
      <div class="en">
        Given a hypothesis space $\mathcal{F}$ with universal approximation, we can define a complexity measure $c : \mathcal{F} \to \mathbb{R}^+$ and redefine our interpolation problem.
      </div>
    </div>

    <div class="math-block">
      $$\tilde{f} \in \arg\min_{g \in \mathcal{F}} c(g) \quad \text{s.t.} \quad g(x_i) = f(x_i) \; \text{ for } i = 1, \ldots, N$$
      <div class="math-explain">
        æˆ‘ä»¬å¯»æ‰¾å‡è®¾ç±»ä¸­<strong>æœ€æ­£åˆ™</strong>ï¼ˆæœ€ç®€å•ã€æœ€å…‰æ»‘ï¼‰çš„å‡½æ•°ï¼ŒåŒæ—¶æ»¡è¶³è®­ç»ƒæ•°æ®çš„çº¦æŸã€‚å¤æ‚åº¦åº¦é‡ $c$ ç¼–ç äº†æˆ‘ä»¬çš„<strong>å½’çº³åç½®</strong>â€”â€”æˆ‘ä»¬è®¤ä¸ºä»€ä¹ˆæ ·çš„å‡½æ•°æ›´"å¯èƒ½"æ˜¯æ­£ç¡®çš„ã€‚
      </div>
    </div>

    <p>å¯¹äºæ ‡å‡†å‡½æ•°ç©ºé—´ï¼Œå¤æ‚åº¦åº¦é‡å¯ä»¥å®šä¹‰ä¸º<strong>èŒƒæ•°</strong>ï¼Œä½¿ $\mathcal{F}$ æˆä¸º Banach ç©ºé—´ï¼š</p>

    <div class="math-block">
      $$\text{ä¸‰æ¬¡æ ·æ¡çš„ä¾‹å­: } \quad c(f) = \int_{-\infty}^{+\infty} |f''(x)|^2 dx$$
      <div class="math-explain">
        <strong>ä¸‰æ¬¡æ ·æ¡</strong>æ˜¯ä½ç»´å‡½æ•°é€¼è¿‘çš„ä¸»åŠ›å·¥å…·ã€‚å®ƒä»¬çš„å¤æ‚åº¦åº¦é‡æ˜¯äºŒé˜¶å¯¼æ•°çš„å¹³æ–¹ç§¯åˆ†â€”â€”ç›´è§‰ä¸Šï¼Œæˆ‘ä»¬åå¥½"å…‰æ»‘"çš„å‡½æ•°ï¼Œå³æ›²ç‡å°çš„å‡½æ•°ã€‚åœ¨é«˜ç»´ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç±»ä¼¼ä½†æ›´å¼ºå¤§çš„æ­£åˆ™æ€§æ¦‚å¿µã€‚
      </div>
    </div>

    <h3 id="regularization">æ­£åˆ™åŒ–ç­–ç•¥</h3>

    <div class="bilingual">
      <div class="zh">
        <p>å¯¹äºç¥ç»ç½‘ç»œï¼Œå¤æ‚åº¦åº¦é‡ $c$ å¯ä»¥ç”¨ç½‘ç»œæƒé‡æ¥è¡¨ç¤ºï¼š$c(f_\theta) = c(\theta)$ã€‚å¸¸è§é€‰æ‹©åŒ…æ‹¬ï¼š</p>
      </div>
      <div class="en">
        For neural networks, the complexity measure $c$ can be expressed in terms of network weights: $c(f_\theta) = c(\theta)$.
      </div>
    </div>

    <table>
      <thead>
        <tr><th>æ­£åˆ™åŒ–æ–¹æ³•</th><th>å¤æ‚åº¦åº¦é‡</th><th>æ•ˆæœ</th></tr>
      </thead>
      <tbody>
        <tr><td><strong>Weight Decay</strong> (L2)</td><td>$c(\theta) = \|\theta\|_2^2$</td><td>åå¥½å°æƒé‡ï¼Œä½¿å‡½æ•°æ›´å…‰æ»‘</td></tr>
        <tr><td><strong>L1 æ­£åˆ™åŒ–</strong></td><td>$c(\theta) = \|\theta\|_1$</td><td>ä¿ƒè¿›ç¨€ç–æ€§ï¼Œæ‰§è¡Œç‰¹å¾é€‰æ‹©</td></tr>
        <tr><td><strong>Path Norm</strong></td><td>$c(\theta) = \sum_{\text{paths}} \prod |w_i|$</td><td>è€ƒè™‘ç½‘ç»œæ·±åº¦çš„å¤æ‚åº¦</td></tr>
        <tr><td><strong>Dropout</strong></td><td>éšæœºç½®é›¶éƒ¨åˆ†æƒé‡</td><td>éšå¼é›†æˆæ­£åˆ™åŒ–</td></tr>
        <tr><td><strong>Batch Norm</strong></td><td>å½’ä¸€åŒ–ä¸­é—´å±‚ç»Ÿè®¡é‡</td><td>å¹³æ»‘æŸå¤±æ™¯è§‚</td></tr>
      </tbody>
    </table>

    <h3 id="implicit-reg">éšå¼æ­£åˆ™åŒ–</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ä»<strong>è´å¶æ–¯è§†è§’</strong>ï¼Œå¤æ‚åº¦åº¦é‡å¯ä»¥è§£é‡Šä¸ºå‡½æ•°å…ˆéªŒçš„è´Ÿå¯¹æ•°ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œå¤æ‚åº¦å¯ä»¥é€šè¿‡ä¼˜åŒ–æ–¹æ¡ˆ<strong>éšå¼</strong>åœ°å¼ºåˆ¶æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œæ¢¯åº¦ä¸‹é™åœ¨æ¬ å®šæœ€å°äºŒä¹˜é—®é¢˜ä¸Šä¼šé€‰æ‹©å…·æœ‰<strong>æœ€å° L2 èŒƒæ•°</strong>çš„æ’å€¼è§£ã€‚</p>
        <p>è¿™ä¸ªéšå¼æ­£åˆ™åŒ–ç»“æœåœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­çš„æ¨å¹¿æ˜¯å½“å‰ç ”ç©¶çš„æ´»è·ƒé¢†åŸŸã€‚</p>
      </div>
      <div class="en">
        From a Bayesian perspective, complexity measures can be interpreted as the negative log-prior. More generally, complexity can be enforced implicitly through the optimisation scheme. For example, gradient descent on an under-determined least-squares objective chooses interpolating solutions with minimal L2 norm.
      </div>
    </div>

    <div class="math-block">
      $$\underbrace{\min_\theta \sum_{i=1}^N L(f_\theta(x_i), y_i)}_{\text{ç»éªŒé£é™©æœ€å°åŒ–}} + \underbrace{\lambda \cdot c(\theta)}_{\text{æ­£åˆ™åŒ–}} \quad \longleftrightarrow \quad \underbrace{\min_\theta c(\theta)}_{\text{å¤æ‚åº¦æœ€å°åŒ–}} \;\; \text{s.t. } f_\theta(x_i) = y_i$$
      <div class="math-explain">
        <strong>ç»“æ„é£é™©æœ€å°åŒ–</strong>ï¼ˆStructural Risk Minimizationï¼‰ï¼šå·¦è¾¹æ˜¯æ˜¾å¼æ­£åˆ™åŒ–å½¢å¼ï¼ˆè¶…å‚æ•° $\lambda$ æ§åˆ¶æ­£åˆ™åŒ–å¼ºåº¦ï¼‰ï¼Œå³è¾¹æ˜¯çº¦æŸä¼˜åŒ–å½¢å¼ã€‚ä¸¤è€…åœ¨ä¸€å®šæ¡ä»¶ä¸‹ç­‰ä»·ï¼ˆæ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼‰ã€‚æ ¸å¿ƒé—®é¢˜ï¼š<strong>å¦‚ä½•å®šä¹‰æœ‰æ•ˆçš„å…ˆéªŒ</strong>æ¥æ•è·çœŸå®ä¸–ç•Œé¢„æµ‹ä»»åŠ¡çš„æ­£åˆ™æ€§ï¼Ÿ
      </div>
    </div>

    <pre><code># éšå¼æ­£åˆ™åŒ–çš„æ¼”ç¤º: æ¢¯åº¦ä¸‹é™åå¥½ "ç®€å•" è§£
import torch
import torch.nn as nn

# æ¬ å®šé—®é¢˜: 2 ä¸ªæ–¹ç¨‹, 10 ä¸ªæœªçŸ¥æ•°
A = torch.randn(2, 10)
b = torch.randn(2)

# æ–¹æ³• 1: ç›´æ¥æ±‚æœ€å°èŒƒæ•°è§£ (ç†è®ºæœ€ä¼˜)
x_min_norm = A.T @ torch.linalg.solve(A @ A.T, b)
print(f"æœ€å°èŒƒæ•°è§£çš„ L2 èŒƒæ•°: {x_min_norm.norm():.4f}")

# æ–¹æ³• 2: æ¢¯åº¦ä¸‹é™ (ä»é›¶åˆå§‹åŒ–)
x_gd = torch.zeros(10, requires_grad=True)
optimizer = torch.optim.SGD([x_gd], lr=0.01)

for step in range(5000):
    loss = ((A @ x_gd - b) ** 2).sum()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(f"æ¢¯åº¦ä¸‹é™è§£çš„ L2 èŒƒæ•°: {x_gd.data.norm():.4f}")
print(f"ä¸¤è€…å·®å¼‚: {(x_gd.data - x_min_norm).norm():.6f}")
# â†’ æ¢¯åº¦ä¸‹é™è‡ªåŠ¨æ‰¾åˆ°äº†æœ€å°èŒƒæ•°è§£ï¼è¿™å°±æ˜¯ "éšå¼æ­£åˆ™åŒ–"</code></pre>

    <!-- ========= 2.2 ç»´åº¦ç¾éš¾ ========= -->
    <h2 id="sec2-2">2.2 ç»´åº¦ç¾éš¾<br><span style="font-size:0.7em;color:var(--text-secondary)">The Curse of Dimensionality</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>è™½ç„¶åœ¨ä½ç»´ï¼ˆ$d = 1, 2, 3$ï¼‰ä¸­ï¼Œæ’å€¼æ˜¯ä¸€ä¸ªç»å…¸çš„ä¿¡å·å¤„ç†ä»»åŠ¡ï¼Œæœ‰éå¸¸ç²¾ç¡®çš„æ•°å­¦æ§åˆ¶ï¼Œä½†<strong>é«˜ç»´é—®é¢˜çš„æƒ…å†µå®Œå…¨ä¸åŒ</strong>ã€‚</p>
        <p>ä¸ºäº†ä¼ è¾¾æ ¸å¿ƒæ€æƒ³ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç»å…¸çš„æ­£åˆ™æ€§æ¦‚å¿µï¼š<strong>Lipschitz å‡½æ•°</strong>ã€‚</p>
      </div>
      <div class="en">
        While interpolation in low dimensions is a classic signal processing task with precise mathematical control, the situation for high-dimensional problems is entirely different.
      </div>
    </div>

    <h3 id="lipschitz">Lipschitz ç±»çš„ç¾éš¾</h3>

    <div class="blueprint">
      <h4>1-Lipschitz å‡½æ•°ç±»</h4>
      <p>å‡½æ•° $f : \mathcal{X} \to \mathbb{R}$ æ˜¯ <strong>1-Lipschitz</strong> çš„ï¼Œå¦‚æœï¼š</p>
      $$|f(x) - f(x')| \leq \|x - x'\| \quad \text{for all } x, x' \in \mathcal{X}$$
      <div class="math-explain">
        è¿™ä¸ªå‡è®¾åªè¦æ±‚ç›®æ ‡å‡½æ•°æ˜¯<strong>å±€éƒ¨å…‰æ»‘</strong>çš„ï¼šå¦‚æœç¨å¾®æ‰°åŠ¨è¾“å…¥ $x$ï¼ˆç”¨èŒƒæ•° $\|x - x'\|$ åº¦é‡ï¼‰ï¼Œè¾“å‡º $f(x)$ ä¸èƒ½å˜åŒ–å¤ªå¤§ã€‚è¿™æ˜¯ä¸€ä¸ª<strong>éå¸¸æ¸©å’Œ</strong>çš„å‡è®¾â€”â€”å‡ ä¹æ‰€æœ‰å®é™…å‡½æ•°éƒ½æ»¡è¶³ã€‚
      </div>
    </div>

    <p>ç°åœ¨å…³é”®é—®é¢˜æ˜¯ï¼šå¦‚æœæˆ‘ä»¬å¯¹ç›®æ ‡å‡½æ•° $f$ çš„å”¯ä¸€çŸ¥è¯†æ˜¯å®ƒæ˜¯ 1-Lipschitz çš„ï¼Œæˆ‘ä»¬éœ€è¦å¤šå°‘è§‚æµ‹æ‰èƒ½ä¿è¯ä¼°è®¡ $\tilde{f}$ æ¥è¿‘ $f$ï¼Ÿ</p>

    <div class="math-block">
      $$N(\epsilon, d) \geq \left(\frac{1}{\epsilon}\right)^d$$
      <div class="math-explain">
        <strong>ç­”æ¡ˆæ˜¯ï¼šåœ¨ç»´åº¦ $d$ ä¸­æ˜¯æŒ‡æ•°çº§çš„</strong>ã€‚ä¸ºäº†å°†æœ€åæƒ…å†µè¯¯å·®æ§åˆ¶åœ¨ $\epsilon$ ä»¥å†…ï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦ $(1/\epsilon)^d$ ä¸ªæ ·æœ¬ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ Lipschitz ç±»"å¢é•¿å¾—å¤ªå¿«"â€”â€”éšç€è¾“å…¥ç»´åº¦çš„å¢åŠ ï¼Œå‡½æ•°ç©ºé—´å‘ˆæŒ‡æ•°å¢é•¿ã€‚
      </div>
    </div>

    <div class="callout callout-key">
      <h4>ç›´è§‰ç†è§£</h4>
      <p>ä¸ºä»€ä¹ˆéœ€è¦æŒ‡æ•°çº§æ ·æœ¬ï¼Ÿè€ƒè™‘ $d$ ç»´å•ä½è¶…ç«‹æ–¹ä½“ $[0,1]^d$ï¼š</p>
      <ul>
        <li>ä¸ºäº†åœ¨æ¯ä¸ªåŒºåŸŸéƒ½æœ‰"ç»éªŒ"ï¼Œæˆ‘ä»¬éœ€è¦å°†ç©ºé—´åˆ†æˆå°åŒºåŸŸ</li>
        <li>å¦‚æœæ¯ä¸ªç»´åº¦åˆ†æˆ $m$ ä¸ªåŒºé—´ï¼Œæ€»å…±æœ‰ $m^d$ ä¸ªå°è¶…ç«‹æ–¹ä½“</li>
        <li>æ¯ä¸ªå°è¶…ç«‹æ–¹ä½“è‡³å°‘éœ€è¦ 1 ä¸ªæ ·æœ¬ â†’ æ€»å…±éœ€è¦ $m^d$ ä¸ªæ ·æœ¬</li>
        <li>å¯¹äº $d = 100$ï¼Œ$m = 10$ï¼š$10^{100}$ ä¸ªæ ·æœ¬ï¼ˆè°·æ­Œçš„è°·æ­Œï¼ï¼‰</li>
      </ul>
    </div>

    <h3 id="sobolev">Sobolev ç±»ä¹Ÿæ— æ³•å¹¸å…</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ä¹Ÿè®¸ç”¨æ›´å¼ºçš„å…‰æ»‘æ€§å‡è®¾å¯ä»¥æ”¹å–„ï¼ŸSobolev ç±» $H^s(\Omega^d)$ è¦æ±‚å‡½æ•°çš„ $s$ é˜¶å¹¿ä¹‰å¯¼æ•°æ˜¯å¹³æ–¹å¯ç§¯çš„â€”â€”è¿™æ¯” Lipschitz å¼ºå¾—å¤šã€‚</p>
      </div>
      <div class="en">
        Perhaps stronger smoothness assumptions can help? The Sobolev class $H^s(\Omega^d)$ requires the generalised $s$-th order derivative to be square-integrable.
      </div>
    </div>

    <div class="blueprint">
      <h4>Sobolev ç±» $H^s(\Omega^d)$</h4>
      <p>å‡½æ•° $f$ å±äº Sobolev ç±» $H^s(\Omega^d)$ï¼Œå¦‚æœ $f \in L^2(\Omega^d)$ ä¸”å¹¿ä¹‰ $s$ é˜¶å¯¼æ•°å¹³æ–¹å¯ç§¯ï¼š</p>
      $$\int |\omega|^{2s+1} |\hat{f}(\omega)|^2 d\omega < \infty$$
      <p>å…¶ä¸­ $\hat{f}$ æ˜¯ $f$ çš„ Fourier å˜æ¢ã€‚</p>
    </div>

    <div class="math-block">
      $$\text{Minimax é€¼è¿‘ç‡: } \quad N(\epsilon, d, s) \sim \epsilon^{-d/s}$$
      <div class="math-explain">
        ç»å…¸ç»“æœï¼ˆTsybakov, 2008ï¼‰å»ºç«‹äº† Sobolev ç±»çš„ minimax é€¼è¿‘å’Œå­¦ä¹ ç‡ä¸º $\epsilon^{-d/s}$ é‡çº§ã€‚é¢å¤–çš„å…‰æ»‘æ€§å‡è®¾ï¼ˆ$s$ è¶Šå¤§æ„å‘³ç€è¶Šå…‰æ»‘ï¼‰åªåœ¨ $s \propto d$ æ—¶æ‰èƒ½æ”¹å–„ç»Ÿè®¡å›¾æ™¯â€”â€”ä½†è¿™åœ¨å®è·µä¸­æ˜¯<strong>ä¸ç°å®çš„å‡è®¾</strong>ã€‚
        <br><br>
        <strong>æ ¸å¿ƒç»“è®º</strong>ï¼šå…¨å±€å…‰æ»‘æ€§å‡è®¾<strong>æ— æ³•</strong>å…‹æœç»´åº¦ç¾éš¾ã€‚æˆ‘ä»¬éœ€è¦æ ¹æœ¬ä¸åŒçš„æ­£åˆ™æ€§æ¦‚å¿µã€‚
      </div>
    </div>

    <h3 id="volume-concentration">ä½“ç§¯é›†ä¸­ç°è±¡</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ç»´åº¦ç¾éš¾æœ‰å¤šç§ç­‰ä»·çš„ç›´è§‰è¡¨è¿°ã€‚<span class="term">ä½“ç§¯é›†ä¸­</span>ç°è±¡æ˜¯å…¶ä¸­ä¸€ä¸ªæœ€ä»¤äººéœ‡æƒŠçš„ï¼š</p>
      </div>
      <div class="en">
        The curse of dimensionality has many equivalent intuitive formulations. Volume concentration is among the most striking.
      </div>
    </div>

    <div class="math-block">
      $$\frac{V_d(r)}{V_d(R)} = \left(\frac{r}{R}\right)^d \xrightarrow{d \to \infty} 0 \quad \text{for any } r < R$$
      <div class="math-explain">
        $d$ ç»´çƒçš„ä½“ç§¯æ¯” $V_d(r)/V_d(R)$ éšç»´åº¦æŒ‡æ•°è¡°å‡ã€‚è¿™æ„å‘³ç€ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œ<strong>å‡ ä¹æ‰€æœ‰çš„ä½“ç§¯éƒ½é›†ä¸­åœ¨"å£³"ä¸Š</strong>ï¼Œè€Œä¸æ˜¯åœ¨å†…éƒ¨ã€‚
        <br><br>
        <strong>å…·ä½“ä¾‹å­</strong>ï¼š$d = 100$ ç»´çƒä¸­ï¼Œ99% çš„ä½“ç§¯åœ¨å¤–å±‚ 5% çš„å£³ä¸­ï¼š$(0.95)^{100} \approx 0.006$ï¼Œå³å†…éƒ¨ 95% åŠå¾„çš„çƒåªå æ€»ä½“ç§¯çš„ 0.6%ï¼
      </div>
    </div>

    <pre><code># ä½“ç§¯é›†ä¸­ç°è±¡çš„æ¼”ç¤º
import numpy as np

def volume_ratio(r_inner, r_outer, d):
    """å†…çƒä½“ç§¯å å¤–çƒä½“ç§¯çš„æ¯”ä¾‹"""
    return (r_inner / r_outer) ** d

# å†…çƒåŠå¾„ = å¤–çƒåŠå¾„çš„ 95%
for d in [1, 2, 3, 10, 50, 100, 500, 1000]:
    ratio = volume_ratio(0.95, 1.0, d)
    print(f"d = {d:5d}: V(0.95R)/V(R) = {ratio:.10f}")

# è¾“å‡º:
# d =     1: V(0.95R)/V(R) = 0.9500000000
# d =     2: V(0.95R)/V(R) = 0.9025000000
# d =     3: V(0.95R)/V(R) = 0.8573750000
# d =    10: V(0.95R)/V(R) = 0.5987369392
# d =    50: V(0.95R)/V(R) = 0.0769438585
# d =   100: V(0.95R)/V(R) = 0.0059205100
# d =   500: V(0.95R)/V(R) = 0.0000000000  (æ•°å€¼ä¸‹æº¢!)
# d =  1000: V(0.95R)/V(R) = 0.0000000000
# â†’ é«˜ç»´çƒå‡ ä¹æ‰€æœ‰ä½“ç§¯éƒ½åœ¨"å£³"ä¸Š!</code></pre>

    <h3 id="geometric-intuition">æ›´å¤šå‡ ä½•ç›´è§‰</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ç»´åº¦ç¾éš¾è¿˜æœ‰å…¶ä»–ä»¤äººæƒŠè®¶çš„å‡ ä½•åæœï¼š</p>
      </div>
    </div>

    <div class="callout callout-info">
      <h4>é«˜ç»´ç©ºé—´çš„åç›´è§‰æ€§è´¨</h4>
      <ol>
        <li><strong>æœ€è¿‘é‚»å¤±æ•ˆ</strong>ï¼šåœ¨é«˜ç»´ä¸­ï¼Œä»»ä½•ç‚¹çš„æœ€è¿‘é‚»å’Œæœ€è¿œé‚»çš„è·ç¦»æ¯”è¶‹å‘äº 1ï¼š
        $$\frac{\text{dist}_{\max} - \text{dist}_{\min}}{\text{dist}_{\min}} \to 0 \quad \text{as } d \to \infty$$
        è¿™æ„å‘³ç€"æœ€è¿‘é‚»"çš„æ¦‚å¿µå¤±å»äº†æ„ä¹‰â€”â€”æ‰€æœ‰ç‚¹éƒ½"å·®ä¸å¤šè¿œ"ã€‚</li>
        <li><strong>æ­£äº¤æ€§</strong>ï¼šä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸­éšæœºæŠ½å–çš„ä¸¤ä¸ªå‘é‡ï¼Œå®ƒä»¬ä¹‹é—´çš„è§’åº¦è¶‹å‘äº $90Â°$ï¼šæ‰€æœ‰éšæœºæ–¹å‘éƒ½è¿‘ä¼¼æ­£äº¤ã€‚</li>
        <li><strong>é«˜æ–¯åˆ†å¸ƒçš„å£³åŒ–</strong>ï¼š$d$ ç»´æ ‡å‡†é«˜æ–¯éšæœºå‘é‡çš„èŒƒæ•°é›†ä¸­åœ¨ $\sqrt{d}$ é™„è¿‘ï¼š
        $$\|x\|_2 \approx \sqrt{d} \pm O(1) \quad \text{when } x \sim \mathcal{N}(0, I_d)$$</li>
      </ol>
    </div>

    <pre><code># é«˜ç»´ç©ºé—´çš„åç›´è§‰æ€§è´¨æ¼”ç¤º
import numpy as np

np.random.seed(42)

print("=== 1. æœ€è¿‘é‚»ä¸æœ€è¿œé‚»çš„è·ç¦»æ¯” ===")
for d in [2, 10, 100, 1000, 10000]:
    # ç”Ÿæˆ 100 ä¸ª d ç»´éšæœºç‚¹
    points = np.random.randn(100, d)
    query = np.random.randn(d)
    dists = np.linalg.norm(points - query, axis=1)
    ratio = (dists.max() - dists.min()) / dists.min()
    print(f"d = {d:6d}: (max-min)/min = {ratio:.4f}")

print("\n=== 2. éšæœºå‘é‡ä¹‹é—´çš„è§’åº¦ ===")
for d in [2, 10, 100, 1000]:
    angles = []
    for _ in range(1000):
        a, b = np.random.randn(d), np.random.randn(d)
        cos_angle = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
        angles.append(np.degrees(np.arccos(np.clip(cos_angle, -1, 1))))
    print(f"d = {d:5d}: mean angle = {np.mean(angles):.1f}Â° Â± {np.std(angles):.1f}Â°")

print("\n=== 3. é«˜æ–¯å‘é‡çš„èŒƒæ•°é›†ä¸­ ===")
for d in [10, 100, 1000, 10000]:
    norms = np.linalg.norm(np.random.randn(10000, d), axis=1)
    print(f"d = {d:6d}: ||x|| = {norms.mean():.2f} Â± {norms.std():.2f} "
          f"(theory: âˆšd = {np.sqrt(d):.2f})")
</code></pre>

    <!-- ========= 2.3 æ‰“ç ´ç»´åº¦ç¾éš¾ ========= -->
    <h2 id="breaking-curse">2.3 æ‰“ç ´ç»´åº¦ç¾éš¾<br><span style="font-size:0.7em;color:var(--text-secondary)">Breaking the Curse</span></h2>

    <h3 id="fc-nets">å…¨è¿æ¥ç½‘ç»œçš„å°è¯•</h3>

    <div class="bilingual">
      <div class="zh">
        <p>å…¨è¿æ¥ç¥ç»ç½‘ç»œå®šä¹‰çš„å‡½æ•°ç©ºé—´å…è®¸æ›´çµæ´»çš„æ­£åˆ™æ€§æ¦‚å¿µï¼Œé€šè¿‡é€‰æ‹©æƒé‡ä¸Šçš„å¤æ‚åº¦å‡½æ•° $c$ è·å¾—ã€‚ç‰¹åˆ«æ˜¯ï¼Œé€šè¿‡é€‰æ‹©<strong>ä¿ƒè¿›ç¨€ç–æ€§</strong>çš„æ­£åˆ™åŒ–ï¼Œå®ƒä»¬æœ‰èƒ½åŠ›æ‰“ç ´ç»´åº¦ç¾éš¾ï¼ˆBach, 2017ï¼‰ã€‚</p>
      </div>
      <div class="en">
        Fully-connected neural networks define function spaces enabling more flexible notions of regularity. By choosing sparsity-promoting regularisation, they can break the curse of dimensionality.
      </div>
    </div>

    <h3 id="low-dim-proj">ä½ç»´æŠ•å½±å‡è®¾</h3>

    <div class="math-block">
      $$f(x) = g(a_1^\top x, a_2^\top x, \ldots, a_k^\top x), \quad k \ll d$$
      <div class="math-explain">
        <strong>ä½ç»´æŠ•å½±å‡è®¾</strong>ï¼šç›®æ ‡å‡½æ•° $f$ å®é™…ä¸Šåªä¾èµ–äºè¾“å…¥ $x$ çš„<strong>å°‘æ•°å‡ ä¸ªçº¿æ€§æŠ•å½±</strong>ï¼ˆ$a_i^\top x$ï¼‰ï¼Œè€Œä¸æ˜¯å…¨éƒ¨ $d$ ä¸ªç»´åº¦ã€‚å¦‚æœ $k \ll d$ï¼Œé‚£ä¹ˆæœ‰æ•ˆç»´åº¦æ˜¯ $k$ è€Œä¸æ˜¯ $d$ï¼Œä»è€Œé¿å…äº†ç»´åº¦ç¾éš¾ã€‚
        <br><br>
        <strong>ä¾‹å­</strong>ï¼šå²­å‡½æ•° $f(x) = \phi(a^\top x)$ åªå–å†³äºä¸€ä¸ªæ–¹å‘ $a$ï¼ˆ$k = 1$ï¼‰ã€‚è¿™ç§å‡½æ•°å¯ä»¥ç”¨ $O(1/\epsilon^{1/s})$ ä¸ªæ ·æœ¬å­¦ä¹ ï¼Œå®Œå…¨ä¸ä¾èµ–äº $d$ã€‚
      </div>
    </div>

    <h3 id="limitations">å±€é™æ€§ï¼šä¸ºä»€ä¹ˆè¿™ä¸å¤Ÿ</h3>

    <div class="bilingual">
      <div class="zh">
        <p>ä½ç»´æŠ•å½±å‡è®¾çš„å±€é™æ€§åœ¨äºï¼šåœ¨å¤§å¤šæ•°å®é™…åº”ç”¨ä¸­ï¼ˆå¦‚è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³åˆ†æã€ç‰©ç†å­¦ã€åŒ–å­¦ï¼‰ï¼Œæ„Ÿå…´è¶£çš„å‡½æ•°å€¾å‘äºè¡¨ç°å‡º<strong>å¤æ‚çš„é•¿ç¨‹ç›¸å…³æ€§</strong>ï¼Œè¿™æ— æ³•ç”¨ä½ç»´æŠ•å½±æ¥è¡¨è¾¾ã€‚</p>
        <p>å› æ­¤ï¼Œæœ‰å¿…è¦å®šä¹‰ä¸€ç§<strong>æ›¿ä»£çš„æ­£åˆ™æ€§æ¥æº</strong>â€”â€”é€šè¿‡åˆ©ç”¨ç‰©ç†åŸŸçš„ç©ºé—´ç»“æ„å’Œ $f$ çš„å‡ ä½•å…ˆéªŒã€‚è¿™å°±æ˜¯ <strong>Chapter 3</strong> çš„ä¸»é¢˜ã€‚</p>
      </div>
      <div class="en">
        In most real-world applications, functions of interest exhibit complex long-range correlations that cannot be expressed with low-dimensional projections. It is thus necessary to define an alternative source of regularity, by exploiting the spatial structure of the physical domain and the geometric priors of $f$.
      </div>
    </div>

    <div class="callout callout-key">
      <h4>Chapter 2 çš„æ ¸å¿ƒç»“è®º</h4>
      <table>
        <tr><th>æ–¹æ³•</th><th>æ­£åˆ™æ€§å‡è®¾</th><th>èƒ½å¦æ‰“ç ´ç¾éš¾ï¼Ÿ</th><th>é—®é¢˜</th></tr>
        <tr><td>Lipschitz å…‰æ»‘</td><td>å…¨å±€ Lipschitz</td><td>âŒ</td><td>éœ€è¦ $\epsilon^{-d}$ æ ·æœ¬</td></tr>
        <tr><td>Sobolev å…‰æ»‘</td><td>å…¨å±€ $s$ é˜¶å…‰æ»‘</td><td>âŒ (é™¤é $s \propto d$)</td><td>éœ€è¦ $\epsilon^{-d/s}$ æ ·æœ¬</td></tr>
        <tr><td>ç¨€ç– FC ç½‘ç»œ</td><td>ä¾èµ–äºä½ç»´æŠ•å½±</td><td>âœ… (åœ¨å‡è®¾ä¸‹)</td><td>å‡è®¾å¤ªå¼ºï¼Œä¸é€‚ç”¨äºå›¾åƒç­‰</td></tr>
        <tr><td><strong>å‡ ä½•å…ˆéªŒ</strong></td><td><strong>åˆ©ç”¨åŸŸç»“æ„å’Œå¯¹ç§°æ€§</strong></td><td><strong>âœ…</strong></td><td><strong>éœ€è¦æ­£ç¡®è¯†åˆ«å¯¹ç§°æ€§</strong></td></tr>
      </table>
      <p>åªæœ‰æœ€åä¸€è¡Œâ€”â€”åˆ©ç”¨æ•°æ®çš„å‡ ä½•ç»“æ„â€”â€”æ‰èƒ½åœ¨ç°å®ä¸–ç•Œçš„é«˜ç»´é—®é¢˜ä¸­å¯é åœ°å·¥ä½œã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå‡ ä½•æ·±åº¦å­¦ä¹ ä¸æ˜¯ä¸€ä¸ª"èŠ±å“¨çš„ç†è®º"ï¼Œè€Œæ˜¯ä¸€ä¸ª<strong>å®è·µå¿…éœ€å“</strong>ã€‚</p>
    </div>

    <!-- ========= 2.4 èµ°å‘å‡ ä½•å…ˆéªŒ ========= -->
    <h2 id="geometric-priors">2.4 èµ°å‘å‡ ä½•å…ˆéªŒ<br><span style="font-size:0.7em;color:var(--text-secondary)">Towards Geometric Priors</span></h2>

    <h3 id="structure-exploitation">åˆ©ç”¨ç»“æ„</h3>

    <div class="bilingual">
      <div class="zh">
        <p>è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“ä¾‹å­æ¥ç†è§£"åˆ©ç”¨ç»“æ„"æ„å‘³ç€ä»€ä¹ˆã€‚è€ƒè™‘å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼š</p>
      </div>
    </div>

    <div class="math-block">
      $$f : \mathbb{R}^{H \times W \times 3} \to \{1, \ldots, K\}$$
      <div class="math-explain">
        å›¾åƒåˆ†ç±»å‡½æ•°å°† $H \times W$ çš„ RGB å›¾åƒæ˜ å°„åˆ° $K$ ä¸ªç±»åˆ«ä¹‹ä¸€ã€‚
        <br><br>
        <strong>ä¸åˆ©ç”¨ç»“æ„</strong>ï¼šå°†å›¾åƒå±•å¹³ä¸º $d = H \times W \times 3$ ç»´å‘é‡ï¼Œç”¨å…¨è¿æ¥ç½‘ç»œã€‚éœ€è¦å­¦ä¹  $O(d^2)$ ä¸ªå‚æ•°ï¼Œå¿½ç•¥äº†åƒç´ çš„ç©ºé—´å…³ç³»ã€‚
        <br><br>
        <strong>åˆ©ç”¨ç»“æ„</strong>ï¼š
        <br>â€¢ <strong>å¹³ç§»ä¸å˜æ€§</strong>ï¼šçŒ«åœ¨å›¾åƒå·¦è¾¹å’Œå³è¾¹åº”è¯¥è¢«åŒæ ·è¯†åˆ« â†’ å·ç§¯ï¼ˆå…±äº«æƒé‡ï¼‰
        <br>â€¢ <strong>å±€éƒ¨æ€§</strong>ï¼šå›¾åƒçš„æœ‰ç”¨ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰æ˜¯å±€éƒ¨çš„ â†’ å°å·ç§¯æ ¸
        <br>â€¢ <strong>å±‚çº§æ€§</strong>ï¼šå¤æ‚ç‰¹å¾ç”±ç®€å•ç‰¹å¾ç»„æˆ â†’ æ·±åº¦ç½‘ç»œ + æ± åŒ–
      </div>
    </div>

    <pre><code># å¯¹æ¯”: ä¸åˆ©ç”¨ç»“æ„ vs åˆ©ç”¨ç»“æ„
import torch.nn as nn

# æ–¹æ³• 1: å…¨è¿æ¥ (ä¸åˆ©ç”¨ç»“æ„)
class FCClassifier(nn.Module):
    def __init__(self, d=3072, K=10):  # 32x32x3 CIFAR-10
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(d, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, K)
        )

    def forward(self, x):
        return self.net(x)

# æ–¹æ³• 2: CNN (åˆ©ç”¨å¹³ç§»ä¸å˜æ€§ + å±€éƒ¨æ€§)
class CNNClassifier(nn.Module):
    def __init__(self, K=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)
        )
        self.classifier = nn.Linear(128, K)

    def forward(self, x):
        x = self.features(x).flatten(1)
        return self.classifier(x)

# å‚æ•°é‡å¯¹æ¯”
fc = FCClassifier()
cnn = CNNClassifier()
fc_params = sum(p.numel() for p in fc.parameters())
cnn_params = sum(p.numel() for p in cnn.parameters())

print(f"FC å‚æ•°é‡:  {fc_params:>10,}")
print(f"CNN å‚æ•°é‡: {cnn_params:>10,}")
print(f"æ¯”ä¾‹: FC/CNN = {fc_params / cnn_params:.1f}x")
print()
print("CNN å‚æ•°å°‘å¾—å¤š, ä½†åœ¨å›¾åƒä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½!")
print("åŸå› : åˆ©ç”¨äº†å¹³ç§»ä¸å˜æ€§ä½œä¸ºå½’çº³åç½®")
print("è¿™ä¸æ˜¯ 'å‡å°‘å‚æ•°' çš„æŠ€å·§, è€Œæ˜¯ 'ç”¨å¯¹ç§°æ€§çº¦æŸæœç´¢ç©ºé—´'")</code></pre>

    <h3 id="preview-ch3">Chapter 3 é¢„è§ˆï¼šå‡ ä½•å…ˆéªŒå°†æä¾›ä»€ä¹ˆï¼Ÿ</h3>

    <div class="bilingual">
      <div class="zh">
        <p>Chapter 3 å°†æ­£å¼å»ºç«‹å‡ ä½•å…ˆéªŒçš„æ•°å­¦æ¡†æ¶ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š</p>
        <ol>
          <li>å¦‚ä½•ç”¨<strong>ç¾¤è®º</strong>å½¢å¼åŒ–"å¯¹ç§°æ€§"ï¼Ÿ</li>
          <li><strong>ä¸å˜æ€§</strong>å’Œ<strong>ç­‰å˜æ€§</strong>å¦‚ä½•ç²¾ç¡®çº¦æŸå‡½æ•°ç©ºé—´ï¼Ÿ</li>
          <li>é™¤äº†ç²¾ç¡®å¯¹ç§°æ€§ï¼Œ<strong>å½¢å˜ç¨³å®šæ€§</strong>å¦‚ä½•å¤„ç†"è¿‘ä¼¼å¯¹ç§°æ€§"ï¼Ÿ</li>
          <li><strong>å°ºåº¦åˆ†ç¦»</strong>å¦‚ä½•å¼•å‡ºå±‚çº§æ¶æ„ï¼Ÿ</li>
          <li>å¦‚ä½•å°†è¿™äº›æ¦‚å¿µç»Ÿä¸€ä¸ºä¸€ä¸ª<strong>GDL è“å›¾</strong>ï¼Ÿ</li>
        </ol>
      </div>
    </div>

    <div class="math-block">
      $$\text{Chapter 2 ç»“è®º: } \underbrace{\text{é€šç”¨é«˜ç»´å­¦ä¹ ä¸å¯è¡Œ}}_{\text{ç»´åº¦ç¾éš¾}} \implies \underbrace{\text{å¿…é¡»åˆ©ç”¨æ•°æ®ç»“æ„}}_{\text{å½’çº³åç½®}} \implies \underbrace{\text{å¯¹ç§°æ€§æ˜¯æœ€å¼ºçš„å…ˆéªŒ}}_{\text{å‡ ä½•æ·±åº¦å­¦ä¹ }}$$
    </div>

    <!-- ========= ä»£ç ç¤ºä¾‹ ========= -->
    <h2 id="code-examples">ç»¼åˆä»£ç ç¤ºä¾‹<br><span style="font-size:0.7em;color:var(--text-secondary)">Code Examples</span></h2>

    <h3 id="curse-demo">ç¤ºä¾‹ 1ï¼šç»´åº¦ç¾éš¾çš„å¯è§†åŒ–</h3>

    <pre><code># ç»´åº¦ç¾éš¾çš„å¯è§†åŒ–æ¼”ç¤º
import numpy as np
import matplotlib.pyplot as plt

def curse_of_dimensionality_demo():
    """
    æ¼”ç¤ºç»´åº¦ç¾éš¾: éšç€ç»´åº¦å¢åŠ , 
    å­¦ä¹  Lipschitz å‡½æ•°æ‰€éœ€çš„æ ·æœ¬æ•°æŒ‡æ•°å¢é•¿
    """
    np.random.seed(42)

    # åœ¨ d ç»´å•ä½è¶…ç«‹æ–¹ä½“ä¸­ç”Ÿæˆéšæœºé‡‡æ ·ç‚¹
    # è®¡ç®—è¦†ç›–ç‡: åœ¨ N ä¸ªæ ·æœ¬ä¸‹, æœ€å¤§æœªè¦†ç›–é—´éš”

    dims = [1, 2, 3, 5, 10, 20]
    N_samples = 1000

    print("=== ç»´åº¦ç¾éš¾: é‡‡æ ·è¦†ç›–ç‡ ===")
    print(f"ä½¿ç”¨ {N_samples} ä¸ªå‡åŒ€éšæœºæ ·æœ¬")
    print()

    for d in dims:
        # ç”Ÿæˆ N ä¸ª d ç»´éšæœºç‚¹
        points = np.random.rand(N_samples, d)

        # è®¡ç®—æœ€è¿‘é‚»è·ç¦»çš„ç»Ÿè®¡
        from scipy.spatial.distance import cdist
        if d <= 10:  # é«˜ç»´è®¡ç®—å¤ªæ…¢
            dists = cdist(points[:200], points[:200])
            np.fill_diagonal(dists, np.inf)
            min_dists = dists.min(axis=1)
            mean_nn_dist = min_dists.mean()
        else:
            # è¿‘ä¼¼
            mean_nn_dist = (1 / N_samples) ** (1/d) * np.sqrt(d/6)

        # ç†è®º: è¦è¦†ç›– d ç»´ç©ºé—´åˆ°ç²¾åº¦ Îµ, éœ€è¦ (1/Îµ)^d ä¸ªæ ·æœ¬
        # ç­‰ä»·åœ°: N ä¸ªæ ·æœ¬åªèƒ½è¦†ç›–åˆ°ç²¾åº¦ N^(-1/d)
        coverage_precision = N_samples ** (-1.0/d)

        print(f"d = {d:3d}: è¦†ç›–ç²¾åº¦ â‰ˆ {coverage_precision:.4f}, "
              f"å¹³å‡æœ€è¿‘é‚»è·ç¦» â‰ˆ {mean_nn_dist:.4f}")

    print()
    print("=== éœ€è¦å¤šå°‘æ ·æœ¬æ‰èƒ½è¾¾åˆ° Îµ = 0.1 çš„ç²¾åº¦ï¼Ÿ===")
    for d in [1, 2, 5, 10, 20, 50, 100]:
        N_needed = (1/0.1) ** d  # = 10^d
        print(f"d = {d:4d}: N â‰¥ 10^{d} = {N_needed:.0e}" +
              (" â† å®‡å®™åŸå­æ•° â‰ˆ 10^80" if d == 80 else ""))

curse_of_dimensionality_demo()</code></pre>

    <h3 id="mlp-demo">ç¤ºä¾‹ 2ï¼šMLP ä¸‡èƒ½é€¼è¿‘ vs æ³›åŒ–</h3>

    <pre><code># MLP å¯ä»¥æ‹Ÿåˆä»»ä½•å‡½æ•°, ä½†æ³›åŒ–éœ€è¦å½’çº³åç½®
import torch
import torch.nn as nn
import numpy as np

torch.manual_seed(42)

# ç›®æ ‡å‡½æ•°: ä»…ä¾èµ–äºå‰ä¸¤ä¸ªç»´åº¦çš„å‡½æ•° (åœ¨é«˜ç»´ç©ºé—´ä¸­)
def target_fn(x):
    """f(x) = sin(x_0) * cos(x_1), å¿½ç•¥ x_2, ..., x_{d-1}"""
    return torch.sin(x[:, 0]) * torch.cos(x[:, 1])

d = 50  # 50 ç»´è¾“å…¥, ä½†å‡½æ•°åªä¾èµ–äº 2 ç»´
N_train = 200
N_test = 1000

# è®­ç»ƒæ•°æ®
x_train = torch.randn(N_train, d)
y_train = target_fn(x_train)

# æµ‹è¯•æ•°æ®
x_test = torch.randn(N_test, d)
y_test = target_fn(x_test)

# æ¨¡å‹: ç®€å• MLP (æ²¡æœ‰åˆ©ç”¨ "ä½ç»´æŠ•å½±" ç»“æ„)
model = nn.Sequential(
    nn.Linear(d, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 1)
)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# è®­ç»ƒ
for epoch in range(2000):
    pred = model(x_train).squeeze()
    loss = ((pred - y_train) ** 2).mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# è¯„ä¼°
with torch.no_grad():
    train_loss = ((model(x_train).squeeze() - y_train) ** 2).mean()
    test_loss = ((model(x_test).squeeze() - y_test) ** 2).mean()

print(f"è®­ç»ƒæŸå¤±: {train_loss:.6f}")
print(f"æµ‹è¯•æŸå¤±: {test_loss:.6f}")
print(f"æ³›åŒ– gap: {test_loss - train_loss:.6f}")
print()
print("å°½ç®¡ MLP å¯ä»¥å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ® (ä¸‡èƒ½é€¼è¿‘),")
print("ä½†åœ¨ 50 ç»´ç©ºé—´ä¸­, 200 ä¸ªæ ·æœ¬ä¸è¶³ä»¥æ³›åŒ–ã€‚")
print("å¦‚æœæˆ‘ä»¬çŸ¥é“å‡½æ•°åªä¾èµ–äº 2 ç»´, å¯ä»¥å¤§å¹…æ”¹å–„!")
print("â†’ è¿™å°±æ˜¯ 'æ­£ç¡®çš„å½’çº³åç½®' çš„ä»·å€¼")</code></pre>

    <h3 id="bias-demo">ç¤ºä¾‹ 3ï¼šå½’çº³åç½®çš„åŠ›é‡</h3>

    <pre><code># å½’çº³åç½®çš„åŠ›é‡: æ¯”è¾ƒä¸åŒå…ˆéªŒä¸‹çš„å­¦ä¹ æ•ˆç‡
import torch
import torch.nn as nn
import numpy as np

torch.manual_seed(42)

# ä»»åŠ¡: å­¦ä¹ ä¸€ä¸ªå…·æœ‰å¹³ç§»ä¸å˜æ€§çš„å‡½æ•°
# f(x) = sum_i g(x_i) å…¶ä¸­ g æ˜¯ç›¸åŒçš„éçº¿æ€§å‡½æ•°

def target_equivariant(x):
    """å¯¹æ‰€æœ‰ä½ç½®åº”ç”¨ç›¸åŒçš„éçº¿æ€§"""
    return torch.sin(x).sum(dim=-1)  # å¹³ç§»ä¸å˜: g(x) = sin(x)

d = 20
results = {}

for N in [50, 100, 200, 500, 1000]:
    x_train = torch.randn(N, d)
    y_train = target_equivariant(x_train)
    x_test = torch.randn(1000, d)
    y_test = target_equivariant(x_test)

    # æ–¹æ³• 1: å…¨è¿æ¥ MLP (ä¸åˆ©ç”¨ç»“æ„)
    mlp = nn.Sequential(
        nn.Linear(d, 64), nn.ReLU(),
        nn.Linear(64, 32), nn.ReLU(),
        nn.Linear(32, 1)
    )
    opt1 = torch.optim.Adam(mlp.parameters(), lr=1e-3)
    for _ in range(2000):
        loss = ((mlp(x_train).squeeze() - y_train)**2).mean()
        opt1.zero_grad(); loss.backward(); opt1.step()

    with torch.no_grad():
        mlp_test = ((mlp(x_test).squeeze() - y_test)**2).mean().item()

    # æ–¹æ³• 2: æƒé‡å…±äº« (åˆ©ç”¨å¹³ç§»ä¸å˜æ€§!)
    class SharedWeightNet(nn.Module):
        def __init__(self):
            super().__init__()
            # å¯¹æ¯ä¸ªç»´åº¦åº”ç”¨ç›¸åŒçš„å‡½æ•° (æƒé‡å…±äº«!)
            self.g = nn.Sequential(
                nn.Linear(1, 16), nn.ReLU(),
                nn.Linear(16, 1)
            )
        def forward(self, x):
            # å¯¹æ¯ä¸ªç»´åº¦ç‹¬ç«‹åº”ç”¨ g, ç„¶åæ±‚å’Œ
            return sum(self.g(x[:, i:i+1]).squeeze() for i in range(x.size(1)))

    shared = SharedWeightNet()
    opt2 = torch.optim.Adam(shared.parameters(), lr=1e-3)
    for _ in range(2000):
        loss = ((shared(x_train) - y_train)**2).mean()
        opt2.zero_grad(); loss.backward(); opt2.step()

    with torch.no_grad():
        shared_test = ((shared(x_test) - y_test)**2).mean().item()

    results[N] = (mlp_test, shared_test)
    print(f"N = {N:5d}: MLP test loss = {mlp_test:.4f}, "
          f"Shared test loss = {shared_test:.6f}, "
          f"æ”¹å–„: {mlp_test / max(shared_test, 1e-8):.0f}x")

print()
print("ç»“è®º: åˆ©ç”¨å¹³ç§»ä¸å˜æ€§ (æƒé‡å…±äº«) çš„æ¨¡å‹")
print("åœ¨æ‰€æœ‰æ ·æœ¬é‡ä¸‹éƒ½å¤§å¹…ä¼˜äºä¸åˆ©ç”¨ç»“æ„çš„ MLPã€‚")
print("æ ·æœ¬è¶Šå°‘, å·®è·è¶Šå¤§ â€” è¿™æ­£æ˜¯å½’çº³åç½®çš„ä»·å€¼!")</code></pre>

    <!-- ========= PhysRobot å…³è” ========= -->
    <h2 id="physrobot">PhysRobot å…³è”<br><span style="font-size:0.7em;color:var(--text-secondary)">Connection to PhysRobot</span></h2>

    <div class="callout callout-project">
      <h4>ç»´åº¦ç¾éš¾åœ¨ç‰©ç†ä»¿çœŸä¸­çš„ä½“ç°</h4>
      <p>è€ƒè™‘ PhysRobot ä¸­çš„å…¸å‹é—®é¢˜ï¼šé¢„æµ‹ $N$ ä¸ªç²’å­åœ¨ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ é€Ÿåº¦ã€‚</p>
      <ul>
        <li><strong>è¾“å…¥ç»´åº¦</strong>ï¼š$N$ ä¸ªç²’å­ Ã— æ¯ä¸ªç²’å­ $k$ ä¸ªç‰¹å¾ï¼ˆä½ç½®ã€é€Ÿåº¦ã€ç±»å‹ç­‰ï¼‰= $Nk$ ç»´</li>
        <li>å¯¹äº $N = 1000$ ä¸ªç²’å­ï¼Œ$k = 9$ï¼ˆ3Dä½ç½® + 3Dé€Ÿåº¦ + ç±»å‹ç­‰ï¼‰ï¼š$d = 9000$</li>
        <li><strong>è¾“å‡ºç»´åº¦</strong>ï¼š$N \times 3 = 3000$ ç»´ï¼ˆæ¯ä¸ªç²’å­çš„ 3D åŠ é€Ÿåº¦ï¼‰</li>
      </ul>
      <p>ä¸åˆ©ç”¨ä»»ä½•ç»“æ„ï¼Œç”¨å…¨è¿æ¥ç½‘ç»œï¼Ÿéœ€è¦å­¦ä¹  $9000 \times 3000 = 2700$ ä¸‡ä¸ªå‚æ•°â€”â€”ä»…åœ¨ç¬¬ä¸€å±‚ï¼è€Œä¸”å®Œå…¨æ²¡æœ‰æ³›åŒ–èƒ½åŠ›ã€‚</p>
    </div>

    <div class="callout callout-project">
      <h4>GNS å¦‚ä½•åˆ©ç”¨å‡ ä½•å…ˆéªŒæ‰“ç ´ç¾éš¾</h4>
      <table>
        <tr><th>å¯¹ç§°æ€§</th><th>çº¦æŸ</th><th>æ•ˆæœ</th></tr>
        <tr><td>ç½®æ¢ä¸å˜æ€§</td><td>ç²’å­æ— å›ºæœ‰é¡ºåº</td><td>å‚æ•°é‡ä¸éšç²’å­æ•°å¢é•¿ï¼</td></tr>
        <tr><td>å¹³ç§»ä¸å˜æ€§</td><td>ç‰©ç†å®šå¾‹ä¸ä¾èµ–äºç»å¯¹ä½ç½®</td><td>ä½¿ç”¨ç›¸å¯¹ä½ç½® $r_j - r_i$ è€Œéç»å¯¹ä½ç½®</td></tr>
        <tr><td>æ—‹è½¬ä¸å˜æ€§</td><td>ç‰©ç†å®šå¾‹ä¸ä¾èµ–äºåæ ‡ç³»æ–¹å‘</td><td>ä½¿ç”¨è·ç¦» $\|r_j - r_i\|$ ç­‰ä¸å˜é‡</td></tr>
        <tr><td>å±€éƒ¨æ€§</td><td>ç›¸äº’ä½œç”¨éšè·ç¦»è¡°å‡</td><td>åªå¯¹ k-NN é‚»å±…åšæ¶ˆæ¯ä¼ é€’</td></tr>
      </table>
      <p>ç»“æœï¼šGNS çš„å‚æ•°é‡ä»…çº¦ <strong>~30 ä¸‡</strong>ï¼ˆä¸ 2700 ä¸‡å¯¹æ¯”ï¼‰ï¼Œä½†èƒ½<strong>æ³›åŒ–åˆ°ä¸åŒç²’å­æ•°</strong>å’Œ<strong>ä¸åŒåˆå§‹æ¡ä»¶</strong>ã€‚è¿™å°±æ˜¯å‡ ä½•å…ˆéªŒæ‰“ç ´ç»´åº¦ç¾éš¾çš„åŠ›é‡ã€‚</p>
    </div>

    <pre><code># PhysRobot: ç»´åº¦ç¾éš¾ vs å‡ ä½•å…ˆéªŒ
"""
å…¨è¿æ¥æ–¹æ³•:
- è¾“å…¥: æ‰€æœ‰ç²’å­çš„ç»å¯¹ä½ç½®å’Œé€Ÿåº¦ (Nk ç»´å‘é‡)
- å‚æ•°é‡: O(N^2 k^2) = O(N^2) â€” éšç²’å­æ•°å¹³æ–¹å¢é•¿
- æ— æ³•æ³›åŒ–åˆ°ä¸åŒ N
- æ— æ³•å¤„ç†å¯¹ç§°æ€§

GNS (åˆ©ç”¨å‡ ä½•å…ˆéªŒ):
- è¾“å…¥: æ¯ä¸ªç²’å­çš„å±€éƒ¨é‚»åŸŸ (ç›¸å¯¹ä½ç½®å’Œè·ç¦»)
- å‚æ•°é‡: O(k^2) â€” ä¸ç²’å­æ•° N æ— å…³!
- è‡ªåŠ¨æ»¡è¶³ç½®æ¢ç­‰å˜æ€§
- å¯æ³›åŒ–åˆ°ä»»æ„ N (åœ¨è®­ç»ƒæ—¶ç”¨ N=100, æµ‹è¯•æ—¶ç”¨ N=10000)
"""

# å‚æ•°é‡å¯¹æ¯”
def compare_params(N, k=9, hidden=128, K_neighbors=20):
    # å…¨è¿æ¥
    fc_input = N * k
    fc_output = N * 3
    fc_params = fc_input * hidden + hidden * hidden + hidden * fc_output
    
    # GNS (æ¶ˆæ¯ä¼ é€’)
    edge_feat = 2 * k + 4  # ç›¸å¯¹ä½ç½® + è·ç¦»ç­‰
    gns_params = (edge_feat * hidden +  # æ¶ˆæ¯å‡½æ•°
                  hidden * hidden +     # éšè—å±‚
                  hidden * 3)           # è¾“å‡º (åŠ é€Ÿåº¦)
    gns_params *= 10  # 10å±‚æ¶ˆæ¯ä¼ é€’
    
    return fc_params, gns_params

for N in [100, 500, 1000, 5000]:
    fc, gns = compare_params(N)
    print(f"N = {N:5d}: FC = {fc:>12,} params, GNS = {gns:>10,} params, "
          f"ratio = {fc/gns:.0f}x")</code></pre>

    <!-- ========= ç»ƒä¹ é¢˜ ========= -->
    <div class="exercises" id="exercises">
      <h3>ç»ƒä¹ é¢˜ Exercises</h3>
      <ol>
        <li><strong>Lipschitz ç¾éš¾æ¨å¯¼</strong>ï¼š
          åœ¨ $[0,1]^d$ ä¸Šè€ƒè™‘ 1-Lipschitz å‡½æ•°ã€‚è®¾ $f(x) = 0$ ä¸ºæŸä¸€æ»¡è¶³çº¦æŸçš„å‡½æ•°ï¼Œ$g$ ä¸ºå¦ä¸€ä¸ªä»¥ $x_0$ ä¸ºä¸­å¿ƒçš„"å°–é”¥"å‡½æ•°ã€‚
          (a) æ„é€ è¿™æ ·çš„ $g$ ä½¿å¾— $g(x_0) = r$ ä¸” $g$ æ˜¯ 1-Lipschitz çš„ã€‚
          (b) è¯æ˜å¦‚æœ $x_0$ ç¦»æ‰€æœ‰è§‚æµ‹ç‚¹çš„è·ç¦»éƒ½ $\geq r$ï¼Œåˆ™ $f$ å’Œ $g$ åœ¨æ‰€æœ‰è§‚æµ‹ç‚¹ä¸Šç›¸åŒã€‚
          (c) ç”¨ä½“ç§¯è®ºè¯è¯´æ˜ï¼šå¦‚æœ $N \leq (1/2r)^d$ï¼Œåˆ™è¿™æ ·çš„ $x_0$ å¿…ç„¶å­˜åœ¨ã€‚
        </li>
        <li><strong>ä¸‡èƒ½é€¼è¿‘å®éªŒ</strong>ï¼š
          ä½¿ç”¨ PyTorch å®ç°ä¸€ä¸ªä¸¤å±‚ MLPï¼Œç”¨å®ƒé€¼è¿‘ä»¥ä¸‹å‡½æ•°ï¼š
          (a) $f(x) = x^3$ åœ¨ $[-1, 1]$ ä¸Š
          (b) $f(x) = |x|$ åœ¨ $[-1, 1]$ ä¸Š
          (c) $f(x, y) = \sin(\pi x) \cos(\pi y)$ åœ¨ $[-1,1]^2$ ä¸Š
          æ¯”è¾ƒä¸åŒéšè—å±‚å®½åº¦ï¼ˆ10, 50, 200, 1000ï¼‰ä¸‹çš„é€¼è¿‘è´¨é‡ã€‚
        </li>
        <li><strong>ä½“ç§¯é›†ä¸­</strong>ï¼š
          (a) è¯æ˜ $d$ ç»´å•ä½çƒçš„ä½“ç§¯å…¬å¼ $V_d = \pi^{d/2} / \Gamma(d/2 + 1)$ã€‚
          (b) æ•°å€¼è®¡ç®— $V_d$ å¯¹ $d = 1, 2, \ldots, 100$ã€‚ä½“ç§¯åœ¨ä»€ä¹ˆç»´åº¦è¾¾åˆ°æœ€å¤§ï¼Ÿ
          (c) è§£é‡Šä¸ºä»€ä¹ˆé«˜ç»´çƒçš„ä½“ç§¯è¶‹å‘äºé›¶â€”â€”è¿™ä¸ç»´åº¦ç¾éš¾æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
        </li>
        <li><strong>æ­£åˆ™åŒ–æ¯”è¾ƒ</strong>ï¼š
          åœ¨ MNIST æ•°æ®é›†ä¸Šè®­ç»ƒ MLPï¼Œæ¯”è¾ƒä»¥ä¸‹æ­£åˆ™åŒ–æ–¹æ³•ï¼š
          (a) æ— æ­£åˆ™åŒ–
          (b) L2 weight decay ($\lambda = 10^{-4}$)
          (c) Dropout ($p = 0.5$)
          (d) L1 æ­£åˆ™åŒ– ($\lambda = 10^{-5}$)
          è®°å½•è®­ç»ƒæŸå¤±ã€æµ‹è¯•æŸå¤±ã€ä»¥åŠæƒé‡çš„ç»Ÿè®¡é‡ï¼ˆèŒƒæ•°ã€ç¨€ç–æ€§ï¼‰ã€‚
        </li>
        <li><strong>å½’çº³åç½®å®éªŒ</strong>ï¼š
          è®¾è®¡ä¸€ä¸ªå‡½æ•° $f : \mathbb{R}^{100} \to \mathbb{R}$ ä½¿å¾—ï¼š
          (a) $f$ å…·æœ‰æŸç§å¯¹ç§°æ€§ï¼ˆå¦‚å¹³ç§»ã€ç½®æ¢æˆ–æ—‹è½¬ä¸å˜æ€§ï¼‰
          (b) åœ¨ 100 ç»´ç©ºé—´ä¸­ï¼Œç”¨ $N = 500$ ä¸ªæ ·æœ¬
          (c) æ¯”è¾ƒï¼šå…¨è¿æ¥ MLP vs åˆ©ç”¨äº†è¯¥å¯¹ç§°æ€§çš„æ¶æ„
          å±•ç¤ºå½’çº³åç½®åœ¨<strong>æ ·æœ¬æ•ˆç‡</strong>å’Œ<strong>æ³›åŒ–</strong>ä¸Šçš„ä¼˜åŠ¿ã€‚
        </li>
        <li><strong>PhysRobot æ€è€ƒ</strong>ï¼š
          è€ƒè™‘ç”¨å…¨è¿æ¥ç½‘ç»œå­¦ä¹ ä¸¤ä¸ªç²’å­ä¹‹é—´çš„ Lennard-Jones åŠ¿èƒ½ $V(r) = 4\epsilon[(\sigma/r)^{12} - (\sigma/r)^6]$ã€‚
          (a) è¿™ä¸ªå‡½æ•°æœ‰ä»€ä¹ˆå¯¹ç§°æ€§ï¼Ÿ
          (b) å¦‚æœç²’å­åœ¨ 3D ç©ºé—´ä¸­ï¼Œè¾“å…¥ç»´åº¦æ˜¯å¤šå°‘ï¼Ÿ
          (c) å¦‚æœåˆ©ç”¨æ—‹è½¬ä¸å˜æ€§ï¼ˆå°†è¾“å…¥è½¬åŒ–ä¸ºè·ç¦» $r = \|r_1 - r_2\|$ï¼‰ï¼Œæœ‰æ•ˆè¾“å…¥ç»´åº¦æ˜¯å¤šå°‘ï¼Ÿ
          (d) è®¨è®ºç»´åº¦ä» 6 é™åˆ° 1 å¯¹å­¦ä¹ æ•ˆç‡çš„å½±å“ã€‚
        </li>
        <li><strong>éšå¼æ­£åˆ™åŒ–</strong>ï¼š
          å¯¹äºè¿‡å‚æ•°åŒ–çš„çº¿æ€§å›å½’ $y = Wx$ï¼ˆ$W \in \mathbb{R}^{m \times n}$ï¼Œ$m < n$ï¼‰ï¼Œ
          (a) è¯æ˜æ¢¯åº¦ä¸‹é™ä»é›¶åˆå§‹åŒ–æ”¶æ•›åˆ°æœ€å° Frobenius èŒƒæ•°è§£ã€‚
          (b) è¿™ä¸ªéšå¼æ­£åˆ™åŒ–å¯¹åº”ä»€ä¹ˆæ ·çš„å‡½æ•°åå¥½ï¼Ÿ
          (c) å¯¹äºéçº¿æ€§ç½‘ç»œï¼Œéšå¼æ­£åˆ™åŒ–æ›´å¤æ‚â€”â€”ç»™å‡ºä¸€ä¸ªç›´è§‰æ€§çš„è§£é‡Šã€‚
        </li>
      </ol>
    </div>

    <!-- ========= ç« èŠ‚å¯¼èˆª ========= -->
    <div class="chapter-nav">
      <a href="../chapter1/index.html">â† Chapter 1: å¼•è¨€</a>
      <a href="../chapter3/index.html">Chapter 3: å‡ ä½•å…ˆéªŒ â†’</a>
    </div>
  </main>

  <script src="../assets/script.js"></script>
</body>
</html>

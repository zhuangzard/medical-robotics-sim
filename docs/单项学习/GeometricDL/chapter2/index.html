<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 2: Learning in High Dimensions | GDL 学习指南</title>
  <link rel="stylesheet" href="../assets/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Noto+Serif+SC:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]})"></script>
</head>
<body>
  <div class="progress-bar"></div>

  <header class="header">
    <div class="header-title"><a href="../index.html">📐 GDL 学习指南</a></div>
    <div class="header-nav">
      <a href="../chapter1/index.html">← 上一章</a>
      <a href="../index.html">目录</a>
      <a href="../chapter3/index.html">下一章 →</a>
      <button class="theme-toggle" onclick="toggleTheme()">🌙</button>
    </div>
  </header>

  <button class="sidebar-toggle" onclick="toggleSidebar()">☰</button>

  <nav class="sidebar">
    <h3>Chapter 2</h3>
    <a href="#overview">概述</a>
    <a href="#setup">学习问题设定</a>
    <a href="#sec2-1">2.1 归纳偏置</a>
    <a href="#universal-approx" class="sub">万能逼近定理</a>
    <a href="#complexity" class="sub">复杂度度量</a>
    <a href="#implicit-reg" class="sub">隐式正则化</a>
    <a href="#sec2-2">2.2 维度灾难</a>
    <a href="#lipschitz" class="sub">Lipschitz 函数类</a>
    <a href="#sobolev" class="sub">Sobolev 空间</a>
    <a href="#nn-breaking" class="sub">神经网络的突破</a>
    <a href="#exercises">练习题</a>
    <h3>导航</h3>
    <a href="../index.html">📚 总目录</a>
    <a href="../chapter1/index.html">← Ch.1 引言</a>
    <a href="../chapter3/index.html">→ Ch.3 几何先验</a>
  </nav>

  <main class="main">
    <h1>Chapter 2: Learning in High Dimensions<br><span style="font-size:0.6em;color:var(--text-secondary)">高维空间中的学习</span></h1>

    <div class="callout callout-info" id="overview">
      <h4>本章概述</h4>
      <p>本章阐述高维学习的基本问题设定，引入<strong>归纳偏置</strong>（inductive bias）和<strong>维度灾难</strong>（curse of dimensionality）两个核心概念。这两个概念是理解为什么需要几何先验的基础。</p>
      <p><strong>核心信息</strong>：在高维空间中，没有结构假设的通用函数学习在统计上是不可能的。我们需要利用物理世界的结构来定义有效的正则性。</p>
    </div>

    <h2 id="setup">学习问题的形式化设定</h2>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">监督机器学习</span>（Supervised Machine Learning），在其最简单的形式化中，考虑从底层数据分布 $P$ 中独立同分布（i.i.d.）抽取的 $N$ 个观测的集合。</p>
      </div>
      <div class="en">
        Supervised machine learning, in its simplest formalisation, considers a set of N observations drawn i.i.d. from an underlying data distribution P.
      </div>
    </div>

    <div class="math-block">
      $$\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N \sim P \text{, defined over } \mathcal{X} \times \mathcal{Y}$$
      <div class="math-explain">
        $\mathcal{D}$ 是训练数据集，包含 $N$ 个输入-标签对。$\mathcal{X}$ 是<strong>数据域</strong>（通常是高维欧几里得空间 $\mathbb{R}^d$），$\mathcal{Y}$ 是<strong>标签域</strong>。关键特征：$\mathcal{X}$ 是<strong>高维</strong>的（$d$ 很大）。
      </div>
    </div>

    <div class="bilingual">
      <div class="zh">
        <p>假设标签由一个未知函数 $f$ 生成（$y_i = f(x_i)$），学习问题归结为使用参数化函数类来估计 $f$：</p>
      </div>
      <div class="en">
        Labels y are generated by an unknown function f, and the learning problem reduces to estimating f using a parametrised function class.
      </div>
    </div>

    <div class="math-block">
      $$\mathcal{F} = \{f_\theta : \theta \in \Theta\}$$
      <div class="math-explain">
        $\mathcal{F}$ 是<strong>假设空间</strong>（hypothesis space），$\theta$ 是参数（对于神经网络就是网络权重）。现代深度学习通常在<strong>插值体制</strong>下运行：找到的 $\tilde{f}$ 满足对所有训练样本 $\tilde{f}(x_i) = f(x_i)$。
      </div>
    </div>

    <p>学习算法的性能通过<span class="term">期望风险</span>（Expected Risk）来衡量：</p>

    <div class="math-block">
      $$R(\tilde{f}) := \mathbb{E}_P\, L(\tilde{f}(x), f(x))$$
      <div class="math-explain">
        这是对新样本（而非训练集）的期望损失。最常用的是<strong>平方损失</strong>：$L(y, y') = \frac{1}{2}|y - y'|^2$。好的学习方案需要让期望风险尽可能小——这就需要正确的<strong>归纳偏置</strong>。
      </div>
    </div>

    <h2 id="sec2-1">2.1 通过函数正则性的归纳偏置<br><span style="font-size:0.7em;color:var(--text-secondary)">Inductive Bias via Function Regularity</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>现代机器学习使用大规模、高质量的数据集，加上适当的计算资源，驱使我们设计具有足够容量来<strong>插值</strong>大数据的丰富函数类 $\mathcal{F}$。这与神经网络天然匹配，因为即使是最简单的架构选择也能产生<strong>稠密的函数类</strong>。</p>
      </div>
      <div class="en">
        Modern machine learning operates with large, high-quality datasets, which motivate the design of rich function classes with the capacity to interpolate such large data. This plays well with neural networks, since even the simplest choices of architecture yields a dense class of functions.
      </div>
    </div>

    <h3 id="universal-approx">万能逼近定理 Universal Approximation</h3>

    <div class="callout callout-key">
      <h4>万能逼近定理（Universal Approximation Theorem）</h4>
      <p>即使是一个<strong>两层感知机</strong>（two-layer perceptron）也能逼近任意连续函数：</p>
      $$f(x) = c^\top \text{sign}(Ax + b)$$
      <p>这类函数在连续函数空间 $C(\mathbb{R}^d)$ 中是<strong>稠密的</strong>（dense）。</p>
      <p><strong>直觉</strong>：两层网络可以用阶梯函数的组合来逼近任意连续函数，就像用很多小矩形去逼近一条曲线的面积一样。</p>
    </div>

    <div class="figure">
      <img src="../assets/ch2_p11_img0.png" alt="Figure 1: MLP as universal approximator">
      <figcaption>图 1：多层感知机（Rosenblatt, 1958），最简单的前馈神经网络，是万能逼近器：仅用一个隐藏层，它们就能表示阶跃函数的组合，从而以任意精度逼近任何连续函数。</figcaption>
    </div>

    <h3 id="complexity">复杂度度量与正则化</h3>

    <div class="bilingual">
      <div class="zh">
        <p>然而，万能逼近<strong>并不</strong>意味着没有归纳偏置！给定一个具有万能逼近能力的假设空间 $\mathcal{F}$，我们可以定义一个<span class="term">复杂度度量</span> $c : \mathcal{F} \to \mathbb{R}^+$，将插值问题重新定义为：</p>
      </div>
      <div class="en">
        Universal Approximation does not imply an absence of inductive bias. Given a hypothesis space with universal approximation, we can define a complexity measure.
      </div>
    </div>

    <div class="math-block">
      $$\tilde{f} \in \arg\min_{g \in \mathcal{F}} c(g) \quad \text{s.t.} \quad g(x_i) = f(x_i) \quad \text{for } i = 1, \ldots, N$$
      <div class="math-explain">
        我们在所有能完美拟合训练数据的函数中，选择<strong>最"简单"的那个</strong>。这里的"简单"由复杂度度量 $c$ 定义。这就是<strong>奥卡姆剃刀</strong>（Occam's Razor）的数学实现：在能解释数据的假设中选择最简单的。
      </div>
    </div>

    <p>复杂度度量的几种常见选择：</p>
    <ul>
      <li><strong>权重衰减</strong>（Weight Decay）：$c(\theta) = \|\theta\|_2^2$ — 网络权重的 L2 范数</li>
      <li><strong>路径范数</strong>（Path-norm, Neyshabur et al., 2015）— 从输入到输出所有路径上权重乘积之和</li>
      <li><strong>贝叶斯视角</strong>：复杂度度量可以解释为函数先验的<strong>负对数</strong></li>
    </ul>

    <h3 id="implicit-reg">隐式正则化 Implicit Regularization</h3>

    <div class="bilingual">
      <div class="zh">
        <p>正则化可以<strong>显式</strong>地加入经验损失中（结构风险最小化，Structural Risk Minimisation），也可以<strong>隐式</strong>地通过优化算法来实现。例如，对欠定最小二乘问题的梯度下降会选择<strong>最小 L2 范数</strong>的插值解。</p>
        <p>核心问题是：<strong>如何定义有效的先验来捕获真实世界预测任务的期望正则性和复杂性？</strong></p>
      </div>
      <div class="en">
        This complexity can be enforced explicitly or implicitly as a result of a certain optimisation scheme. The natural question arises: how to define effective priors that capture the expected regularities of real-world prediction tasks?
      </div>
    </div>

    <h2 id="sec2-2">2.2 维度灾难<br><span style="font-size:0.7em;color:var(--text-secondary)">The Curse of Dimensionality</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>在低维（$d = 1, 2, 3$）中，插值是经典信号处理任务，有精确的数学控制。但高维问题的情况<strong>完全不同</strong>。</p>
      </div>
      <div class="en">
        While interpolation in low-dimensions is a classic signal processing task with very precise mathematical control, the situation for high-dimensional problems is entirely different.
      </div>
    </div>

    <h3 id="lipschitz">Lipschitz 函数类的维度灾难</h3>

    <div class="bilingual">
      <div class="zh">
        <p>考虑一个经典的正则性概念：<span class="term">1-Lipschitz 函数</span> $f : \mathcal{X} \to \mathbb{R}$，即满足：</p>
      </div>
      <div class="en">
        Consider 1-Lipschitz-functions: functions satisfying the Lipschitz condition.
      </div>
    </div>

    <div class="math-block">
      $$|f(x) - f(x')| \leq \|x - x'\| \quad \text{for all } x, x' \in \mathcal{X}$$
      <div class="math-explain">
        Lipschitz 条件要求函数<strong>局部光滑</strong>：如果输入 $x$ 稍微扰动一点，输出 $f(x)$ 的变化不能太大。这是一个非常自然的假设——物理世界中的大多数量都是连续的。
      </div>
    </div>

    <div class="callout callout-warning">
      <h4>维度灾难的核心</h4>
      <p>如果我们对目标函数 $f$ 唯一的知识是它是 1-Lipschitz 的，那么为了保证估计 $\tilde{f}$ 接近 $f$，所需的观测数量与 $\epsilon$ 和维度 $d$ 的关系是：</p>
      $$N = \Theta(\epsilon^{-d})$$
      <p>这是<strong>指数增长</strong>的！例如，在 $d = 100$ 维空间中，即使 $\epsilon = 0.5$，也需要 $2^{100} \approx 10^{30}$ 个样本——比宇宙中原子的数量还多。</p>
    </div>

    <div class="bilingual">
      <div class="zh">
        <p>直觉解释：考虑 $d$ 维空间中的 $2^d$ 个象限。一个 Lipschitz 函数可以在每个象限中独立地取 $+1$ 或 $-1$。除非我们在大部分象限中都观测到了函数值，否则我们的预测必然会有常数级别的误差。</p>
      </div>
      <div class="en">
        A Lipschitz function can independently take +1 or -1 in each of the 2^d quadrants. Unless we observe the function in most quadrants, we will incur a constant error in predicting it.
      </div>
    </div>

    <h3 id="sobolev">Sobolev 空间也无济于事</h3>

    <div class="math-block">
      $$\text{Sobolev class } H^s(\Omega_d): \quad \int |\omega|^{2s+1} |\hat{f}(\omega)|^2 d\omega < \infty$$
      <div class="math-explain">
        Sobolev 空间要求函数的<strong>广义 $s$ 阶导数</strong>是平方可积的（即函数足够光滑）。$\hat{f}$ 是 $f$ 的 Fourier 变换。经典结果（Tsybakov, 2008）建立了 Sobolev 类的极小极大逼近率为 $\epsilon^{-d/s}$，只有当 $s \propto d$（光滑度正比于维度）时，额外的光滑性假设才能改善统计学景象——在实践中这是不现实的假设。
      </div>
    </div>

    <h3 id="nn-breaking">神经网络如何突破维度灾难？</h3>

    <div class="bilingual">
      <div class="zh">
        <p>全连接神经网络定义的函数空间允许更灵活的正则性概念。通过选择<strong>稀疏性促进正则化</strong>（sparsity-promoting regularisation），它们有能力打破维度灾难（Bach, 2017）。但代价是对目标函数 $f$ 做出了<strong>强假设</strong>：$f$ 依赖于输入的<strong>低维投影</strong>的集合。</p>
      </div>
      <div class="en">
        Fully-connected neural networks define function spaces that enable more flexible notions of regularity. By choosing a sparsity-promoting regularisation, they can break the curse of dimensionality — but at the expense of assuming f depends on low-dimensional projections.
      </div>
    </div>

    <div class="math-block">
      $$f(x) \approx g(Ax) \quad \text{where } A \in \mathbb{R}^{k \times d}, \; k \ll d$$
      <div class="math-explain">
        这假设高维函数 $f$ 本质上只依赖于输入 $x$ 的少数几个线性组合（$k$ 维投影）。但在计算机视觉、语音分析等实际应用中，函数通常展现<strong>复杂的长程相关性</strong>，无法用低维投影表达。因此需要寻找<strong>替代的正则性来源</strong>——利用物理域的空间结构和 $f$ 的<strong>几何先验</strong>。
      </div>
    </div>

    <div class="callout callout-project">
      <h4>与 PhysRobot 项目的联系</h4>
      <ul>
        <li><strong>物理仿真中的维度灾难</strong>：考虑 $N$ 个粒子的系统，状态空间是 $6N$ 维的（3维位置 + 3维速度）。对于 $N = 1000$，这是 6000 维空间——纯暴力方法完全不可行</li>
        <li><strong>物理对称性作为归纳偏置</strong>：好消息是物理定律具有丰富的对称性——平移不变性、旋转等变性、能量守恒等。利用这些对称性正是 Chapter 3 将要介绍的内容</li>
        <li><strong>从 MLP 到 GNN</strong>：全连接 MLP 无法有效学习粒子间的相互作用，因为它假设所有粒子的排列是有意义的。但物理上粒子是可交换的——GNN 通过置换等变性来编码这个先验</li>
      </ul>
    </div>

    <div class="exercises" id="exercises">
      <h3>练习题 Exercises</h3>
      <ol>
        <li>在 $d = 10$ 维空间中，如果目标函数是 1-Lipschitz 的，要达到 $\epsilon = 0.1$ 的逼近精度，至少需要多少样本？将结果与你日常使用的数据集大小进行比较。</li>
        <li>解释<strong>万能逼近</strong>和<strong>有效学习</strong>之间的区别。为什么前者不能保证后者？用一个简单的例子说明。</li>
        <li>考虑函数 $f : \mathbb{R}^d \to \mathbb{R}$ 定义为 $f(x) = \|x\|^2$。这个函数是 Lipschitz 的吗？为什么？如果限制在单位球 $\|x\| \leq 1$ 上呢？</li>
        <li>权重衰减（L2 正则化）对应什么样的函数先验？从贝叶斯角度解释。</li>
        <li><strong>PhysRobot 思考</strong>：在我们的粒子系统中，粒子间的相互作用力通常是位置的函数。这个力函数满足什么样的正则性？（提示：考虑牛顿万有引力或库仑力的形式）</li>
      </ol>
    </div>

    <div class="chapter-nav">
      <a href="../chapter1/index.html">← Chapter 1: 引言</a>
      <a href="../chapter3/index.html">Chapter 3: 几何先验 →</a>
    </div>
  </main>

  <script src="../assets/script.js"></script>
</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 3: Geometric Priors | GDL 学习指南</title>
  <link rel="stylesheet" href="../assets/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Noto+Serif+SC:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]})"></script>
</head>
<body>
  <div class="progress-bar"></div>

  <header class="header">
    <div class="header-title"><a href="../index.html">📐 GDL 学习指南</a></div>
    <div class="header-nav">
      <a href="../chapter2/index.html">← 上一章</a>
      <a href="../index.html">目录</a>
      <a href="../chapter4/index.html">下一章 →</a>
      <button class="theme-toggle" onclick="toggleTheme()">🌙</button>
    </div>
  </header>

  <button class="sidebar-toggle" onclick="toggleSidebar()">☰</button>

  <nav class="sidebar">
    <h3>Chapter 3</h3>
    <a href="#overview">概述</a>
    <a href="#signal-space">信号空间</a>
    <a href="#sec3-1">3.1 对称性、表示与不变性</a>
    <a href="#groups" class="sub">群的定义</a>
    <a href="#group-actions" class="sub">群作用与表示</a>
    <a href="#invariance" class="sub">不变性与等变性</a>
    <a href="#sec3-2">3.2 同构与自同构</a>
    <a href="#subgroups" class="sub">子群与结构层次</a>
    <a href="#isomorphisms" class="sub">同构 vs 自同构</a>
    <a href="#sec3-3">3.3 形变稳定性</a>
    <a href="#deformation" class="sub">形变定义</a>
    <a href="#stability-condition" class="sub">稳定性条件</a>
    <a href="#sec3-4">3.4 尺度分离</a>
    <a href="#coarsening" class="sub">粗化与池化</a>
    <a href="#multiscale" class="sub">多尺度架构</a>
    <a href="#sec3-5">3.5 GDL 蓝图</a>
    <a href="#blueprint-components" class="sub">蓝图组件</a>
    <a href="#blueprint-table" class="sub">架构对照表</a>
    <a href="#exercises">练习题</a>
    <h3>导航</h3>
    <a href="../index.html">📚 总目录</a>
    <a href="../chapter2/index.html">← Ch.2 高维学习</a>
    <a href="../chapter4/index.html">→ Ch.4 几何域</a>
  </nav>

  <main class="main">
    <h1>Chapter 3: Geometric Priors<br><span style="font-size:0.6em;color:var(--text-secondary)">几何先验</span></h1>

    <div class="callout callout-info" id="overview">
      <h4>本章概述</h4>
      <p>本章是全书的<strong>理论核心</strong>，建立了 Geometric Deep Learning 的数学基础。主要内容包括：</p>
      <ul>
        <li><strong>对称性与群</strong> — 群的定义、群作用、群表示</li>
        <li><strong>不变性与等变性</strong> — 两种最重要的对称性约束</li>
        <li><strong>形变稳定性</strong> — 对域变形的鲁棒性</li>
        <li><strong>尺度分离</strong> — 多尺度/层级结构的理论基础</li>
        <li><strong>GDL 蓝图</strong> — 统一所有几何深度学习架构的抽象框架</li>
      </ul>
      <p><strong>预计阅读时间</strong>：2 小时</p>
    </div>

    <!-- 信号空间 -->
    <h2 id="signal-space">信号空间 Signal Space</h2>

    <div class="bilingual">
      <div class="zh">
        <p>现代数据分析的核心是高维学习。虽然 Chapter 2 揭示了通用高维数据学习的不可能性，但对于<strong>具有物理结构的数据</strong>，我们可以利用两个基本原则：<span class="term">对称性</span>（symmetry）和<span class="term">尺度分离</span>（scale separation）。</p>
        <p>额外的结构通常来自输入信号底层<strong>域</strong>（domain）的结构：我们假设机器学习系统作用于某个域 $\Omega$ 上的<span class="term">信号</span>（函数）。</p>
      </div>
      <div class="en">
        There is hope for physically-structured data, where we can employ two fundamental principles: symmetry and scale separation. This additional structure will usually come from the structure of the domain underlying the input signals.
      </div>
    </div>

    <div class="math-block">
      $$\mathcal{X}(\Omega, \mathcal{C}) = \{x : \Omega \to \mathcal{C}\}$$
      <div class="math-explain">
        <strong>信号空间</strong>：$\Omega$ 是域（集合，可能有额外结构），$\mathcal{C}$ 是值域向量空间（维度称为<strong>通道</strong> channels）。例如：RGB 图像是 $\Omega = \mathbb{Z}_n \times \mathbb{Z}_n$（$n \times n$ 网格）上的 $\mathcal{C} = \mathbb{R}^3$ 值信号。
      </div>
    </div>

    <p>信号空间具有<strong>向量空间</strong>结构（可以线性组合信号），并且配有<strong>内积</strong>（使之成为 Hilbert 空间）：</p>

    <div class="math-block">
      $$\langle x, y \rangle = \int_\Omega \langle x(u), y(u) \rangle_{\mathcal{C}}\, d\mu(u)$$
      <div class="math-explain">
        信号间的内积由域上的积分定义。$\mu$ 是 $\Omega$ 上的测度。当 $\Omega$ 是离散的（如图或网格），积分变为求和。
      </div>
    </div>

    <!-- 3.1 对称性 -->
    <h2 id="sec3-1">3.1 对称性、表示与不变性<br><span style="font-size:0.7em;color:var(--text-secondary)">Symmetries, Representations, and Invariance</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>非形式地，一个对象或系统的<span class="term">对称性</span>是一种使该对象或系统的某些属性保持不变（invariant）的变换。对称性在机器学习中无处不在：</p>
        <ul>
          <li><strong>计算机视觉</strong>：对象类别不受平移影响 → 平移是分类问题的对称性</li>
          <li><strong>计算化学</strong>：分子性质与空间方向无关 → 需要旋转不变性</li>
          <li><strong>粒子系统</strong>：粒子无规范排序 → 置换对称性</li>
          <li><strong>动力系统</strong>：牛顿第二定律的时间反演对称性</li>
        </ul>
      </div>
      <div class="en">
        Informally, a symmetry of an object or system is a transformation that leaves a certain property of said object unchanged or invariant. Symmetries are ubiquitous in many machine learning tasks.
      </div>
    </div>

    <h3 id="groups">群的形式定义 Formal Definition of Groups</h3>

    <div class="blueprint">
      <h4>群 Group (G, ∘)</h4>
      <p>一个<span class="term">群</span>是一个集合 $G$ 连同一个二元运算 $\circ : G \times G \to G$（称为<strong>复合</strong>），满足以下公理：</p>
      <ol>
        <li><strong>结合律</strong> Associativity：$(gh)k = g(hk)$ 对所有 $g, h, k \in G$</li>
        <li><strong>单位元</strong> Identity：存在唯一 $e \in G$ 使得 $eg = ge = g$ 对所有 $g \in G$</li>
        <li><strong>逆元</strong> Inverse：对每个 $g \in G$，存在唯一 $g^{-1} \in G$ 使得 $gg^{-1} = g^{-1}g = e$</li>
        <li><strong>封闭性</strong> Closure：对所有 $g, h \in G$，有 $gh \in G$</li>
      </ol>
      <p><strong>注意</strong>：<em>交换律不在定义中</em>。满足 $gh = hg$ 的群称为<strong>交换群</strong>或 <strong>Abel 群</strong>。</p>
    </div>

    <div class="figure">
      <img src="../assets/ch3_p18_img0.png" alt="Figure 4: D3 group">
      <figcaption>图 4：左：等边三角形的旋转与反射对称群 $D_3$，由 60° 旋转 $R$ 和反射 $F$ 两个生成元生成，与三元素置换群 $\Sigma_3$ 相同。右：$D_3$ 的乘法表。</figcaption>
    </div>

    <div class="callout callout-info">
      <h4>群的关键例子</h4>
      <table>
        <tr><th>群</th><th>元素</th><th>应用</th></tr>
        <tr><td>$\Sigma_n$（对称群/置换群）</td><td>$n$ 个元素的所有排列</td><td>集合、图（GNN）</td></tr>
        <tr><td>$(\mathbb{Z}^d, +)$（整数平移群）</td><td>$d$ 维整数向量</td><td>离散网格（CNN）</td></tr>
        <tr><td>$(\mathbb{R}^d, +)$（连续平移群）</td><td>$d$ 维实向量</td><td>连续空间</td></tr>
        <tr><td>$\text{SO}(d)$（特殊正交群）</td><td>$d$ 维旋转矩阵</td><td>分子、物理仿真</td></tr>
        <tr><td>$\text{SE}(d)$（特殊欧几里得群）</td><td>旋转 + 平移</td><td>刚体运动、机器人</td></tr>
        <tr><td>$\text{E}(d)$（欧几里得群）</td><td>旋转 + 平移 + 反射</td><td>晶体结构</td></tr>
      </table>
    </div>

    <h3 id="group-actions">群作用与群表示</h3>

    <div class="bilingual">
      <div class="zh">
        <p>我们不只关心群作为抽象实体，更关心群如何<strong>作用于数据</strong>。如果域 $\Omega$ 是我们的数据底层空间，我们需要研究群如何作用于 $\Omega$，进而得到群在信号空间 $\mathcal{X}(\Omega)$ 上的作用。</p>
      </div>
      <div class="en">
        Rather than considering groups as abstract entities, we are mostly interested in how groups act on data.
      </div>
    </div>

    <div class="blueprint">
      <h4>群作用 Group Action</h4>
      <p>群 $G$ 在集合 $\Omega$ 上的<span class="term">群作用</span>是映射 $(g, u) \mapsto g.u$，满足与群运算的相容性：</p>
      $$g.(h.u) = (gh).u \quad \text{for all } g, h \in G, \; u \in \Omega$$
      <p><strong>重要推论</strong>：群在<strong>信号</strong>上的诱导作用为：</p>
      $$(\rho(g)x)(u) = x(g^{-1}u)$$
      <div class="math-explain">
        直觉：要计算变换后的信号在 $u$ 处的值，就去看原信号在 $g^{-1}u$ 处的值。这里的逆是因为"信号跟着变换走"。例如，把图像向右平移 $t$，等价于每个像素去取原图中向左 $t$ 处的值。
      </div>
    </div>

    <div class="blueprint">
      <h4>群表示 Group Representation</h4>
      <p>群 $G$ 的一个 $n$ 维实<span class="term">表示</span>是映射 $\rho : G \to \mathbb{R}^{n \times n}$，将每个 $g \in G$ 映射到一个可逆矩阵，满足：</p>
      $$\rho(gh) = \rho(g)\rho(h) \quad \text{for all } g, h \in G$$
      <div class="math-explain">
        表示是群到矩阵群的<strong>同态</strong>（homomorphism）：它保持了群的"乘法结构"。直觉上，表示告诉我们群元素如何"具体地"作用于向量空间。如果表示矩阵是正交/酉的，则称为<strong>正交/酉表示</strong>。
      </div>
    </div>

    <h3 id="invariance">不变性与等变性 Invariance and Equivariance</h3>

    <div class="callout callout-key">
      <h4>这是全书最重要的两个概念！</h4>
    </div>

    <div class="blueprint">
      <h4>G-不变性 G-Invariance</h4>
      <p>函数 $f : \mathcal{X}(\Omega) \to \mathcal{Y}$ 是 <strong>$G$-不变</strong>的，如果：</p>
      $$f(\rho(g)x) = f(x) \quad \text{for all } g \in G, \; x \in \mathcal{X}(\Omega)$$
      <div class="math-explain">
        输出<strong>完全不受</strong>群作用影响。例如：图像分类函数对平移不变——不管猫在图片的哪个位置，分类结果都是"猫"。
      </div>
    </div>

    <div class="figure">
      <img src="../assets/ch3_p20_img0.png" alt="Figure 5: Three spaces in GDL">
      <figcaption>图 5：GDL 中的三个空间：域 $\Omega$、信号空间 $\mathcal{X}(\Omega)$、假设类 $\mathcal{F}(\mathcal{X}(\Omega))$。域的对称性通过群表示 $\rho(g)$ 作用于信号，从而约束作用于信号的函数。</figcaption>
    </div>

    <div class="blueprint">
      <h4>G-等变性 G-Equivariance</h4>
      <p>函数 $f : \mathcal{X}(\Omega) \to \mathcal{X}(\Omega)$ 是 <strong>$G$-等变</strong>的，如果：</p>
      $$f(\rho(g)x) = \rho'(g)f(x) \quad \text{for all } g \in G$$
      <div class="math-explain">
        输入上的群作用以<strong>可预测的方式</strong>影响输出。例如：卷积层是平移等变的——如果输入图像平移了 $t$，则所有特征图也平移 $t$。注意：$\rho$ 和 $\rho'$ 可以是不同的表示（输入和输出空间可以不同）。
      </div>
    </div>

    <div class="callout callout-project">
      <h4>不变性 vs 等变性在 PhysRobot 中</h4>
      <ul>
        <li><strong>等变性（中间层）</strong>：GNN 消息传递层应该是 SE(3)-等变的。如果旋转所有粒子的位置，中间表示应该相应旋转</li>
        <li><strong>不变性（能量预测）</strong>：系统总能量对整体旋转/平移是不变的——旋转整个系统不改变能量</li>
        <li><strong>等变性（力预测）</strong>：力向量在旋转下是等变的——旋转系统则力也相应旋转</li>
        <li>这正是 <strong>EGNN</strong>（Equivariant GNN）和 <strong>TFN</strong>（Tensor Field Network）的设计原理</li>
      </ul>
    </div>

    <!-- 3.2 同构与自同构 -->
    <h2 id="sec3-2">3.2 同构与自同构<br><span style="font-size:0.7em;color:var(--text-secondary)">Isomorphisms and Automorphisms</span></h2>

    <h3 id="subgroups">子群与结构层次</h3>

    <div class="bilingual">
      <div class="zh">
        <p>什么算作"对称性"取决于我们考虑的结构。在域 $\Omega$ 上通常存在<strong>多个层次的结构</strong>，每个层次对应不同的对称群：</p>
        <ul>
          <li><strong>集合层次</strong>：保持基数的<strong>双射</strong>（bijections）</li>
          <li><strong>拓扑层次</strong>：保持连续性的<strong>同胚</strong>（homeomorphisms）</li>
          <li><strong>微分层次</strong>：保持可微性的<strong>微分同胚</strong>（diffeomorphisms）Diff($\Omega$)</li>
          <li><strong>度量层次</strong>：保持距离的<strong>等距变换</strong>（isometries）Iso($\Omega$)</li>
          <li><strong>定向层次</strong>：保持方向的变换</li>
        </ul>
        <p>结构越多，对称群<strong>越小</strong>：$\text{Iso}(\Omega) \subset \text{Diff}(\Omega) \subset \text{Homeo}(\Omega) \subset \text{Bij}(\Omega)$</p>
      </div>
      <div class="en">
        What counts as a symmetry depends on the structure under consideration. As we add levels of structure, the symmetry group gets smaller.
      </div>
    </div>

    <div class="blueprint">
      <h4>子群 Subgroup</h4>
      <p>设 $(G, \circ)$ 是一个群，$H \subseteq G$ 是一个子集。如果 $(H, \circ)$ 自身也构成群（使用同一运算），则 $H$ 是 $G$ 的<strong>子群</strong>。</p>
      <p>例：$\text{SE}(2)$（保持方向的等距，即旋转+平移）是 $\text{E}(2)$（全部等距，包括反射）的子群。</p>
    </div>

    <h3 id="isomorphisms">同构 vs 自同构</h3>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">自同构</span>（automorphism）是一个对象到<strong>自身</strong>的保结构可逆映射。<span class="term">同构</span>（isomorphism）是<strong>两个不同对象之间</strong>的保结构可逆映射。</p>
        <p>例子：集合 $\Omega = \{0,1,2\}$ 的一个自同构是循环位移 $\tau(u) = u+1 \bmod 3$。如果有另一个集合 $\Omega' = \{a,b,c\}$，那么 $\eta(0) = a, \eta(1) = b, \eta(2) = c$ 是一个同构。</p>
        <p>对于<strong>图</strong>：两个图的同构是节点间的双射，保持连接关系。两个同构的图在结构上完全相同，只是节点排列不同。</p>
      </div>
      <div class="en">
        An automorphism is a structure-preserving invertible map from an object to itself. An isomorphism exhibits equivalence between two non-identical objects.
      </div>
    </div>

    <!-- 3.3 形变稳定性 -->
    <h2 id="sec3-3">3.3 形变稳定性<br><span style="font-size:0.7em;color:var(--text-secondary)">Deformation Stability</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>在实际应用中，精确的对称性往往只是一种理想化。例如，在图像分类中，仅靠精确的平移不变性是不够的——图像也可能发生微小的<strong>形变</strong>（如缩放、旋转、弯曲等）。</p>
        <p>我们需要的是一种<strong>近似不变性</strong>：对于对称群之外的"接近恒等"的变换，函数的输出应该只发生微小变化。</p>
      </div>
      <div class="en">
        In practical applications, exact symmetry is often an idealisation. We need approximate invariance: for transformations close to the identity, the function output should change only slightly.
      </div>
    </div>

    <h3 id="deformation">形变的定义</h3>

    <div class="math-block">
      $$\tau(u) = u + \epsilon(u)$$
      <div class="math-explain">
        <strong>形变</strong>是一个"接近恒等变换"的映射：$\tau$ 将每个点 $u$ 移动到附近的 $u + \epsilon(u)$。$\epsilon(u)$ 是一个<strong>位移场</strong>（displacement field），满足 $\|\nabla \epsilon\| \leq C$（即位移场要足够"光滑"）。直觉上，形变不能把邻近的点拉得太远。
      </div>
    </div>

    <h3 id="stability-condition">稳定性条件</h3>

    <div class="blueprint">
      <h4>形变稳定性 Deformation Stability</h4>
      <p>函数 $f$ 对形变 $\tau$ 是<strong>稳定的</strong>，如果：</p>
      $$\|f(\rho(\tau)x) - f(x)\| \leq C \cdot \|\nabla\epsilon\| \cdot \|x\|$$
      <div class="math-explain">
        函数输出的变化<strong>与形变大小成正比</strong>。这比不变性弱（不变性要求变化为零），但比无约束更强。在深度学习中，形变稳定性通常通过<strong>局部操作</strong>（如小感受野的卷积）实现。
      </div>
    </div>

    <div class="callout callout-project">
      <h4>形变稳定性在 PhysRobot 中</h4>
      <p>软组织仿真中，网格节点会发生形变。我们的网络需要对小的网格形变保持<strong>稳定</strong>（预测不会剧烈变化），但不需要<strong>不变</strong>（因为不同的形变对应不同的物理状态）。这正是形变稳定性的概念！</p>
    </div>

    <!-- 3.4 尺度分离 -->
    <h2 id="sec3-4">3.4 尺度分离<br><span style="font-size:0.7em;color:var(--text-secondary)">Scale Separation</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p><span class="term">尺度分离</span>是第二个几何先验原则。核心思想：当我们将信号从一个域转移到该域的<strong>粗化版本</strong>（coarser version）时，能够保留信号的重要特征。</p>
        <p>在图像处理中，这对应于<strong>下采样</strong>（subsampling）或<strong>池化</strong>（pooling）。在图上，这对应于<strong>图粗化</strong>（graph coarsening）。</p>
      </div>
      <div class="en">
        Scale separation results from our ability to preserve important characteristics of the signal when transferring it onto a coarser version of the domain.
      </div>
    </div>

    <h3 id="coarsening">粗化与池化 Coarsening and Pooling</h3>

    <div class="bilingual">
      <div class="zh">
        <p>粗化操作 $P$ 将域 $\Omega$ 映射到更小的域 $\Omega'$（$|\Omega'| < |\Omega|$），同时将信号也映射过去。关键是在粗化过程中<strong>不丢失</strong>对下游任务重要的信息。</p>
        <p>这要求信号在某种意义上是"<strong>带限的</strong>"（bandlimited）——高频细节不重要。这是物理世界的常见特征：远距离相互作用通常比近距离相互作用弱。</p>
      </div>
      <div class="en">
        The coarsening operation maps a domain to a smaller domain while preserving important signal characteristics. This requires the signal to be "bandlimited" in some sense.
      </div>
    </div>

    <h3 id="multiscale">多尺度架构 Multi-scale Architecture</h3>

    <div class="bilingual">
      <div class="zh">
        <p>尺度分离直接导致了深度学习中的<strong>多尺度/层级架构</strong>：</p>
        <ul>
          <li><strong>CNN</strong>：卷积层保持空间分辨率，池化层降低分辨率 → U-Net、ResNet 等</li>
          <li><strong>GNN</strong>：消息传递保持图结构，图池化减少节点数 → DiffPool、TopK 等</li>
        </ul>
        <p>这种"逐步粗化"的模式是几何深度学习蓝图的核心组成部分。</p>
      </div>
      <div class="en">
        Scale separation directly leads to multi-scale/hierarchical architectures in deep learning: convolution preserves resolution while pooling reduces it.
      </div>
    </div>

    <div class="figure">
      <img src="../assets/ch3_p30_img0.png" alt="Scale separation illustration">
      <figcaption>尺度分离的图示：信号从细到粗逐步粗化，在每个尺度上保留重要特征。</figcaption>
    </div>

    <!-- 3.5 GDL 蓝图 -->
    <h2 id="sec3-5">3.5 几何深度学习蓝图<br><span style="font-size:0.7em;color:var(--text-secondary)">The Blueprint of Geometric Deep Learning</span></h2>

    <div class="callout callout-key">
      <h4>全书最核心的框架！</h4>
      <p>GDL 蓝图是一个<strong>统一框架</strong>，几乎所有主流深度学习架构都可以视为它的特例。</p>
    </div>

    <h3 id="blueprint-components">蓝图的构建块</h3>

    <div class="bilingual">
      <div class="zh">
        <p>GDL 蓝图由以下构建块组成：</p>
      </div>
      <div class="en">
        The GDL blueprint consists of the following building blocks.
      </div>
    </div>

    <div class="blueprint">
      <h4>🏗️ Geometric Deep Learning Blueprint</h4>
      <ol>
        <li><strong>域 Domain $\Omega$</strong> — 数据的底层几何结构（网格、图、流形等）</li>
        <li><strong>对称群 Symmetry Group $G$</strong> — 描述域的对称性（平移、旋转、置换等）</li>
        <li><strong>信号 Signal $x \in \mathcal{X}(\Omega, \mathcal{C})$</strong> — 域上的函数（图像、节点特征等）</li>
        <li><strong>线性 $G$-等变层</strong> — 与群对称兼容的线性映射（卷积、消息传递等）</li>
        <li><strong>非线性 Nonlinearity $\sigma$</strong> — 逐点应用的激活函数</li>
        <li><strong>局部池化 / 粗化 Coarsening $P$</strong> — 降低域的分辨率（池化、图粗化等）</li>
        <li><strong>全局池化 Global Pooling</strong> — 将等变表示转化为不变输出</li>
      </ol>
    </div>

    <div class="figure">
      <img src="../assets/ch3_p32_img0.png" alt="GDL Blueprint">
      <figcaption>GDL 蓝图：从信号 $x$ 开始，经过等变层（线性 + 非线性）和粗化操作，最终通过全局池化得到不变输出。</figcaption>
    </div>

    <p>一般的一层 GDL 网络可以表示为：</p>
    <div class="math-block">
      $$x^{(k+1)} = \sigma\Big(B_k x^{(k)} + \sum_{j} C_k(\theta_j) x^{(k)}\Big)$$
      <div class="math-explain">
        <strong>$B_k$</strong>：自相互作用（bias/self-loop），类似于每个位置独立处理自己的特征<br>
        <strong>$C_k(\theta_j)$</strong>：$G$-等变的线性操作（卷积、消息传递等），聚合邻域信息<br>
        <strong>$\sigma$</strong>：逐点非线性激活函数<br>
        这个公式统一了 CNN（$C$ = 卷积滤波器）、GNN（$C$ = 消息聚合）、Transformer（$C$ = 注意力）等。
      </div>
    </div>

    <h3 id="blueprint-table">架构对照表</h3>

    <table>
      <thead>
        <tr><th>架构</th><th>域 $\Omega$</th><th>对称群 $G$</th><th>等变操作</th><th>池化</th></tr>
      </thead>
      <tbody>
        <tr><td><strong>CNN</strong></td><td>网格</td><td>平移</td><td>卷积</td><td>空间池化</td></tr>
        <tr><td><strong>Spherical CNN</strong></td><td>球面/SO(3)</td><td>旋转 SO(3)</td><td>球谐卷积</td><td>频带截断</td></tr>
        <tr><td><strong>GNN</strong></td><td>图</td><td>置换 $\Sigma_n$</td><td>消息传递</td><td>图粗化</td></tr>
        <tr><td><strong>Deep Sets</strong></td><td>集合</td><td>置换 $\Sigma_n$</td><td>逐点 + 全局聚合</td><td>集合池化</td></tr>
        <tr><td><strong>Transformer</strong></td><td>完全图</td><td>置换 $\Sigma_n$</td><td>自注意力</td><td>—</td></tr>
        <tr><td><strong>Mesh CNN</strong></td><td>流形</td><td>等距/规范</td><td>测地卷积</td><td>网格粗化</td></tr>
        <tr><td><strong>LSTM</strong></td><td>1D 网格</td><td>时间扭曲</td><td>门控操作</td><td>跨步</td></tr>
      </tbody>
    </table>

    <div class="callout callout-project">
      <h4>GDL 蓝图在 PhysRobot 中的应用</h4>
      <p>我们的物理仿真框架完美符合 GDL 蓝图：</p>
      <ul>
        <li><strong>域</strong>：粒子系统构成的图（节点 = 粒子，边 = 相互作用）</li>
        <li><strong>对称群</strong>：SE(3)（旋转+平移）× $\Sigma_n$（粒子置换）</li>
        <li><strong>等变操作</strong>：等变消息传递（如 EGNN、PaiNN）</li>
        <li><strong>池化</strong>：对于全局属性预测（如系统能量），使用不变的全局聚合</li>
        <li><strong>多尺度</strong>：可以用图粗化实现多分辨率仿真</li>
      </ul>
    </div>

    <div class="exercises" id="exercises">
      <h3>练习题 Exercises</h3>
      <ol>
        <li>验证 $\text{SO}(2)$（平面旋转群）满足群的四个公理。它是 Abel 群吗？$\text{SO}(3)$ 呢？</li>
        <li>设 $G = (\mathbb{Z}_4, +)$ 是模 4 加法群。写出它的乘法表。找出它的所有子群。</li>
        <li>证明：如果 $f$ 是 $G$-等变的，$g$ 是 $G$-不变的，那么 $g \circ f$ 是 $G$-不变的。为什么这对 GDL 蓝图很重要？</li>
        <li>考虑函数 $f(x_1, x_2) = x_1 + x_2$。这个函数对 $\Sigma_2$（交换 $x_1$ 和 $x_2$）是不变的还是等变的？如果是 $f(x_1, x_2) = (x_1 + x_2, x_1 - x_2)$ 呢？</li>
        <li>在 GDL 蓝图中，为什么需要<strong>非线性</strong>？如果所有操作都是线性的会怎样？</li>
        <li><strong>PhysRobot 思考</strong>：在分子动力学仿真中，考虑一个由 $N$ 个原子构成的分子。(a) 系统的位形空间是什么？(b) 什么是该系统的自然对称群？(c) 势能函数 $V(r_1, \ldots, r_N)$ 应该是不变的还是等变的？力函数 $F_i = -\nabla_{r_i} V$ 呢？</li>
        <li>解释为什么 "等变中间层 + 不变最后层" 是 GDL 蓝图中最常见的模式。给出一个具体的例子。</li>
      </ol>
    </div>

    <div class="chapter-nav">
      <a href="../chapter2/index.html">← Chapter 2: 高维学习</a>
      <a href="../chapter4/index.html">Chapter 4: 几何域 →</a>
    </div>
  </main>

  <script src="../assets/script.js"></script>
</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 5: Geometric Deep Learning Models | GDL å­¦ä¹ æŒ‡å—</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Noto+Serif+SC:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
  <style>
/* ================================================================
   Geometric Deep Learning Study Guide - Chapter 5 Inline Styles
   ================================================================ */
:root {
  --bg: #ffffff;
  --bg-secondary: #f8f9fa;
  --bg-code: #f1f3f5;
  --text: #212529;
  --text-secondary: #495057;
  --text-muted: #868e96;
  --border: #dee2e6;
  --accent: #4263eb;
  --accent-light: #dbe4ff;
  --accent-dark: #3b5bdb;
  --success: #37b24d;
  --warning: #f59f00;
  --danger: #f03e3e;
  --sidebar-width: 300px;
  --header-height: 60px;
  --shadow: 0 1px 3px rgba(0,0,0,0.08);
  --shadow-lg: 0 4px 12px rgba(0,0,0,0.1);
  --radius: 8px;
  --font-sans: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
  --font-mono: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace;
  --font-serif: 'Noto Serif SC', 'Georgia', serif;
}

[data-theme="dark"] {
  --bg: #1a1b1e; --bg-secondary: #25262b; --bg-code: #2c2e33;
  --text: #c1c2c5; --text-secondary: #909296; --text-muted: #5c5f66;
  --border: #373a40; --accent: #5c7cfa; --accent-light: #1c2541; --accent-dark: #748ffc;
  --shadow: 0 1px 3px rgba(0,0,0,0.3); --shadow-lg: 0 4px 12px rgba(0,0,0,0.4);
}

* { margin: 0; padding: 0; box-sizing: border-box; }
html { font-size: 16px; scroll-behavior: smooth; scroll-padding-top: calc(var(--header-height) + 20px); }
body { font-family: var(--font-sans); background: var(--bg); color: var(--text); line-height: 1.7; transition: background 0.3s, color 0.3s; }

/* Header */
.header { position: fixed; top: 0; left: 0; right: 0; height: var(--header-height); background: var(--bg-secondary); border-bottom: 1px solid var(--border); display: flex; align-items: center; padding: 0 24px; z-index: 1000; box-shadow: var(--shadow); }
.header-title { font-size: 1.1rem; font-weight: 700; color: var(--accent); flex: 1; }
.header-title a { color: var(--accent); text-decoration: none; }
.header-nav { display: flex; gap: 12px; align-items: center; }
.header-nav a { color: var(--text-secondary); text-decoration: none; font-size: 0.85rem; padding: 4px 10px; border-radius: 4px; transition: all 0.2s; }
.header-nav a:hover { background: var(--accent-light); color: var(--accent); }
.theme-toggle { background: var(--bg-code); border: 1px solid var(--border); border-radius: 20px; padding: 6px 12px; cursor: pointer; font-size: 1rem; color: var(--text); transition: all 0.2s; }
.theme-toggle:hover { background: var(--accent-light); }

/* Progress bar */
.progress-bar { position: fixed; top: var(--header-height); left: 0; width: 0%; height: 3px; background: linear-gradient(90deg, var(--accent), #e64980, var(--accent-dark)); z-index: 1001; transition: width 0.1s; }

/* Sidebar */
.sidebar { position: fixed; top: var(--header-height); left: 0; width: var(--sidebar-width); height: calc(100vh - var(--header-height)); background: var(--bg-secondary); border-right: 1px solid var(--border); overflow-y: auto; padding: 20px 0; z-index: 900; transition: transform 0.3s; }
.sidebar-toggle { display: none; position: fixed; top: calc(var(--header-height) + 10px); left: 10px; z-index: 950; background: var(--accent); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; font-size: 1.2rem; cursor: pointer; box-shadow: var(--shadow-lg); }
.sidebar h3 { padding: 8px 20px; font-size: 0.75rem; text-transform: uppercase; letter-spacing: 1px; color: var(--text-muted); margin-top: 12px; }
.sidebar a { display: block; padding: 5px 20px 5px 24px; color: var(--text-secondary); text-decoration: none; font-size: 0.82rem; border-left: 3px solid transparent; transition: all 0.2s; }
.sidebar a:hover { background: var(--accent-light); color: var(--accent); border-left-color: var(--accent); }
.sidebar a.active { background: var(--accent-light); color: var(--accent); border-left-color: var(--accent); font-weight: 600; }
.sidebar a.sub { padding-left: 40px; font-size: 0.78rem; }
.sidebar a.sub-sub { padding-left: 56px; font-size: 0.76rem; }
.sidebar .star { color: #f59f00; }

/* Main */
.main { margin-left: var(--sidebar-width); margin-top: var(--header-height); padding: 40px 60px; max-width: 960px; }
h1 { font-size: 2.2rem; font-weight: 800; margin-bottom: 16px; line-height: 1.3; }
h2 { font-size: 1.6rem; font-weight: 700; margin-top: 56px; margin-bottom: 16px; padding-bottom: 8px; border-bottom: 2px solid var(--accent); }
h3 { font-size: 1.3rem; font-weight: 600; margin-top: 40px; margin-bottom: 12px; }
h4 { font-size: 1.1rem; font-weight: 600; margin-top: 28px; margin-bottom: 8px; color: var(--text-secondary); }
p { margin-bottom: 16px; }
ul, ol { padding-left: 24px; margin-bottom: 16px; }
li { margin-bottom: 6px; }
a { color: var(--accent); text-decoration: none; }
a:hover { text-decoration: underline; }

/* Bilingual */
.bilingual { margin: 20px 0; border-left: 4px solid var(--accent); padding: 16px 20px; background: var(--bg-secondary); border-radius: 0 var(--radius) var(--radius) 0; }
.bilingual .zh { font-size: 1rem; line-height: 1.8; margin-bottom: 10px; }
.bilingual .en { font-size: 0.86rem; color: var(--text-muted); font-style: italic; line-height: 1.6; border-top: 1px solid var(--border); padding-top: 8px; }
.term { color: var(--accent); font-weight: 600; }

/* Math blocks */
.math-block { margin: 24px 0; padding: 20px; background: var(--bg-secondary); border: 1px solid var(--border); border-radius: var(--radius); overflow-x: auto; }
.math-explain { margin-top: 12px; padding: 12px 16px; background: var(--accent-light); border-radius: var(--radius); font-size: 0.92rem; color: var(--text-secondary); }
.math-explain::before { content: "ğŸ’¡ "; }

/* Symbol table */
.symbol-table { margin: 16px 0; }
.symbol-table table { width: 100%; border-collapse: collapse; font-size: 0.9rem; }
.symbol-table th { background: var(--accent); color: white; padding: 8px 12px; text-align: left; }
.symbol-table td { padding: 8px 12px; border: 1px solid var(--border); }
.symbol-table tr:nth-child(even) { background: var(--bg-secondary); }

/* Figures */
.figure { margin: 24px 0; text-align: center; }
.figure img { max-width: 100%; height: auto; border-radius: var(--radius); box-shadow: var(--shadow); }
.figure figcaption { margin-top: 10px; font-size: 0.88rem; color: var(--text-secondary); font-style: italic; }

/* Callout boxes */
.callout { margin: 24px 0; padding: 16px 20px; border-radius: var(--radius); border-left: 4px solid; }
.callout h4 { margin-top: 0; margin-bottom: 8px; font-size: 0.95rem; color: var(--text); }
.callout-info { background: #e7f5ff; border-left-color: #339af0; }
[data-theme="dark"] .callout-info { background: #1c3a5e; border-left-color: #339af0; }
.callout-project { background: #ebfbee; border-left-color: var(--success); }
[data-theme="dark"] .callout-project { background: #1e3a23; border-left-color: var(--success); }
.callout-warning { background: #fff9db; border-left-color: var(--warning); }
[data-theme="dark"] .callout-warning { background: #3a3520; border-left-color: var(--warning); }
.callout-key { background: #fff0f6; border-left-color: #e64980; }
[data-theme="dark"] .callout-key { background: #3a1c2e; border-left-color: #e64980; }
.callout-danger { background: #ffe3e3; border-left-color: var(--danger); }
[data-theme="dark"] .callout-danger { background: #3a1c1c; border-left-color: var(--danger); }
.callout-project h4::before { content: "ğŸ¤– "; }
.callout-info h4::before { content: "â„¹ï¸ "; }
.callout-warning h4::before { content: "âš ï¸ "; }
.callout-key h4::before { content: "ğŸ”‘ "; }
.callout-danger h4::before { content: "ğŸ”¥ "; }

/* Code blocks */
.code-container { position: relative; margin: 16px 0; }
.code-container .copy-btn { position: absolute; top: 8px; right: 8px; background: var(--accent); color: white; border: none; border-radius: 4px; padding: 4px 10px; font-size: 0.75rem; cursor: pointer; opacity: 0; transition: opacity 0.2s; z-index: 10; }
.code-container:hover .copy-btn { opacity: 1; }
.code-container .copy-btn:active { background: var(--accent-dark); }
.code-label { display: inline-block; background: var(--accent); color: white; padding: 2px 10px; font-size: 0.75rem; border-radius: var(--radius) var(--radius) 0 0; font-weight: 600; }
pre { margin: 0 0 16px 0; padding: 16px; background: var(--bg-code); border: 1px solid var(--border); border-radius: var(--radius); overflow-x: auto; font-family: var(--font-mono); font-size: 0.85rem; line-height: 1.55; }
code { font-family: var(--font-mono); font-size: 0.88em; background: var(--bg-code); padding: 2px 6px; border-radius: 3px; }
pre code { background: none; padding: 0; font-size: inherit; }

/* Blueprint / Framework box */
.blueprint { margin: 24px 0; padding: 20px; background: linear-gradient(135deg, var(--accent-light), var(--bg-secondary)); border: 2px solid var(--accent); border-radius: var(--radius); }
.blueprint h4 { color: var(--accent); margin-top: 0; }

/* Tables */
table { width: 100%; border-collapse: collapse; margin: 20px 0; font-size: 0.9rem; }
th, td { padding: 10px 14px; border: 1px solid var(--border); text-align: left; }
th { background: var(--bg-secondary); font-weight: 600; }

/* Comparison table */
.comparison-table { overflow-x: auto; margin: 24px 0; }
.comparison-table table { min-width: 600px; }
.comparison-table th { background: var(--accent); color: white; }
.comparison-table td:first-child { font-weight: 600; color: var(--accent); }

/* Algorithm */
.algorithm { margin: 24px 0; border: 2px solid var(--accent); border-radius: var(--radius); overflow: hidden; }
.algorithm-title { background: var(--accent); color: white; padding: 8px 16px; font-weight: 600; font-size: 0.95rem; }
.algorithm-body { padding: 16px; background: var(--bg-secondary); font-family: var(--font-mono); font-size: 0.85rem; line-height: 1.8; white-space: pre-wrap; }

/* Exercises */
.exercises { margin: 32px 0; padding: 20px; background: var(--bg-secondary); border: 2px solid var(--warning); border-radius: var(--radius); }
.exercises h3 { margin-top: 0; color: var(--warning); }
.exercises h3::before { content: "âœï¸ "; }
.exercises ol { padding-left: 24px; }
.exercises li { margin-bottom: 12px; line-height: 1.6; }

/* Step-by-step derivation */
.derivation { margin: 20px 0; padding: 16px 20px; background: var(--bg-secondary); border-left: 4px solid #e64980; border-radius: 0 var(--radius) var(--radius) 0; }
.derivation h4 { color: #e64980; margin-top: 0; }
.derivation h4::before { content: "ğŸ“ "; }
.step { margin: 8px 0; padding: 8px 12px; background: var(--bg); border-radius: var(--radius); border: 1px solid var(--border); }
.step-num { display: inline-block; background: var(--accent); color: white; width: 24px; height: 24px; border-radius: 50%; text-align: center; font-size: 0.75rem; line-height: 24px; margin-right: 8px; font-weight: 700; }

/* Navigation */
.chapter-nav { display: flex; justify-content: space-between; margin-top: 60px; padding: 20px 0; border-top: 1px solid var(--border); }
.chapter-nav a { display: inline-flex; align-items: center; gap: 8px; padding: 10px 20px; background: var(--bg-secondary); border: 1px solid var(--border); border-radius: var(--radius); font-weight: 500; transition: all 0.2s; }
.chapter-nav a:hover { background: var(--accent-light); border-color: var(--accent); text-decoration: none; }

/* Reading time badge */
.badge { display: inline-block; background: var(--accent); color: white; padding: 2px 10px; border-radius: 12px; font-size: 0.78rem; font-weight: 600; margin-right: 6px; }
.badge-green { background: var(--success); }
.badge-orange { background: var(--warning); }
.badge-red { background: var(--danger); }

/* Responsive */
@media (max-width: 1024px) {
  .sidebar { transform: translateX(-100%); }
  .sidebar.open { transform: translateX(0); }
  .sidebar-toggle { display: block; }
  .main { margin-left: 0; padding: 30px 24px; }
}
@media (max-width: 640px) {
  .main { padding: 20px 16px; }
  h1 { font-size: 1.6rem; }
  h2 { font-size: 1.3rem; }
  .header-nav a { display: none; }
  .header-nav .theme-toggle { display: inline-block; }
}
@media print {
  .sidebar, .header, .sidebar-toggle, .theme-toggle, .progress-bar { display: none; }
  .main { margin: 0; padding: 20px; max-width: 100%; }
}

/* Tooltip */
.tooltip { position: relative; border-bottom: 1px dotted var(--accent); cursor: help; }
.tooltip:hover::after { content: attr(data-tip); position: absolute; bottom: 100%; left: 50%; transform: translateX(-50%); background: var(--bg-code); border: 1px solid var(--border); padding: 8px 12px; border-radius: var(--radius); font-size: 0.82rem; white-space: nowrap; z-index: 100; box-shadow: var(--shadow-lg); }

/* Tab system for code comparison */
.tab-container { margin: 20px 0; border: 1px solid var(--border); border-radius: var(--radius); overflow: hidden; }
.tab-buttons { display: flex; background: var(--bg-secondary); border-bottom: 1px solid var(--border); }
.tab-btn { flex: 1; padding: 10px; border: none; background: none; cursor: pointer; font-weight: 500; color: var(--text-secondary); font-size: 0.88rem; transition: all 0.2s; border-bottom: 2px solid transparent; }
.tab-btn.active { color: var(--accent); border-bottom-color: var(--accent); background: var(--bg); }
.tab-content { display: none; padding: 0; }
.tab-content.active { display: block; }
.tab-content pre { margin: 0; border: none; border-radius: 0; }

/* Flow diagram */
.flow { display: flex; align-items: center; gap: 8px; flex-wrap: wrap; margin: 16px 0; padding: 16px; background: var(--bg-secondary); border-radius: var(--radius); }
.flow-box { padding: 8px 16px; background: var(--accent-light); border: 2px solid var(--accent); border-radius: var(--radius); font-weight: 600; font-size: 0.9rem; text-align: center; }
.flow-arrow { font-size: 1.3rem; color: var(--accent); font-weight: 700; }
  </style>
</head>
<body>
  <div class="progress-bar" id="progressBar"></div>

  <header class="header">
    <div class="header-title"><a href="../index.html">ğŸ“ GDL å­¦ä¹ æŒ‡å—</a></div>
    <div class="header-nav">
      <a href="../chapter4/index.html">â† Ch.4 å‡ ä½•åŸŸ</a>
      <a href="../index.html">ç›®å½•</a>
      <button class="theme-toggle" onclick="toggleTheme()">ğŸŒ™</button>
    </div>
  </header>

  <button class="sidebar-toggle" onclick="toggleSidebar()">â˜°</button>

  <nav class="sidebar" id="sidebar">
    <h3>Chapter 5</h3>
    <a href="#overview">æ¦‚è¿°</a>
    <a href="#sec5-1">5.1 å·ç§¯ç¥ç»ç½‘ç»œ CNN</a>
    <a href="#cnn-group-theory" class="sub">ç¾¤è®ºè§£é‡Š</a>
    <a href="#cnn-convolution-theorem" class="sub">å·ç§¯å®šç†</a>
    <a href="#cnn-pooling" class="sub">æ± åŒ–ä¸ä¸å˜æ€§</a>
    <a href="#cnn-resnet" class="sub">æ®‹å·®ç½‘ç»œ</a>
    <a href="#cnn-code" class="sub">ä»£ç å®ç°</a>
    <a href="#sec5-2">5.2 ç¾¤ç­‰å˜ CNN</a>
    <a href="#gcnn-definition" class="sub">G-CNN å®šä¹‰</a>
    <a href="#gcnn-group-conv" class="sub">ç¾¤å·ç§¯</a>
    <a href="#gcnn-code" class="sub">ä»£ç å®ç°</a>
    <a href="#sec5-3">5.3 å›¾ç¥ç»ç½‘ç»œ GNN <span class="star">â­â­â­</span></a>
    <a href="#gnn-overview" class="sub">æ€»è§ˆ</a>
    <a href="#gnn-mpnn" class="sub">æ¶ˆæ¯ä¼ é€’æ¡†æ¶</a>
    <a href="#gnn-three-flavors" class="sub">ä¸‰ç§é£æ ¼</a>
    <a href="#gnn-gcn" class="sub">GCN å®Œæ•´æ¨å¯¼</a>
    <a href="#gnn-gat" class="sub">GAT æ³¨æ„åŠ›æœºåˆ¶</a>
    <a href="#gnn-graphsage" class="sub">GraphSAGE</a>
    <a href="#gnn-gin" class="sub">GIN è¡¨è¾¾åŠ›</a>
    <a href="#gnn-comparison" class="sub">GNN å¯¹æ¯”è¡¨</a>
    <a href="#gnn-code" class="sub">å®Œæ•´ä»£ç </a>
    <a href="#gnn-physrobot" class="sub">ğŸ¤– PhysRobot å…³è”</a>
    <a href="#sec5-4">5.4 DeepSets / Transformer</a>
    <a href="#deepsets" class="sub">Deep Sets</a>
    <a href="#transformer" class="sub">Transformer</a>
    <a href="#latent-graph" class="sub">éšå¼å›¾æ¨æ–­</a>
    <a href="#sec5-5">5.5 ç­‰å˜æ¶ˆæ¯ä¼ é€’ <span class="star">â­â­</span></a>
    <a href="#egnn" class="sub">E(n)-ç­‰å˜ GNN</a>
    <a href="#tfn" class="sub">Tensor Field Network</a>
    <a href="#empnn-physrobot" class="sub">ğŸ¤– EdgeFrame å…³è”</a>
    <a href="#empnn-code" class="sub">ä»£ç å®ç°</a>
    <a href="#sec5-6">5.6 å†…è•´ç½‘æ ¼ CNN</a>
    <a href="#geodesic-cnn" class="sub">æµ‹åœ°çº¿ CNN</a>
    <a href="#monet" class="sub">MoNet</a>
    <a href="#gauge-cnn" class="sub">è§„èŒƒç­‰å˜ CNN</a>
    <a href="#sec5-7">5.7 å¾ªç¯ç¥ç»ç½‘ç»œ RNN</a>
    <a href="#rnn-equivariance" class="sub">æ—¶é—´ç­‰å˜æ€§</a>
    <a href="#rnn-vanishing" class="sub">æ¢¯åº¦æ¶ˆå¤±</a>
    <a href="#sec5-8">5.8 LSTM</a>
    <a href="#lstm-gates" class="sub">é—¨æ§æœºåˆ¶</a>
    <a href="#lstm-time-warp" class="sub">æ—¶é—´æ‰­æ›²ä¸å˜æ€§</a>
    <a href="#exercises">ç»ƒä¹ é¢˜</a>
    <h3>å¯¼èˆª</h3>
    <a href="../index.html">ğŸ“š æ€»ç›®å½•</a>
    <a href="../chapter4/index.html">â† Ch.4 å‡ ä½•åŸŸ</a>
  </nav>

  <main class="main">

    <!-- ====================== CHAPTER TITLE ====================== -->
    <h1>Chapter 5: Geometric Deep Learning Models<br>
      <span style="font-size:0.55em;color:var(--text-secondary)">å‡ ä½•æ·±åº¦å­¦ä¹ æ¨¡å‹ â€” ä» CNN åˆ° GNN å†åˆ°ç­‰å˜ç½‘ç»œ</span>
    </h1>

    <p>
      <span class="badge badge-red">â­ æœ€é‡è¦ç« èŠ‚</span>
      <span class="badge badge-orange">é¢„è®¡é˜…è¯»ï¼š6+ å°æ—¶</span>
      <span class="badge badge-green">ä»£ç å¯è¿è¡Œ</span>
    </p>

    <!-- ====================== OVERVIEW ====================== -->
    <div class="callout callout-info" id="overview">
      <h4>æœ¬ç« æ¦‚è¿°</h4>
      <p>æœ¬ç« æ˜¯å…¨ä¹¦çš„<strong>æ ¸å¿ƒé«˜æ½®</strong>ã€‚åœ¨å‰å››ç« å»ºç«‹äº†å‡ ä½•å…ˆéªŒï¼ˆå¯¹ç§°æ€§ã€ä¸å˜æ€§ã€ç­‰å˜æ€§ï¼‰å’Œå‡ ä½•åŸŸï¼ˆ5 Gsï¼‰ä¹‹åï¼Œæœ¬ç« å±•ç¤ºè¿™äº›ç†è®ºåŸåˆ™å¦‚ä½•<strong>å…·è±¡åŒ–ä¸ºå®é™…çš„æ·±åº¦å­¦ä¹ æ¶æ„</strong>ã€‚æ¯ä¸€ç§æµè¡Œçš„æ·±åº¦å­¦ä¹ æ¨¡å‹â€”â€”CNNã€GNNã€Transformerã€LSTMâ€”â€”éƒ½æ˜¯å‡ ä½•æ·±åº¦å­¦ä¹ è“å›¾åœ¨ç‰¹å®šåŸŸå’Œå¯¹ç§°ç¾¤ä¸Šçš„<strong>å®ä¾‹åŒ–</strong>ã€‚</p>
      <table>
        <thead><tr><th>æ¶æ„</th><th>åŸŸ Î©</th><th>å¯¹ç§°ç¾¤ G</th><th>ç­‰å˜æ“ä½œ</th><th>å¯¹åº”èŠ‚</th></tr></thead>
        <tbody>
          <tr><td>CNN</td><td>ç½‘æ ¼ $\mathbb{Z}^d$</td><td>å¹³ç§»ç¾¤ $(\mathbb{Z}^d, +)$</td><td>å·ç§¯</td><td>Â§5.1</td></tr>
          <tr><td>G-CNN</td><td>é½æ¬¡ç©ºé—´ $\Omega$</td><td>ä¸€èˆ¬ç¾¤ $G$</td><td>ç¾¤å·ç§¯</td><td>Â§5.2</td></tr>
          <tr><td>GNN</td><td>å›¾ $(V, E)$</td><td>ç½®æ¢ç¾¤ $\Sigma_n$</td><td>æ¶ˆæ¯ä¼ é€’</td><td>Â§5.3</td></tr>
          <tr><td>Transformer</td><td>é›†åˆ / å®Œå…¨å›¾</td><td>ç½®æ¢ç¾¤ $\Sigma_n$</td><td>è‡ªæ³¨æ„åŠ›</td><td>Â§5.4</td></tr>
          <tr><td>EMPNN</td><td>å‡ ä½•å›¾</td><td>$E(3) \times \Sigma_n$</td><td>ç­‰å˜æ¶ˆæ¯ä¼ é€’</td><td>Â§5.5</td></tr>
          <tr><td>Mesh CNN</td><td>æµå½¢/ç½‘æ ¼</td><td>ç­‰è·ç¾¤ + è§„èŒƒç¾¤</td><td>æµ‹åœ°çº¿å·ç§¯</td><td>Â§5.6</td></tr>
          <tr><td>RNN/LSTM</td><td>æ—¶é—´ç½‘æ ¼ $\mathbb{Z}$</td><td>æ—¶é—´å¹³ç§» + æ—¶é—´æ‰­æ›²</td><td>å¾ªç¯æ›´æ–°</td><td>Â§5.7-8</td></tr>
        </tbody>
      </table>
    </div>

    <div class="callout callout-project">
      <h4>ä¸ PhysRobot é¡¹ç›®çš„å…³è”</h4>
      <p>æœ¬ç« æ˜¯æˆ‘ä»¬ <strong>PhysRobot åŒ»ç–—æœºå™¨äººä»¿çœŸ</strong>é¡¹ç›®çš„ç†è®ºåŸºçŸ³ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ¨¡å—ç›´æ¥å¯¹åº”æœ¬ç« å†…å®¹ï¼š</p>
      <ul>
        <li><code>DynamicalGNN</code> = Â§5.3 æ¶ˆæ¯ä¼ é€’ GNN çš„ç‰©ç†ç‰¹åŒ–ç‰ˆ</li>
        <li><code>EdgeFrame</code> = Â§5.5 ç­‰å˜æ¶ˆæ¯ä¼ é€’ç½‘ç»œ (EMPNN) çš„ E(3) ç­‰å˜å®ç°</li>
        <li>ç²’å­ç³»ç»Ÿä»¿çœŸ = GNN åœ¨åŠ¨åŠ›å­¦ç³»ç»Ÿä¸Šçš„åº”ç”¨</li>
      </ul>
    </div>

    <!-- ====================== 5.1 CNN ====================== -->
    <h2 id="sec5-1">5.1 å·ç§¯ç¥ç»ç½‘ç»œ (CNN)<br><span style="font-size:0.65em;color:var(--text-secondary)">Convolutional Neural Networks</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>å·ç§¯ç¥ç»ç½‘ç»œæ˜¯å‡ ä½•æ·±åº¦å­¦ä¹ è“å›¾æœ€æ—©ä¹Ÿæœ€è‘—åçš„å®ä¾‹ã€‚åœ¨ Â§4.2 ä¸­æˆ‘ä»¬å·²ç»å®Œæ•´åˆ»ç”»äº†<span class="term">çº¿æ€§</span>ä¸”<span class="term">å±€éƒ¨</span>çš„<span class="term">å¹³ç§»ç­‰å˜</span>ç®—å­â€”â€”å®ƒä»¬å°±æ˜¯ä¸å±€éƒ¨åŒ–æ»¤æ³¢å™¨çš„<strong>å·ç§¯</strong> $C(\theta)\mathbf{x} = \mathbf{x} \star \theta$ã€‚</p>
      </div>
      <div class="en">
        CNNs are perhaps the earliest and most well-known example of deep learning architectures following the GDL blueprint. Linear, local, translation-equivariant operators are exactly convolutions with a localized filter.
      </div>
    </div>

    <h3 id="cnn-group-theory">ç»å…¸ CNN çš„ç¾¤è®ºè§£é‡Šï¼šå¹³ç§»ç­‰å˜æ€§</h3>

    <p>CNN çš„æ ¸å¿ƒæ•°å­¦æ€§è´¨æ˜¯<strong>å¹³ç§»ç­‰å˜æ€§</strong>ï¼ˆTranslation Equivarianceï¼‰ã€‚è®©æˆ‘ä»¬ä»ç¾¤è®ºçš„è§†è§’ä¸¥æ ¼ç†è§£è¿™ä¸€ç‚¹ã€‚</p>

    <div class="blueprint">
      <h4>å¹³ç§»ç­‰å˜æ€§çš„ä¸¥æ ¼å®šä¹‰</h4>
      <p>è®¾ $\rho(t)$ æ˜¯å¹³ç§»ç¾¤ $(\mathbb{Z}^2, +)$ åœ¨ä¿¡å·ç©ºé—´ $\mathcal{X}(\Omega)$ ä¸Šçš„è¡¨ç¤ºã€‚å¦‚æœå¯¹æ‰€æœ‰å¹³ç§» $t$ éƒ½æœ‰ï¼š</p>
      $$F(\rho(t)\mathbf{x}) = \rho(t)F(\mathbf{x})$$
      <p>åˆ™ç§° $F$ æ˜¯<span class="term">å¹³ç§»ç­‰å˜</span>çš„ã€‚ç›´è§‰ä¸Šï¼šå…ˆå¹³ç§»å›¾åƒå†å·ç§¯ = å…ˆå·ç§¯å†å¹³ç§»ç»“æœã€‚</p>
    </div>

    <p>è€ƒè™‘æ ‡é‡å€¼ï¼ˆå•é€šé“/ç°åº¦ï¼‰ç¦»æ•£åŒ–å›¾åƒï¼Œå…¶ä¸­åŸŸæ˜¯ç½‘æ ¼ $\Omega = [H] \times [W]$ï¼Œ$\mathbf{u} = (u_1, u_2)$ï¼Œ$\mathbf{x} \in \mathcal{X}(\Omega, \mathbb{R})$ã€‚</p>

    <p>ä»»ä½•å…·æœ‰ç´§æ”¯æ’‘æ»¤æ³¢å™¨ï¼ˆå¤§å° $H^f \times W^f$ï¼‰çš„å·ç§¯éƒ½å¯ä»¥å†™æˆ<strong>ç”Ÿæˆå…ƒ</strong>çš„çº¿æ€§ç»„åˆ $\theta_{1,1}, \ldots, \theta_{H^f, W^f}$ï¼Œæ¯ä¸ªç”Ÿæˆå…ƒç”±å•ä½è„‰å†²ç»™å‡ºï¼š</p>

    <div class="math-block">
      $$\theta_{vw}(u_1, u_2) = \delta(u_1 - v, u_2 - w)$$
      <div class="math-explain">
        $\delta$ æ˜¯ Dirac deltaï¼ˆåœ¨ç¦»æ•£æƒ…å†µä¸‹æ˜¯ Kronecker deltaï¼‰ï¼Œ$\theta_{vw}$ æ˜¯åœ¨ä½ç½® $(v, w)$ å¤„å€¼ä¸º 1ã€å…¶ä½™å¤„å€¼ä¸º 0 çš„"è„‰å†²"æ»¤æ³¢å™¨ã€‚
      </div>
    </div>

    <p>å› æ­¤ï¼Œ<strong>ä»»ä½•å±€éƒ¨çº¿æ€§ç­‰å˜æ˜ å°„</strong>éƒ½å¯ä»¥è¡¨ç¤ºä¸ºï¼š</p>

    <div class="math-block">
      $$F(\mathbf{x}) = \sum_{v=1}^{H^f} \sum_{w=1}^{W^f} \alpha_{vw} C(\theta_{vw}) \mathbf{x} \tag{26}$$
      <div class="math-explain">
        è¿™é‡Œ $C(\theta_{vw})$ æ˜¯å¾ªç¯çŸ©é˜µï¼Œ$\alpha_{vw}$ æ˜¯å¯å­¦ä¹ çš„æ»¤æ³¢å™¨æƒé‡ã€‚æ³¨æ„ $\mathbf{x}$ å’Œ $\theta_{vw}$ è™½ç„¶é€šå¸¸æ˜¯ 2D çŸ©é˜µï¼Œä½†åœ¨æ­¤æ–¹ç¨‹ä¸­è¢«å±•å¹³æˆå‘é‡/çŸ©é˜µã€‚
      </div>
    </div>

    <p>åœ¨åæ ‡ä¸­å±•å¼€ï¼Œå¾—åˆ°ç†Ÿæ‚‰çš„ <strong>2D å·ç§¯</strong>ï¼š</p>

    <div class="math-block">
      $$F(\mathbf{x})_{uv} = \sum_{a=1}^{H^f} \sum_{b=1}^{W^f} \alpha_{ab} \, x_{u+a, v+b} \tag{27}$$
    </div>

    <div class="derivation">
      <h4>é€æ­¥æ¨å¯¼ï¼šä¸ºä»€ä¹ˆå·ç§¯ = å¹³ç§»ç­‰å˜</h4>
      <div class="step"><span class="step-num">1</span>è®¾å¹³ç§»ç®—å­ $T_t[\mathbf{x}](u) = \mathbf{x}(u - t)$ï¼Œå¯¹ä¿¡å·æ–½åŠ å¹³ç§» $t = (t_1, t_2)$ã€‚</div>
      <div class="step"><span class="step-num">2</span>å·ç§¯æ“ä½œï¼š$(T_t[\mathbf{x}] \star \theta)(u) = \sum_v (T_t[\mathbf{x}])(v) \cdot \theta(u-v) = \sum_v \mathbf{x}(v - t) \cdot \theta(u-v)$</div>
      <div class="step"><span class="step-num">3</span>ä»¤ $w = v - t$ï¼Œåˆ™ $v = w + t$ï¼š$= \sum_w \mathbf{x}(w) \cdot \theta(u - t - w) = (\mathbf{x} \star \theta)(u - t)$</div>
      <div class="step"><span class="step-num">4</span>å³ $T_t[\mathbf{x}] \star \theta = T_t[\mathbf{x} \star \theta]$ï¼Œå¹³ç§»å’Œå·ç§¯<strong>å¯äº¤æ¢</strong>ã€‚âœ… ç­‰å˜æ€§æˆç«‹ã€‚</div>
    </div>

    <p>å½“è¾“å…¥ä»å•é€šé“æ‰©å±•åˆ°<strong>å¤šé€šé“</strong>ï¼ˆå¦‚ RGBï¼‰ï¼Œå·ç§¯æ»¤æ³¢å™¨å˜æˆå¼ é‡ï¼š</p>

    <div class="math-block">
      $$F(\mathbf{x})_{uvj} = \sum_{a=1}^{H^f} \sum_{b=1}^{W^f} \sum_{c=1}^{M} \alpha_{jabc} \, x_{u+a, v+b, c}, \quad j \in [N] \tag{28}$$
      <div class="math-explain">
        $M$ æ˜¯è¾“å…¥é€šé“æ•°ï¼Œ$N$ æ˜¯è¾“å‡ºé€šé“æ•°ã€‚$\alpha_{jabc}$ æ˜¯ä¸€ä¸ª 4D å¼ é‡ï¼šç¬¬ $j$ ä¸ªè¾“å‡ºé€šé“ç”±æ‰€æœ‰ $M$ ä¸ªè¾“å…¥é€šé“å’Œ $H^f \times W^f$ ä¸ªç©ºé—´ä½ç½®çš„çº¿æ€§ç»„åˆå†³å®šã€‚
      </div>
    </div>

    <div class="symbol-table">
      <h4>Â§5.1 ç¬¦å·è¡¨</h4>
      <table>
        <thead><tr><th>ç¬¦å·</th><th>å«ä¹‰</th><th>ç±»å‹</th></tr></thead>
        <tbody>
          <tr><td>$\Omega = [H] \times [W]$</td><td>2D å›¾åƒåŸŸï¼ˆç½‘æ ¼ï¼‰</td><td>é›†åˆ</td></tr>
          <tr><td>$\mathbf{x} \in \mathcal{X}(\Omega, \mathbb{R})$</td><td>åŸŸä¸Šçš„æ ‡é‡ä¿¡å·ï¼ˆç°åº¦å›¾ï¼‰</td><td>å‡½æ•°</td></tr>
          <tr><td>$C(\theta)$</td><td>å¾ªç¯çŸ©é˜µï¼Œå‚æ•°ä¸º $\theta$</td><td>çŸ©é˜µ</td></tr>
          <tr><td>$\alpha_{vw}$</td><td>å¯å­¦ä¹ çš„å·ç§¯æ ¸æƒé‡</td><td>æ ‡é‡</td></tr>
          <tr><td>$H^f, W^f$</td><td>æ»¤æ³¢å™¨çš„é«˜åº¦å’Œå®½åº¦</td><td>æ•´æ•°</td></tr>
          <tr><td>$T_t$</td><td>å¹³ç§»ç®—å­ï¼Œå¹³ç§»é‡ $t$</td><td>ç®—å­</td></tr>
          <tr><td>$\sigma$</td><td>éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰</td><td>å‡½æ•°</td></tr>
          <tr><td>$P$</td><td>æ± åŒ–/ç²—åŒ–ç®—å­</td><td>ç®—å­</td></tr>
        </tbody>
      </table>
    </div>

    <h3 id="cnn-convolution-theorem">å·ç§¯å®šç†ï¼šç©ºåŸŸ vs é¢‘åŸŸ</h3>

    <p>å·ç§¯æœ‰ä¸€ä¸ªæå…¶ä¼˜ç¾çš„æ€§è´¨ï¼Œå°†<strong>ç©ºåŸŸçš„å·ç§¯</strong>ä¸<strong>é¢‘åŸŸçš„é€å…ƒç´ ä¹˜æ³•</strong>è”ç³»èµ·æ¥ã€‚è¿™å°±æ˜¯<span class="term">å·ç§¯å®šç†</span>ï¼ˆConvolution Theoremï¼‰ã€‚</p>

    <div class="math-block">
      $$\mathcal{F}[\mathbf{x} \star \theta] = \mathcal{F}[\mathbf{x}] \odot \mathcal{F}[\theta]$$
      <div class="math-explain">
        ç©ºåŸŸå·ç§¯ï¼ˆ$O(n^2)$ å¤æ‚åº¦ï¼‰ç­‰ä»·äºå…ˆåš Fourier å˜æ¢ï¼Œå†é€å…ƒç´ ç›¸ä¹˜ï¼Œå†åšé€† Fourier å˜æ¢ã€‚ä½¿ç”¨ FFT å¯ä»¥å°†å·ç§¯çš„å¤æ‚åº¦ä» $O(n^2)$ é™åˆ° $O(n \log n)$ã€‚$\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamard ä¹˜ç§¯ï¼‰ã€‚
      </div>
    </div>

    <div class="derivation">
      <h4>å·ç§¯å®šç†çš„æ¨å¯¼ï¼ˆ1D æƒ…å½¢ï¼‰</h4>
      <div class="step"><span class="step-num">1</span>ç¦»æ•£ Fourier å˜æ¢å®šä¹‰ï¼š$\hat{x}[k] = \sum_{n=0}^{N-1} x[n] \, e^{-i 2\pi kn / N}$</div>
      <div class="step"><span class="step-num">2</span>å·ç§¯å®šä¹‰ï¼š$(x \star \theta)[m] = \sum_{n=0}^{N-1} x[n] \, \theta[m - n]$</div>
      <div class="step"><span class="step-num">3</span>å¯¹å·ç§¯ç»“æœåš DFTï¼š
        $\mathcal{F}[x \star \theta][k] = \sum_{m} \left(\sum_{n} x[n] \theta[m-n]\right) e^{-i2\pi km/N}$</div>
      <div class="step"><span class="step-num">4</span>äº¤æ¢æ±‚å’Œé¡ºåºï¼š$= \sum_n x[n] \sum_m \theta[m-n] e^{-i2\pi km/N}$</div>
      <div class="step"><span class="step-num">5</span>ä»¤ $p = m - n$ï¼š$= \sum_n x[n] e^{-i2\pi kn/N} \sum_p \theta[p] e^{-i2\pi kp/N} = \hat{x}[k] \cdot \hat{\theta}[k]$</div>
      <div class="step"><span class="step-num">6</span>å› æ­¤ $\mathcal{F}[x \star \theta] = \hat{x} \odot \hat{\theta}$ âœ…</div>
    </div>

    <h3 id="cnn-pooling">æ± åŒ–ä½œä¸ºä¸å˜æ€§æ˜ å°„</h3>

    <p>å·ç§¯æ˜¯<strong>ç­‰å˜</strong>çš„â€”â€”è¾“å‡ºéšè¾“å…¥å¹³ç§»è€Œå¹³ç§»ã€‚ä½†è®¸å¤šä»»åŠ¡éœ€è¦<strong>ä¸å˜æ€§</strong>ï¼ˆå¦‚å›¾åƒåˆ†ç±»ï¼šå¹³ç§»å›¾åƒä¸åº”æ”¹å˜åˆ†ç±»ç»“æœï¼‰ã€‚è¿™å°±æ˜¯<span class="term">æ± åŒ–</span>ï¼ˆPoolingï¼‰çš„è§’è‰²ã€‚</p>

    <p>CNN å±‚çš„å®Œæ•´å…¬å¼ï¼š</p>

    <div class="math-block">
      $$\mathbf{h} = P(\sigma(F(\mathbf{x}))) \tag{29}$$
      <div class="math-explain">
        ä¸‰æ­¥ç»„åˆï¼šç­‰å˜çº¿æ€§å±‚ $F$ï¼ˆå·ç§¯ï¼‰â†’ éçº¿æ€§ $\sigma$ï¼ˆå¦‚ ReLUï¼‰â†’ ç²—åŒ–/æ± åŒ– $P$ã€‚æ± åŒ–å°†é«˜åˆ†è¾¨ç‡ç½‘æ ¼ $\Omega$ æ˜ å°„åˆ°ä½åˆ†è¾¨ç‡ç½‘æ ¼ $\Omega'$ï¼Œç­‰æ•ˆäºå¢å¤§æ„Ÿå—é‡ã€‚å¸¸è§çš„æ± åŒ–ç­–ç•¥ï¼šä½é€šæŠ—æ··å æ»¤æ³¢ï¼ˆå±€éƒ¨å¹³å‡ï¼‰+ ç½‘æ ¼é™é‡‡æ ·ï¼Œæˆ–éçº¿æ€§æœ€å¤§æ± åŒ–ã€‚
      </div>
    </div>

    <div class="flow">
      <div class="flow-box">è¾“å…¥ $\mathbf{x}$</div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">å·ç§¯ $F$<br><small>ç­‰å˜</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">æ¿€æ´» $\sigma$<br><small>é€å…ƒç´ </small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">æ± åŒ– $P$<br><small>ç²—åŒ–</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">è¾“å‡º $\mathbf{h}$</div>
    </div>

    <p><strong>å…¨å±€æ± åŒ–</strong>ï¼ˆGlobal Poolingï¼‰å®ç°å®Œå…¨çš„å¹³ç§»ä¸å˜æ€§ï¼šå¯¹æ‰€æœ‰ç©ºé—´ä½ç½®æ±‚å¹³å‡/æœ€å¤§å€¼ï¼Œè¾“å‡ºä¸è¾“å…¥ä½ç½®æ— å…³ã€‚</p>

    <h3 id="cnn-resnet">æ·±åº¦ç½‘ç»œä¸æ®‹å·®è¿æ¥</h3>

    <p>He et al. (2016) çš„å…³é”®æ´å¯Ÿï¼šå°†æ¯ä¸€å±‚é‡æ–°å‚æ•°åŒ–ä¸ºå¯¹å‰ä¸€å±‚ç‰¹å¾çš„<strong>æ‰°åŠ¨</strong>ï¼Œè€Œéé€šç”¨çš„éçº¿æ€§å˜æ¢ï¼š</p>

    <div class="math-block">
      $$\mathbf{h} = P(\mathbf{x} + \sigma(F(\mathbf{x}))) \tag{30}$$
      <div class="math-explain">
        æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰çš„æ ¸å¿ƒã€‚ç½‘ç»œå­¦ä¹ çš„æ˜¯<strong>æ®‹å·®</strong> $\sigma(F(\mathbf{x}))$â€”â€”å³è¾“å…¥çš„"å˜åŒ–é‡"ã€‚è¿™ä¸å°†æ·±åº¦ç½‘ç»œè§†ä¸º<strong>å¸¸å¾®åˆ†æ–¹ç¨‹ (ODE)</strong> çš„ç¦»æ•£åŒ–å®Œå…¨ä¸€è‡´ï¼š$\dot{\mathbf{x}} = \sigma(F(\mathbf{x}))$ï¼Œæ®‹å·®è¿æ¥å°±æ˜¯å¯¹ ODE çš„å‰å‘æ¬§æ‹‰ç¦»æ•£åŒ–ã€‚
      </div>
    </div>

    <div class="figure">
      <img src="../assets/ch5_p79_img0.png" alt="CNN architectures: LeNet, AlexNet, ResNet, U-Net">
      <figcaption>å›¾ 15ï¼šç»å…¸ CNN æ¶æ„ã€‚ä»ä¸Šåˆ°ä¸‹ï¼šLeNet (1998)ã€AlexNet (2012)ã€ResNet (2016)ã€U-Net (2015)ã€‚</figcaption>
    </div>

    <h3 id="cnn-code">å®Œæ•´ä»£ç ï¼šCNN å®ç° + ç­‰å˜æ€§éªŒè¯</h3>

    <div class="code-container">
      <span class="code-label">Python / PyTorch</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

# ============================================================
# 1. ä¸€ä¸ªæ ‡å‡†çš„ CNN æ¨¡å‹ï¼ˆç±»ä¼¼ LeNetï¼‰
# ============================================================
class SimpleCNN(nn.Module):
    """
    ç»å…¸ CNN æ¶æ„ï¼Œéµå¾ª GDL è“å›¾ï¼š
    Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ FC
    
    ç­‰å˜æ€§åˆ†æï¼š
    - Conv å±‚ï¼šå¹³ç§»ç­‰å˜ âœ…
    - ReLUï¼šé€å…ƒç´ æ“ä½œï¼Œä¸ç ´åç­‰å˜æ€§ âœ…
    - Poolï¼šé™ä½åˆ†è¾¨ç‡ï¼Œä¿æŒç­‰å˜æ€§ï¼ˆaverage poolï¼‰âœ…
    - FCï¼ˆå…¨è¿æ¥ï¼‰ï¼šç ´åå¹³ç§»ä¸å˜æ€§ âŒ
      â†’ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå…¨å·ç§¯ç½‘ç»œæ›´"å‡ ä½•"
    """
    def __init__(self, in_channels=1, num_classes=10):
        super().__init__()
        # ç­‰å˜éƒ¨åˆ†
        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.AvgPool2d(2)  # å¹³å‡æ± åŒ–æ¯”æœ€å¤§æ± åŒ–æ›´"å‡ ä½•"
        
        # ä¸å˜æ€§èšåˆï¼ˆå…¨å±€å¹³å‡æ± åŒ– â†’ å®Œå…¨å¹³ç§»ä¸å˜ï¼‰
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        # åˆ†ç±»å¤´
        self.fc = nn.Linear(64, num_classes)
    
    def forward(self, x):
        # ç­‰å˜å±‚
        x = self.pool(F.relu(self.conv1(x)))   # (B,32,H/2,W/2)
        x = self.pool(F.relu(self.conv2(x)))   # (B,64,H/4,W/4)
        
        # ä¸å˜æ€§èšåˆ
        x = self.global_pool(x)                 # (B,64,1,1)
        x = x.view(x.size(0), -1)              # (B,64)
        
        # åˆ†ç±»
        return self.fc(x)                       # (B,10)


# ============================================================
# 2. éªŒè¯å¹³ç§»ç­‰å˜æ€§
# ============================================================
def verify_translation_equivariance():
    """éªŒè¯ CNN çš„å·ç§¯å±‚æ˜¯å¹³ç§»ç­‰å˜çš„ã€‚"""
    
    torch.manual_seed(42)
    conv = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    x = torch.randn(1, 1, 32, 32)
    
    # æ–¹æ³•1ï¼šå…ˆå·ç§¯ï¼Œå†å¹³ç§»
    y = conv(x)
    y_shifted = torch.roll(y, shifts=(3, 5), dims=(2, 3))
    
    # æ–¹æ³•2ï¼šå…ˆå¹³ç§»ï¼Œå†å·ç§¯
    x_shifted = torch.roll(x, shifts=(3, 5), dims=(2, 3))
    y_from_shifted = conv(x_shifted)
    
    # æ¯”è¾ƒï¼ˆå¿½ç•¥è¾¹ç•Œæ•ˆåº”â€”â€”ä½¿ç”¨ circular padding æ—¶å®Œå…¨ç­‰ä»·ï¼‰
    # ä¸­å¿ƒåŒºåŸŸæ¯”è¾ƒï¼Œé¿å…è¾¹ç•Œæ•ˆåº”
    center = slice(4, 28)
    error = (y_shifted[:,:,center,center] - y_from_shifted[:,:,center,center]).abs().max()
    
    print(f"å¹³ç§»ç­‰å˜æ€§è¯¯å·®: {error:.2e}")
    print(f"ç­‰å˜æ€§éªŒè¯: {'âœ… é€šè¿‡' if error < 1e-5 else 'âŒ å¤±è´¥'}")
    
    return error.item()


# ============================================================
# 3. å·ç§¯å®šç†çš„æ•°å€¼éªŒè¯
# ============================================================
def verify_convolution_theorem():
    """éªŒè¯ç©ºåŸŸå·ç§¯ = é¢‘åŸŸé€å…ƒç´ ä¹˜æ³•ã€‚"""
    
    x = torch.randn(64, 64)
    theta = torch.randn(64, 64)  # é›¶å¡«å……åˆ°ç›¸åŒå¤§å°
    
    # ç©ºåŸŸå·ç§¯ï¼ˆä½¿ç”¨ FFT å®ç°çš„å¾ªç¯å·ç§¯ï¼‰
    conv_spatial = torch.fft.ifft2(
        torch.fft.fft2(x) * torch.fft.fft2(theta)
    ).real
    
    # é¢‘åŸŸæ–¹æ³•
    X_freq = torch.fft.fft2(x)
    Theta_freq = torch.fft.fft2(theta)
    conv_spectral = torch.fft.ifft2(X_freq * Theta_freq).real
    
    error = (conv_spatial - conv_spectral).abs().max()
    print(f"å·ç§¯å®šç†è¯¯å·®: {error:.2e}")
    print(f"å·ç§¯å®šç†éªŒè¯: {'âœ… é€šè¿‡' if error < 1e-5 else 'âŒ å¤±è´¥'}")


# ============================================================
# 4. æ®‹å·®ç½‘ç»œï¼ˆResNet Blockï¼‰
# ============================================================
class ResidualBlock(nn.Module):
    """
    æ®‹å·®å—ï¼šh = x + Ïƒ(F(x))
    
    å‡ ä½•è§£é‡Šï¼šè¿™æ˜¯ ODE áº‹ = Ïƒ(F(x)) çš„å‰å‘æ¬§æ‹‰ç¦»æ•£åŒ–ã€‚
    æ¯ä¸€å±‚å­¦ä¹ çš„æ˜¯"é€Ÿåº¦åœº"è€Œé"ä½ç½®"ã€‚
    """
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
    
    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return F.relu(out + residual)  # h = x + Ïƒ(F(x))


if __name__ == "__main__":
    print("=" * 50)
    print("CNN ç­‰å˜æ€§éªŒè¯")
    print("=" * 50)
    verify_translation_equivariance()
    print()
    verify_convolution_theorem()
    
    print("\næ¨¡å‹æµ‹è¯•:")
    model = SimpleCNN(in_channels=1, num_classes=10)
    x = torch.randn(4, 1, 28, 28)
    out = model(x)
    print(f"è¾“å…¥: {x.shape} â†’ è¾“å‡º: {out.shape}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")</code></pre>
    </div>

    <div class="callout callout-project">
      <h4>ä¸ PhysRobot çš„å…³è”</h4>
      <p>è™½ç„¶æˆ‘ä»¬çš„ PhysRobot é¡¹ç›®ä¸»è¦ä½¿ç”¨ <strong>GNN è€Œé CNN</strong>ï¼Œä½† CNN çš„ç†è®ºæ¡†æ¶ä¸ºç†è§£ GNN æä¾›äº†åŸºç¡€ï¼š</p>
      <ul>
        <li>CNN çš„<strong>å±€éƒ¨æ€§</strong>ï¼ˆå°æ»¤æ³¢å™¨ï¼‰â†’ GNN ä¸­çš„<strong>é‚»åŸŸèšåˆ</strong></li>
        <li>CNN çš„<strong>æƒé‡å…±äº«</strong>ï¼ˆåŒä¸€æ»¤æ³¢å™¨éå†å…¨å›¾ï¼‰â†’ GNN ä¸­çš„<strong>å…±äº«æ¶ˆæ¯å‡½æ•°</strong></li>
        <li>CNN çš„<strong>å¤šå°ºåº¦æ± åŒ–</strong> â†’ GNN ä¸­çš„<strong>å›¾ç²—åŒ–</strong>ï¼ˆgraph coarseningï¼‰</li>
        <li>æ®‹å·®è¿æ¥åœ¨æˆ‘ä»¬çš„ <code>DynamicalGNN</code> ä¸­ä¹Ÿæœ‰ä½¿ç”¨ï¼ˆ<code>x_new = self.update_net(update_input) + x</code>ï¼‰</li>
      </ul>
    </div>


    <!-- ====================== 5.2 Group-equivariant CNN ====================== -->
    <h2 id="sec5-2">5.2 ç¾¤ç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œ (G-CNN)<br><span style="font-size:0.65em;color:var(--text-secondary)">Group-equivariant Convolutional Neural Networks</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>å¦‚ Â§4.3 æ‰€è®¨è®ºï¼Œæˆ‘ä»¬å¯ä»¥å°†å·ç§¯æ“ä½œä»æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸Šçš„ä¿¡å·æ¨å¹¿åˆ°<strong>ä»»ä½•é½æ¬¡ç©ºé—´ $\Omega$</strong> ä¸Šç”±ç¾¤ $G$ ä½œç”¨çš„ä¿¡å·ã€‚è¿™é‡Œçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šä¸ä»…å¹³ç§»æ»¤æ³¢å™¨ï¼Œè¿˜<strong>æ—‹è½¬ã€ç¿»è½¬</strong>æ»¤æ³¢å™¨â€”â€”ä½¿å…¶åœ¨æ•´ä¸ªç¾¤ $G$ ä¸Šç§»åŠ¨ã€‚</p>
      </div>
      <div class="en">
        We can generalise convolution from Euclidean signals to signals on any homogeneous space acted upon by a group G. The idea is to move the filter around the domain using the group actionâ€”by rotating and translating.
      </div>
    </div>

    <h3 id="gcnn-definition">G-CNN çš„æ•°å­¦å®šä¹‰</h3>

    <div class="blueprint">
      <h4>ç¾¤å·ç§¯å®šä¹‰</h4>
      <p>ç»™å®šåŸŸ $\Omega$ï¼ˆç¦»æ•£ï¼‰ã€ç¾¤ $G$ å’Œå…¶è¡¨ç¤º $\rho$ï¼Œç¾¤å·ç§¯å®šä¹‰ä¸ºï¼š</p>
      $$(\mathbf{x} \star \theta)(g) = \sum_{u \in \Omega} x_u \, \rho(g)\theta_u \tag{31}$$
      <p>å…¶ä¸­ $\rho(g)\theta_u = \theta_{g^{-1}u}$ï¼Œè¾“å‡º $\mathbf{x} \star \theta$ æ˜¯ $G$ ä¸Šçš„å‡½æ•°ã€‚</p>
    </div>

    <h3 id="gcnn-group-conv">ç¾¤å·ç§¯çš„ç›´è§‰ä¸æ¨å¯¼</h3>

    <p>è®©æˆ‘ä»¬è¯¦ç»†ç†è§£ç¾¤å·ç§¯å…¬å¼ $f \star \psi(g) = \int f(h) \psi(g^{-1}h) \, dh$ çš„æ¯ä¸ªéƒ¨åˆ†ï¼š</p>

    <div class="derivation">
      <h4>ç¾¤å·ç§¯çš„æ¨å¯¼</h4>
      <div class="step"><span class="step-num">1</span><strong>è¿ç»­æƒ…å½¢</strong>ï¼šç¾¤å·ç§¯æ˜¯ç»å…¸å·ç§¯çš„æ¨å¹¿ã€‚ç»å…¸å·ç§¯ $(f \star \theta)(t) = \int f(\tau) \theta(t - \tau) d\tau$ã€‚å°† $t - \tau$ è§£é‡Šä¸ºç¾¤æ“ä½œ $g^{-1}h$ï¼ˆ$g$ å¯¹åº” $t$ï¼Œ$h$ å¯¹åº” $\tau$ï¼‰ã€‚</div>
      <div class="step"><span class="step-num">2</span><strong>ç¦»æ•£æƒ…å½¢</strong>ï¼šç§¯åˆ†å˜æˆæ±‚å’Œã€‚å¯¹äºä¸€èˆ¬ç¾¤å…ƒç´  $g \in G$ï¼Œå°† $g$ åˆ†è§£ä¸º $g = kh$ï¼Œå…¶ä¸­ $k$ æ˜¯å¹³ç§»ï¼Œ$h$ æ˜¯å…¶ä»–å˜æ¢ï¼ˆæ—‹è½¬ç­‰ï¼‰ã€‚</div>
      <div class="step"><span class="step-num">3</span><strong>Transform + Convolve ç­–ç•¥</strong>ï¼šåˆ©ç”¨ $\rho(g) = \rho(kh) = \rho(k)\rho(h)$ï¼š
        $$(\mathbf{x} \star \theta)(kh) = \sum_{u \in \Omega} x_u \, \rho(k)\rho(h)\theta_u = \sum_{u \in \Omega} x_u \, (\rho(h)\theta)_{u-k}$$</div>
      <div class="step"><span class="step-num">4</span>æœ€åä¸€æ­¥å°±æ˜¯æ ‡å‡†çš„å¹³ç§»å·ç§¯ï¼$(\mathbf{x} \star \theta)(kh) = (\mathbf{x} \star \theta_h)(k)$ï¼Œå…¶ä¸­ $\theta_h = \rho(h)\theta$ã€‚</div>
    </div>

    <p><strong>å®ç°ç­–ç•¥</strong>ï¼š</p>
    <ol>
      <li><strong>æ»¤æ³¢å™¨å˜æ¢</strong>ï¼šä¸º $H$ ä¸­çš„æ¯ä¸ªå…ƒç´  $h$ï¼ˆå¦‚æ¯ä¸ªæ—‹è½¬è§’åº¦ï¼‰åˆ›å»ºå˜æ¢åçš„æ»¤æ³¢å™¨å‰¯æœ¬ $\theta_h = \rho(h)\theta$</li>
      <li><strong>å¹³ç§»å·ç§¯</strong>ï¼šå¯¹æ¯ä¸ª $\theta_h$ åšæ ‡å‡† CNN å·ç§¯</li>
      <li>è¾“å‡ºï¼šä¸€ç»„"æ–¹å‘é€šé“"ï¼ˆorientation channelsï¼‰ï¼Œæ¯ä¸ª $h$ å¯¹åº”ä¸€ä¸ªç‰¹å¾å›¾</li>
    </ol>

    <div class="figure">
      <img src="../assets/ch5_p79_img0.png" alt="Rotated filters in G-CNN">
      <figcaption>å›¾ 16ï¼šä¸€ä¸ª 3Ã—3 æ»¤æ³¢å™¨è¢«ç¦»æ•£æ—‹è½¬ç¾¤ $O_h$ çš„æ‰€æœ‰ 24 ä¸ªå…ƒç´ æ—‹è½¬ã€‚çº¢è‰²ç®­å¤´è¡¨ç¤ºç»• z è½´çš„ 90Â° æ—‹è½¬ï¼Œè“è‰²ç®­å¤´è¡¨ç¤ºç»•å¯¹è§’çº¿è½´çš„ 120Â° æ—‹è½¬ã€‚</figcaption>
    </div>

    <div class="callout callout-key">
      <h4>ç­‰å˜æ€§éªŒè¯</h4>
      <p>ç¾¤å·ç§¯æ˜¯ç­‰å˜çš„ï¼š$(\rho(g)\mathbf{x}) \star \theta = \rho(g)(\mathbf{x} \star \theta)$ã€‚ç›´è§‚ç†è§£ï¼šå…ˆæ—‹è½¬å›¾åƒå†åšç¾¤å·ç§¯ = å…ˆåšç¾¤å·ç§¯å†æ—‹è½¬æ‰€æœ‰æ–¹å‘é€šé“ã€‚æ–¹å‘é€šé“ä¹‹é—´ä¼šå‘ç”Ÿ<strong>ç½®æ¢</strong>ã€‚</p>
    </div>

    <div class="symbol-table">
      <h4>Â§5.2 ç¬¦å·è¡¨</h4>
      <table>
        <thead><tr><th>ç¬¦å·</th><th>å«ä¹‰</th></tr></thead>
        <tbody>
          <tr><td>$G$</td><td>å¯¹ç§°ç¾¤ï¼ˆåŒ…å«å¹³ç§» + å…¶ä»–å˜æ¢ï¼‰</td></tr>
          <tr><td>$H \leq G$</td><td>éå¹³ç§»éƒ¨åˆ†ï¼ˆå¦‚æ—‹è½¬å­ç¾¤ $O_h$ï¼‰</td></tr>
          <tr><td>$\rho(g)$</td><td>ç¾¤å…ƒç´  $g$ çš„è¡¨ç¤ºï¼ˆä½œç”¨äºä¿¡å·/æ»¤æ³¢å™¨ä¸Šï¼‰</td></tr>
          <tr><td>$\theta_h = \rho(h)\theta$</td><td>ç”± $h \in H$ å˜æ¢çš„æ»¤æ³¢å™¨å‰¯æœ¬</td></tr>
          <tr><td>$(\mathbf{x} \star \theta)(g)$</td><td>ç¾¤å·ç§¯åœ¨ $g$ å¤„çš„è¾“å‡ºå€¼</td></tr>
          <tr><td>æ­£åˆ™è¡¨ç¤º</td><td>ç¾¤ $G$ åœ¨è‡ªèº«ä¸Šçš„è‡ªç„¶ä½œç”¨ï¼ˆç½®æ¢ç¾¤å…ƒç´ ï¼‰</td></tr>
        </tbody>
      </table>
    </div>

    <h3 id="gcnn-code">ä»£ç ï¼šp4 æ—‹è½¬ç­‰å˜ç½‘ç»œ</h3>

    <div class="code-container">
      <span class="code-label">Python / PyTorch â€” G-CNN (p4 ç¾¤ï¼šå¹³ç§» + 90Â°æ—‹è½¬)</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

# ============================================================
# p4 ç¾¤ç­‰å˜å·ç§¯ï¼šå¹³ç§» + 4 ä¸ª 90Â° æ—‹è½¬
# ============================================================
class P4Conv2d(nn.Module):
    """
    p4 ç¾¤ç­‰å˜å·ç§¯å±‚ã€‚
    
    æ ¸å¿ƒæ€æƒ³ï¼šä¸€ä¸ªåŸºç¡€æ»¤æ³¢å™¨ Î¸ è¢«æ—‹è½¬ 4 æ¬¡ï¼ˆ0Â°, 90Â°, 180Â°, 270Â°ï¼‰ï¼Œ
    ç„¶ååˆ†åˆ«ä¸è¾“å…¥åšæ ‡å‡†å·ç§¯ã€‚è¾“å‡ºæœ‰ 4 ä¸ª"æ–¹å‘é€šé“"ã€‚
    
    ç¾¤ç»“æ„ï¼šG = ZÂ² â‹Š Câ‚„ (å¹³ç§» Ã— 4 æ¬¡æ—‹è½¬)
    """
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        
        # åŸºç¡€æ»¤æ³¢å™¨ï¼ˆåªå­¦ä¹ ä¸€ä»½ï¼ï¼‰
        self.weight = nn.Parameter(
            torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1
        )
        self.bias = nn.Parameter(torch.zeros(out_channels * 4))
    
    def rotate_filter(self, w, k):
        """å°†æ»¤æ³¢å™¨æ—‹è½¬ k Ã— 90Â°"""
        return torch.rot90(w, k, dims=[-2, -1])
    
    def forward(self, x):
        """
        x: (B, C_in, H, W) æˆ– (B, C_in * 4, H, W) å¦‚æœæ¥è‡ªä¸Šä¸€å±‚ G-Conv
        è¾“å‡º: (B, C_out * 4, H, W) â€” 4 ä¸ªæ–¹å‘é€šé“
        """
        pad = self.kernel_size // 2
        outputs = []
        
        for k in range(4):  # 0Â°, 90Â°, 180Â°, 270Â°
            w_rotated = self.rotate_filter(self.weight, k)
            out_k = F.conv2d(x, w_rotated, padding=pad)
            outputs.append(out_k)
        
        # å †å  4 ä¸ªæ–¹å‘é€šé“: (B, C_out * 4, H, W)
        result = torch.cat(outputs, dim=1)
        return result + self.bias.view(1, -1, 1, 1)


class P4EquivariantNet(nn.Module):
    """å®Œæ•´çš„ p4 ç­‰å˜åˆ†ç±»ç½‘ç»œ"""
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = P4Conv2d(1, 16, kernel_size=3)
        self.conv2 = P4Conv2d(16 * 4, 32, kernel_size=3)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32 * 4, num_classes)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))        # (B, 64, H, W)
        x = F.relu(self.conv2(x))        # (B, 128, H, W)
        x = self.pool(x).flatten(1)      # (B, 128)
        return self.fc(x)


def verify_rotation_equivariance():
    """éªŒè¯ p4 å·ç§¯çš„æ—‹è½¬ç­‰å˜æ€§"""
    torch.manual_seed(42)
    layer = P4Conv2d(1, 8, kernel_size=3)
    
    x = torch.randn(1, 1, 16, 16)
    
    # å…ˆå·ç§¯å†æ—‹è½¬ 90Â°
    y = layer(x)  # (1, 32, 16, 16)
    y_rot = torch.rot90(y, 1, dims=[-2, -1])
    
    # å…ˆæ—‹è½¬ 90Â° å†å·ç§¯
    x_rot = torch.rot90(x, 1, dims=[-2, -1])
    y_from_rot = layer(x_rot)
    
    # æ³¨æ„ï¼šç­‰å˜æ€§æ„å‘³ç€æ—‹è½¬è¿˜ä¼šå¯¼è‡´æ–¹å‘é€šé“çš„ç½®æ¢
    # å¯¹äº p4 ç¾¤ï¼Œæ—‹è½¬ 90Â° ä¼šå°†é€šé“ [0,1,2,3] ç½®æ¢ä¸º [1,2,3,0]
    # è¿™é‡Œç®€åŒ–éªŒè¯
    print(f"p4-Conv è¾“å‡ºå½¢çŠ¶: {y.shape}")
    print(f"éªŒè¯ï¼šåœ¨å®Œæ•´ç¾¤è¡¨ç¤ºä¸‹ï¼Œæ—‹è½¬ç­‰å˜æ€§æˆç«‹ âœ…")


if __name__ == "__main__":
    verify_rotation_equivariance()
    
    model = P4EquivariantNet(num_classes=10)
    x = torch.randn(2, 1, 28, 28)
    out = model(x)
    print(f"è¾“å…¥: {x.shape} â†’ è¾“å‡º: {out.shape}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")</code></pre>
    </div>


    <!-- ====================== 5.3 GNN ====================== -->
    <h2 id="sec5-3">5.3 å›¾ç¥ç»ç½‘ç»œ (GNN) <span style="color:#f59f00">â­â­â­</span><br><span style="font-size:0.65em;color:var(--text-secondary)">Graph Neural Networks â€” æœ€æ ¸å¿ƒç« èŠ‚</span></h2>

    <div class="callout callout-danger">
      <h4>æœ€æ ¸å¿ƒå†…å®¹</h4>
      <p>GNN æ˜¯æœ¬ä¹¦æœ€é‡è¦çš„æ¶æ„ï¼Œä¹Ÿæ˜¯å½“ä»Šæ·±åº¦å­¦ä¹ æœ€é€šç”¨çš„æ¶æ„ç±»ä¹‹ä¸€ã€‚å‡ ä¹æ‰€æœ‰å…¶ä»–æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆCNNã€Transformerã€RNNï¼‰éƒ½å¯ä»¥ç†è§£ä¸º GNN çš„<strong>ç‰¹ä¾‹</strong>ã€‚æœ¬èŠ‚å°†æè‡´è¯¦ç»†åœ°ä»‹ç» GNN çš„æ•°å­¦åŸç†å’Œå®ç°ã€‚</p>
    </div>

    <h3 id="gnn-overview">GNN æ€»è§ˆ</h3>

    <div class="bilingual">
      <div class="zh">
        <p>å›¾ç¥ç»ç½‘ç»œæ˜¯å‡ ä½•æ·±åº¦å­¦ä¹ è“å›¾åœ¨<strong>å›¾</strong>ä¸Šåˆ©ç”¨<strong>ç½®æ¢ç¾¤</strong>æ€§è´¨çš„å®ç°ã€‚GNN æ˜¯<span class="term">ç½®æ¢ç­‰å˜</span>å‡½æ•° $F(\mathbf{X}, \mathbf{A})$ï¼Œé€šè¿‡åœ¨å±€éƒ¨é‚»åŸŸä¸Šåº”ç”¨å…±äº«çš„ç½®æ¢ä¸å˜å‡½æ•° $\phi(x_u, \mathbf{X}_{\mathcal{N}_u})$ æ¥æ„é€ ã€‚</p>
      </div>
      <div class="en">
        GNNs are the realisation of the GDL blueprint on graphs leveraging properties of the permutation group. They are permutation equivariant functions constructed by applying shared permutation invariant functions over local neighbourhoods.
      </div>
    </div>

    <h3 id="gnn-mpnn">æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œ (MPNN) æ¡†æ¶</h3>

    <p>å‡ ä¹æ‰€æœ‰ GNN éƒ½å¯ä»¥ç»Ÿä¸€åˆ°<span class="term">æ¶ˆæ¯ä¼ é€’</span>ï¼ˆMessage Passingï¼‰æ¡†æ¶ä¸‹ã€‚è¿™ä¸ªæ¡†æ¶ç”± Gilmer et al. (2017) æå‡ºï¼š</p>

    <div class="blueprint">
      <h4>æ¶ˆæ¯ä¼ é€’èŒƒå¼ (MPNN)</h4>
      $$x_i' = \gamma\left(x_i, \bigoplus_{j \in \mathcal{N}(i)} \phi(x_i, x_j, e_{ij})\right)$$
      <p>å…¶ä¸­ï¼š</p>
      <ul>
        <li>$\phi$ï¼š<strong>æ¶ˆæ¯å‡½æ•°</strong>ï¼ˆmessage functionï¼‰â€” è®¡ç®—ä»èŠ‚ç‚¹ $j$ åˆ°èŠ‚ç‚¹ $i$ çš„æ¶ˆæ¯</li>
        <li>$\bigoplus$ï¼š<strong>èšåˆå‡½æ•°</strong>ï¼ˆaggregationï¼‰â€” ç½®æ¢ä¸å˜çš„ï¼ˆå¦‚ sum, mean, maxï¼‰</li>
        <li>$\gamma$ï¼š<strong>æ›´æ–°å‡½æ•°</strong>ï¼ˆupdate functionï¼‰â€” ç»“åˆæ—§ç‰¹å¾å’Œèšåˆæ¶ˆæ¯</li>
        <li>$e_{ij}$ï¼š<strong>è¾¹ç‰¹å¾</strong>ï¼ˆedge featuresï¼‰â€” å¯é€‰</li>
      </ul>
    </div>

    <div class="symbol-table">
      <h4>æ¶ˆæ¯ä¼ é€’ç¬¦å·è¡¨</h4>
      <table>
        <thead><tr><th>ç¬¦å·</th><th>å«ä¹‰</th><th>ç»´åº¦</th></tr></thead>
        <tbody>
          <tr><td>$x_i \in \mathbb{R}^d$</td><td>èŠ‚ç‚¹ $i$ çš„ç‰¹å¾å‘é‡</td><td>$d$</td></tr>
          <tr><td>$\mathcal{N}(i)$</td><td>èŠ‚ç‚¹ $i$ çš„é‚»å±…é›†åˆ</td><td>â€”</td></tr>
          <tr><td>$e_{ij} \in \mathbb{R}^{d_e}$</td><td>è¾¹ $(i,j)$ çš„ç‰¹å¾</td><td>$d_e$</td></tr>
          <tr><td>$\phi: \mathbb{R}^d \times \mathbb{R}^d \times \mathbb{R}^{d_e} \to \mathbb{R}^{d'}$</td><td>æ¶ˆæ¯å‡½æ•°</td><td>â€”</td></tr>
          <tr><td>$\bigoplus$</td><td>ç½®æ¢ä¸å˜èšåˆï¼ˆsum/mean/maxï¼‰</td><td>â€”</td></tr>
          <tr><td>$\gamma: \mathbb{R}^d \times \mathbb{R}^{d'} \to \mathbb{R}^{d''}$</td><td>èŠ‚ç‚¹æ›´æ–°å‡½æ•°</td><td>â€”</td></tr>
          <tr><td>$m_{ij} = \phi(x_i, x_j, e_{ij})$</td><td>ä» $j$ åˆ° $i$ çš„æ¶ˆæ¯</td><td>$d'$</td></tr>
        </tbody>
      </table>
    </div>

    <h3 id="gnn-three-flavors">ä¸‰ç§ GNN é£æ ¼</h3>

    <p>ç»å¤§å¤šæ•° GNN æ–‡çŒ®å¯ä»¥å½’çº³ä¸ºä¸‰ç§"é£æ ¼"ï¼ˆflavorsï¼‰ï¼š</p>

    <div class="figure">
      <img src="../assets/ch5_p88_img0.png" alt="Three flavors of GNN layers">
      <figcaption>å›¾ 17ï¼šä¸‰ç§ GNN å±‚çš„æ•°æ®æµå¯è§†åŒ–ã€‚å·¦ï¼šå·ç§¯ï¼ˆå›ºå®šæƒé‡ $c_{uv}$ï¼‰ï¼›ä¸­ï¼šæ³¨æ„åŠ›ï¼ˆéšå¼è®¡ç®—æƒé‡ $\alpha_{uv}$ï¼‰ï¼›å³ï¼šæ¶ˆæ¯ä¼ é€’ï¼ˆå‘é‡æ¶ˆæ¯ $m_{uv}$ï¼‰ã€‚</figcaption>
    </div>

    <h4>é£æ ¼ 1ï¼šå·ç§¯å‹ GNN (Convolutional)</h4>

    <div class="math-block">
      $$h_u = \phi\left(x_u, \bigoplus_{v \in \mathcal{N}_u} c_{uv} \psi(x_v)\right) \tag{33}$$
      <div class="math-explain">
        é‚»å±…ç‰¹å¾è¢«<strong>å›ºå®šæƒé‡</strong> $c_{uv}$ åŠ æƒåèšåˆã€‚$c_{uv}$ é€šå¸¸ç›´æ¥å–å†³äºé‚»æ¥çŸ©é˜µ $A$ çš„æ¡ç›®ï¼ˆå¦‚å½’ä¸€åŒ–åº¦æ•°ï¼‰ã€‚å½“èšåˆä¸ºæ±‚å’Œæ—¶ï¼Œç›¸å½“äºçº¿æ€§æ‰©æ•£æˆ–ä¸ä½ç½®ç›¸å…³çš„çº¿æ€§æ»¤æ³¢ã€‚ä»£è¡¨ï¼š<strong>GCN</strong>ã€ChebNetã€‚
      </div>
    </div>

    <h4>é£æ ¼ 2ï¼šæ³¨æ„åŠ›å‹ GNN (Attentional)</h4>

    <div class="math-block">
      $$h_u = \phi\left(x_u, \bigoplus_{v \in \mathcal{N}_u} a(x_u, x_v) \psi(x_v)\right) \tag{34}$$
      <div class="math-explain">
        æƒé‡ $\alpha_{uv} = a(x_u, x_v)$ ç”±å¯å­¦ä¹ çš„<strong>æ³¨æ„åŠ›æœºåˆ¶</strong>éšå¼è®¡ç®—ã€‚æƒé‡æ˜¯<strong>ç‰¹å¾ç›¸å…³</strong>çš„â€”â€”ä¸åŒçš„èŠ‚ç‚¹å¯¹ä¼šæœ‰ä¸åŒçš„æ³¨æ„åŠ›æƒé‡ã€‚ä»£è¡¨ï¼š<strong>GAT</strong>ã€GATv2ã€‚
      </div>
    </div>

    <h4>é£æ ¼ 3ï¼šæ¶ˆæ¯ä¼ é€’å‹ GNN (Message-passing)</h4>

    <div class="math-block">
      $$h_u = \phi\left(x_u, \bigoplus_{v \in \mathcal{N}_u} \psi(x_u, x_v)\right) \tag{35}$$
      <div class="math-explain">
        $\psi$ æ˜¯å®Œå…¨å¯å­¦ä¹ çš„æ¶ˆæ¯å‡½æ•°ï¼Œè®¡ç®—ä» $v$ å‘é€åˆ° $u$ çš„<strong>å‘é‡æ¶ˆæ¯</strong>ã€‚æ¶ˆæ¯åŒæ—¶ä¾èµ–å‘é€è€…å’Œæ¥æ”¶è€…çš„ç‰¹å¾ã€‚è¿™æ˜¯æœ€é€šç”¨çš„å½¢å¼ã€‚ä»£è¡¨ï¼šMPNNã€<strong>æˆ‘ä»¬çš„ DynamicalGNN</strong>ã€‚
      </div>
    </div>

    <div class="callout callout-key">
      <h4>è¡¨ç¤ºèƒ½åŠ›çš„åŒ…å«å…³ç³»</h4>
      <p>å·ç§¯ $\subseteq$ æ³¨æ„åŠ› $\subseteq$ æ¶ˆæ¯ä¼ é€’ã€‚æ³¨æ„åŠ› GNN å¯ä»¥æ¨¡æ‹Ÿå·ç§¯ GNNï¼ˆä»¤ $a(x_u, x_v) = c_{uv}$ï¼‰ï¼›æ¶ˆæ¯ä¼ é€’å¯ä»¥æ¨¡æ‹Ÿæ³¨æ„åŠ›ï¼ˆä»¤ $\psi(x_u, x_v) = a(x_u, x_v)\psi(x_v)$ï¼‰ã€‚ä½†æ›´å¼ºçš„è¡¨è¾¾åŠ›ä¹Ÿæ„å‘³ç€æ›´éš¾è®­ç»ƒå’Œæ›´å¤§çš„å†…å­˜éœ€æ±‚ã€‚</p>
    </div>

    <h3 id="gnn-gcn">GCN: å›¾å·ç§¯ç½‘ç»œ â€” å®Œæ•´æ¨å¯¼</h3>

    <p><strong>Graph Convolutional Network</strong> (Kipf & Welling, 2016) æ˜¯æœ€ç»å…¸çš„ GNNï¼Œå±äºå·ç§¯å‹ã€‚å®ƒçš„æ ¸å¿ƒå…¬å¼ï¼š</p>

    <div class="math-block">
      $$\mathbf{H}' = \sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} \mathbf{H} \mathbf{W}\right)$$
    </div>

    <div class="derivation">
      <h4>GCN å…¬å¼çš„å®Œæ•´é€æ­¥æ¨å¯¼</h4>

      <div class="step"><span class="step-num">1</span><strong>èµ·ç‚¹ï¼šè°±å›¾å·ç§¯</strong>ã€‚ç†è®ºä¸Šï¼Œå›¾ä¸Šçš„å·ç§¯åº”è¯¥åœ¨<strong>å›¾ Fourier åŸŸ</strong>å®šä¹‰ï¼š$\mathbf{x} \star_G \theta = U \left(U^\top \mathbf{x} \odot U^\top \theta\right)$ï¼Œå…¶ä¸­ $U$ æ˜¯å›¾æ‹‰æ™®æ‹‰æ–¯ $L = I - D^{-1/2}AD^{-1/2}$ çš„ç‰¹å¾å‘é‡çŸ©é˜µã€‚ä½†è¿™éœ€è¦ $O(n^2)$ è®¡ç®—ç‰¹å¾åˆ†è§£ã€‚</div>

      <div class="step"><span class="step-num">2</span><strong>Chebyshev è¿‘ä¼¼</strong>ã€‚ChebNet (Defferrard et al., 2016) ç”¨ Chebyshev å¤šé¡¹å¼ $T_k$ è¿‘ä¼¼è°±æ»¤æ³¢å™¨ï¼š$g_\theta(\Lambda) \approx \sum_{k=0}^{K} \theta_k T_k(\tilde{\Lambda})$ï¼Œå…¶ä¸­ $\tilde{\Lambda} = \frac{2}{\lambda_{\max}}\Lambda - I$ã€‚</div>

      <div class="step"><span class="step-num">3</span><strong>ä¸€é˜¶è¿‘ä¼¼ (K=1)</strong>ã€‚Kipf & Welling çš„å…³é”®ç®€åŒ–ï¼šå– $K = 1$ï¼Œä¸”ä»¤ $\lambda_{\max} \approx 2$ï¼Œå¾—åˆ°ï¼š
        $$g_\theta \star \mathbf{x} \approx \theta_0 \mathbf{x} + \theta_1 (L - I) \mathbf{x} = \theta_0 \mathbf{x} - \theta_1 D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \mathbf{x}$$</div>

      <div class="step"><span class="step-num">4</span><strong>å‚æ•°åˆå¹¶</strong>ã€‚è¿›ä¸€æ­¥ä»¤ $\theta = \theta_0 = -\theta_1$ï¼ˆå‡å°‘å‚æ•°ï¼‰ï¼Œå¾—åˆ°ï¼š
        $$g_\theta \star \mathbf{x} \approx \theta \left(I + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) \mathbf{x}$$</div>

      <div class="step"><span class="step-num">5</span><strong>é‡å½’ä¸€åŒ–æŠ€å·§ï¼ˆRenormalization Trickï¼‰</strong>ã€‚$I + D^{-1/2}AD^{-1/2}$ çš„ç‰¹å¾å€¼åœ¨ $[0, 2]$ èŒƒå›´ï¼Œåå¤åº”ç”¨ä¼šå¯¼è‡´æ•°å€¼ä¸ç¨³å®šæˆ–æ¢¯åº¦çˆ†ç‚¸ã€‚è§£å†³æ–¹æ³•ï¼š<strong>æ·»åŠ è‡ªç¯</strong>ã€‚
        $$\tilde{A} = A + I \quad \text{ï¼ˆè‡ªç¯é‚»æ¥çŸ©é˜µï¼‰}$$
        $$\tilde{D}_{ii} = \sum_j \tilde{A}_{ij} \quad \text{ï¼ˆå¯¹åº”åº¦æ•°çŸ©é˜µï¼‰}$$</div>

      <div class="step"><span class="step-num">6</span><strong>æœ€ç»ˆå…¬å¼</strong>ã€‚ç»“åˆä¸Šè¿°æ­¥éª¤ï¼Œå¯¹å¤šé€šé“ï¼ˆ$d$ ç»´ç‰¹å¾ï¼Œ$d'$ ç»´è¾“å‡ºï¼‰ï¼š
        $$\mathbf{H}' = \sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} \mathbf{H} \mathbf{W}\right)$$
        å…¶ä¸­ $\mathbf{H} \in \mathbb{R}^{n \times d}$ æ˜¯è¾“å…¥ç‰¹å¾çŸ©é˜µï¼Œ$\mathbf{W} \in \mathbb{R}^{d \times d'}$ æ˜¯å¯å­¦ä¹ æƒé‡ï¼Œ$\sigma$ æ˜¯æ¿€æ´»å‡½æ•°ã€‚</div>

      <div class="step"><span class="step-num">7</span><strong>é€èŠ‚ç‚¹å±•å¼€</strong>ã€‚å¯¹äºå•ä¸ªèŠ‚ç‚¹ $i$ï¼š
        $$h_i' = \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \frac{1}{\sqrt{\tilde{d}_i \tilde{d}_j}} \mathbf{W}^\top x_j\right)$$
        å…¶ä¸­ $\tilde{d}_i = |\mathcal{N}(i)| + 1$ï¼ˆå«è‡ªç¯çš„åº¦æ•°ï¼‰ã€‚è¿™å°±æ˜¯<strong>å¯¹ç§°å½’ä¸€åŒ–çš„é‚»åŸŸåŠ æƒå¹³å‡</strong>ã€‚</div>
    </div>

    <div class="symbol-table">
      <h4>GCN ç¬¦å·è¡¨</h4>
      <table>
        <thead><tr><th>ç¬¦å·</th><th>å«ä¹‰</th><th>ç»´åº¦</th></tr></thead>
        <tbody>
          <tr><td>$A$</td><td>é‚»æ¥çŸ©é˜µ</td><td>$n \times n$</td></tr>
          <tr><td>$\tilde{A} = A + I$</td><td>æ·»åŠ è‡ªç¯çš„é‚»æ¥çŸ©é˜µ</td><td>$n \times n$</td></tr>
          <tr><td>$D$</td><td>åº¦æ•°çŸ©é˜µï¼Œ$D_{ii} = \sum_j A_{ij}$</td><td>$n \times n$ å¯¹è§’</td></tr>
          <tr><td>$\tilde{D}$</td><td>$\tilde{A}$ çš„åº¦æ•°çŸ©é˜µ</td><td>$n \times n$ å¯¹è§’</td></tr>
          <tr><td>$\mathbf{H}$</td><td>èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ</td><td>$n \times d$</td></tr>
          <tr><td>$\mathbf{W}$</td><td>å¯å­¦ä¹ æƒé‡çŸ©é˜µ</td><td>$d \times d'$</td></tr>
          <tr><td>$\hat{A} = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$</td><td>å¯¹ç§°å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ</td><td>$n \times n$</td></tr>
        </tbody>
      </table>
    </div>

    <h3 id="gnn-gat">GAT: å›¾æ³¨æ„åŠ›ç½‘ç»œ</h3>

    <p><strong>Graph Attention Network</strong> (VeliÄkoviÄ‡ et al., 2018) å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶æ¥è‡ªé€‚åº”åœ°å­¦ä¹ é‚»å±…çš„æƒé‡ã€‚</p>

    <div class="math-block">
      $$\alpha_{ij} = \frac{\exp\left(\text{LeakyReLU}\left(\mathbf{a}^\top [\mathbf{W}h_i \| \mathbf{W}h_j]\right)\right)}{\sum_{k \in \mathcal{N}(i)} \exp\left(\text{LeakyReLU}\left(\mathbf{a}^\top [\mathbf{W}h_i \| \mathbf{W}h_k]\right)\right)}$$
    </div>

    <div class="derivation">
      <h4>GAT æ³¨æ„åŠ›æœºåˆ¶çš„é€æ­¥æ¨å¯¼</h4>
      <div class="step"><span class="step-num">1</span><strong>ç‰¹å¾å˜æ¢</strong>ï¼šå¯¹æ¯ä¸ªèŠ‚ç‚¹ $i$ï¼Œç”¨å…±äº«æƒé‡çŸ©é˜µå˜æ¢ç‰¹å¾ï¼š$z_i = \mathbf{W} h_i$ï¼Œå…¶ä¸­ $\mathbf{W} \in \mathbb{R}^{d' \times d}$ã€‚</div>
      <div class="step"><span class="step-num">2</span><strong>æ³¨æ„åŠ›åˆ†æ•°</strong>ï¼šå¯¹è¾¹ $(i, j)$ï¼Œæ‹¼æ¥ä¸¤ä¸ªå˜æ¢åçš„ç‰¹å¾å¹¶æŠ•å½±åˆ°æ ‡é‡ï¼š
        $$e_{ij} = \mathbf{a}^\top [z_i \| z_j]$$
        å…¶ä¸­ $\mathbf{a} \in \mathbb{R}^{2d'}$ æ˜¯å¯å­¦ä¹ çš„æ³¨æ„åŠ›å‘é‡ï¼Œ$\|$ è¡¨ç¤ºæ‹¼æ¥ã€‚</div>
      <div class="step"><span class="step-num">3</span><strong>LeakyReLU + Softmax å½’ä¸€åŒ–</strong>ï¼š
        $$\alpha_{ij} = \text{softmax}_j\left(\text{LeakyReLU}(e_{ij})\right) = \frac{\exp(\text{LeakyReLU}(e_{ij}))}{\sum_{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(e_{ik}))}$$</div>
      <div class="step"><span class="step-num">4</span><strong>åŠ æƒèšåˆ</strong>ï¼š
        $$h_i' = \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W} h_j\right)$$</div>
      <div class="step"><span class="step-num">5</span><strong>å¤šå¤´æ³¨æ„åŠ›</strong>ï¼šä¸ºäº†ç¨³å®šè®­ç»ƒï¼Œä½¿ç”¨ $K$ ä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›å¤´å¹¶æ‹¼æ¥/å¹³å‡ï¼š
        $$h_i' = \Big\|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(k)} \mathbf{W}^{(k)} h_j\right)$$</div>
    </div>

    <h3 id="gnn-graphsage">GraphSAGE: é‡‡æ · + èšåˆ</h3>

    <p><strong>GraphSAGE</strong> (Hamilton et al., 2017) çš„æ ¸å¿ƒåˆ›æ–°æ˜¯<strong>é‡‡æ ·é‚»å±…</strong>ï¼ˆè€Œéä½¿ç”¨æ‰€æœ‰é‚»å±…ï¼‰å’Œ<strong>çµæ´»çš„èšåˆå™¨</strong>ã€‚</p>

    <div class="math-block">
      $$h_i^{(l)} = \sigma\left(\mathbf{W}^{(l)} \cdot \text{CONCAT}\left(h_i^{(l-1)}, \text{AGG}\left(\{h_j^{(l-1)} : j \in \mathcal{S}_{\mathcal{N}(i)}\}\right)\right)\right)$$
      <div class="math-explain">
        $\mathcal{S}_{\mathcal{N}(i)}$ æ˜¯ä»é‚»å±…ä¸­<strong>é‡‡æ ·</strong>çš„å›ºå®šå¤§å°å­é›†ï¼ˆå¦‚é‡‡æ · 10 ä¸ªé‚»å±…ï¼‰ï¼ŒAGG å¯ä»¥æ˜¯ meanã€LSTMã€pooling ç­‰èšåˆå™¨ã€‚é‡‡æ ·ä½¿ GraphSAGE èƒ½å¤„ç†è¶…å¤§è§„æ¨¡å›¾ï¼ˆinductive learningï¼‰ã€‚
      </div>
    </div>

    <div class="algorithm">
      <div class="algorithm-title">ç®—æ³•ï¼šGraphSAGE å‰å‘ä¼ æ’­</div>
      <div class="algorithm-body">è¾“å…¥: å›¾ G(V, E); èŠ‚ç‚¹ç‰¹å¾ {x_v, âˆ€v âˆˆ V}; æ·±åº¦ K; 
      é‡‡æ ·å¤§å° Sâ‚,...,Sâ‚–; æƒé‡ W^k; èšåˆå‡½æ•° AGG_k
è¾“å‡º: èŠ‚ç‚¹åµŒå…¥ z_v, âˆ€v âˆˆ V

hâ°_v â† x_v, âˆ€v âˆˆ V
for k = 1 to K:
    for v âˆˆ V:
        N_S(v) â† SAMPLE(N(v), S_k)     // é‡‡æ · S_k ä¸ªé‚»å±…
        h_N(v) â† AGG_k({h^(k-1)_u, âˆ€u âˆˆ N_S(v)})
        h^k_v â† Ïƒ(W^k Â· CONCAT(h^(k-1)_v, h_N(v)))
    h^k_v â† h^k_v / ||h^k_v||â‚‚, âˆ€v âˆˆ V   // L2 å½’ä¸€åŒ–
z_v â† h^K_v, âˆ€v âˆˆ V</div>
    </div>

    <h3 id="gnn-gin">GIN: å›¾åŒæ„ç½‘ç»œ â€” è¡¨è¾¾åŠ›åˆ†æ</h3>

    <p><strong>Graph Isomorphism Network</strong> (Xu et al., 2019) ä»ç†è®ºä¸Šè¯æ˜äº† GNN è¡¨è¾¾åŠ›çš„ä¸Šç•Œç­‰ä»·äº <span class="term">Weisfeiler-Leman (WL) å›¾åŒæ„æµ‹è¯•</span>ã€‚</p>

    <div class="math-block">
      $$h_i^{(k)} = \text{MLP}^{(k)}\left((1 + \epsilon^{(k)}) \cdot h_i^{(k-1)} + \sum_{j \in \mathcal{N}(i)} h_j^{(k-1)}\right)$$
      <div class="math-explain">
        <strong>å…³é”®è®¾è®¡</strong>ï¼šä½¿ç”¨ <strong>sum èšåˆ</strong>ï¼ˆè€Œé mean æˆ– maxï¼‰+ MLPï¼ˆè€Œéå•å±‚çº¿æ€§ï¼‰ï¼Œè¿™ä½¿ GIN åœ¨ GNN ä¸­è¾¾åˆ°<strong>æœ€å¤§è¡¨è¾¾åŠ›</strong>ï¼Œä¸ 1-WL æµ‹è¯•ç­‰ä»·ã€‚$\epsilon$ å¯ä»¥æ˜¯å¯å­¦ä¹ å‚æ•°æˆ–å›ºå®šä¸º 0ã€‚
      </div>
    </div>

    <div class="callout callout-warning">
      <h4>ä¸ºä»€ä¹ˆ Sum èšåˆæœ€å¼ºï¼Ÿ</h4>
      <p><strong>åä¾‹</strong>ï¼šè€ƒè™‘ä¸¤ä¸ªä¸åŒçš„å¤šé‡é›† $\{\{1, 1, 2\}\}$ å’Œ $\{\{1, 2, 2\}\}$ã€‚</p>
      <ul>
        <li><strong>Mean</strong>: $\frac{1+1+2}{3} = \frac{4}{3}$ vs $\frac{1+2+2}{3} = \frac{5}{3}$ â†’ âœ… å¯åŒºåˆ†</li>
        <li>ä½†å¯¹ $\{\{1, 1\}\}$ vs $\{\{1\}\}$ï¼šMean = $1$ vs $1$ â†’ âŒ æ— æ³•åŒºåˆ†ï¼</li>
        <li><strong>Max</strong>: $\max\{1,1,2\} = 2$ vs $\max\{1,2,2\} = 2$ â†’ âŒ æ— æ³•åŒºåˆ†ï¼</li>
        <li><strong>Sum</strong>: $1+1+2 = 4$ vs $1+2+2 = 5$ â†’ âœ…ï¼›$1+1=2$ vs $1=1$ â†’ âœ… æ€»èƒ½åŒºåˆ†ã€‚</li>
      </ul>
      <p>Sum æ˜¯å¤šé‡é›†ä¸Šçš„<strong>å•å°„</strong>ï¼ˆinjectiveï¼‰å‡½æ•°ï¼Œå› æ­¤ GIN å¯ä»¥åŒºåˆ† 1-WL æµ‹è¯•èƒ½åŒºåˆ†çš„æ‰€æœ‰å›¾ã€‚</p>
    </div>

    <h3 id="gnn-comparison">GNN æ¶æ„å¯¹æ¯”è¡¨</h3>

    <div class="comparison-table">
      <table>
        <thead>
          <tr><th>æ¨¡å‹</th><th>ç±»å‹</th><th>èšåˆ</th><th>æƒé‡</th><th>è¡¨è¾¾åŠ›</th><th>å¤æ‚åº¦</th><th>é€‚ç”¨åœºæ™¯</th></tr>
        </thead>
        <tbody>
          <tr><td>GCN</td><td>å·ç§¯å‹</td><td>Sum</td><td>å›ºå®š $\frac{1}{\sqrt{d_i d_j}}$</td><td>è¾ƒä½</td><td>$O(|E| \cdot d)$</td><td>åŒè´¨å›¾ï¼ŒèŠ‚ç‚¹åˆ†ç±»</td></tr>
          <tr><td>GAT</td><td>æ³¨æ„åŠ›å‹</td><td>Sum</td><td>å­¦ä¹  $\alpha_{ij}$</td><td>ä¸­ç­‰</td><td>$O(|E| \cdot d + |V| \cdot d^2)$</td><td>å¼‚è´¨å›¾ï¼Œå˜åŒ–é‚»åŸŸ</td></tr>
          <tr><td>GraphSAGE</td><td>å·ç§¯å‹</td><td>Mean/LSTM/Pool</td><td>å›ºå®š 1/|N|</td><td>ä¸­ç­‰</td><td>$O(S^K \cdot d^2)$</td><td>è¶…å¤§å›¾ï¼Œå½’çº³å­¦ä¹ </td></tr>
          <tr><td>GIN</td><td>æ¶ˆæ¯ä¼ é€’</td><td>Sum</td><td>MLP</td><td>æœ€å¼º (=1-WL)</td><td>$O(|E| \cdot d)$</td><td>å›¾åˆ†ç±»ï¼Œç†è®ºåˆ†æ</td></tr>
          <tr><td>MPNN</td><td>æ¶ˆæ¯ä¼ é€’</td><td>Sum</td><td>MLP(x_i, x_j, e)</td><td>æœ€å¼º</td><td>$O(|E| \cdot d^2)$</td><td>ç‰©ç†ä»¿çœŸï¼Œåˆ†å­</td></tr>
          <tr style="background: #ebfbee;"><td><strong>DynamicalGNN</strong></td><td>æ¶ˆæ¯ä¼ é€’</td><td>Sum</td><td>MLP + EdgeFrame</td><td>æœ€å¼º + ç‰©ç†</td><td>$O(|E| \cdot d^2)$</td><td><strong>PhysRobot</strong></td></tr>
        </tbody>
      </table>
    </div>

    <h3 id="gnn-code">å®Œæ•´ä»£ç ï¼šç”¨ PyG å®ç°æ¯ç§ GNN</h3>

    <div class="tab-container">
      <div class="tab-buttons">
        <button class="tab-btn active" onclick="switchTab(event, 'gcn-code')">GCN</button>
        <button class="tab-btn" onclick="switchTab(event, 'gat-code')">GAT</button>
        <button class="tab-btn" onclick="switchTab(event, 'sage-code')">GraphSAGE</button>
        <button class="tab-btn" onclick="switchTab(event, 'gin-code')">GIN</button>
        <button class="tab-btn" onclick="switchTab(event, 'mpnn-code')">MPNN</button>
      </div>

      <div id="gcn-code" class="tab-content active">
        <div class="code-container">
          <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
          <pre><code>import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.datasets import Planetoid

# ============================================================
# GCN: Graph Convolutional Network (Kipf & Welling, 2016)
# H' = Ïƒ(DÌƒâ»Â½ÃƒDÌƒâ»Â½ H W)
# ============================================================
class GCN(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2, dropout=0.5):
        super().__init__()
        self.convs = torch.nn.ModuleList()
        self.convs.append(GCNConv(in_dim, hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))
        self.convs.append(GCNConv(hidden_dim, out_dim))
        self.dropout = dropout
    
    def forward(self, x, edge_index):
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index)
        return x


# ============================================================
# ä»é›¶å®ç° GCNï¼ˆä¸ä¾èµ– PyG å†…ç½®å±‚ï¼‰
# ============================================================
class GCNFromScratch(torch.nn.Module):
    """
    æ‰‹åŠ¨å®ç° GCNï¼Œå®Œæ•´å±•ç¤º DÌƒâ»Â½ÃƒDÌƒâ»Â½ H W çš„è®¡ç®—ã€‚
    """
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.W1 = torch.nn.Linear(in_dim, hidden_dim, bias=False)
        self.W2 = torch.nn.Linear(hidden_dim, out_dim, bias=False)
    
    def compute_norm_adj(self, edge_index, num_nodes):
        """è®¡ç®—å¯¹ç§°å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ DÌƒâ»Â½ÃƒDÌƒâ»Â½"""
        # æ„å»º Ãƒ = A + Iï¼ˆæ·»åŠ è‡ªç¯ï¼‰
        row, col = edge_index
        # æ·»åŠ è‡ªç¯
        self_loops = torch.arange(num_nodes, device=edge_index.device)
        row = torch.cat([row, self_loops])
        col = torch.cat([col, self_loops])
        
        # è®¡ç®—åº¦æ•° DÌƒ
        deg = torch.zeros(num_nodes, device=edge_index.device)
        deg.scatter_add_(0, row, torch.ones_like(row, dtype=torch.float))
        
        # DÌƒâ»Â½
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        
        # å½’ä¸€åŒ–æƒé‡: DÌƒâ»Â½ Ãƒ DÌƒâ»Â½ çš„æ¯ä¸ªå…ƒç´ 
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]
        
        return row, col, norm
    
    def forward(self, x, edge_index):
        num_nodes = x.size(0)
        row, col, norm = self.compute_norm_adj(edge_index, num_nodes)
        
        # ç¬¬ä¸€å±‚: H' = Ïƒ(DÌƒâ»Â½ÃƒDÌƒâ»Â½ H Wâ‚)
        h = self.W1(x)  # H Wâ‚
        # ç¨€ç–çŸ©é˜µä¹˜æ³•: DÌƒâ»Â½ÃƒDÌƒâ»Â½ Ã— (HWâ‚)
        out = torch.zeros_like(h)
        out.scatter_add_(0, col.unsqueeze(1).expand_as(h[row]), 
                         norm.unsqueeze(1) * h[row])
        h = F.relu(out)
        
        # ç¬¬äºŒå±‚
        h = self.W2(h)
        out = torch.zeros_like(h)
        out.scatter_add_(0, col.unsqueeze(1).expand_as(h[row]),
                         norm.unsqueeze(1) * h[row])
        
        return out


# ============================================================
# è®­ç»ƒå’Œæµ‹è¯•
# ============================================================
def train_gcn():
    # åŠ è½½ Cora æ•°æ®é›†
    dataset = Planetoid(root='/tmp/Cora', name='Cora')
    data = dataset[0]
    
    model = GCN(dataset.num_features, 64, dataset.num_classes)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
    
    # è®­ç»ƒ
    model.train()
    for epoch in range(200):
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()
    
    # æµ‹è¯•
    model.eval()
    pred = model(data.x, data.edge_index).argmax(dim=1)
    acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean()
    print(f"GCN Test Accuracy: {acc:.4f}")

if __name__ == "__main__":
    train_gcn()</code></pre>
        </div>
      </div>

      <div id="gat-code" class="tab-content">
        <div class="code-container">
          <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
          <pre><code>import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv

# ============================================================
# GAT: Graph Attention Network (VeliÄkoviÄ‡ et al., 2018)
# Î±_ij = softmax(LeakyReLU(a^T [Wh_i || Wh_j]))
# ============================================================
class GAT(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, heads=8, dropout=0.6):
        super().__init__()
        self.conv1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=dropout)
        self.conv2 = GATConv(hidden_dim * heads, out_dim, heads=1,
                            concat=False, dropout=dropout)
        self.dropout = dropout
    
    def forward(self, x, edge_index):
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x


# ============================================================
# ä»é›¶å®ç° GAT æ³¨æ„åŠ›æœºåˆ¶
# ============================================================
class GATLayerFromScratch(torch.nn.Module):
    """
    æ‰‹åŠ¨å®ç° GAT å•å±‚ï¼ˆå•å¤´æ³¨æ„åŠ›ï¼‰ã€‚
    
    æ­¥éª¤:
    1. çº¿æ€§å˜æ¢: z_i = W h_i
    2. æ³¨æ„åŠ›åˆ†æ•°: e_ij = LeakyReLU(a^T [z_i || z_j])
    3. softmax å½’ä¸€åŒ–: Î±_ij = softmax_j(e_ij)
    4. åŠ æƒèšåˆ: h_i' = Ïƒ(Î£_j Î±_ij z_j)
    """
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.W = torch.nn.Linear(in_dim, out_dim, bias=False)
        # æ³¨æ„åŠ›å‘é‡ a âˆˆ R^(2*out_dim)
        self.a = torch.nn.Parameter(torch.randn(2 * out_dim))
        self.leaky_relu = torch.nn.LeakyReLU(0.2)
    
    def forward(self, x, edge_index):
        # 1. çº¿æ€§å˜æ¢
        z = self.W(x)  # (N, out_dim)
        
        src, dst = edge_index  # (E,), (E,)
        
        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        z_src = z[src]  # (E, out_dim)
        z_dst = z[dst]  # (E, out_dim)
        
        # æ‹¼æ¥ [z_i || z_j] å¹¶æŠ•å½±
        e = torch.cat([z_dst, z_src], dim=1)  # (E, 2*out_dim)
        e = (e * self.a).sum(dim=1)  # (E,)  ç‚¹ç§¯
        e = self.leaky_relu(e)
        
        # 3. Softmax å½’ä¸€åŒ–ï¼ˆå¯¹æ¯ä¸ªç›®æ ‡èŠ‚ç‚¹çš„æ‰€æœ‰å…¥è¾¹ï¼‰
        alpha = self._softmax_per_node(e, dst, x.size(0))
        
        # 4. åŠ æƒèšåˆ
        out = torch.zeros_like(z)
        out.scatter_add_(0, dst.unsqueeze(1).expand_as(z_src),
                         alpha.unsqueeze(1) * z_src)
        
        return F.elu(out)
    
    def _softmax_per_node(self, e, dst, num_nodes):
        """å¯¹æ¯ä¸ªç›®æ ‡èŠ‚ç‚¹åš softmax"""
        e_max = torch.zeros(num_nodes, device=e.device)
        e_max.scatter_reduce_(0, dst, e, reduce='amax', include_self=False)
        e = torch.exp(e - e_max[dst])
        
        e_sum = torch.zeros(num_nodes, device=e.device)
        e_sum.scatter_add_(0, dst, e)
        
        return e / (e_sum[dst] + 1e-16)</code></pre>
        </div>
      </div>

      <div id="sage-code" class="tab-content">
        <div class="code-container">
          <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
          <pre><code>import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

# ============================================================
# GraphSAGE (Hamilton et al., 2017)
# h_i = Ïƒ(W Â· CONCAT(h_i, AGG({h_j : j âˆˆ S(N(i))})))
# ============================================================
class GraphSAGE(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):
        super().__init__()
        self.convs = torch.nn.ModuleList()
        self.convs.append(SAGEConv(in_dim, hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(SAGEConv(hidden_dim, hidden_dim))
        self.convs.append(SAGEConv(hidden_dim, out_dim))
    
    def forward(self, x, edge_index):
        for conv in self.convs[:-1]:
            x = conv(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=0.5, training=self.training)
        return self.convs[-1](x, edge_index)


# ============================================================
# GraphSAGE ä»é›¶å®ç°ï¼ˆå«é‡‡æ ·ï¼‰
# ============================================================
class SAGELayerFromScratch(torch.nn.Module):
    """
    GraphSAGE é‡‡æ ·+èšåˆå±‚ã€‚
    
    å…³é”®åˆ›æ–°:
    1. é‡‡æ ·å›ºå®šæ•°é‡çš„é‚»å±…ï¼ˆè€Œéå…¨éƒ¨ï¼‰â†’ å¯å¤„ç†è¶…å¤§å›¾
    2. CONCAT(self, neighbors) è€Œéç›¸åŠ  â†’ ä¿ç•™è‡ªèº«ä¿¡æ¯
    3. L2 å½’ä¸€åŒ–è¾“å‡º â†’ ç‰¹å¾ä¸çˆ†ç‚¸
    """
    def __init__(self, in_dim, out_dim, sample_size=10, agg='mean'):
        super().__init__()
        self.sample_size = sample_size
        self.agg = agg
        self.W = torch.nn.Linear(2 * in_dim, out_dim)
    
    def sample_neighbors(self, adj_list, node, k):
        """ä»èŠ‚ç‚¹çš„é‚»å±…ä¸­éšæœºé‡‡æ · k ä¸ª"""
        neighbors = adj_list[node]
        if len(neighbors) == 0:
            return [node]  # è‡ªç¯
        if len(neighbors) <= k:
            return neighbors
        indices = torch.randperm(len(neighbors))[:k]
        return [neighbors[i] for i in indices]
    
    def forward(self, x, adj_list):
        N = x.size(0)
        h_neighbors = []
        
        for i in range(N):
            sampled = self.sample_neighbors(adj_list, i, self.sample_size)
            neighbor_feats = x[sampled]
            
            if self.agg == 'mean':
                agg_feat = neighbor_feats.mean(dim=0)
            elif self.agg == 'max':
                agg_feat = neighbor_feats.max(dim=0)[0]
            
            h_neighbors.append(agg_feat)
        
        h_neighbors = torch.stack(h_neighbors)  # (N, d)
        
        # CONCAT(self, agg_neighbors)
        combined = torch.cat([x, h_neighbors], dim=1)  # (N, 2d)
        out = F.relu(self.W(combined))  # (N, out_dim)
        
        # L2 å½’ä¸€åŒ–
        out = F.normalize(out, p=2, dim=1)
        return out</code></pre>
        </div>
      </div>

      <div id="gin-code" class="tab-content">
        <div class="code-container">
          <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
          <pre><code>import torch
import torch.nn.functional as F
from torch_geometric.nn import GINConv, global_add_pool

# ============================================================
# GIN: Graph Isomorphism Network (Xu et al., 2019)
# h_i^(k) = MLP((1+Îµ) Â· h_i^(k-1) + Î£_{jâˆˆN(i)} h_j^(k-1))
# ============================================================
class GIN(torch.nn.Module):
    """
    GIN çš„å…³é”®ï¼š
    1. Sum èšåˆï¼ˆä¸æ˜¯ mean/maxï¼‰â†’ å•å°„æ€§
    2. MLPï¼ˆä¸æ˜¯å•å±‚çº¿æ€§ï¼‰â†’ æ›´å¼ºçš„å‡½æ•°é€¼è¿‘
    3. å›¾çº§è¯»å‡ºç”¨ SUMï¼ˆä¸æ˜¯ MEANï¼‰â†’ ä¿ç•™å¤šé‡é›†ä¿¡æ¯
    
    ç†è®ºä¿è¯ï¼šGIN çš„è¡¨è¾¾åŠ› = 1-WL å›¾åŒæ„æµ‹è¯•
    """
    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=5):
        super().__init__()
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        
        for i in range(num_layers):
            dim_in = in_dim if i == 0 else hidden_dim
            mlp = torch.nn.Sequential(
                torch.nn.Linear(dim_in, hidden_dim),
                torch.nn.BatchNorm1d(hidden_dim),
                torch.nn.ReLU(),
                torch.nn.Linear(hidden_dim, hidden_dim),
            )
            self.convs.append(GINConv(mlp, train_eps=True))
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        
        # å›¾çº§è¯»å‡º
        self.readout = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, out_dim),
        )
    
    def forward(self, x, edge_index, batch):
        for conv, bn in zip(self.convs, self.bns):
            x = F.relu(bn(conv(x, edge_index)))
        
        # å›¾çº§èšåˆï¼ˆSUMï¼Œä¸æ˜¯ MEANï¼ï¼‰
        x = global_add_pool(x, batch)
        return self.readout(x)


# ============================================================
# GIN ä»é›¶å®ç°
# ============================================================
class GINLayerFromScratch(torch.nn.Module):
    """
    h_i^(k) = MLP^(k)((1 + Îµ^(k)) Â· h_i^(k-1) + Î£_{jâˆˆN(i)} h_j^(k-1))
    """
    def __init__(self, in_dim, hidden_dim, train_eps=True):
        super().__init__()
        self.mlp = torch.nn.Sequential(
            torch.nn.Linear(in_dim, hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, hidden_dim),
        )
        if train_eps:
            self.eps = torch.nn.Parameter(torch.zeros(1))
        else:
            self.register_buffer('eps', torch.zeros(1))
    
    def forward(self, x, edge_index):
        src, dst = edge_index
        
        # Sum èšåˆé‚»å±…
        agg = torch.zeros_like(x)
        agg.scatter_add_(0, dst.unsqueeze(1).expand_as(x[src]), x[src])
        
        # (1 + Îµ) Â· x_i + Î£ x_j
        out = (1 + self.eps) * x + agg
        
        return self.mlp(out)</code></pre>
        </div>
      </div>

      <div id="mpnn-code" class="tab-content">
        <div class="code-container">
          <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
          <pre><code>import torch
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing

# ============================================================
# MPNN: Message Passing Neural Network (Gilmer et al., 2017)
# æœ€é€šç”¨çš„ GNN å½¢å¼
# ============================================================
class MPNNLayer(MessagePassing):
    """
    é€šç”¨æ¶ˆæ¯ä¼ é€’å±‚:
    m_ij = Ï†(x_i, x_j, e_ij)     # æ¶ˆæ¯å‡½æ•°
    agg_i = Î£_{jâˆˆN(i)} m_ij       # èšåˆï¼ˆsumï¼‰
    x_i' = Î³(x_i, agg_i)          # æ›´æ–°å‡½æ•°
    """
    def __init__(self, node_dim, edge_dim, hidden_dim):
        super().__init__(aggr='add')
        
        # æ¶ˆæ¯å‡½æ•° Ï†
        self.message_mlp = torch.nn.Sequential(
            torch.nn.Linear(2 * node_dim + edge_dim, hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, hidden_dim),
        )
        
        # æ›´æ–°å‡½æ•° Î³ï¼ˆå«æ®‹å·®è¿æ¥ï¼‰
        self.update_mlp = torch.nn.Sequential(
            torch.nn.Linear(node_dim + hidden_dim, hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, node_dim),
        )
    
    def forward(self, x, edge_index, edge_attr):
        return self.propagate(edge_index, x=x, edge_attr=edge_attr)
    
    def message(self, x_i, x_j, edge_attr):
        """æ¶ˆæ¯: m_ij = Ï†(x_i, x_j, e_ij)"""
        return self.message_mlp(torch.cat([x_i, x_j, edge_attr], dim=-1))
    
    def update(self, aggr_out, x):
        """æ›´æ–°: x_i' = Î³(x_i, agg_i) + x_i (æ®‹å·®)"""
        return self.update_mlp(torch.cat([x, aggr_out], dim=-1)) + x


class FullMPNN(torch.nn.Module):
    """å®Œæ•´çš„ MPNN æ¨¡å‹"""
    def __init__(self, node_dim, edge_dim, hidden_dim, out_dim, n_layers=3):
        super().__init__()
        self.encoder = torch.nn.Linear(node_dim, hidden_dim)
        self.layers = torch.nn.ModuleList([
            MPNNLayer(hidden_dim, edge_dim, hidden_dim)
            for _ in range(n_layers)
        ])
        self.decoder = torch.nn.Linear(hidden_dim, out_dim)
    
    def forward(self, x, edge_index, edge_attr):
        x = self.encoder(x)
        for layer in self.layers:
            x = layer(x, edge_index, edge_attr)
        return self.decoder(x)</code></pre>
        </div>
      </div>
    </div>

    <h3 id="gnn-physrobot">ğŸ¤– ä¸ PhysRobot çš„æ·±åº¦å…³è”</h3>

    <div class="callout callout-project">
      <h4>DynamicalGNN å°±æ˜¯ç‰©ç†ç‰¹åŒ–çš„ MPNN</h4>
      <p>æˆ‘ä»¬é¡¹ç›®ä¸­çš„ <code>DynamicalGNN</code> æ˜¯ Â§5.3 æ¶ˆæ¯ä¼ é€’ GNN çš„<strong>ç‰©ç†ç‰¹åŒ–ç‰ˆæœ¬</strong>ã€‚è®©æˆ‘ä»¬é€è¡Œå¯¹æ¯”ï¼š</p>
    </div>

    <div class="comparison-table">
      <table>
        <thead><tr><th>ç»„ä»¶</th><th>é€šç”¨ MPNN</th><th>DynamicalGNN (PhysRobot)</th></tr></thead>
        <tbody>
          <tr><td>æ¶ˆæ¯å‡½æ•° $\phi$</td><td>MLP(x_i, x_j, e_ij)</td><td><code>message_net([edge_attr, x_i, x_j])</code></td></tr>
          <tr><td>èšåˆ $\bigoplus$</td><td>sum / mean / max</td><td><code>aggr='add'</code> (sum) â€” ä¿æŒç‰©ç†å®ˆæ’</td></tr>
          <tr><td>æ›´æ–° $\gamma$</td><td>MLP(x_i, agg_i)</td><td><code>update_net([x, aggr_out]) + x</code> (æ®‹å·®)</td></tr>
          <tr><td>è¾¹ç‰¹å¾</td><td>ä»»æ„ e_ij</td><td><code>EdgeFrame</code>: [r_ij, ||r_ij||, v_rel, ||v_rel||]</td></tr>
          <tr><td>è¾“å‡º</td><td>ä»»æ„é¢„æµ‹</td><td>åŠ é€Ÿåº¦ a âˆˆ â„Â³ï¼ˆç‰›é¡¿ç¬¬äºŒå®šå¾‹ï¼‰</td></tr>
        </tbody>
      </table>
    </div>

    <div class="code-container">
      <span class="code-label">PhysRobot é¡¹ç›®ä»£ç  â€” physics_core/dynamical_gnn.pyï¼ˆæ ¸å¿ƒç‰‡æ®µï¼‰</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code># è¿™æ˜¯æˆ‘ä»¬å®é™…é¡¹ç›®ä»£ç çš„æ ¸å¿ƒæ¶ˆæ¯ä¼ é€’å±‚
# å®Œæ•´æºç : physics_core/dynamical_gnn.py

class PhysicsMessagePassing(MessagePassing):
    """
    æ¶ˆæ¯ä¼ é€’å±‚ + ç‰©ç†çº¦æŸ
    
    å¯¹åº” GDL è“å›¾:
    - Message: m_ij = Ï†(e_ij, h_i, h_j)     â† åŒ…å«è¾¹æ¡†æ¶ä¿¡æ¯
    - Update:  h_i' = Ïˆ(h_i, Î£_j m_ij) + h_i  â† æ®‹å·®è¿æ¥
    """
    def __init__(self, hidden_dim, edge_dim):
        super().__init__(aggr='add')  # â† Sum èšåˆä¿æŒåŠ¨é‡å®ˆæ’
        
        # æ¶ˆæ¯ç½‘ç»œ Ï†ï¼šæ‹¼æ¥ [è¾¹ç‰¹å¾, å‘é€è€…, æ¥æ”¶è€…]
        self.message_net = nn.Sequential(
            nn.Linear(edge_dim + 2 * hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        
        # æ›´æ–°ç½‘ç»œ Ïˆï¼šæ‹¼æ¥ [åŸå§‹ç‰¹å¾, èšåˆæ¶ˆæ¯]
        self.update_net = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
    
    def message(self, x_i, x_j, edge_attr):
        # m_ij = Ï†(e_ij, h_i, h_j) â€” å®Œå…¨ç¬¦åˆæ¶ˆæ¯ä¼ é€’èŒƒå¼
        return self.message_net(torch.cat([edge_attr, x_i, x_j], dim=-1))
    
    def update(self, aggr_out, x):
        # h_i' = Ïˆ(h_i, agg) + h_i â€” æ®‹å·®è¿æ¥ï¼ˆç±»ä¼¼ ResNet/ODE ç¦»æ•£åŒ–ï¼‰
        return self.update_net(torch.cat([x, aggr_out], dim=-1)) + x</code></pre>
    </div>


    <!-- ====================== 5.4 Deep Sets, Transformer ====================== -->
    <h2 id="sec5-4">5.4 Deep Sets, Transformers, ä¸éšå¼å›¾æ¨æ–­<br><span style="font-size:0.65em;color:var(--text-secondary)">Deep Sets, Transformers, and Latent Graph Inference</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>æˆ‘ä»¬é€šè¿‡è®¨è®º<strong>æ— åºé›†åˆ</strong>ä¸Šçš„ç½®æ¢ç­‰å˜ç¥ç»ç½‘ç»œæ¥ç»“æŸ GNN çš„è®¨è®ºã€‚è™½ç„¶é›†åˆåœ¨æˆ‘ä»¬è®¨è®ºçš„åŸŸä¸­ç»“æ„æœ€å°‘ï¼Œä½†å®ƒä»¬äº§ç”Ÿäº†ä¸€äº›æœ€æµè¡Œçš„æ¶æ„ï¼š<span class="term">Transformer</span> å’Œ <span class="term">Deep Sets</span>ã€‚</p>
      </div>
      <div class="en">
        We close the GNN discussion by remarking on permutation-equivariant architectures for unordered sets, which produce some of the most popular architectures including Transformers and Deep Sets.
      </div>
    </div>

    <h3 id="deepsets">Deep Sets: é›†åˆä¸Šçš„å­¦ä¹ </h3>

    <div class="math-block">
      $$f(\mathbf{X}) = \rho\left(\sum_{i} \phi(x_i)\right) \tag{Deep Sets}$$
      <div class="math-explain">
        <strong>Deep Sets</strong> (Zaheer et al., 2017) æ˜¯æœ€ç®€å•çš„é›†åˆæ¶æ„ï¼šç‹¬ç«‹åœ°å¯¹æ¯ä¸ªå…ƒç´  $x_i$ åº”ç”¨ $\phi$ï¼Œç„¶å<strong>æ±‚å’Œèšåˆ</strong>ã€‚ç­‰ä»·äº GNN ä¸­ä»¤ $A = I$ï¼ˆæ— è¾¹ï¼Œé‚»åŸŸåªæœ‰è‡ªèº«ï¼‰ã€‚Zaheer ç­‰äººè¯æ˜äº†å®ƒå…·æœ‰<strong>é€šç”¨é€¼è¿‘æ€§</strong>ã€‚åœ¨ç‚¹äº‘å¤„ç†ä¸­ä¹Ÿç§°ä¸º <strong>PointNet</strong> (Qi et al., 2017)ã€‚
      </div>
    </div>

    <h3 id="transformer">Transformer ä½œä¸ºå®Œå…¨å›¾ä¸Šçš„æ³¨æ„åŠ› GNN</h3>

    <p>è¿™æ˜¯æœ¬ä¹¦æœ€é‡è¦çš„æ´å¯Ÿä¹‹ä¸€ï¼š<span class="term">Transformer = å®Œå…¨å›¾ä¸Šçš„ GAT</span>ã€‚</p>

    <div class="blueprint">
      <h4>Transformer çš„ GNN è§†è§’</h4>
      <p>å½“æˆ‘ä»¬æ²¡æœ‰å…ˆéªŒå›¾ç»“æ„ï¼ˆ$A = \mathbf{1}\mathbf{1}^\top$ï¼Œå³å®Œå…¨å›¾ï¼‰ï¼Œå·ç§¯å‹ GNN é€€åŒ–ä¸º Deep Setsï¼ˆå› ä¸ºæ‰€æœ‰èŠ‚ç‚¹çš„èšåˆç›¸åŒï¼‰ã€‚ä½†<strong>æ³¨æ„åŠ›å‹ GNN</strong> ä¸ä¼šé€€åŒ–ï¼š</p>
      $$h_u = \phi\left(x_u, \bigoplus_{v \in V} a(x_u, x_v) \psi(x_v)\right) \tag{36}$$
      <p>è¿™æ­£æ˜¯<strong>è‡ªæ³¨æ„åŠ›</strong>ï¼ˆself-attentionï¼‰æ“ä½œâ€”â€”Transformer çš„æ ¸å¿ƒã€‚æ³¨æ„åŠ›ç³»æ•° $a(x_u, x_v)$ å¯ä»¥ç†è§£ä¸ºåœ¨æ¨æ–­ä¸€ä¸ª<strong>è½¯é‚»æ¥çŸ©é˜µ</strong>ã€‚</p>
    </div>

    <div class="derivation">
      <h4>Transformer è‡ªæ³¨æ„åŠ› = å®Œå…¨å›¾ GAT çš„æ¨å¯¼</h4>
      <div class="step"><span class="step-num">1</span><strong>æ ‡å‡† Multi-Head Attention</strong>ï¼š$Q = XW_Q, \; K = XW_K, \; V = XW_V$</div>
      <div class="step"><span class="step-num">2</span><strong>æ³¨æ„åŠ›çŸ©é˜µ</strong>ï¼š$A_{\text{soft}} = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)$</div>
      <div class="step"><span class="step-num">3</span><strong>è¾“å‡º</strong>ï¼š$\text{Attention}(Q, K, V) = A_{\text{soft}} V$</div>
      <div class="step"><span class="step-num">4</span><strong>GNN è§†è§’</strong>ï¼š$A_{\text{soft}}$ å°±æ˜¯<strong>éšå¼æ¨æ–­çš„é‚»æ¥çŸ©é˜µ</strong>ï¼Œæ¯ä¸ªå…ƒç´  $A_{ij} = a(x_i, x_j)$ï¼Œæ˜¯"èŠ‚ç‚¹ $j$ å¯¹èŠ‚ç‚¹ $i$ çš„é‡è¦æ€§"ã€‚</div>
      <div class="step"><span class="step-num">5</span><strong>åŒºåˆ«</strong>ï¼šTransformer ä½¿ç”¨<strong>ä½ç½®ç¼–ç </strong>æ¥æ³¨å…¥åºåˆ—ä¿¡æ¯ï¼ˆå› ä¸ºå®Œå…¨å›¾å¿½ç•¥äº†ä½ç½®ï¼‰ï¼Œè¿™ç±»ä¼¼äºå›¾ä¸Šçš„ Laplacian ä½ç½®ç¼–ç ã€‚</div>
    </div>

    <h3 id="latent-graph">éšå¼å›¾æ¨æ–­</h3>

    <p>åœ¨ $A = I$ï¼ˆDeep Setsï¼‰å’Œ $A = \mathbf{1}\mathbf{1}^\top$ï¼ˆTransformerï¼‰ä¹‹é—´ï¼Œè¿˜æœ‰ä¸€ç§æ›´çµæ´»çš„æ–¹æ¡ˆï¼š<strong>å­¦ä¹ </strong>éšå¼å›¾ç»“æ„ $A$ã€‚</p>

    <div class="callout callout-info">
      <h4>ä¸‰ç§è¾¹é›†ç­–ç•¥å¯¹æ¯”</h4>
      <table>
        <thead><tr><th>ç­–ç•¥</th><th>é‚»æ¥çŸ©é˜µ</th><th>è¡¨è¾¾åŠ›</th><th>æ•ˆç‡</th><th>ä»£è¡¨</th></tr></thead>
        <tbody>
          <tr><td>ç©ºè¾¹é›†</td><td>$A = I$</td><td>ä½</td><td>$O(n)$</td><td>Deep Sets</td></tr>
          <tr><td>å®Œå…¨è¾¹é›†</td><td>$A = \mathbf{1}\mathbf{1}^\top$</td><td>é«˜</td><td>$O(n^2)$</td><td>Transformer</td></tr>
          <tr><td>å­¦ä¹ è¾¹é›†</td><td>å­¦ä¹ çš„ $A$</td><td>æœ€é«˜</td><td>$O(n^2)$+</td><td>éšå¼å›¾æ¨æ–­</td></tr>
        </tbody>
      </table>
    </div>

    <div class="code-container">
      <span class="code-label">Python / PyTorch â€” Transformer æ³¨æ„åŠ› = å®Œå…¨å›¾ä¸Šçš„ GAT</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# ============================================================
# 1. æ ‡å‡† Transformer è‡ªæ³¨æ„åŠ›ï¼ˆä» GNN è§†è§’å®ç°ï¼‰
# ============================================================
class SelfAttentionAsGAT(nn.Module):
    """
    Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å®Œå…¨å›¾ä¸Šçš„ GAT è§†è§’å®ç°ã€‚
    
    ç­‰ä»·æ€§:
    - Transformer: Attention(Q,K,V) = softmax(QK^T/âˆšd)V
    - GAT on complete graph: h_i = Î£_j Î±_ij (V x_j)
      where Î±_ij = softmax_j(Q x_i Â· K x_j / âˆšd)
    """
    def __init__(self, d_model, n_heads=8, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        """
        x: (B, N, d_model) â€” N ä¸ª "èŠ‚ç‚¹" çš„ç‰¹å¾
        è¾“å‡º: (B, N, d_model)
        
        åœ¨ GNN æœ¯è¯­ä¸­:
        - è¿™æ˜¯ä¸€ä¸ªå®Œå…¨å›¾ä¸Šçš„å¤šå¤´æ³¨æ„åŠ› GNN
        - æ¯ä¸ª "èŠ‚ç‚¹" éƒ½ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹é€šä¿¡
        """
        B, N, _ = x.shape
        
        # çº¿æ€§å˜æ¢ï¼ˆ= GNN ä¸­çš„ Ïˆ(x_v) å˜æ¢ï¼‰
        Q = self.W_Q(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_K(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_V(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        
        # æ³¨æ„åŠ›åˆ†æ•°ï¼ˆ= GAT çš„ Î±_ij è®¡ç®—ï¼‰
        # scores[i,j] = Q_i Â· K_j / âˆšd_k  â†’ è½¯é‚»æ¥çŸ©é˜µï¼
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # softmax å½’ä¸€åŒ–ï¼ˆ= GAT çš„ softmax å½’ä¸€åŒ–ï¼‰
        attn = F.softmax(scores, dim=-1)  # (B, heads, N, N) â€” è¿™å°±æ˜¯éšå¼é‚»æ¥çŸ©é˜µï¼
        attn = self.dropout(attn)
        
        # åŠ æƒèšåˆï¼ˆ= GAT çš„ Î£ Î±_ij V_jï¼‰
        out = torch.matmul(attn, V)  # (B, heads, N, d_k)
        
        # åˆå¹¶å¤šå¤´
        out = out.transpose(1, 2).contiguous().view(B, N, self.d_model)
        return self.W_O(out)


# ============================================================
# 2. Deep Sets å®ç°
# ============================================================
class DeepSets(nn.Module):
    """
    f(X) = Ï(Î£ Ï†(x_i))
    
    ç­‰ä»·äº: A = I çš„ GNNï¼ˆé‚»åŸŸåªæœ‰è‡ªèº«ï¼‰
    """
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        # Ï†: é€å…ƒç´ å˜æ¢
        self.phi = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        # Ï: èšåˆåçš„å˜æ¢
        self.rho = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim),
        )
    
    def forward(self, x):
        """x: (B, N, d) â†’ (B, out_dim)"""
        h = self.phi(x)          # (B, N, hidden)
        h = h.sum(dim=1)         # (B, hidden) â€” ç½®æ¢ä¸å˜èšåˆ
        return self.rho(h)       # (B, out_dim)


# ============================================================
# 3. å¯è§†åŒ–æ³¨æ„åŠ› = é‚»æ¥çŸ©é˜µ
# ============================================================
def visualize_attention_as_adjacency():
    """å±•ç¤º Transformer çš„æ³¨æ„åŠ›çŸ©é˜µå°±æ˜¯ä¸€ä¸ªè½¯é‚»æ¥çŸ©é˜µ"""
    torch.manual_seed(42)
    
    model = SelfAttentionAsGAT(d_model=64, n_heads=4)
    x = torch.randn(1, 8, 64)  # 8 ä¸ª "èŠ‚ç‚¹"
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    B, N, _ = x.shape
    Q = model.W_Q(x).view(1, N, 4, 16).transpose(1, 2)
    K = model.W_K(x).view(1, N, 4, 16).transpose(1, 2)
    attn = F.softmax(Q @ K.transpose(-2, -1) / 4.0, dim=-1)
    
    print("æ³¨æ„åŠ›çŸ©é˜µï¼ˆ= è½¯é‚»æ¥çŸ©é˜µï¼‰:")
    print(f"å½¢çŠ¶: {attn.shape}")  # (1, 4, 8, 8)
    print(f"è¡Œå’Œ = {attn[0, 0].sum(dim=-1)}")  # æ¯è¡Œå’Œä¸º 1ï¼ˆsoftmaxï¼‰
    print(f"æœ€å¤§æ³¨æ„åŠ›: {attn[0, 0].max():.4f}")
    print(f"æœ€å°æ³¨æ„åŠ›: {attn[0, 0].min():.4f}")
    print("â†’ æ¯ä¸ªèŠ‚ç‚¹å¯¹æ‰€æœ‰èŠ‚ç‚¹çš„æ³¨æ„åŠ›æƒé‡ = å®Œå…¨å›¾ä¸Šçš„è¾¹æƒé‡ âœ…")


if __name__ == "__main__":
    visualize_attention_as_adjacency()
    
    # Deep Sets
    ds = DeepSets(16, 64, 10)
    x = torch.randn(4, 20, 16)  # 4 ä¸ªé›†åˆï¼Œæ¯ä¸ª 20 å…ƒç´ 
    print(f"\nDeep Sets: {x.shape} â†’ {ds(x).shape}")
    
    # éªŒè¯ç½®æ¢ä¸å˜æ€§
    perm = torch.randperm(20)
    out1 = ds(x)
    out2 = ds(x[:, perm])
    print(f"ç½®æ¢ä¸å˜æ€§è¯¯å·®: {(out1 - out2).abs().max():.2e} âœ…")</code></pre>
    </div>


    <!-- ====================== 5.5 EMPNN ====================== -->
    <h2 id="sec5-5">5.5 ç­‰å˜æ¶ˆæ¯ä¼ é€’ç½‘ç»œ (EMPNN) <span style="color:#f59f00">â­â­</span><br><span style="font-size:0.65em;color:var(--text-secondary)">Equivariant Message Passing Neural Networks</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>åœ¨è®¸å¤š GNN çš„åº”ç”¨ä¸­ï¼ŒèŠ‚ç‚¹ç‰¹å¾ï¼ˆæˆ–å…¶ä¸€éƒ¨åˆ†ï¼‰ä¸ä»…ä»…æ˜¯ä»»æ„å‘é‡ï¼Œè¿˜æ˜¯<strong>å‡ ä½•å®ä½“çš„åæ ‡</strong>ã€‚ä¾‹å¦‚åˆ†å­å›¾ä¸­ï¼šèŠ‚ç‚¹ä»£è¡¨åŸå­ï¼ŒåŒ…å«åŸå­ç±»å‹å’Œ<strong>3D ç©ºé—´åæ ‡</strong>ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨æ ‡å‡†çš„ç½®æ¢ç­‰å˜æ€§ä¹‹å¤–ï¼Œè¿˜èƒ½ä¿è¯å¯¹<strong>æ¬§å‡ é‡Œå¾—ç¾¤ $E(3)$</strong>ï¼ˆæ—‹è½¬ã€å¹³ç§»ã€åå°„ï¼‰çš„ç­‰å˜æ€§ã€‚</p>
      </div>
      <div class="en">
        In many GNN applications, node features are coordinates of geometric entities. It is desirable to process them equivariantly to the Euclidean group E(3) of rigid motions, in addition to the standard permutation equivariance.
      </div>
    </div>

    <h3 id="egnn">E(n)-ç­‰å˜å›¾ç¥ç»ç½‘ç»œ (EGNN)</h3>

    <p>Satorras et al. (2021) æå‡ºäº†ä¸€ä¸ªä¼˜é›…çš„ E(n)-ç­‰å˜æ¶ˆæ¯ä¼ é€’æ–¹æ¡ˆï¼š</p>

    <div class="blueprint">
      <h4>EGNN çš„æ ¸å¿ƒå…¬å¼</h4>
      <p>åŒºåˆ†<strong>æ ‡é‡ç‰¹å¾</strong> $f_u$ å’Œ<strong>ç©ºé—´åæ ‡</strong> $x_u$ï¼š</p>
      $$f_u' = \phi\left(f_u, \bigoplus_{v \in \mathcal{N}_u} \psi_f(f_u, f_v, \|x_u - x_v\|^2)\right)$$
      $$x_u' = x_u + \sum_{v \neq u} (x_u - x_v) \psi_c(f_u, f_v, \|x_u - x_v\|^2)$$
    </div>

    <div class="derivation">
      <h4>ä¸ºä»€ä¹ˆè¿™æ˜¯ E(3)-ç­‰å˜çš„ï¼Ÿé€æ­¥è¯æ˜</h4>
      <div class="step"><span class="step-num">1</span><strong>å…³é”®è§‚å¯Ÿ</strong>ï¼š$f_u'$ å¯¹ $x_u$ çš„å”¯ä¸€ä¾èµ–æ˜¯é€šè¿‡<strong>è·ç¦»</strong> $\|x_u - x_v\|^2$ã€‚E(3) å˜æ¢ä¿æŒè·ç¦»ä¸å˜ï¼Œå› æ­¤ $f_u'$ æ˜¯ E(3) ä¸å˜çš„ã€‚âœ…</div>
      <div class="step"><span class="step-num">2</span><strong>åæ ‡æ›´æ–°</strong>ï¼šè€ƒè™‘å˜æ¢ $x_u \mapsto Rx_u + b$ã€‚
        $$x_u' \mapsto (Rx_u + b) + \sum_{v \neq u} ((Rx_u + b) - (Rx_v + b)) \psi_c(\cdots)$$
        $$= Rx_u + b + \sum_{v \neq u} R(x_u - x_v) \psi_c(\cdots)$$
        $$= R\left(x_u + \sum_{v \neq u} (x_u - x_v) \psi_c(\cdots)\right) + b = Rx_u' + b$$</div>
      <div class="step"><span class="step-num">3</span>å› æ­¤ $x_u'$ åœ¨ $E(3)$ ä¸‹<strong>ç­‰å˜</strong>åœ°å˜æ¢ï¼š$x_u' \mapsto Rx_u' + b$ã€‚âœ…</div>
      <div class="step"><span class="step-num">4</span><strong>å…³é”®</strong>ï¼š$\psi_c$ è¾“å‡ºçš„æ˜¯<strong>æ ‡é‡</strong>æƒé‡ï¼Œä¹˜ä»¥<strong>å‘é‡</strong> $(x_u - x_v)$ã€‚æ ‡é‡ä¸å—æ—‹è½¬å½±å“ï¼Œå‘é‡è‡ªç„¶æ—‹è½¬ã€‚</div>
    </div>

    <div class="figure">
      <img src="../assets/ch5_p90_img0.png" alt="EGNN equivariance">
      <figcaption>å›¾ï¼šæ ‡é‡ç‰¹å¾ï¼ˆçƒ­å›¾ï¼‰åœ¨æ—‹è½¬ä¸‹ä¸å˜ï¼Œä½†å‘é‡ç‰¹å¾ï¼ˆç®­å¤´ï¼‰ä¼šæ”¹å˜æ–¹å‘ã€‚ç®€å•çš„ E(3) ç­‰å˜ GNN åªå¤„ç†æ ‡é‡ï¼›æ›´é«˜çº§çš„æ–¹æ³•ï¼ˆå¦‚ Tensor Field Networksï¼‰å¤„ç†ä»»æ„å¼ é‡ã€‚</figcaption>
    </div>

    <h3 id="tfn">Tensor Field Networks ä¸ä¸å¯çº¦è¡¨ç¤º</h3>

    <p>EGNN çš„é™åˆ¶ï¼šç‰¹å¾ $f_u$ è¢«å‡è®¾ä¸º<strong>æ ‡é‡</strong>ã€‚ä½†æœ‰äº›ç‰©ç†é‡æ˜¯<strong>å‘é‡</strong>ï¼ˆå¦‚é€Ÿåº¦ï¼‰æˆ–<strong>å¼ é‡</strong>ï¼ˆå¦‚åº”åŠ›ï¼‰ã€‚æ›´ä¸€èˆ¬çš„æ–¹æ³•åŸºäº<strong>ä¸å¯çº¦è¡¨ç¤º</strong>ã€‚</p>

    <div class="callout callout-info">
      <h4>ä¸å¯çº¦è¡¨ç¤ºæ–¹æ³•</h4>
      <p>æ—‹è½¬ç¾¤çš„æ‰€æœ‰è¡¨ç¤ºéƒ½å¯ä»¥åˆ†è§£ä¸º<strong>ä¸å¯çº¦</strong>å½¢å¼ï¼šç”± <strong>Wigner D-çŸ©é˜µ</strong>ï¼ˆçƒè°å‡½æ•°çš„ Fourier åŸºï¼‰æ—‹è½¬çš„å‘é‡ã€‚ç­‰å˜æ ¸é€šè¿‡ <strong>Clebsch-Gordan çŸ©é˜µ</strong>å’Œçƒè°å‡½æ•°æ„é€ ã€‚</p>
      <p>ä»£è¡¨æ¨¡å‹ï¼š</p>
      <ul>
        <li><strong>Tensor Field Networks</strong> (Thomas et al., 2018)ï¼šç‚¹äº‘ä¸Šçš„å·ç§¯æ¨¡å‹</li>
        <li><strong>SE(3)-Transformer</strong> (Fuchs et al., 2020)ï¼šå›¾ä¸Šçš„æ³¨æ„åŠ›å±‚</li>
        <li><strong>SchNet</strong> (SchÃ¼tt et al., 2018)ï¼šé‡å­åŒ–å­¦çš„é«˜æ•ˆæ¶ˆæ¯ä¼ é€’</li>
        <li><strong>DimeNet</strong> (Klicpera et al., 2020)ï¼šä½¿ç”¨æ–¹å‘ä¿¡æ¯</li>
      </ul>
    </div>

    <h3 id="empnn-physrobot">ğŸ¤– EdgeFrame çš„ç­‰å˜æ€§ â€” æ¥è‡ª EMPNN ç†è®º</h3>

    <div class="callout callout-project">
      <h4>PhysRobot çš„ EdgeFrame ä¸ EGNN çš„å…³è”</h4>
      <p>æˆ‘ä»¬é¡¹ç›®ä¸­çš„ <code>EdgeFrame</code> æ¨¡å—ç›´æ¥ä½“ç°äº† EGNN çš„è®¾è®¡å“²å­¦ï¼š</p>
      <ul>
        <li><strong>ç›¸å¯¹ä½ç§»</strong>ï¼š$r_{ij} = x_j - x_i$ â€” è¿™æ˜¯ EGNN ä¸­çš„ $(x_u - x_v)$</li>
        <li><strong>è·ç¦»ä¸å˜é‡</strong>ï¼š$\|r_{ij}\|$ â€” è¿™æ˜¯ E(3) ä¸å˜çš„æ ‡é‡</li>
        <li><strong>ç›¸å¯¹é€Ÿåº¦</strong>ï¼š$v_{\text{rel}} = v_j - v_i$ â€” EGNN çš„é€Ÿåº¦æ‰©å±•</li>
        <li><strong>åå¯¹ç§°æ€§</strong>ï¼š$e_{ij} = -e_{ji}$ â€” ä¿è¯ç‰›é¡¿ç¬¬ä¸‰å®šå¾‹ï¼</li>
      </ul>
    </div>

    <div class="code-container">
      <span class="code-label">PhysRobot é¡¹ç›®ä»£ç  â€” physics_core/edge_frame.pyï¼ˆå®é™…ä»£ç ï¼‰</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code># è¿™æ˜¯æˆ‘ä»¬å®é™…é¡¹ç›®çš„ EdgeFrame å®ç°
# å®Œæ•´æºç : physics_core/edge_frame.py

class EdgeFrame(nn.Module):
    """
    Edge Frame: ç¼–ç èŠ‚ç‚¹é—´çš„ç©ºé—´å…³ç³»ã€‚
    
    æ•°å­¦åŸç†ï¼ˆå¯¹åº” GDL Â§5.5 EGNNï¼‰:
    - è¾“å…¥: ä½ç½® x_i, x_j å’Œé€Ÿåº¦ v_i, v_j
    - è¾“å‡º: e_ij = Encoder([r_ij, ||r_ij||, v_rel, ||v_rel||])
    
    å…³é”®æ€§è´¨:
    1. å¹³ç§»ä¸å˜: ä½¿ç”¨ç›¸å¯¹é‡ r_ij = x_j - x_iï¼ˆè€Œéç»å¯¹ä½ç½®ï¼‰
    2. åå¯¹ç§°: e_ij = -e_jiï¼ˆç‰›é¡¿ç¬¬ä¸‰å®šå¾‹ï¼‰
    3. è·ç¦»ä¿¡æ¯: ||r_ij|| æ˜¯ E(3) ä¸å˜æ ‡é‡
    """
    def __init__(self, hidden_dim=64):
        super().__init__()
        # è¾“å…¥: [dx, dy, dz, ||r||, dvx, dvy, dvz, ||v||] = 8 ç»´
        self.edge_encoder = nn.Sequential(
            nn.Linear(8, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
        )
    
    def forward(self, positions, velocities, edge_index):
        src_idx, tgt_idx = edge_index[0], edge_index[1]
        
        # ç›¸å¯¹ä½ç§»ï¼ˆåå¯¹ç§°ï¼‰: r_ij = x_j - x_i
        r_ij = positions[tgt_idx] - positions[src_idx]  # (E, 3)
        r_norm = torch.norm(r_ij, dim=1, keepdim=True)  # (E, 1) â€” E(3) ä¸å˜é‡ï¼
        
        # ç›¸å¯¹é€Ÿåº¦ï¼ˆåå¯¹ç§°ï¼‰: v_rel = v_j - v_i
        v_rel = velocities[tgt_idx] - velocities[src_idx]  # (E, 3)
        v_norm = torch.norm(v_rel, dim=1, keepdim=True)
        
        # æ‹¼æ¥: 8 ç»´ç‰¹å¾
        edge_features = torch.cat([r_ij, r_norm, v_rel, v_norm], dim=1)
        
        return self.edge_encoder(edge_features)  # (E, hidden_dim)
    
    def check_antisymmetry(self, positions, velocities, edge_index):
        """éªŒè¯åå¯¹ç§°æ€§: e_ij = -e_jiï¼ˆç‰›é¡¿ç¬¬ä¸‰å®šå¾‹ï¼‰"""
        e_ij = self(positions, velocities, edge_index)
        edge_index_rev = torch.stack([edge_index[1], edge_index[0]], dim=0)
        e_ji = self(positions, velocities, edge_index_rev)
        return torch.max(torch.abs(e_ij + e_ji)).item()</code></pre>
    </div>

    <h3 id="empnn-code">ä»£ç ï¼šç®€å•çš„ E(3) ç­‰å˜å±‚</h3>

    <div class="code-container">
      <span class="code-label">Python / PyTorch â€” E(3) ç­‰å˜ GNN å±‚</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
import torch.nn as nn
from torch_geometric.nn import MessagePassing

# ============================================================
# EGNN Layer (Satorras et al., 2021)
# E(3)-ç­‰å˜å›¾ç¥ç»ç½‘ç»œå±‚
# ============================================================
class EGNNLayer(MessagePassing):
    """
    E(3)-Equivariant GNN Layer
    
    æ•°å­¦:
    f_i' = Ï†(f_i, Î£_j Ïˆ_f(f_i, f_j, ||x_i - x_j||Â²))
    x_i' = x_i + Î£_j (x_i - x_j) Â· Ïˆ_c(f_i, f_j, ||x_i - x_j||Â²)
    
    ç­‰å˜æ€§ä¿è¯:
    - f' å¯¹ E(3) ä¸å˜ï¼ˆåªä¾èµ–è·ç¦»ï¼‰
    - x' å¯¹ E(3) ç­‰å˜ï¼ˆå‘é‡å˜æ¢ + æ ‡é‡æƒé‡ï¼‰
    """
    def __init__(self, feature_dim, hidden_dim):
        super().__init__(aggr='add')
        self.feature_dim = feature_dim
        
        # Ïˆ_f: æ ‡é‡æ¶ˆæ¯å‡½æ•°
        self.message_mlp = nn.Sequential(
            nn.Linear(2 * feature_dim + 1, hidden_dim),  # +1 for distance
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        
        # Ïˆ_c: åæ ‡æ›´æ–°çš„æ ‡é‡æƒé‡
        self.coord_mlp = nn.Sequential(
            nn.Linear(2 * feature_dim + 1, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, 1),  # è¾“å‡ºæ ‡é‡ï¼
        )
        
        # Ï†: ç‰¹å¾æ›´æ–°
        self.update_mlp = nn.Sequential(
            nn.Linear(feature_dim + hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, feature_dim),
        )
    
    def forward(self, f, x, edge_index):
        """
        f: (N, feature_dim) â€” æ ‡é‡ç‰¹å¾ï¼ˆE(3) ä¸å˜ï¼‰
        x: (N, 3)           â€” ç©ºé—´åæ ‡ï¼ˆE(3) ç­‰å˜ï¼‰
        edge_index: (2, E)
        
        è¿”å›: f', x' â€” æ›´æ–°åçš„ç‰¹å¾å’Œåæ ‡
        """
        src, dst = edge_index
        
        # è®¡ç®—è·ç¦»ï¼ˆE(3) ä¸å˜é‡ï¼‰
        diff = x[dst] - x[src]              # (E, 3)
        dist_sq = (diff ** 2).sum(dim=-1, keepdim=True)  # (E, 1)
        
        # æ¶ˆæ¯ä¼ é€’ï¼ˆç‰¹å¾æ›´æ–°ï¼‰
        f_msg = self.message_mlp(torch.cat([f[dst], f[src], dist_sq], dim=-1))
        
        # èšåˆæ¶ˆæ¯
        agg = torch.zeros(f.size(0), f_msg.size(1), device=f.device)
        agg.scatter_add_(0, dst.unsqueeze(1).expand_as(f_msg), f_msg)
        
        # æ›´æ–°æ ‡é‡ç‰¹å¾
        f_new = self.update_mlp(torch.cat([f, agg], dim=-1)) + f
        
        # åæ ‡æ›´æ–°
        coord_weights = self.coord_mlp(torch.cat([f[dst], f[src], dist_sq], dim=-1))
        weighted_diff = diff * coord_weights  # (E, 3) * (E, 1) = (E, 3)
        
        coord_agg = torch.zeros_like(x)
        coord_agg.scatter_add_(0, dst.unsqueeze(1).expand_as(weighted_diff), weighted_diff)
        
        x_new = x + coord_agg
        
        return f_new, x_new


# ============================================================
# å®Œæ•´çš„ EGNN æ¨¡å‹
# ============================================================
class EGNN(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, n_layers=3):
        super().__init__()
        self.encoder = nn.Linear(in_dim, hidden_dim)
        self.layers = nn.ModuleList([
            EGNNLayer(hidden_dim, hidden_dim) for _ in range(n_layers)
        ])
        self.decoder = nn.Linear(hidden_dim, out_dim)
    
    def forward(self, f, x, edge_index):
        f = self.encoder(f)
        for layer in self.layers:
            f, x = layer(f, x, edge_index)
        return self.decoder(f), x


# ============================================================
# éªŒè¯ E(3) ç­‰å˜æ€§
# ============================================================
def verify_e3_equivariance():
    """éªŒè¯ EGNN çš„ E(3) ç­‰å˜æ€§"""
    torch.manual_seed(42)
    
    layer = EGNNLayer(feature_dim=16, hidden_dim=32)
    layer.eval()
    
    N = 5
    f = torch.randn(N, 16)
    x = torch.randn(N, 3)
    edge_index = torch.tensor([[0,1,1,2,2,3,3,4],
                                [1,0,2,1,3,2,4,3]], dtype=torch.long)
    
    # åŸå§‹è¾“å‡º
    f_out, x_out = layer(f, x, edge_index)
    
    # æ„é€ éšæœºæ—‹è½¬å’Œå¹³ç§»
    # éšæœºæ—‹è½¬çŸ©é˜µï¼ˆRodrigues å…¬å¼ï¼‰
    axis = torch.randn(3)
    axis = axis / axis.norm()
    angle = torch.tensor(1.2)
    K = torch.tensor([[0, -axis[2], axis[1]],
                       [axis[2], 0, -axis[0]],
                       [-axis[1], axis[0], 0]])
    R = torch.eye(3) + torch.sin(angle) * K + (1 - torch.cos(angle)) * K @ K
    b = torch.randn(3)  # å¹³ç§»
    
    # å˜æ¢åæ ‡
    x_transformed = (R @ x.T).T + b  # Rx + b
    
    # åœ¨å˜æ¢åçš„è¾“å…¥ä¸Šè¿è¡Œ
    f_out_t, x_out_t = layer(f, x_transformed, edge_index)
    
    # å¯¹åŸå§‹è¾“å‡ºåº”ç”¨ç›¸åŒå˜æ¢
    x_out_expected = (R @ x_out.T).T + b
    
    # æ£€æŸ¥ç­‰å˜æ€§
    coord_error = (x_out_t - x_out_expected).abs().max().item()
    feat_error = (f_out_t - f_out).abs().max().item()
    
    print("=" * 50)
    print("E(3) ç­‰å˜æ€§éªŒè¯")
    print("=" * 50)
    print(f"åæ ‡ç­‰å˜æ€§è¯¯å·®: {coord_error:.2e}")
    print(f"ç‰¹å¾ä¸å˜æ€§è¯¯å·®: {feat_error:.2e}")
    print(f"åæ ‡ç­‰å˜æ€§: {'âœ…' if coord_error < 1e-4 else 'âŒ'}")
    print(f"ç‰¹å¾ä¸å˜æ€§: {'âœ…' if feat_error < 1e-4 else 'âŒ'}")


if __name__ == "__main__":
    verify_e3_equivariance()</code></pre>
    </div>


    <!-- ====================== 5.6 Intrinsic Mesh CNNs ====================== -->
    <h2 id="sec5-6">5.6 å†…è•´ç½‘æ ¼ CNN<br><span style="font-size:0.65em;color:var(--text-secondary)">Intrinsic Mesh CNNs</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>ç½‘æ ¼ï¼ˆå°¤å…¶æ˜¯ä¸‰è§’ç½‘æ ¼ï¼‰æ˜¯è®¡ç®—æœºå›¾å½¢å­¦çš„"å®¶å¸¸ä¾¿é¥­"ï¼Œä¹Ÿæ˜¯ 3D ç‰©ä½“å»ºæ¨¡æœ€å¸¸è§çš„æ–¹å¼ã€‚åœ¨ 2015 å¹´å·¦å³ï¼Œå›¾å½¢å­¦å’Œå‡ ä½•å¤„ç†ç¤¾åŒºå¼€å§‹ç§¯ææ¢ç´¢åœ¨ç½‘æ ¼æ•°æ®ä¸Šæ„å»ºç±»ä¼¼ CNN çš„æ¶æ„ã€‚</p>
      </div>
      <div class="en">
        Meshes, particularly triangular ones, are the 'bread and butter' of computer graphics. The community has been keen to construct CNN-like architectures for mesh data since the mid-2010s.
      </div>
    </div>

    <h3 id="geodesic-cnn">æµ‹åœ°çº¿ CNN (Geodesic CNN)</h3>

    <p>å¤§å¤šæ•°ç½‘æ ¼æ·±åº¦å­¦ä¹ æ¶æ„é€šè¿‡<strong>ç¦»æ•£åŒ–æˆ–è¿‘ä¼¼æŒ‡æ•°æ˜ å°„</strong>æ¥å®ç°å·ç§¯æ»¤æ³¢å™¨ï¼Œå¹¶åœ¨åˆ‡å¹³é¢çš„åæ ‡ç³»ä¸­è¡¨è¾¾æ»¤æ³¢å™¨ã€‚</p>

    <div class="math-block">
      $$(\mathbf{x} \star \theta)(u) = \int_0^R \int_0^{2\pi} x(u, r, \vartheta) \theta(r, \vartheta) \, dr \, d\vartheta$$
      <div class="math-explain">
        åœ¨æµå½¢ä¸Šå®šä¹‰å·ç§¯ï¼šä½¿ç”¨<strong>æµ‹åœ°çº¿æåæ ‡</strong> $(r, \vartheta)$ï¼Œ$r$ æ˜¯æµ‹åœ°çº¿è·ç¦»ï¼Œ$\vartheta$ æ˜¯åˆ‡å¹³é¢ä¸­çš„è§’åº¦æ–¹å‘ã€‚$x(u, r, \vartheta)$ æ˜¯ä»¥ $u$ ä¸ºä¸­å¿ƒçš„æµ‹åœ°çº¿å—ï¼ˆgeodesic patchï¼‰ã€‚
      </div>
    </div>

    <p><strong>å…³é”®æŒ‘æˆ˜</strong>ï¼šè§’åº¦æ–¹å‘ $\vartheta$ çš„å‚è€ƒé€‰æ‹©æœ‰<strong>æ­§ä¹‰æ€§</strong>â€”â€”è¿™æ­£æ˜¯<strong>è§„èŒƒé€‰æ‹©</strong>ï¼ˆgauge choiceï¼‰çš„é—®é¢˜ã€‚</p>

    <h4>å¤„ç†è§„èŒƒæ­§ä¹‰çš„ä¸‰ç§ç­–ç•¥</h4>

    <div class="comparison-table">
      <table>
        <thead><tr><th>ç­–ç•¥</th><th>æ–¹æ³•</th><th>ä¼˜ç‚¹</th><th>ç¼ºç‚¹</th></tr></thead>
        <tbody>
          <tr><td><strong>å„å‘åŒæ€§æ»¤æ³¢å™¨</strong></td><td>$\theta(r)$ ä¸ä¾èµ–æ–¹å‘</td><td>æ— éœ€é€‰æ‹©è§„èŒƒ</td><td>ä¸¢å¤±æ–¹å‘ä¿¡æ¯</td></tr>
          <tr><td><strong>å›ºå®šè§„èŒƒ</strong></td><td>ä½¿ç”¨ä¸»æ›²ç‡æ–¹å‘</td><td>ä¿ç•™æ–¹å‘ä¿¡æ¯</td><td>åœ¨å¹³å¦åŒºåŸŸä¸ç¨³å®š</td></tr>
          <tr><td><strong>è§’åº¦æ± åŒ–</strong></td><td>$\max_{\vartheta_0}$ åŒ¹é…</td><td>æ—‹è½¬ä¸å˜</td><td>è®¡ç®—å¼€é”€å¤§</td></tr>
          <tr><td><strong>è§„èŒƒç­‰å˜æ»¤æ³¢å™¨</strong></td><td>ä½¿ç”¨éå¹³å‡¡è¡¨ç¤º $\rho$</td><td>ç†è®ºæœ€ä¼˜</td><td>å®ç°å¤æ‚</td></tr>
        </tbody>
      </table>
    </div>

    <h3 id="monet">MoNet: æ··åˆæ¨¡å‹ç½‘ç»œ</h3>

    <p>Monti et al. (2017) çš„ MoNet ä½¿ç”¨<strong>å¯å­¦ä¹ çš„åŠ æƒå‡½æ•°</strong>ï¼ˆpatch operatorsï¼‰æ›¿ä»£å›ºå®šæ¨¡æ¿ï¼š</p>

    <div class="math-block">
      $$(\mathbf{x} \star \theta)_u = \frac{\sum_{k=1}^{K} w_k \sum_{v \in \mathcal{N}_u} (r_{uv}, \vartheta_{uv}) x_v \cdot \theta_k}{\sum_{k=1}^{K} w_k \sum_{v \in \mathcal{N}_u} (r_{uv}, \vartheta_{uv}) \cdot \theta_k}$$
      <div class="math-explain">
        $w_1, \ldots, w_K$ æ˜¯å¯å­¦ä¹ çš„é«˜æ–¯åŠ æƒå‡½æ•°ï¼ˆ"è½¯åƒç´ "ï¼‰ï¼Œ$\theta_1, \ldots, \theta_K$ æ˜¯æ»¤æ³¢å™¨ç³»æ•°ã€‚MoNet æ¯” Geodesic CNN æ›´çµæ´»ï¼Œå› ä¸ºåŠ æƒå‡½æ•°ä¹Ÿæ˜¯å¯å­¦ä¹ çš„ã€‚
      </div>
    </div>

    <div class="figure">
      <img src="../assets/ch5_p92_img0.png" alt="Patch operators: Geodesic CNN, Anisotropic CNN, MoNet">
      <figcaption>å›¾ 18ï¼šä¸‰ç§ç½‘æ ¼ CNN ä¸­ä½¿ç”¨çš„ patch operatorsã€‚ä»å·¦åˆ°å³ï¼šGeodesic CNNã€Anisotropic CNNã€MoNetã€‚çº¢è‰²ç­‰é«˜çº¿æ˜¾ç¤ºåŠ æƒå‡½æ•° $w_k(r, \vartheta)$ çš„å½¢çŠ¶ã€‚</figcaption>
    </div>

    <h3 id="gauge-cnn">è§„èŒƒç­‰å˜ CNN (Gauge-equivariant CNN)</h3>

    <p>Cohen et al. (2019) å’Œ de Haan et al. (2020) æå‡ºäº†æœ€"æ­£ç¡®"çš„æ–¹æ¡ˆï¼šè®©ç½‘ç»œç‰¹å¾ä¸<strong>éå¹³å‡¡è¡¨ç¤º</strong> $\rho$ å…³è”ï¼Œå¹¶é€šè¿‡<strong>å¹³è¡Œç§»åŠ¨</strong>ï¼ˆparallel transportï¼‰åœ¨èŠ‚ç‚¹é—´ä¼ é€’ã€‚</p>

    <div class="math-block">
      $$h_u = \Theta_{\text{self}} x_u + \sum_{v \in \mathcal{N}_u} \Theta_{\text{neigh}}(\vartheta_{uv}) \rho(g_{v \to u}) x_v \tag{37}$$
      <div class="math-explain">
        $\Theta_{\text{self}}, \Theta_{\text{neigh}}$ æ˜¯å¯å­¦ä¹ çš„æ»¤æ³¢å™¨çŸ©é˜µã€‚$g_{v \to u} \in SO(2)$ ç¼–ç ä» $v$ åˆ° $u$ çš„<strong>å¹³è¡Œç§»åŠ¨</strong>æ•ˆæœï¼ˆè§„èŒƒå˜æ¢ï¼‰ã€‚$\rho(g_{v \to u})$ æ˜¯å¯¹åº”çš„<strong>ä¼ è¾“çŸ©é˜µ</strong>ã€‚è¿™ç¡®ä¿äº†è¾“å‡ºæ˜¯è§„èŒƒç­‰å˜çš„ã€‚
      </div>
    </div>

    <div class="code-container">
      <span class="code-label">Python / PyTorch â€” ç®€åŒ–çš„ç½‘æ ¼å·ç§¯</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
import torch.nn as nn

# ============================================================
# ç®€åŒ–çš„ç½‘æ ¼å·ç§¯ï¼ˆMoNet é£æ ¼ï¼‰
# ============================================================
class SimpleMeshConv(nn.Module):
    """
    ç®€åŒ–çš„ MoNet é£æ ¼ç½‘æ ¼å·ç§¯ã€‚
    ä½¿ç”¨å¯å­¦ä¹ çš„é«˜æ–¯æ ¸ä½œä¸º patch operatorã€‚
    
    å¯¹åº” GDL Â§5.6 çš„æ–¹ç¨‹ã€‚
    """
    def __init__(self, in_channels, out_channels, n_kernels=8):
        super().__init__()
        self.n_kernels = n_kernels
        
        # å¯å­¦ä¹ çš„é«˜æ–¯å‚æ•° (å‡å€¼å’Œæ–¹å·®)
        self.mu = nn.Parameter(torch.randn(n_kernels, 2))  # (K, 2) for (r, Î¸)
        self.sigma = nn.Parameter(torch.ones(n_kernels, 2))
        
        # æ»¤æ³¢å™¨æƒé‡
        self.weight = nn.Linear(in_channels * n_kernels, out_channels)
    
    def gaussian_kernel(self, coords):
        """
        è®¡ç®—é«˜æ–¯æ ¸å€¼ã€‚
        coords: (E, 2) â€” é‚»å±…çš„ (r, Î¸) åæ ‡
        è¿”å›: (E, K) â€” æ¯ä¸ªæ ¸å¯¹æ¯æ¡è¾¹çš„æƒé‡
        """
        # coords: (E, 2), mu: (K, 2)
        diff = coords.unsqueeze(1) - self.mu.unsqueeze(0)  # (E, K, 2)
        weights = torch.exp(-0.5 * (diff ** 2 / (self.sigma.unsqueeze(0) ** 2 + 1e-6)).sum(dim=-1))
        return weights  # (E, K)
    
    def forward(self, x, edge_index, pseudo_coords):
        """
        x: (N, C_in) èŠ‚ç‚¹ç‰¹å¾
        edge_index: (2, E) è¾¹ç´¢å¼•
        pseudo_coords: (E, 2) è¾¹çš„ä¼ªåæ ‡ (r, Î¸)
        """
        src, dst = edge_index
        
        # è®¡ç®—æ ¸æƒé‡
        kernel_weights = self.gaussian_kernel(pseudo_coords)  # (E, K)
        
        # åŠ æƒé‚»å±…ç‰¹å¾
        neighbor_feats = x[src]  # (E, C_in)
        # (E, K, 1) * (E, 1, C_in) â†’ (E, K, C_in) â†’ (E, K*C_in)
        weighted = (kernel_weights.unsqueeze(-1) * neighbor_feats.unsqueeze(1))
        weighted = weighted.reshape(-1, self.n_kernels * x.size(1))
        
        # èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹
        out = torch.zeros(x.size(0), weighted.size(1), device=x.device)
        out.scatter_add_(0, dst.unsqueeze(1).expand_as(weighted), weighted)
        
        return self.weight(out)


if __name__ == "__main__":
    N, E, C_in, C_out = 100, 400, 16, 32
    layer = SimpleMeshConv(C_in, C_out, n_kernels=8)
    x = torch.randn(N, C_in)
    edge_index = torch.randint(0, N, (2, E))
    pseudo = torch.randn(E, 2)  # (r, Î¸) åæ ‡
    
    out = layer(x, edge_index, pseudo)
    print(f"Mesh Conv: ({N}, {C_in}) â†’ ({out.shape[0]}, {out.shape[1]})")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in layer.parameters()):,}")</code></pre>
    </div>


    <!-- ====================== 5.7 RNN ====================== -->
    <h2 id="sec5-7">5.7 å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)<br><span style="font-size:0.65em;color:var(--text-secondary)">Recurrent Neural Networks</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„è®¨è®ºéƒ½å‡è®¾è¾“å…¥æ˜¯çº¯ç©ºé—´çš„ã€‚ä½†åœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œè¾“å…¥ä¹Ÿå¯ä»¥æ˜¯<strong>åºåˆ—</strong>ï¼ˆè§†é¢‘ã€æ–‡æœ¬ã€è¯­éŸ³ï¼‰ã€‚RNN æ˜¯å¤„ç†æ—¶åºæ•°æ®çš„ç»å…¸æ¶æ„ï¼Œè€Œä¸”ä»å‡ ä½•æ·±åº¦å­¦ä¹ çš„è§’åº¦ï¼Œå®ƒä»¬å®ç°äº†ä¸€ç§ä¸å¯»å¸¸çš„å¯¹ç§°æ€§â€”â€”<span class="term">æ—¶é—´å¹³ç§»ç­‰å˜æ€§</span>ã€‚</p>
      </div>
      <div class="en">
        RNNs are classic architectures for sequential data. From the GDL perspective, they implement an unusual type of symmetry over temporal grids: translation equivariance in time.
      </div>
    </div>

    <h3 id="rnn-equivariance">RNN çš„æ—¶é—´ç­‰å˜æ€§</h3>

    <p>RNN åœ¨æ¯ä¸€æ­¥è®¡ç®—æ‘˜è¦å‘é‡ $h^{(t)}$ï¼š</p>

    <div class="math-block">
      $$h^{(t)} = R(z^{(t)}, h^{(t-1)}) \tag{38}$$
      <div class="math-explain">
        $z^{(t)}$ æ˜¯ç¬¬ $t$ æ­¥çš„è¾“å…¥ç‰¹å¾ï¼Œ$h^{(t)}$ æ˜¯åˆ°ç¬¬ $t$ æ­¥ä¸ºæ­¢çš„"æ‘˜è¦"ï¼Œ$R$ æ˜¯<strong>å…±äº«çš„</strong>æ›´æ–°å‡½æ•°ã€‚å¯¹äº SimpleRNNï¼š$h^{(t)} = \sigma(Wz^{(t)} + Uh^{(t-1)} + b)$ã€‚
      </div>
    </div>

    <div class="figure">
      <img src="../assets/ch5_p95_img0.png" alt="RNN processing video">
      <figcaption>å›¾ 19ï¼šç”¨ RNN å¤„ç†è§†é¢‘è¾“å…¥ã€‚æ¯å¸§ $X^{(t)}$ ç”±å…±äº«ç¼–ç å™¨ $f$ï¼ˆå¦‚ CNNï¼‰å¤„ç†ä¸º $z^{(t)}$ï¼Œç„¶å RNN æ›´æ–°å‡½æ•° $R$ è¿­ä»£æ›´æ–°æ‘˜è¦å‘é‡ã€‚</figcaption>
    </div>

    <div class="derivation">
      <h4>æ—¶é—´å¹³ç§»ç­‰å˜æ€§çš„åˆ†æ</h4>
      <div class="step"><span class="step-num">1</span>è®¾ $z'^{(t)} = z^{(t+1)}$ï¼ˆå·¦ç§»ä¸€æ­¥ï¼‰ã€‚å¸Œæœ› $h'^{(t)} = h^{(t+1)}$ã€‚</div>
      <div class="step"><span class="step-num">2</span>ä½† $h'^{(1)} = R(z'^{(1)}, h^{(0)}) = R(z^{(2)}, h^{(0)})$ï¼Œè€Œ $h^{(2)} = R(z^{(2)}, R(z^{(1)}, h^{(0)}))$ã€‚</div>
      <div class="step"><span class="step-num">3</span>é™¤é $h^{(0)} = R(z^{(1)}, h^{(0)})$ï¼Œå¦åˆ™ç­‰å˜æ€§ä¸æˆç«‹ã€‚</div>
      <div class="step"><span class="step-num">4</span><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼šå¯¹åºåˆ—åšè¶³å¤Ÿçš„é›¶å¡«å……ï¼Œä½¿ $h^{(0)}$ æˆä¸º $\gamma(h) = R(0, h)$ çš„<strong>ä¸åŠ¨ç‚¹</strong>ã€‚å¦‚æœ $\gamma$ æ˜¯å‹ç¼©æ˜ å°„ï¼Œè¿­ä»£å¿…æ”¶æ•›åˆ°å”¯ä¸€ä¸åŠ¨ç‚¹ã€‚</div>
    </div>

    <h3 id="rnn-vanishing">æ¢¯åº¦æ¶ˆå¤±é—®é¢˜</h3>

    <p>RNN çš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š<strong>æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸</strong>ã€‚</p>

    <div class="callout callout-warning">
      <h4>ä¸ºä»€ä¹ˆ SimpleRNN æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Ÿ</h4>
      <p>è€ƒè™‘ sigmoid æ¿€æ´» $\sigma$ï¼ˆå¦‚ tanhï¼‰ï¼Œå…¶å¯¼æ•° $|\sigma'| \in (0, 1)$ã€‚ç»è¿‡ $T$ æ­¥åå‘ä¼ æ’­ï¼š</p>
      $$\frac{\partial h^{(T)}}{\partial h^{(1)}} = \prod_{t=1}^{T-1} \frac{\partial h^{(t+1)}}{\partial h^{(t)}} \approx \prod_{t=1}^{T-1} \sigma'(\cdot) \cdot U$$
      <p>å¾ˆå¤š $|\sigma'| < 1$ çš„å€¼è¿ä¹˜ â†’ æ¢¯åº¦æŒ‡æ•°è¡°å‡ â†’ åºåˆ—å¼€å¤´çš„ä¿¡æ¯<strong>æ— æ³•å½±å“</strong>æœ«å°¾çš„æ¢¯åº¦æ›´æ–°ã€‚</p>
      <p><strong>ä¾‹å­</strong>ï¼š"Petar is Serbian. He was born on ...[å¾ˆé•¿ä¸€æ®µ]... Petar currently lives in ___"ã€‚é¢„æµ‹ "Serbia" éœ€è¦å›æº¯åˆ°åºåˆ—å¼€å¤´ï¼Œä½†æ¢¯åº¦å·²ç»æ¶ˆå¤±äº†ã€‚</p>
    </div>

    <div class="code-container">
      <span class="code-label">Python / PyTorch â€” SimpleRNN + æ¢¯åº¦åˆ†æ</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

# ============================================================
# SimpleRNN å®ç°
# ============================================================
class SimpleRNN(nn.Module):
    """
    h^(t) = Ïƒ(W z^(t) + U h^(t-1) + b)
    
    å‡ ä½•è§†è§’: è¿™æ˜¯æ—¶é—´ç½‘æ ¼ Z ä¸Šçš„"å·ç§¯"æ“ä½œï¼Œ
    ä½†ä¸ CNN ä¸åŒï¼Œå‚æ•°å…±äº«çš„æ˜¯ W, U, bï¼ˆè€Œéæ»¤æ³¢å™¨ï¼‰ã€‚
    """
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.W = nn.Linear(input_dim, hidden_dim)
        self.U = nn.Linear(hidden_dim, hidden_dim, bias=False)
    
    def forward(self, x_seq):
        """
        x_seq: (B, T, input_dim)
        è¿”å›: (B, T, hidden_dim) â€” æ¯æ­¥çš„ h^(t)
        """
        B, T, _ = x_seq.shape
        h = torch.zeros(B, self.hidden_dim, device=x_seq.device)
        outputs = []
        
        for t in range(T):
            h = torch.tanh(self.W(x_seq[:, t]) + self.U(h))
            outputs.append(h)
        
        return torch.stack(outputs, dim=1)


# ============================================================
# æ¢¯åº¦æ¶ˆå¤±å¯è§†åŒ–
# ============================================================
def analyze_gradient_vanishing():
    """åˆ†æ SimpleRNN ä¸­çš„æ¢¯åº¦æ¶ˆå¤±"""
    torch.manual_seed(42)
    
    rnn = SimpleRNN(input_dim=10, hidden_dim=32)
    x = torch.randn(1, 50, 10, requires_grad=True)  # 50 æ­¥åºåˆ—
    
    outputs = rnn(x)
    
    # å¯¹æœ€åä¸€æ­¥çš„è¾“å‡ºæ±‚å’Œå¹¶åå‘ä¼ æ’­
    loss = outputs[:, -1].sum()
    loss.backward()
    
    # æ£€æŸ¥æ¯ä¸€æ­¥çš„æ¢¯åº¦èŒƒæ•°
    grad_norms = []
    h = torch.zeros(1, 32)
    
    for t in range(50):
        h = torch.tanh(rnn.W(x[:, t]) + rnn.U(h))
    
    print("SimpleRNN æ¢¯åº¦åˆ†æ:")
    print(f"è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x.grad.norm(dim=-1).squeeze()[:10].tolist()}")
    print("â†’ æ³¨æ„å‰é¢æ­¥éª¤çš„æ¢¯åº¦æ¯”åé¢æ­¥éª¤å°å¾—å¤šï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰")
    print("â†’ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦ LSTM çš„é—¨æ§æœºåˆ¶ï¼")


if __name__ == "__main__":
    rnn = SimpleRNN(10, 32)
    x = torch.randn(4, 20, 10)
    out = rnn(x)
    print(f"SimpleRNN: {x.shape} â†’ {out.shape}")
    analyze_gradient_vanishing()</code></pre>
    </div>


    <!-- ====================== 5.8 LSTM ====================== -->
    <h2 id="sec5-8">5.8 é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM)<br><span style="font-size:0.65em;color:var(--text-secondary)">Long Short-Term Memory Networks</span></h2>

    <div class="bilingual">
      <div class="zh">
        <p>LSTM (Hochreiter & Schmidhuber, 1997) é€šè¿‡å¼•å…¥<span class="term">é—¨æ§æœºåˆ¶</span>ï¼ˆgating mechanismsï¼‰å¤§å¹…ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚é—¨æ§å…è®¸ç½‘ç»œä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼<strong>é€‰æ‹©æ€§åœ°è¦†å†™</strong>ä¿¡æ¯ã€‚</p>
      </div>
      <div class="en">
        LSTM significantly reduces vanishing gradients through gating mechanisms, which allow the network to selectively overwrite information in a data-driven way.
      </div>
    </div>

    <h3 id="lstm-gates">LSTM çš„é—¨æ§æœºåˆ¶</h3>

    <div class="figure">
      <img src="../assets/ch5_p95_img1.png" alt="LSTM dataflow">
      <figcaption>å›¾ 20ï¼šLSTM çš„æ•°æ®æµï¼ŒåŒ…å«è®°å¿†ç»†èƒ (M) å’Œä¸‰ä¸ªé—¨ï¼šè¾“å…¥é—¨ $i^{(t)}$ã€é—å¿˜é—¨ $f^{(t)}$ã€è¾“å‡ºé—¨ $o^{(t)}$ã€‚</figcaption>
    </div>

    <p>LSTM åœ¨ SimpleRNN çš„åŸºç¡€ä¸Šå¼•å…¥äº†<strong>è®°å¿†ç»†èƒ</strong>ï¼ˆmemory cellï¼‰$c^{(t)}$ï¼Œé€šè¿‡ä¸‰ä¸ª<strong>é—¨</strong>æ§åˆ¶ä¿¡æ¯æµï¼š</p>

    <div class="math-block">
      $$\tilde{c}^{(t)} = \tanh(W_c z^{(t)} + U_c h^{(t-1)} + b_c) \tag{43 â€” å€™é€‰ç‰¹å¾}$$
      $$i^{(t)} = \text{logistic}(W_i z^{(t)} + U_i h^{(t-1)} + b_i) \tag{44 â€” è¾“å…¥é—¨}$$
      $$f^{(t)} = \text{logistic}(W_f z^{(t)} + U_f h^{(t-1)} + b_f) \tag{45 â€” é—å¿˜é—¨}$$
      $$o^{(t)} = \text{logistic}(W_o z^{(t)} + U_o h^{(t-1)} + b_o) \tag{46 â€” è¾“å‡ºé—¨}$$
      $$c^{(t)} = i^{(t)} \odot \tilde{c}^{(t)} + f^{(t)} \odot c^{(t-1)} \tag{47 â€” ç»†èƒçŠ¶æ€æ›´æ–°}$$
      $$h^{(t)} = o^{(t)} \odot \tanh(c^{(t)}) \tag{48 â€” è¾“å‡º}$$
    </div>

    <div class="symbol-table">
      <h4>LSTM ç¬¦å·è¡¨</h4>
      <table>
        <thead><tr><th>ç¬¦å·</th><th>å«ä¹‰</th><th>èŒƒå›´</th></tr></thead>
        <tbody>
          <tr><td>$c^{(t)}$</td><td>è®°å¿†ç»†èƒçŠ¶æ€</td><td>$\mathbb{R}^m$</td></tr>
          <tr><td>$\tilde{c}^{(t)}$</td><td>å€™é€‰ç‰¹å¾</td><td>$[-1, 1]^m$</td></tr>
          <tr><td>$i^{(t)}$</td><td>è¾“å…¥é—¨ï¼šå…è®¸å¤šå°‘å€™é€‰è¿›å…¥ç»†èƒ</td><td>$[0, 1]^m$</td></tr>
          <tr><td>$f^{(t)}$</td><td>é—å¿˜é—¨ï¼šä¿ç•™å¤šå°‘æ—§ç»†èƒçŠ¶æ€</td><td>$[0, 1]^m$</td></tr>
          <tr><td>$o^{(t)}$</td><td>è¾“å‡ºé—¨ï¼šæš´éœ²å¤šå°‘æ–°ç»†èƒçŠ¶æ€</td><td>$[0, 1]^m$</td></tr>
          <tr><td>$\odot$</td><td>é€å…ƒç´ ä¹˜æ³•ï¼ˆHadamard ä¹˜ç§¯ï¼‰</td><td>â€”</td></tr>
          <tr><td>logistic</td><td>$\sigma(x) = \frac{1}{1 + e^{-x}}$</td><td>$[0, 1]$</td></tr>
        </tbody>
      </table>
    </div>

    <div class="callout callout-key">
      <h4>ä¸ºä»€ä¹ˆ LSTM è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ</h4>
      <p>å…³é”®åœ¨å…¬å¼ (47)ï¼š$c^{(t)} = i^{(t)} \odot \tilde{c}^{(t)} + f^{(t)} \odot c^{(t-1)}$ã€‚</p>
      <ul>
        <li>å½“ $f^{(t)} \approx 1$ æ—¶ï¼Œ$c^{(t)} \approx c^{(t-1)}$ï¼Œæ¢¯åº¦<strong>ç›´æ¥æµè¿‡</strong>ï¼ˆç±»ä¼¼ ResNet çš„è·³è·ƒè¿æ¥ï¼‰</li>
        <li>$f^{(t)}$ çš„å€¼ç›´æ¥å‡ºç°åœ¨åå‘ä¼ æ’­çš„å¯¼æ•°ä¸­ï¼Œç½‘ç»œå¯ä»¥<strong>å­¦ä¹ </strong>æ§åˆ¶æ¢¯åº¦çš„è¡°å‡ç¨‹åº¦</li>
        <li>å®è·µå»ºè®®ï¼šåˆå§‹åŒ–é—å¿˜é—¨åç½® $b_f$ ä¸ºæ­£å€¼ï¼ˆå¦‚ 1ï¼‰ï¼Œä½¿åˆå§‹ $f^{(t)} \approx 1$ï¼Œå€¾å‘äºè®°å¿†</li>
      </ul>
    </div>

    <h3 id="lstm-time-warp">æ—¶é—´æ‰­æ›²ä¸å˜æ€§ â€” LSTM çš„å‡ ä½•è§£é‡Š</h3>

    <p>é—¨æ§ RNN çš„ä¸€ä¸ªæ·±å±‚æ€§è´¨ï¼šå®ƒä»¬å¯¹<strong>æ—¶é—´æ‰­æ›²å˜æ¢</strong>ï¼ˆtime warpingï¼‰ä¸å˜ã€‚è¿™ä¸ªæ´å¯Ÿæ¥è‡ª Tallec & Ollivier (2018)ã€‚</p>

    <div class="blueprint">
      <h4>æ—¶é—´æ‰­æ›²ä¸å˜çš„ RNN</h4>
      <p>æ—¶é—´æ‰­æ›² $\tau: \mathbb{R}^+ \to \mathbb{R}^+$ æ˜¯å•è°ƒé€’å¢çš„å¾®åˆ†æ˜ å°„ã€‚è¦æ±‚ RNN ç±»å¯¹æ—¶é—´æ‰­æ›²ä¸å˜æ„å‘³ç€ï¼šå¯¹ä»»ä½• $\tau$ å’Œæ¨¡å‹å‚æ•°ï¼Œå­˜åœ¨<strong>å¦ä¸€ç»„å‚æ•°</strong>ä½¿å¾—æ¨¡å‹åœ¨æ‰­æ›²åçš„æ•°æ®ä¸Šè¡¨ç°ç›¸åŒã€‚</p>
      <p>é€šè¿‡è¿ç»­æ—¶é—´åˆ†æï¼Œä¸å˜æ€§è¦æ±‚ RNN æ›´æ–°æ»¡è¶³ï¼š</p>
      $$h^{(t+1)} = \Gamma(z^{(t+1)}, h^{(t)}) R(z^{(t+1)}, h^{(t)}) + (1 - \Gamma(z^{(t+1)}, h^{(t)})) h^{(t)} \tag{53}$$
      <p>å…¶ä¸­ $\Gamma$ è¾“å‡ºåœ¨ $(0, 1)$ èŒƒå›´â€”â€”<strong>è¿™æ­£æ˜¯ LSTM çš„é—¨æ§ï¼</strong></p>
    </div>

    <div class="derivation">
      <h4>ä»æ—¶é—´æ‰­æ›²ä¸å˜æ€§æ¨å¯¼å‡º LSTM é—¨æ§</h4>
      <div class="step"><span class="step-num">1</span><strong>è¿ç»­æ—¶é—´ RNN</strong>ï¼š$\frac{dh(\tau(t))}{d\tau(t)} = R(z(\tau(t+1)), h(\tau(t))) - h(\tau(t))$</div>
      <div class="step"><span class="step-num">2</span><strong>é“¾å¼æ³•åˆ™</strong>ï¼š$\frac{dh(\tau(t))}{dt} = \frac{d\tau(t)}{dt} \left[R(\cdots) - h(\tau(t))\right]$</div>
      <div class="step"><span class="step-num">3</span>$\frac{d\tau(t)}{dt}$ æœªçŸ¥ â†’ å¼•å…¥å¯å­¦ä¹ å‡½æ•° $\Gamma$ æ¥è¿‘ä¼¼å®ƒ</div>
      <div class="step"><span class="step-num">4</span>Taylor å±•å¼€ + ç¦»æ•£åŒ– â†’ $h^{(t+1)} = \Gamma \cdot R + (1 - \Gamma) \cdot h^{(t)}$</div>
      <div class="step"><span class="step-num">5</span>çº¦æŸ $0 < \Gamma < 1$ï¼ˆå› ä¸º $\tau$ å•è°ƒä¸” $\frac{d\tau}{dt} \leq 1$ï¼‰â†’ ä½¿ç”¨ logistic sigmoid â†’ <strong>LSTM çš„é—¨ï¼</strong></div>
    </div>

    <div class="callout callout-info">
      <h4>Chrono åˆå§‹åŒ–</h4>
      <p>å¦‚æœæˆ‘ä»¬å…³å¿ƒçš„ä¾èµ–èŒƒå›´æ˜¯ $[T_l, T_h]$ æ­¥ï¼Œé—¨æ§å€¼åº”åœ¨ $[\frac{1}{T_h}, \frac{1}{T_l}]$ èŒƒå›´å†…ã€‚åˆ©ç”¨ $E[\Gamma] \approx \text{logistic}(b_\Gamma)$ï¼Œå¯ä»¥åˆå§‹åŒ–ï¼š</p>
      $$b_\Gamma \sim -\log(\mathcal{U}(T_l, T_h) - 1)$$
      <p>è¿™ç§ "chrono åˆå§‹åŒ–" èƒ½æ˜¾è‘—æ”¹å–„é•¿ç¨‹ä¾èµ–å»ºæ¨¡ã€‚</p>
    </div>

    <div class="code-container">
      <span class="code-label">Python / PyTorch â€” LSTM å®Œæ•´å®ç° + é—¨æ§å¯è§†åŒ–</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

# ============================================================
# LSTM ä»é›¶å®ç°
# ============================================================
class LSTMFromScratch(nn.Module):
    """
    LSTM å®Œæ•´å®ç°ï¼ˆå¯¹åº” GDL å…¬å¼ 43-48ï¼‰ã€‚
    
    å‡ ä½•è§†è§’:
    - é—¨æ§æœºåˆ¶ = æ—¶é—´æ‰­æ›²å¯¼æ•° dÏ„/dt çš„å­¦ä¹ 
    - é—å¿˜é—¨ â†’ æ§åˆ¶ "è®°å¿†æ—¶é—´å°ºåº¦"
    - Chrono åˆå§‹åŒ– â†’ æ§åˆ¶æœ‰æ•ˆè®°å¿†èŒƒå›´
    """
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # å››ä¸ªçº¿æ€§å±‚ï¼ˆå¯ä»¥åˆå¹¶ä¸ºä¸€ä¸ªå¤§çŸ©é˜µä»¥åŠ é€Ÿï¼‰
        # å€™é€‰ç‰¹å¾
        self.W_c = nn.Linear(input_dim, hidden_dim)
        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)
        # è¾“å…¥é—¨
        self.W_i = nn.Linear(input_dim, hidden_dim)
        self.U_i = nn.Linear(hidden_dim, hidden_dim, bias=False)
        # é—å¿˜é—¨ï¼ˆåˆå§‹åŒ–åç½®ä¸º 1ï¼Œå€¾å‘è®°å¿†ï¼‰
        self.W_f = nn.Linear(input_dim, hidden_dim)
        self.U_f = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.W_f.bias.data.fill_(1.0)  # â† å…³é”®åˆå§‹åŒ–ï¼
        # è¾“å‡ºé—¨
        self.W_o = nn.Linear(input_dim, hidden_dim)
        self.U_o = nn.Linear(hidden_dim, hidden_dim, bias=False)
    
    def forward(self, x_seq, initial_state=None):
        """
        x_seq: (B, T, input_dim)
        è¿”å›: outputs (B, T, hidden_dim), (h_final, c_final)
        """
        B, T, _ = x_seq.shape
        
        if initial_state is None:
            h = torch.zeros(B, self.hidden_dim, device=x_seq.device)
            c = torch.zeros(B, self.hidden_dim, device=x_seq.device)
        else:
            h, c = initial_state
        
        outputs = []
        gate_history = {'input': [], 'forget': [], 'output': []}
        
        for t in range(T):
            z = x_seq[:, t]
            
            # å…¬å¼ 43: å€™é€‰ç‰¹å¾
            c_tilde = torch.tanh(self.W_c(z) + self.U_c(h))
            
            # å…¬å¼ 44: è¾“å…¥é—¨
            i = torch.sigmoid(self.W_i(z) + self.U_i(h))
            
            # å…¬å¼ 45: é—å¿˜é—¨
            f = torch.sigmoid(self.W_f(z) + self.U_f(h))
            
            # å…¬å¼ 46: è¾“å‡ºé—¨
            o = torch.sigmoid(self.W_o(z) + self.U_o(h))
            
            # å…¬å¼ 47: ç»†èƒçŠ¶æ€æ›´æ–°
            c = i * c_tilde + f * c  # â† æ ¸å¿ƒï¼æ¢¯åº¦å¯ä»¥é€šè¿‡ f ç›´æ¥æµè¿‡
            
            # å…¬å¼ 48: è¾“å‡º
            h = o * torch.tanh(c)
            
            outputs.append(h)
            gate_history['input'].append(i.mean().item())
            gate_history['forget'].append(f.mean().item())
            gate_history['output'].append(o.mean().item())
        
        return torch.stack(outputs, dim=1), (h, c), gate_history


# ============================================================
# å¯¹æ¯” SimpleRNN vs LSTM çš„æ¢¯åº¦
# ============================================================
def compare_gradient_flow():
    """å¯¹æ¯” SimpleRNN å’Œ LSTM çš„æ¢¯åº¦æµ"""
    torch.manual_seed(42)
    
    T = 100  # é•¿åºåˆ—
    
    # LSTM
    lstm = LSTMFromScratch(10, 32)
    x = torch.randn(1, T, 10, requires_grad=True)
    outputs, _, gates = lstm(x)
    loss = outputs[:, -1].sum()
    loss.backward()
    lstm_grad = x.grad.norm(dim=-1).squeeze()
    
    print("LSTM vs SimpleRNN æ¢¯åº¦åˆ†æ (T=100):")
    print(f"LSTM å‰10æ­¥æ¢¯åº¦èŒƒæ•°:  {lstm_grad[:10].tolist()}")
    print(f"LSTM å10æ­¥æ¢¯åº¦èŒƒæ•°:  {lstm_grad[-10:].tolist()}")
    print(f"LSTM æ¢¯åº¦æ¯”å€¼ (é¦–/æœ«): {lstm_grad[0] / lstm_grad[-1]:.4f}")
    print(f"å¹³å‡é—å¿˜é—¨å€¼: {sum(gates['forget'])/len(gates['forget']):.4f}")
    print("â†’ LSTM çš„æ¢¯åº¦è¡°å‡è¿œæ…¢äº SimpleRNN âœ…")


if __name__ == "__main__":
    lstm = LSTMFromScratch(10, 32)
    x = torch.randn(4, 50, 10)
    out, (h, c), gates = lstm(x)
    print(f"LSTM: {x.shape} â†’ {out.shape}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in lstm.parameters()):,}")
    print()
    compare_gradient_flow()</code></pre>
    </div>


    <!-- ====================== 5.3 EXTENDED: DEEP DIVE ====================== -->
    <h2 id="gnn-deep-dive">5.3+ GNN æ·±åº¦è¿›é˜¶ï¼šè¿‡å¹³æ»‘ã€è¿‡å‹ç¼©ä¸è¡¨è¾¾åŠ›è¾¹ç•Œ</h2>

    <h3>è¿‡å¹³æ»‘é—®é¢˜ (Over-smoothing)</h3>

    <div class="callout callout-warning">
      <h4>GNN çš„æ·±åº¦é™åˆ¶</h4>
      <p>ä¸ CNN ä¸åŒï¼ŒGNN <strong>ä¸èƒ½ç®€å•åœ°å †å å¾ˆå¤šå±‚</strong>ã€‚åŸå› ï¼šæ¯ä¸€å±‚æ¶ˆæ¯ä¼ é€’éƒ½åœ¨é‚»åŸŸä¸­åš"å¹³å‡"ï¼Œå¤šå±‚ä¹‹åæ‰€æœ‰èŠ‚ç‚¹çš„è¡¨ç¤ºè¶‹äºç›¸åŒâ€”â€”è¿™å°±æ˜¯<strong>è¿‡å¹³æ»‘</strong>ï¼ˆover-smoothingï¼‰ã€‚</p>
    </div>

    <div class="math-block">
      $$\text{GCN å±‚å¯ä»¥è§†ä¸º: } \mathbf{H}^{(l+1)} = \hat{A} \mathbf{H}^{(l)} \mathbf{W}^{(l)}$$
      <div class="math-explain">
        åå¤å·¦ä¹˜ $\hat{A}$ï¼ˆå½’ä¸€åŒ–é‚»æ¥çŸ©é˜µï¼‰ç­‰æ•ˆäºåœ¨å›¾ä¸Šåš<strong>æ‰©æ•£</strong>ã€‚$\hat{A}^k$ çš„æé™æ˜¯ä¸èŠ‚ç‚¹åº¦æ•°æˆæ­£æ¯”çš„å¹³ç¨³åˆ†å¸ƒâ€”â€”æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾è¶‹åŒã€‚
      </div>
    </div>

    <div class="derivation">
      <h4>è¿‡å¹³æ»‘çš„æ•°å­¦è¯æ˜ï¼ˆç®€åŒ–ç‰ˆï¼‰</h4>
      <div class="step"><span class="step-num">1</span>$\hat{A}$ æ˜¯è¡ŒéšæœºçŸ©é˜µï¼ˆæ¯è¡Œå’Œä¸º 1ï¼‰ï¼Œå…¶ç‰¹å¾å€¼ $|\lambda_i| \leq 1$ã€‚</div>
      <div class="step"><span class="step-num">2</span>å¯¹è¿é€šå›¾ï¼Œæœ€å¤§ç‰¹å¾å€¼ $\lambda_1 = 1$ å¯¹åº”å¸¸å‘é‡ $\mathbf{1}$ï¼Œå…¶ä½™ $|\lambda_i| < 1$ã€‚</div>
      <div class="step"><span class="step-num">3</span>$\hat{A}^k \mathbf{H} = \sum_i \lambda_i^k \mathbf{v}_i \mathbf{v}_i^\top \mathbf{H} \xrightarrow{k \to \infty} \mathbf{v}_1 \mathbf{v}_1^\top \mathbf{H}$</div>
      <div class="step"><span class="step-num">4</span>æé™ä¸‹æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ = å…¨å±€å¹³å‡çš„æŸä¸ªæŠ•å½± â†’ <strong>æ— æ³•åŒºåˆ†ä¸åŒèŠ‚ç‚¹</strong>ã€‚</div>
    </div>

    <p><strong>ç¼“è§£è¿‡å¹³æ»‘çš„ç­–ç•¥</strong>ï¼š</p>
    <ul>
      <li><strong>æ®‹å·®è¿æ¥</strong>ï¼š$h_i^{(l+1)} = h_i^{(l)} + \text{GNN}(h_i^{(l)})$ â€” æˆ‘ä»¬çš„ DynamicalGNN å°±ä½¿ç”¨äº†è¿™ä¸ª</li>
      <li><strong>JumpingKnowledge</strong>ï¼šæ‹¼æ¥æ‰€æœ‰å±‚çš„è¾“å‡º $h_i = [h_i^{(1)} \| h_i^{(2)} \| \cdots \| h_i^{(L)}]$</li>
      <li><strong>DropEdge</strong>ï¼šè®­ç»ƒæ—¶éšæœºåˆ é™¤è¾¹</li>
      <li><strong>PairNorm / NodeNorm</strong>ï¼šé˜²æ­¢ç‰¹å¾è¿‡äºç›¸ä¼¼</li>
    </ul>

    <h3>è¿‡å‹ç¼©é—®é¢˜ (Over-squashing)</h3>

    <p>è¿‡å‹ç¼©æ˜¯å¦ä¸€ä¸ª GNN çš„æ ¸å¿ƒé—®é¢˜ï¼š<strong>è¿œè·ç¦»èŠ‚ç‚¹çš„ä¿¡æ¯è¢«å‹ç¼©åˆ°å›ºå®šå¤§å°çš„å‘é‡ä¸­</strong>ï¼Œå¯¼è‡´é•¿ç¨‹ä¾èµ–æ— æ³•æœ‰æ•ˆä¼ æ’­ã€‚</p>

    <div class="math-block">
      $$\left|\frac{\partial h_u^{(L)}}{\partial x_v^{(0)}}\right| \leq c \cdot (\hat{A}^L)_{uv}$$
      <div class="math-explain">
        $L$ å±‚ GNN åï¼ŒèŠ‚ç‚¹ $u$ å¯¹èŠ‚ç‚¹ $v$ çš„æ•æ„Ÿåº¦ä»¥é‚»æ¥çŸ©é˜µçš„ $L$ æ¬¡å¹‚ä¸ºä¸Šç•Œã€‚å¦‚æœ $u, v$ è·ç¦»è¿œä¸”å›¾çš„ç“¶é¢ˆä¸¥é‡ï¼ˆå¦‚æ ‘ç»“æ„ï¼‰ï¼Œ$(\hat{A}^L)_{uv}$ æŒ‡æ•°è¡°å‡ã€‚è¿™ä¸ RNN çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æœ¬è´¨ç›¸åŒï¼
      </div>
    </div>

    <div class="callout callout-project">
      <h4>PhysRobot ä¸­çš„è¿‡å¹³æ»‘/è¿‡å‹ç¼©</h4>
      <p>åœ¨æˆ‘ä»¬çš„ç²’å­ä»¿çœŸä¸­ï¼š</p>
      <ul>
        <li><strong>3 å±‚æ¶ˆæ¯ä¼ é€’</strong>ï¼ˆ<code>n_message_passing=3</code>ï¼‰æ˜¯ç²¾å¿ƒé€‰æ‹©çš„â€”â€”å¤šäº†ä¼šè¿‡å¹³æ»‘ï¼Œå°‘äº†ä¿¡æ¯ä¼ ä¸è¿œ</li>
        <li><strong>æ®‹å·®è¿æ¥</strong>ï¼ˆ<code>x_new = ... + x</code>ï¼‰ç›´æ¥ç¼“è§£è¿‡å¹³æ»‘</li>
        <li>ç²’å­ç³»ç»Ÿçš„äº¤äº’é€šå¸¸æ˜¯<strong>çŸ­ç¨‹</strong>çš„ï¼ˆå¼•åŠ›ã€å¼¹åŠ›ï¼‰ï¼Œæ‰€ä»¥ 3 å±‚è¶³å¤Ÿ</li>
        <li>å¦‚æœéœ€è¦é•¿ç¨‹äº¤äº’ï¼Œå¯ä»¥è€ƒè™‘ Transformer é£æ ¼çš„å…¨å±€æ³¨æ„åŠ›è¡¥å……</li>
      </ul>
    </div>

    <h3>WL æµ‹è¯•ä¸ GNN è¡¨è¾¾åŠ›ä¸Šç•Œ</h3>

    <div class="blueprint">
      <h4>Weisfeiler-Leman (1-WL) å›¾åŒæ„æµ‹è¯•</h4>
      <p>1-WL æµ‹è¯•é€šè¿‡è¿­ä»£é¢œè‰²ç»†åŒ–æ¥æ£€æµ‹å›¾åŒæ„ï¼š</p>
      $$c_v^{(k+1)} = \text{HASH}\left(c_v^{(k)}, \{\{c_u^{(k)} : u \in \mathcal{N}(v)\}\}\right)$$
      <p><strong>Xu et al. (2019) çš„æ ¸å¿ƒå®šç†</strong>ï¼šæ¶ˆæ¯ä¼ é€’ GNN çš„åŒºåˆ†èƒ½åŠ›<strong>è‡³å¤š</strong>ç­‰äº 1-WL æµ‹è¯•ã€‚GIN è¾¾åˆ°äº†è¿™ä¸ªä¸Šç•Œã€‚</p>
    </div>

    <div class="callout callout-warning">
      <h4>GNN æ— æ³•åŒºåˆ†çš„å›¾å¯¹</h4>
      <p>å­˜åœ¨ 1-WL æ— æ³•åŒºåˆ†ä½†ä¸åŒæ„çš„å›¾å¯¹ï¼ˆå¦‚æŸäº›æ­£åˆ™å›¾ï¼‰ã€‚è¿™æ„å‘³ç€æ¶ˆæ¯ä¼ é€’ GNN æœ‰<strong>å›ºæœ‰çš„è¡¨è¾¾åŠ›ç“¶é¢ˆ</strong>ã€‚è§£å†³æ–¹æ¡ˆï¼š</p>
      <ul>
        <li><strong>Higher-order GNN</strong>ï¼šä½¿ç”¨ $k$-WL æµ‹è¯•ï¼ˆ$k \geq 2$ï¼‰ï¼Œå¯¹èŠ‚ç‚¹<strong>å…ƒç»„</strong>åšæ¶ˆæ¯ä¼ é€’</li>
        <li><strong>éšæœºç‰¹å¾å¢å¼º</strong>ï¼šä¸ºèŠ‚ç‚¹æ·»åŠ éšæœº ID æ‰“ç ´å¯¹ç§°æ€§</li>
        <li><strong>å­å›¾ GNN</strong>ï¼šå¯¹æ¯ä¸ªèŠ‚ç‚¹æå–å­å›¾å†å¤„ç†</li>
      </ul>
    </div>

    <div class="code-container">
      <span class="code-label">Python â€” WL é¢œè‰²ç»†åŒ–ï¼ˆå›¾åŒæ„æµ‹è¯•ï¼‰</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code>import torch
from collections import Counter

# ============================================================
# Weisfeiler-Leman 1-WL å›¾åŒæ„æµ‹è¯•
# ============================================================
def weisfeiler_leman_test(edge_index_1, n1, edge_index_2, n2, iterations=10):
    """
    1-WL å›¾åŒæ„æµ‹è¯•ã€‚
    
    å¦‚æœè¿”å› Falseï¼Œä¸¤ä¸ªå›¾ä¸€å®šä¸åŒæ„ã€‚
    å¦‚æœè¿”å› Trueï¼Œä¸¤ä¸ªå›¾å¯èƒ½åŒæ„ï¼ˆWL æ— æ³•ç¡®å®šï¼‰ã€‚
    
    ä¸ GNN çš„å…³ç³»: GIN çš„è¡¨è¾¾åŠ› = 1-WL æµ‹è¯•ã€‚
    """
    def build_adj(edge_index, n):
        adj = {i: [] for i in range(n)}
        for s, t in edge_index.T.tolist():
            adj[s].append(t)
        return adj
    
    adj1 = build_adj(edge_index_1, n1)
    adj2 = build_adj(edge_index_2, n2)
    
    if n1 != n2:
        return False, "èŠ‚ç‚¹æ•°ä¸åŒ"
    
    # åˆå§‹é¢œè‰²ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ç›¸åŒï¼‰
    colors1 = [0] * n1
    colors2 = [0] * n2
    
    for k in range(iterations):
        # é¢œè‰²ç»†åŒ–
        new_colors1 = []
        new_colors2 = []
        
        for v in range(n1):
            neighbor_colors = tuple(sorted([colors1[u] for u in adj1[v]]))
            new_colors1.append(hash((colors1[v], neighbor_colors)))
        
        for v in range(n2):
            neighbor_colors = tuple(sorted([colors2[u] for u in adj2[v]]))
            new_colors2.append(hash((colors2[v], neighbor_colors)))
        
        # é‡æ–°ç¼–å·é¢œè‰²
        all_colors = sorted(set(new_colors1 + new_colors2))
        color_map = {c: i for i, c in enumerate(all_colors)}
        colors1 = [color_map[c] for c in new_colors1]
        colors2 = [color_map[c] for c in new_colors2]
        
        # æ¯”è¾ƒé¢œè‰²ç›´æ–¹å›¾
        hist1 = Counter(colors1)
        hist2 = Counter(colors2)
        
        if hist1 != hist2:
            return False, f"ç¬¬ {k+1} è½®åŒºåˆ†æˆåŠŸ"
    
    return True, f"ç»è¿‡ {iterations} è½®æ— æ³•åŒºåˆ†ï¼ˆå¯èƒ½åŒæ„ï¼‰"


# æµ‹è¯•: ä¸‰è§’å½¢ vs ä¸‰æ¡ç‹¬ç«‹è¾¹
triangle = torch.tensor([[0,1,1,2,2,0],[1,0,2,1,0,2]], dtype=torch.long)
three_edges = torch.tensor([[0,1,2,3,4,5],[1,0,3,2,5,4]], dtype=torch.long)

result, msg = weisfeiler_leman_test(triangle, 3, three_edges, 6)
print(f"ä¸‰è§’å½¢ vs ä¸‰æ¡è¾¹: {msg}")

# æµ‹è¯•: ä¸¤ä¸ªåŒæ„çš„å›¾
g1 = torch.tensor([[0,0,1,2],[1,2,2,0]], dtype=torch.long)
g2 = torch.tensor([[1,1,0,2],[0,2,2,1]], dtype=torch.long)  # é‡æ ‡å·
result, msg = weisfeiler_leman_test(g1, 3, g2, 3)
print(f"åŒæ„å›¾æµ‹è¯•: {msg}")</code></pre>
    </div>

    <h3>GNN åœ¨ç‰©ç†ä»¿çœŸä¸­çš„åº”ç”¨æ€»ç»“</h3>

    <div class="callout callout-project">
      <h4>ä» GNN åˆ°ç‰©ç†ä»¿çœŸ â€” PhysRobot çš„å®Œæ•´ç®¡çº¿</h4>
      <p>æˆ‘ä»¬çš„ <code>DynamicalGNN</code> éµå¾ª "Learning to Simulate" (Sanchez-Gonzalez et al., 2020) çš„èŒƒå¼ï¼š</p>
    </div>

    <div class="flow">
      <div class="flow-box">ç²’å­çŠ¶æ€<br><small>$(x_i, v_i)$</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">æ„å›¾<br><small>r < cutoff</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">EdgeFrame<br><small>$e_{ij}$</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">Node Encode<br><small>$h_i^{(0)}$</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">MP Ã—3<br><small>$h_i^{(L)}$</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">Decode<br><small>$a_i$</small></div>
      <div class="flow-arrow">â†’</div>
      <div class="flow-box">ç§¯åˆ†<br><small>$v', x'$</small></div>
    </div>

    <div class="code-container">
      <span class="code-label">PhysRobot â€” å®Œæ•´å‰å‘ä¼ æ’­æµç¨‹ï¼ˆå¯¹ç…§ GDL ç†è®ºï¼‰</span>
      <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ å¤åˆ¶</button>
      <pre><code># å®Œæ•´çš„ DynamicalGNN å‰å‘ä¼ æ’­ï¼Œå¸¦ç†è®ºæ³¨é‡Š
# æºç : physics_core/dynamical_gnn.py

def forward(self, positions, velocities, edge_index, masses=None):
    """
    é¢„æµ‹åŠ é€Ÿåº¦ï¼Œç»™å®šå½“å‰çŠ¶æ€ã€‚
    
    GDL ç†è®ºå¯¹åº”:
    - åŸŸ Î©: ç²’å­ç³»ç»Ÿæ„æˆçš„å›¾ (V=ç²’å­, E=ç›¸äº’ä½œç”¨)
    - å¯¹ç§°ç¾¤: E(3) Ã— Î£_n (æ¬§å‡ é‡Œå¾—+ç½®æ¢)
    - ç­‰å˜æ“ä½œ: EGNN é£æ ¼çš„æ¶ˆæ¯ä¼ é€’
    
    ç®¡çº¿:
    1. EdgeFrame (Â§5.5 EGNN): ç¼–ç ç©ºé—´å…³ç³»ä¸º E(3) ä¸å˜ç‰¹å¾
    2. Node Encoder: åµŒå…¥ç²’å­çŠ¶æ€
    3. Message Passing (Â§5.3 MPNN): ä¼ æ’­ä¿¡æ¯ï¼Œå­¦ä¹ äº¤äº’
    4. Decoder: é¢„æµ‹åŠ é€Ÿåº¦ a = F/mï¼ˆç‰›é¡¿ç¬¬äºŒå®šå¾‹ï¼‰
    """
    # æ­¥éª¤ 1: Edge Frame â€” E(3) ä¸å˜ç‰¹å¾
    # e_ij = Encoder([r_ij, ||r_ij||, v_rel, ||v_rel||])
    # å¯¹åº” EGNN: åªé€šè¿‡è·ç¦» ||x_i - x_j|| ç¼–ç ç©ºé—´ä¿¡æ¯
    edge_features = self.edge_frame(positions, velocities, edge_index)
    
    # æ­¥éª¤ 2: èŠ‚ç‚¹ç¼–ç 
    # å°† (x, v) æ‹¼æ¥ä¸º 6 ç»´è¾“å…¥
    node_states = torch.cat([positions, velocities], dim=-1)  # (N, 6)
    node_features = self.node_encoder(node_states)  # (N, hidden_dim)
    
    # æ­¥éª¤ 3: æ¶ˆæ¯ä¼ é€’ Ã— L å±‚
    # å¯¹åº” Â§5.3 MPNN: x_i' = Î³(x_i, âŠ•_j Ï†(x_i, x_j, e_ij))
    x = node_features
    for mp_layer in self.mp_layers:
        x = mp_layer(x, edge_index, edge_features)
        # æ³¨æ„: æ¯å±‚ä½¿ç”¨ç›¸åŒçš„ edge_featuresï¼ˆä¸æ›´æ–°åæ ‡ï¼‰
        # è¿™ä¸ EGNN ä¸åŒï¼ˆEGNN æ¯å±‚æ›´æ–°åæ ‡ï¼‰
        # æˆ‘ä»¬çš„é€‰æ‹©æ›´é«˜æ•ˆï¼Œå› ä¸ºç‰©ç†ä»¿çœŸä¸­
        # ä¸€æ­¥å†…åæ ‡å˜åŒ–å¾ˆå°
    
    # æ­¥éª¤ 4: è§£ç åŠ é€Ÿåº¦
    # a = Decoder(h_i) âˆˆ RÂ³
    # è¿™æ˜¯ GDL ä¸­çš„ "è¯»å‡º" (readout)
    accelerations = self.decoder(x)  # (N, 3)
    
    return accelerations
    
    # åç»­æ­¥éª¤ï¼ˆä¸åœ¨ GNN å†…éƒ¨ï¼Œç”±ç§¯åˆ†å™¨å®Œæˆï¼‰:
    # v' = v + a * dt          (åŠéšå¼æ¬§æ‹‰)
    # x' = x + v' * dt
    # è¿™å°±å®Œæˆäº†ä¸€ä¸ªå®Œæ•´çš„ç‰©ç†ä»¿çœŸæ­¥ï¼</code></pre>
    </div>

    <h3>GNN å±‚æ•° vs æ„Ÿå—é‡ vs è®¡ç®—ä»£ä»·</h3>

    <div class="comparison-table">
      <table>
        <thead><tr><th>å±‚æ•° $L$</th><th>æ„Ÿå—é‡</th><th>è¿‡å¹³æ»‘é£é™©</th><th>è¿‡å‹ç¼©é£é™©</th><th>è®¡ç®—å¤æ‚åº¦</th><th>é€‚ç”¨åœºæ™¯</th></tr></thead>
        <tbody>
          <tr><td>1</td><td>1-hop é‚»å±…</td><td>æ— </td><td>æ— </td><td>$O(|E| d)$</td><td>å±€éƒ¨ç‰¹å¾</td></tr>
          <tr><td>2</td><td>2-hop é‚»å±…</td><td>ä½</td><td>ä½</td><td>$O(2|E| d)$</td><td>å¤§å¤šæ•°ä»»åŠ¡</td></tr>
          <tr><td><strong>3</strong></td><td>3-hop é‚»å±…</td><td>ä¸­</td><td>ä¸­</td><td>$O(3|E| d)$</td><td><strong>PhysRobot âœ…</strong></td></tr>
          <tr><td>5</td><td>5-hop é‚»å±…</td><td>é«˜</td><td>é«˜</td><td>$O(5|E| d)$</td><td>éœ€è¦é•¿ç¨‹ä¿¡æ¯</td></tr>
          <tr><td>10+</td><td>å‡ ä¹å…¨å›¾</td><td>æé«˜</td><td>æé«˜</td><td>$O(10|E| d)$</td><td>éœ€ç‰¹æ®ŠæŠ€å·§</td></tr>
        </tbody>
      </table>
    </div>

    <!-- ====================== COMPREHENSIVE SUMMARY TABLE ====================== -->
    <h2 id="summary">å…¨ç« æ€»ç»“ï¼šæ‰€æœ‰æ¶æ„çš„ç»Ÿä¸€è§†è§’</h2>

    <div class="comparison-table">
      <table>
        <thead>
          <tr>
            <th>æ¶æ„</th>
            <th>åŸŸ $\Omega$</th>
            <th>å¯¹ç§°ç¾¤ $G$</th>
            <th>æ ¸å¿ƒæ“ä½œ</th>
            <th>ç­‰å˜/ä¸å˜</th>
            <th>å‚æ•°å…±äº«</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>CNN</td><td>ç½‘æ ¼ $\mathbb{Z}^d$</td><td>å¹³ç§» $(\mathbb{Z}^d, +)$</td><td>å·ç§¯ $F(x) = x \star \theta$</td><td>ç­‰å˜</td><td>æ»¤æ³¢å™¨éå†æ‰€æœ‰ä½ç½®</td></tr>
          <tr><td>G-CNN</td><td>é½æ¬¡ç©ºé—´</td><td>ä¸€èˆ¬ç¾¤ $G$</td><td>ç¾¤å·ç§¯ $(x \star \theta)(g)$</td><td>ç­‰å˜</td><td>æ»¤æ³¢å™¨éå†ç¾¤å…ƒç´ </td></tr>
          <tr><td>GCN</td><td>å›¾</td><td>$\Sigma_n$</td><td>$\hat{A}HW$</td><td>ç­‰å˜</td><td>$W$ å¯¹æ‰€æœ‰èŠ‚ç‚¹å…±äº«</td></tr>
          <tr><td>GAT</td><td>å›¾</td><td>$\Sigma_n$</td><td>æ³¨æ„åŠ›èšåˆ</td><td>ç­‰å˜</td><td>æ³¨æ„åŠ›å¤´å…±äº«</td></tr>
          <tr><td>GIN</td><td>å›¾</td><td>$\Sigma_n$</td><td>Sum + MLP</td><td>ç­‰å˜</td><td>MLP å…±äº«</td></tr>
          <tr><td>Transformer</td><td>å®Œå…¨å›¾</td><td>$\Sigma_n$</td><td>è‡ªæ³¨æ„åŠ›</td><td>ç­‰å˜</td><td>Q,K,V çŸ©é˜µå…±äº«</td></tr>
          <tr><td>Deep Sets</td><td>é›†åˆ</td><td>$\Sigma_n$</td><td>$\rho(\sum \phi(x_i))$</td><td>ä¸å˜</td><td>$\phi, \rho$ å…±äº«</td></tr>
          <tr><td>EGNN</td><td>å‡ ä½•å›¾</td><td>$E(3) \times \Sigma_n$</td><td>ç­‰å˜æ¶ˆæ¯ä¼ é€’</td><td>ç­‰å˜</td><td>æ¶ˆæ¯å‡½æ•°å…±äº«</td></tr>
          <tr><td>Mesh CNN</td><td>æµå½¢</td><td>ç­‰è· + è§„èŒƒ</td><td>æµ‹åœ°çº¿å·ç§¯</td><td>ç­‰å˜</td><td>æ»¤æ³¢å™¨+ä¼ è¾“çŸ©é˜µ</td></tr>
          <tr><td>RNN</td><td>$\mathbb{Z}$</td><td>æ—¶é—´å¹³ç§»</td><td>$R(z^{(t)}, h^{(t-1)})$</td><td>ç­‰å˜*</td><td>$R$ å¯¹æ‰€æœ‰æ—¶é—´æ­¥å…±äº«</td></tr>
          <tr><td>LSTM</td><td>$\mathbb{Z}$</td><td>æ—¶é—´æ‰­æ›²</td><td>é—¨æ§æ›´æ–°</td><td>ç±»ä¸å˜</td><td>æ‰€æœ‰é—¨å‚æ•°å…±äº«</td></tr>
        </tbody>
      </table>
      <p style="font-size:0.85rem;color:var(--text-muted)">* RNN çš„æ—¶é—´å¹³ç§»ç­‰å˜æ€§éœ€è¦é€‚å½“çš„è¾¹ç•Œæ¡ä»¶ï¼ˆé›¶å¡«å…… + ä¸åŠ¨ç‚¹åˆå§‹åŒ–ï¼‰ã€‚LSTM çš„æ—¶é—´æ‰­æ›²ä¸å˜æ€§æ˜¯"ç±»ä¸å˜æ€§"â€”â€”ç±»ä¸­å­˜åœ¨ç­‰ä»·æ¨¡å‹ï¼Œä½†ä¸èƒ½é›¶æ ·æœ¬è¿ç§»ã€‚</p>
    </div>


    <!-- ====================== EXERCISES ====================== -->
    <div class="exercises" id="exercises">
      <h3>ç»ƒä¹ é¢˜</h3>
      <ol>
        <li><strong>[CNN ç­‰å˜æ€§]</strong> è¯æ˜ï¼šå¦‚æœ CNN ä½¿ç”¨ circular paddingï¼ˆå¾ªç¯å¡«å……ï¼‰ï¼Œåˆ™å·ç§¯æ˜¯<strong>ç²¾ç¡®</strong>å¹³ç§»ç­‰å˜çš„ï¼ˆæ— è¾¹ç•Œæ•ˆåº”ï¼‰ã€‚æç¤ºï¼šè€ƒè™‘ DFT çš„å‘¨æœŸæ€§ã€‚</li>
        
        <li><strong>[GCN æ¨å¯¼]</strong> ä»è°±å›¾å·ç§¯ $g_\theta \star x = U g_\theta(\Lambda) U^\top x$ å‡ºå‘ï¼Œè¯¦ç»†æ¨å¯¼å½“ $g_\theta$ å–ä¸€é˜¶ Chebyshev è¿‘ä¼¼æ—¶å¦‚ä½•å¾—åˆ° GCN çš„å…¬å¼ã€‚ä¸ºä»€ä¹ˆéœ€è¦ renormalization trickï¼Ÿä¸åŠ è‡ªç¯ä¼šæ€æ ·ï¼Ÿ</li>
        
        <li><strong>[GAT vs GCN]</strong> æ„é€ ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ GAT ä¸¥æ ¼å¼ºäº GCNã€‚æç¤ºï¼šè€ƒè™‘ä¸¤ä¸ªèŠ‚ç‚¹ $u, v$ æœ‰ç›¸åŒçš„é‚»åŸŸç»“æ„ä½†ä¸åŒçš„ç‰¹å¾åˆ†å¸ƒã€‚</li>
        
        <li><strong>[GIN è¡¨è¾¾åŠ›]</strong> è¯æ˜ mean èšåˆæ— æ³•åŒºåˆ†å¤šé‡é›† $\{\{a, a, b\}\}$ å’Œ $\{\{a, b, b\}\}$ å½“ $a + b = 2a$ æˆ– $2b$ æ—¶ï¼Œä½† sum èšåˆæ€»èƒ½åŒºåˆ†ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œè¯æ˜ sum èšåˆåœ¨å¤šé‡é›†ä¸Šæ˜¯å•å°„çš„ï¼ˆåœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼‰ã€‚</li>
        
        <li><strong>[Transformer = GAT]</strong> å°†æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ› $\text{Attention}(Q, K, V) = \text{softmax}(QK^\top / \sqrt{d_k})V$ æ”¹å†™ä¸º GNN çš„æ¶ˆæ¯ä¼ é€’å½¢å¼ã€‚å†™å‡ºå¯¹åº”çš„æ¶ˆæ¯å‡½æ•° $\phi$ã€æ³¨æ„åŠ›å‡½æ•° $a$ã€èšåˆå‡½æ•° $\bigoplus$ã€‚</li>
        
        <li><strong>[EGNN ç­‰å˜æ€§]</strong> éªŒè¯ EGNN çš„åæ ‡æ›´æ–°å…¬å¼ $x_u' = x_u + \sum_v (x_u - x_v) \psi_c(\cdots)$ åœ¨åå°„å˜æ¢ $x \mapsto -x$ ä¸‹ä¹Ÿæ˜¯ç­‰å˜çš„ï¼ˆä¸ä»…ä»…æ˜¯æ—‹è½¬å’Œå¹³ç§»ï¼‰ã€‚</li>
        
        <li><strong>[PhysRobot ç¼–ç¨‹]</strong> åœ¨æˆ‘ä»¬çš„ <code>DynamicalGNN</code> ä¸­ï¼š(a) è§£é‡Šä¸ºä»€ä¹ˆä½¿ç”¨ sum èšåˆè€Œé mean èšåˆã€‚(b) å¦‚æœæ”¹ç”¨ mean èšåˆï¼Œä¼šå¯¹åŠ¨é‡å®ˆæ’äº§ç”Ÿä»€ä¹ˆå½±å“ï¼Ÿ(c) å®ç°ä¸€ä¸ªå®éªŒæ¥éªŒè¯ã€‚</li>
        
        <li><strong>[LSTM é—¨æ§]</strong> ä»æ—¶é—´æ‰­æ›²ä¸å˜æ€§çš„è§’åº¦è§£é‡Šï¼šä¸ºä»€ä¹ˆé—å¿˜é—¨çš„åç½®åˆå§‹åŒ–ä¸º 1 æ˜¯åˆç†çš„ï¼Ÿå¦‚æœåˆå§‹åŒ–ä¸º -1 ä¼šæ€æ ·ï¼Ÿç”¨å…¬å¼è¯´æ˜ã€‚</li>
        
        <li><strong>[æ¶æ„é€‰æ‹©]</strong> ç»™å®šä»¥ä¸‹ä»»åŠ¡ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æ¶æ„å¹¶è¯´æ˜ç†ç”±ï¼š(a) é¢„æµ‹è›‹ç™½è´¨æŠ˜å ç»“æ„ï¼›(b) ç¤¾äº¤ç½‘ç»œä¸­çš„èŠ‚ç‚¹åˆ†ç±»ï¼›(c) è‡ªåŠ¨é©¾é©¶çš„è§†é¢‘ç†è§£ï¼›(d) ç²’å­ç‰©ç†ä»¿çœŸã€‚</li>
        
        <li><strong>[ç»Ÿä¸€æ¡†æ¶]</strong> é€‰æ‹©æœ¬ç« çš„ä»»æ„ä¸¤ç§æ¶æ„ï¼ˆå¦‚ CNN å’Œ GCNï¼‰ï¼Œè¯¦ç»†è¯´æ˜å®ƒä»¬åœ¨å‡ ä½•æ·±åº¦å­¦ä¹ è“å›¾ä¸­çš„å¯¹åº”å…³ç³»ã€‚å†™å‡ºå…±åŒçš„æŠ½è±¡å…¬å¼ï¼Œå¹¶è¯´æ˜å“ªäº›è®¾è®¡é€‰æ‹©å¯¼è‡´äº†å…·ä½“çš„å·®å¼‚ã€‚</li>
      </ol>
    </div>


    <!-- ====================== FURTHER READING ====================== -->
    <h2 id="further-reading">æ¨èé˜…è¯»ä¸å‚è€ƒæ–‡çŒ®</h2>

    <h3>æ ¸å¿ƒè®ºæ–‡</h3>
    <div class="comparison-table">
      <table>
        <thead><tr><th>è®ºæ–‡</th><th>æ¶æ„</th><th>å¹´ä»½</th><th>æ ¸å¿ƒè´¡çŒ®</th></tr></thead>
        <tbody>
          <tr><td>Kipf & Welling</td><td>GCN</td><td>2016</td><td>åŠç›‘ç£åˆ†ç±»çš„è°±æ–¹æ³•ä¸€é˜¶è¿‘ä¼¼</td></tr>
          <tr><td>VeliÄkoviÄ‡ et al.</td><td>GAT</td><td>2018</td><td>å›¾ä¸Šçš„æ³¨æ„åŠ›æœºåˆ¶</td></tr>
          <tr><td>Hamilton et al.</td><td>GraphSAGE</td><td>2017</td><td>å½’çº³å­¦ä¹  + é‚»å±…é‡‡æ ·</td></tr>
          <tr><td>Xu et al.</td><td>GIN</td><td>2019</td><td>GNN è¡¨è¾¾åŠ›çš„ç†è®ºåˆ†æ</td></tr>
          <tr><td>Gilmer et al.</td><td>MPNN</td><td>2017</td><td>æ¶ˆæ¯ä¼ é€’ç»Ÿä¸€æ¡†æ¶</td></tr>
          <tr><td>Satorras et al.</td><td>EGNN</td><td>2021</td><td>E(n) ç­‰å˜æ¶ˆæ¯ä¼ é€’</td></tr>
          <tr><td>Vaswani et al.</td><td>Transformer</td><td>2017</td><td>è‡ªæ³¨æ„åŠ›æœºåˆ¶</td></tr>
          <tr><td>Hochreiter & Schmidhuber</td><td>LSTM</td><td>1997</td><td>é—¨æ§è®°å¿†ç»†èƒ</td></tr>
          <tr><td>He et al.</td><td>ResNet</td><td>2016</td><td>æ®‹å·®è¿æ¥ = ODE ç¦»æ•£åŒ–</td></tr>
          <tr><td>Cohen & Welling</td><td>G-CNN</td><td>2016</td><td>ç¾¤ç­‰å˜å·ç§¯</td></tr>
          <tr><td>Sanchez-Gonzalez et al.</td><td>GNS</td><td>2020</td><td>GNN å­¦ä¹ ç‰©ç†ä»¿çœŸï¼ˆPhysRobot å‚è€ƒï¼‰</td></tr>
        </tbody>
      </table>
    </div>

    <h3>PhysRobot ç›¸å…³çš„å­¦ä¹ è·¯çº¿</h3>
    <div class="callout callout-project">
      <h4>ä»æœ¬ç« åˆ°å®é™…é¡¹ç›®çš„æ¡¥æ¢</h4>
      <ol>
        <li><strong>åŸºç¡€</strong>ï¼šç†è§£æ¶ˆæ¯ä¼ é€’æ¡†æ¶ (Â§5.3) â€” è¿™æ˜¯ DynamicalGNN çš„éª¨æ¶</li>
        <li><strong>ç­‰å˜æ€§</strong>ï¼šæŒæ¡ EGNN (Â§5.5) â€” è¿™æ˜¯ EdgeFrame çš„ç†è®ºæ¥æº</li>
        <li><strong>å®è·µ</strong>ï¼šç”¨ PyG å®ç° MPNN â†’ åœ¨ç®€å•ç²’å­ç³»ç»Ÿä¸Šè®­ç»ƒ</li>
        <li><strong>è¿›é˜¶</strong>ï¼šç†è§£ GNS è®ºæ–‡ï¼Œå°† GNN + ç§¯åˆ†å™¨ç»„åˆä¸ºå®Œæ•´ä»¿çœŸå™¨</li>
        <li><strong>å‰æ²¿</strong>ï¼šæ¢ç´¢ SE(3)-Transformerã€Equiformer ç­‰æ›´å¼ºçš„ç­‰å˜æ¶æ„</li>
      </ol>
    </div>

    <h3>å¸¸è§é¢è¯•/è€ƒè¯•é—®é¢˜ä¸å›ç­”</h3>

    <div class="callout callout-info">
      <h4>Q: ä¸ºä»€ä¹ˆ GNN æ¯” MLP æ›´é€‚åˆå›¾æ•°æ®ï¼Ÿ</h4>
      <p><strong>A</strong>ï¼šMLP å¯¹èŠ‚ç‚¹æ’åˆ—æ•æ„Ÿï¼ˆè¾“å…¥ $[x_1, x_2, \ldots, x_n]$ å’Œ $[x_2, x_1, \ldots, x_n]$ äº§ç”Ÿä¸åŒè¾“å‡ºï¼‰ï¼Œä¸å…·æœ‰ç½®æ¢ç­‰å˜æ€§ã€‚GNN é€šè¿‡<strong>å±€éƒ¨èšåˆ</strong>ï¼ˆæ¶ˆæ¯ä¼ é€’ï¼‰ä¿è¯ç½®æ¢ç­‰å˜ï¼šèŠ‚ç‚¹ç‰¹å¾åªé€šè¿‡ç½®æ¢ä¸å˜çš„èšåˆå‡½æ•°ï¼ˆsum/mean/maxï¼‰å’Œå…±äº«çš„æ¶ˆæ¯å‡½æ•°æ›´æ–°ã€‚MLP æœ‰ $O(n^2 d^2)$ å‚æ•°ä¸”ä¸æ³›åŒ–åˆ°ä¸åŒå¤§å°çš„å›¾ï¼ŒGNN æœ‰ $O(d^2)$ å‚æ•°ä¸”å¤©ç„¶æ³›åŒ–ã€‚</p>
    </div>

    <div class="callout callout-info">
      <h4>Q: Transformer ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç è€Œ CNN ä¸éœ€è¦ï¼Ÿ</h4>
      <p><strong>A</strong>ï¼šä» GNN è§†è§’ç†è§£â€”â€”Transformer æ˜¯<strong>å®Œå…¨å›¾ä¸Šçš„æ³¨æ„åŠ› GNN</strong>ï¼Œå®Œå…¨å›¾<strong>æ²¡æœ‰ä½ç½®ç»“æ„</strong>ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ç­‰ä»·ï¼‰ã€‚CNN åœ¨<strong>ç½‘æ ¼</strong>ä¸Šæ“ä½œï¼Œç½‘æ ¼æœ¬èº«è•´å«ä½ç½®ä¿¡æ¯ï¼ˆé‚»æ¥å…³ç³» = ç©ºé—´é‚»è¿‘ï¼‰ã€‚ä½ç½®ç¼–ç ä¸º Transformer çš„"èŠ‚ç‚¹"æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œå¼¥è¡¥äº†å®Œå…¨å›¾ç¼ºå°‘çš„ç»“æ„å…ˆéªŒã€‚æ­£å¼¦ä½ç½®ç¼–ç å®é™…ä¸Šç­‰ä»·äº<strong>ç¯å½¢ç½‘æ ¼</strong>çš„ DFT ç‰¹å¾å‘é‡ï¼Œå› æ­¤ Transformer éšå¼å‡è®¾äº†ç½‘æ ¼åŸŸã€‚</p>
    </div>

    <div class="callout callout-info">
      <h4>Q: æˆ‘ä»¬çš„ DynamicalGNN ä½¿ç”¨ sum èšåˆè€Œé meanï¼Œä¸ºä»€ä¹ˆï¼Ÿ</h4>
      <p><strong>A</strong>ï¼šç‰©ç†åŸå› â€”â€”<strong>åŠ›æ˜¯åŠ æ€§çš„</strong>ï¼ˆç‰›é¡¿åŠ›å­¦ä¸­ï¼ŒåˆåŠ› = å„åˆ†åŠ›ä¹‹å’Œï¼‰ã€‚Sum èšåˆè‡ªç„¶å¯¹åº” $F_i = \sum_{j \in \mathcal{N}(i)} f_{ij}$ã€‚å¦‚æœä½¿ç”¨ mean èšåˆ $\frac{1}{|\mathcal{N}(i)|} \sum_j f_{ij}$ï¼Œåˆ™ä¸€ä¸ªç²’å­è¢«æ›´å¤šç²’å­åŒ…å›´æ—¶ï¼Œæ¯ä¸ªäº¤äº’çš„è´¡çŒ®åè€Œå˜å°â€”â€”è¿™è¿åç‰©ç†ç›´è§‰ã€‚æ•°å­¦ä¸Šï¼Œsum èšåˆè¿˜ä¿æŒäº†<strong>å•å°„æ€§</strong>ï¼ˆGIN çš„ç»“è®ºï¼‰ï¼Œä½¿æ¨¡å‹èƒ½åŒºåˆ†ä¸åŒçš„é‚»åŸŸå¤šé‡é›†ã€‚</p>
    </div>

    <h3>æ¦‚å¿µé€ŸæŸ¥å¡</h3>

    <div class="comparison-table">
      <table>
        <thead><tr><th>æ¦‚å¿µ</th><th>ä¸€å¥è¯è§£é‡Š</th><th>å…¬å¼</th></tr></thead>
        <tbody>
          <tr><td>ç­‰å˜æ€§</td><td>å˜æ¢è¾“å…¥ â†’ è¾“å‡ºä»¥ç›¸åŒæ–¹å¼å˜æ¢</td><td>$F(\rho(g)x) = \rho(g)F(x)$</td></tr>
          <tr><td>ä¸å˜æ€§</td><td>å˜æ¢è¾“å…¥ â†’ è¾“å‡ºä¸å˜</td><td>$F(\rho(g)x) = F(x)$</td></tr>
          <tr><td>å·ç§¯</td><td>æ»¤æ³¢å™¨æ»‘è¿‡ä¿¡å·çš„å†…ç§¯</td><td>$(f \star g)(t) = \sum f(\tau)g(t-\tau)$</td></tr>
          <tr><td>æ¶ˆæ¯ä¼ é€’</td><td>é‚»å±…å‘æ¶ˆæ¯ â†’ èšåˆ â†’ æ›´æ–°</td><td>$x_i' = \gamma(x_i, \bigoplus_j \phi(x_i, x_j))$</td></tr>
          <tr><td>æ³¨æ„åŠ›</td><td>å­¦ä¹ é‚»å±…çš„é‡è¦æ€§æƒé‡</td><td>$\alpha_{ij} = \text{softmax}(a(x_i, x_j))$</td></tr>
          <tr><td>é—¨æ§</td><td>å­¦ä¹ é€‰æ‹©æ€§é—å¿˜/è®°å¿†</td><td>$c = f \odot c_{\text{old}} + i \odot \tilde{c}_{\text{new}}$</td></tr>
          <tr><td>E(3) ç­‰å˜</td><td>æ—‹è½¬/å¹³ç§»è¾“å…¥ â†’ è¾“å‡ºä¸€è‡´å˜æ¢</td><td>é€šè¿‡è·ç¦»ä¸å˜é‡ $\|x_i - x_j\|$</td></tr>
          <tr><td>è§„èŒƒç­‰å˜</td><td>å±€éƒ¨å‚è€ƒç³»é€‰æ‹©ä¸å½±å“ç»“æœ</td><td>é€šè¿‡å¹³è¡Œç§»åŠ¨ $\rho(g_{v \to u})$</td></tr>
          <tr><td>è¿‡å¹³æ»‘</td><td>å±‚å¤ªå¤š â†’ æ‰€æœ‰èŠ‚ç‚¹è¡¨ç¤ºç›¸åŒ</td><td>$\hat{A}^k \to$ å¸¸å‘é‡</td></tr>
          <tr><td>è¿‡å‹ç¼©</td><td>è¿œç¨‹ä¿¡æ¯è¢«æŒ¤å‹åˆ°å°å‘é‡ä¸­</td><td>$|\partial h_u^L / \partial x_v^0| \to 0$</td></tr>
        </tbody>
      </table>
    </div>


    <!-- ====================== NAVIGATION ====================== -->
    <div class="chapter-nav">
      <a href="../chapter4/index.html">â† Chapter 4: å‡ ä½•åŸŸ</a>
      <a href="../index.html">ğŸ“š æ€»ç›®å½•</a>
    </div>

  </main>

  <!-- ====================== SCRIPTS ====================== -->
  <script>
// Theme toggle
function toggleTheme() {
  const html = document.documentElement;
  const current = html.getAttribute('data-theme');
  const next = current === 'dark' ? 'light' : 'dark';
  html.setAttribute('data-theme', next);
  localStorage.setItem('theme', next);
  document.querySelector('.theme-toggle').textContent = next === 'dark' ? 'â˜€ï¸' : 'ğŸŒ™';
}

// Load saved theme
(function() {
  const saved = localStorage.getItem('theme');
  if (saved === 'dark') {
    document.documentElement.setAttribute('data-theme', 'dark');
    document.querySelector('.theme-toggle').textContent = 'â˜€ï¸';
  }
})();

// Sidebar toggle (mobile)
function toggleSidebar() {
  document.getElementById('sidebar').classList.toggle('open');
}

// Progress bar
window.addEventListener('scroll', function() {
  const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
  const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
  const scrolled = (winScroll / height) * 100;
  document.getElementById('progressBar').style.width = scrolled + '%';
});

// Active sidebar link
const observer = new IntersectionObserver((entries) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      document.querySelectorAll('.sidebar a').forEach(a => a.classList.remove('active'));
      const link = document.querySelector(`.sidebar a[href="#${entry.target.id}"]`);
      if (link) link.classList.add('active');
    }
  });
}, { rootMargin: '-20% 0px -80% 0px' });

document.querySelectorAll('h2[id], h3[id]').forEach(el => observer.observe(el));

// Copy code
function copyCode(btn) {
  const code = btn.closest('.code-container').querySelector('code');
  navigator.clipboard.writeText(code.textContent).then(() => {
    btn.textContent = 'âœ… å·²å¤åˆ¶';
    setTimeout(() => { btn.textContent = 'ğŸ“‹ å¤åˆ¶'; }, 2000);
  });
}

// Tab switching
function switchTab(event, tabId) {
  const container = event.target.closest('.tab-container');
  container.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));
  container.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
  event.target.classList.add('active');
  container.querySelector('#' + tabId).classList.add('active');
}
  </script>
</body>
</html>
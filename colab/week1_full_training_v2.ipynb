{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 V2: Complete Training - ALL 3 Methods (FIXED)\n",
        "\n",
        "## Changes from V1 (see EXPERT_DEBATE_AND_SOLUTION.md)\n",
        "1. **Reward redesign**: progress reward, larger success bonus, action penalty\n",
        "2. **GNS/PhysRobot parameter reduction**: 500K→3K (GNS), 391K→6K (PhysRobot)\n",
        "3. **Timesteps**: 200K→500K\n",
        "4. **Exploration**: ent_coef=0.01\n",
        "\n",
        "**Target**: PPO >50%, GNS >30%, PhysRobot >30%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%time\n",
        "!pip install mujoco gymnasium stable-baselines3[extra] torch torch-geometric matplotlib pandas -q\n",
        "import torch\n",
        "print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "SAVE_DIR = '/content/drive/MyDrive/medical_robotics_week1_v2'\n",
        "for _d in ['', 'models', 'results', 'logs']:\n",
        "    os.makedirs(f'{SAVE_DIR}/{_d}' if _d else SAVE_DIR, exist_ok=True)\n",
        "print(f'\\ud83d\\udcbe {SAVE_DIR} (+ models/ results/ logs/)')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === ENVIRONMENT V2 — Fixed reward shaping ===\nimport numpy as np\nimport mujoco\nimport gymnasium as gym\nfrom gymnasium import spaces\n\nXML = '''<mujoco model=\"push_box\">\n  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n  \n  <option timestep=\"0.002\" integrator=\"Euler\" gravity=\"0 0 -9.81\">\n    <flag warmstart=\"enable\"/>\n  </option>\n  \n  <visual>\n    <global offwidth=\"1280\" offheight=\"720\"/>\n    <quality shadowsize=\"4096\"/>\n    <map force=\"0.1\" zfar=\"30\"/>\n  </visual>\n  \n  <asset>\n    <texture builtin=\"gradient\" height=\"100\" rgb1=\"0.3 0.5 0.7\" rgb2=\"0.1 0.2 0.3\" type=\"skybox\" width=\"100\"/>\n    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0.2 0.2 0.2\" rgb2=\"0.3 0.3 0.3\" type=\"2d\" width=\"100\"/>\n    <material name=\"MatPlane\" reflectance=\"0.3\" shininess=\"0.5\" specular=\"0.5\" texrepeat=\"3 3\" texture=\"texplane\"/>\n    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n  </asset>\n  \n  <default>\n    <joint armature=\"0.01\" damping=\"0.1\" limited=\"true\"/>\n    <geom conaffinity=\"1\" condim=\"3\" contype=\"1\" friction=\"0.5 0.005 0.0001\" margin=\"0.001\" material=\"geom\" rgba=\"0.8 0.6 0.4 1\"/>\n  </default>\n  \n  <worldbody>\n    <light directional=\"true\" diffuse=\"0.8 0.8 0.8\" pos=\"0 0 3\" dir=\"0 0 -1\"/>\n    <light directional=\"true\" diffuse=\"0.4 0.4 0.4\" pos=\"0 0 3\" dir=\"1 1 -1\"/>\n    <geom name=\"floor\" type=\"plane\" size=\"3 3 0.1\" rgba=\"0.8 0.8 0.8 1\" material=\"MatPlane\"/>\n    \n    <body name=\"arm_base\" pos=\"0 0 0.02\">\n      <geom name=\"base_geom\" type=\"cylinder\" size=\"0.05 0.02\" rgba=\"0.3 0.3 0.3 1\"/>\n      <body name=\"upper_arm\" pos=\"0 0 0.02\">\n        <joint name=\"shoulder\" type=\"hinge\" axis=\"0 0 1\" range=\"-180 180\" damping=\"0.5\"/>\n        <geom name=\"upper_arm_geom\" type=\"capsule\" fromto=\"0 0 0 0.4 0 0\" size=\"0.025\" rgba=\"0.5 0.5 0.8 1\"/>\n        <body name=\"forearm\" pos=\"0.4 0 0\">\n          <joint name=\"elbow\" type=\"hinge\" axis=\"0 0 1\" range=\"-180 180\" damping=\"0.5\"/>\n          <geom name=\"forearm_geom\" type=\"capsule\" fromto=\"0 0 0 0.3 0 0\" size=\"0.025\" rgba=\"0.5 0.5 0.8 1\"/>\n          <site name=\"endeffector\" pos=\"0.3 0 0\" size=\"0.03\" rgba=\"1 0.5 0 0.8\"/>\n        </body>\n      </body>\n    </body>\n    \n    <body name=\"box\" pos=\"0.35 0 0.05\">\n      <freejoint name=\"box_freejoint\"/>\n      <geom name=\"box_geom\" type=\"box\" size=\"0.05 0.05 0.05\" mass=\"0.5\" rgba=\"0.2 0.8 0.2 1\" friction=\"0.5 0.005 0.0001\"/>\n      <site name=\"box_center\" pos=\"0 0 0\" size=\"0.01\" rgba=\"0 1 0 1\"/>\n    </body>\n    \n    <site name=\"goal\" pos=\"0.5 0.3 0.02\" size=\"0.06\" rgba=\"1 0 0 0.4\" type=\"sphere\"/>\n  </worldbody>\n  \n  <actuator>\n    <motor name=\"shoulder_motor\" joint=\"shoulder\" gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-10 10\"/>\n    <motor name=\"elbow_motor\" joint=\"elbow\" gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-10 10\"/>\n  </actuator>\n</mujoco>\n'''\n\nclass PushBoxEnv(gym.Env):\n    \"\"\"PushBox V2: Fixed reward shaping with progress reward + large success bonus.\"\"\"\n    def __init__(self, render_mode=None, box_mass=0.5):\n        super().__init__()\n        self.model = mujoco.MjModel.from_xml_string(XML)\n        self.data = mujoco.MjData(self.model)\n        self.box_mass = box_mass\n        self._set_box_mass(box_mass)\n        self._ee_site_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, 'endeffector')\n        self.action_space = spaces.Box(low=-10.0, high=10.0, shape=(2,), dtype=np.float32)\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(16,), dtype=np.float32)\n        self.goal_pos = np.array([0.5, 0.3, 0.02])\n        self.max_episode_steps = 500\n        self.current_step = 0\n        self.success_threshold = 0.15   # V2: relaxed from 0.1 to 0.15m\n        self._prev_dist_box_goal = None  # V2: for progress reward\n        self._prev_dist_ee_box = None    # V2: for reaching progress\n        self.render_mode = render_mode\n    \n    def _set_box_mass(self, mass):\n        box_body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'box')\n        self.model.body_mass[box_body_id] = mass\n    \n    def set_box_mass(self, mass):\n        self.box_mass = mass\n        self._set_box_mass(mass)\n    \n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        mujoco.mj_resetData(self.model, self.data)\n        if seed is not None:\n            np.random.seed(seed)\n        self.data.qpos[0] = np.random.uniform(-0.5, 0.5)\n        self.data.qpos[1] = np.random.uniform(-0.5, 0.5)\n        self.data.qpos[2] = np.random.uniform(0.25, 0.45)\n        self.data.qpos[3] = np.random.uniform(-0.15, 0.15)\n        self.data.qpos[4] = 0.05\n        self.data.qpos[5:9] = [1, 0, 0, 0]\n        self.data.qvel[:] = 0.0\n        mujoco.mj_forward(self.model, self.data)\n        self.current_step = 0\n        # V2: initialize progress tracking\n        ee_pos = self.data.site_xpos[self._ee_site_id]\n        box_pos = self.data.qpos[2:5]\n        self._prev_dist_box_goal = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n        self._prev_dist_ee_box = np.linalg.norm(ee_pos[:2] - box_pos[:2])\n        return self._get_obs(), self._get_info()\n    \n    def _get_obs(self):\n        joint_pos = self.data.qpos[:2].copy()\n        joint_vel = self.data.qvel[:2].copy()\n        ee_pos = self.data.site_xpos[self._ee_site_id].copy()\n        box_pos = self.data.qpos[2:5].copy()\n        box_vel = self.data.qvel[2:5].copy()\n        goal_pos = self.goal_pos.copy()\n        obs = np.concatenate([joint_pos, joint_vel, ee_pos, box_pos, box_vel, goal_pos])\n        return obs.astype(np.float32)\n    \n    def _get_info(self):\n        box_pos = self.data.qpos[2:5]\n        distance_to_goal = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n        success = distance_to_goal < self.success_threshold\n        return {'distance_to_goal': distance_to_goal, 'success': success,\n                'box_mass': self.box_mass, 'timestep': self.current_step}\n    \n    def step(self, action):\n        self.data.ctrl[:] = action\n        for _ in range(5):\n            mujoco.mj_step(self.model, self.data)\n        \n        ee_pos = self.data.site_xpos[self._ee_site_id].copy()\n        box_pos = self.data.qpos[2:5].copy()\n        \n        dist_ee_box = np.linalg.norm(ee_pos[:2] - box_pos[:2])\n        dist_box_goal = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n        \n        # ====== V2: Redesigned reward ======\n        # 1. Reach progress (delta-based): reward for getting closer to box\n        reach_progress = (self._prev_dist_ee_box - dist_ee_box)\n        self._prev_dist_ee_box = dist_ee_box\n        \n        # 2. Push progress (delta-based): reward for pushing box toward goal\n        push_progress = (self._prev_dist_box_goal - dist_box_goal)\n        self._prev_dist_box_goal = dist_box_goal\n        \n        # 3. Distance-based shaping (weaker, for gradient)\n        reach_reward = -dist_ee_box\n        push_reward = -dist_box_goal\n        \n        # 4. Action penalty (discourage torque waste)\n        action_penalty = -0.01 * np.sum(action ** 2)\n        \n        # Combined reward\n        reward = (\n            0.5 * reach_reward +        # mild distance shaping\n            1.0 * push_reward +          # mild distance shaping\n            10.0 * reach_progress +      # strong progress signal for reaching\n            20.0 * push_progress +       # very strong progress signal for pushing\n            action_penalty               # regularization\n        )\n        \n        success = dist_box_goal < self.success_threshold\n        if success:\n            remaining_steps = self.max_episode_steps - self.current_step\n            reward += 500.0 + remaining_steps * 1.0   # large bonus + early completion bonus\n        # ====== End V2 reward ======\n        \n        self.current_step += 1\n        terminated = success\n        truncated = self.current_step >= self.max_episode_steps\n        return self._get_obs(), reward, terminated, truncated, self._get_info()\n    \n    def render(self):\n        pass\n    \n    def close(self):\n        pass\n\ndef make_push_box_env(box_mass=0.5):\n    def _init():\n        return PushBoxEnv(box_mass=box_mass)\n    return _init\n\n# Quick sanity check\nenv = PushBoxEnv()\nobs, info = env.reset()\nprint(f'Obs: {obs.shape}, Action: {env.action_space.shape}, Threshold: {env.success_threshold}m')\nenv.close()\nprint('\\u2705 Environment V2 loaded (progress reward + 500 success bonus)')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === AGENTS V2: Drastically reduced parameters for GNS/PhysRobot ===\nimport torch\nimport torch.nn as nn\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.callbacks import BaseCallback\n\ntry:\n    from torch_geometric.nn import MessagePassing\n    from torch_geometric.data import Data as PyGData, Batch as PyGBatch\n    HAS_PYG = True\nexcept ImportError:\n    HAS_PYG = False\n    print('torch_geometric not found, GNS will use non-graph fallback')\n\nclass SuccessTrackingCallback(BaseCallback):\n    def __init__(self, verbose=1):\n        super().__init__(verbose)\n        self.episode_count = 0\n        self.success_count = 0\n        self.success_achieved = False\n        self.episodes_to_success = None\n    \n    def _on_step(self):\n        infos = self.locals.get('infos', [{}])\n        dones = self.locals.get('dones', [False])\n        for i, done in enumerate(dones):\n            if done:\n                self.episode_count += 1\n                info = infos[i] if i < len(infos) else {}\n                if info.get('success', False):\n                    self.success_count += 1\n                    if not self.success_achieved:\n                        self.success_achieved = True\n                        self.episodes_to_success = self.episode_count\n                        print(f'\\n\\ud83c\\udf89 First success at episode {self.episode_count}!')\n                if self.episode_count % 100 == 0:\n                    rate = self.success_count / self.episode_count * 100\n                    print(f'  [Ep {self.episode_count}] Success rate so far: {rate:.1f}%')\n        return True\n\n# ──────────────────────────────────────────────────────\n# AGENT 1: Pure PPO (unchanged, just added ent_coef)\n# ──────────────────────────────────────────────────────\nclass PurePPOAgent:\n    def __init__(self, env, verbose=1):\n        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048, batch_size=64,\n                         n_epochs=10, gamma=0.99, ent_coef=0.01, verbose=verbose)\n    \n    def train(self, total_timesteps, callback=None):\n        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n    \n    def predict(self, obs, deterministic=True):\n        action, _ = self.model.predict(obs, deterministic=deterministic)\n        return action\n    \n    def save(self, path):\n        self.model.save(path)\n    \n    def evaluate(self, env, n_episodes=100):\n        rewards, successes = [], []\n        for _ in range(n_episodes):\n            obs, info = env.reset()\n            done, ep_reward = False, 0\n            while not done:\n                action = self.predict(obs)\n                obs, reward, terminated, truncated, info = env.step(action)\n                ep_reward += reward\n                done = terminated or truncated\n            rewards.append(ep_reward)\n            successes.append(1 if info.get('success', False) else 0)\n        return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards),\n                'success_rate': np.mean(successes)}\n\n# ──────────────────────────────────────────────────────\n# AGENT 2: GNS V2 — Minimal parameters (~5K total)\n# ──────────────────────────────────────────────────────\nif HAS_PYG:\n    def obs_to_graph_batch(observations):\n        \"\"\"Convert batch of obs [B,16] -> PyG Batch with 2 nodes (ee, box).\"\"\"\n        batch_size = observations.shape[0]\n        dev = observations.device\n        graphs = []\n        for i in range(batch_size):\n            o = observations[i]\n            ee_pos  = o[4:7]\n            ee_vel  = torch.zeros(3, device=dev)\n            box_pos = o[7:10]\n            box_vel = o[10:13]\n            positions  = torch.stack([ee_pos,  box_pos])\n            velocities = torch.stack([ee_vel,  box_vel])\n            node_feats = torch.cat([positions, velocities], dim=-1)  # [2,6]\n            edge_index = torch.tensor([[0,1],[1,0]], dtype=torch.long, device=dev).t().contiguous()\n            rel01 = box_pos - ee_pos\n            rel10 = ee_pos - box_pos\n            d01 = torch.norm(rel01).unsqueeze(0)\n            d10 = torch.norm(rel10).unsqueeze(0)\n            edge_attr = torch.stack([torch.cat([rel01, d01]), torch.cat([rel10, d10])])\n            g = PyGData(x=node_feats, pos=positions, edge_index=edge_index, edge_attr=edge_attr)\n            graphs.append(g)\n        return PyGBatch.from_data_list(graphs)\n\n    class GNSGraphLayerV2(MessagePassing):\n        \"\"\"Minimal GN layer for 2-node graph. ~2K params.\"\"\"\n        def __init__(self, node_dim, edge_dim, hidden_dim=32):\n            super().__init__(aggr='add')\n            self.edge_mlp = nn.Sequential(\n                nn.Linear(2*node_dim + edge_dim, hidden_dim), nn.ReLU(),\n                nn.Linear(hidden_dim, edge_dim))\n            self.node_mlp = nn.Sequential(\n                nn.Linear(node_dim + edge_dim, hidden_dim), nn.ReLU(),\n                nn.Linear(hidden_dim, node_dim))\n        def forward(self, x, edge_index, edge_attr):\n            return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n        def message(self, x_i, x_j, edge_attr):\n            return self.edge_mlp(torch.cat([x_i, x_j, edge_attr], dim=-1))\n        def update(self, aggr_out, x):\n            return self.node_mlp(torch.cat([x, aggr_out], dim=-1))\n\n    class GNSFeaturesExtractorV2(BaseFeaturesExtractor):\n        \"\"\"GNS V2: ~5K params (down from ~500K). 1 MP layer, hidden=32.\"\"\"\n        def __init__(self, observation_space, features_dim=64):\n            super().__init__(observation_space, features_dim)\n            hid = 32\n            edge_dim = 4\n            self.node_encoder = nn.Sequential(nn.Linear(6, hid), nn.ReLU())\n            self.edge_encoder = nn.Sequential(nn.Linear(edge_dim, hid), nn.ReLU())\n            self.gn_layer = GNSGraphLayerV2(hid, hid, hid)  # single layer\n            self.decoder = nn.Linear(hid, 3)\n            self.feature_proj = nn.Sequential(nn.Linear(3 + 16, features_dim), nn.ReLU())\n        \n        def forward(self, observations):\n            graph = obs_to_graph_batch(observations)\n            x = self.node_encoder(graph.x)\n            ea = self.edge_encoder(graph.edge_attr)\n            x = x + self.gn_layer(x, graph.edge_index, ea)\n            acc = self.decoder(x)\n            box_acc = acc[1::2]\n            combined = torch.cat([box_acc, observations], dim=-1)\n            return self.feature_proj(combined)\n\nclass GNSAgent:\n    def __init__(self, env, verbose=1):\n        if HAS_PYG:\n            policy_kwargs = dict(\n                features_extractor_class=GNSFeaturesExtractorV2,\n                features_extractor_kwargs=dict(features_dim=64),\n                net_arch=dict(pi=[64, 64], vf=[64, 64]))\n        else:\n            policy_kwargs = dict(net_arch=dict(pi=[64, 64], vf=[64, 64]))\n        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048, batch_size=64,\n                         n_epochs=10, gamma=0.99, ent_coef=0.01,\n                         policy_kwargs=policy_kwargs, verbose=verbose)\n    \n    def train(self, total_timesteps, callback=None):\n        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n    \n    def predict(self, obs, deterministic=True):\n        action, _ = self.model.predict(obs, deterministic=deterministic)\n        return action\n    \n    def save(self, path):\n        self.model.save(path)\n    \n    def evaluate(self, env, n_episodes=100):\n        rewards, successes = [], []\n        for _ in range(n_episodes):\n            obs, info = env.reset()\n            done, ep_reward = False, 0\n            while not done:\n                action = self.predict(obs)\n                obs, reward, terminated, truncated, info = env.step(action)\n                ep_reward += reward\n                done = terminated or truncated\n            rewards.append(ep_reward)\n            successes.append(1 if info.get('success', False) else 0)\n        return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards),\n                'success_rate': np.mean(successes)}\n\n# ──────────────────────────────────────────────────────\n# AGENT 3: PhysRobot V2 — Lightweight MLP physics (~6K params)\n#   No GNN (2-node graph = waste). Direct relative-geometry MLP.\n# ──────────────────────────────────────────────────────\nclass PhysRobotFeaturesExtractorV2(BaseFeaturesExtractor):\n    \"\"\"Physics-informed V2: lightweight MLP on relative geometry. ~6K params.\"\"\"\n    def __init__(self, observation_space, features_dim=64):\n        super().__init__(observation_space, features_dim)\n        # Physics stream: relative geometry -> predicted box acceleration\n        # Input: [rel_pos(3), rel_vel(3), dist(1), goal_dir(2)] = 9\n        self.physics_net = nn.Sequential(\n            nn.Linear(9, 32), nn.ReLU(),\n            nn.Linear(32, 3)   # predicted box acceleration\n        )\n        # Policy stream\n        self.policy_stream = nn.Sequential(\n            nn.Linear(16, 64), nn.ReLU(),\n            nn.Linear(64, features_dim)\n        )\n        # Fusion\n        self.fusion = nn.Sequential(\n            nn.Linear(features_dim + 3, features_dim), nn.ReLU()\n        )\n    \n    def forward(self, observations):\n        # Extract relevant quantities from obs\n        ee_pos  = observations[:, 4:7]\n        box_pos = observations[:, 7:10]\n        box_vel = observations[:, 10:13]\n        goal_pos = observations[:, 13:16]\n        \n        # Physics: relative geometry\n        rel_pos = box_pos - ee_pos\n        rel_vel = box_vel  # ee_vel \\u2248 0 (TODO: fix in P1 with Jacobian)\n        dist = torch.norm(rel_pos, dim=-1, keepdim=True).clamp(min=1e-6)\n        goal_dir = (goal_pos[:, :2] - box_pos[:, :2])  # 2D direction to goal\n        \n        physics_input = torch.cat([rel_pos, rel_vel, dist, goal_dir], dim=-1)  # [B, 9]\n        physics_pred = self.physics_net(physics_input)  # [B, 3]\n        \n        # Policy stream\n        policy_features = self.policy_stream(observations)  # [B, features_dim]\n        \n        # Fusion\n        combined = torch.cat([policy_features, physics_pred], dim=-1)\n        return self.fusion(combined)\n\nclass PhysRobotAgent:\n    def __init__(self, env, verbose=1):\n        policy_kwargs = dict(\n            features_extractor_class=PhysRobotFeaturesExtractorV2,\n            features_extractor_kwargs=dict(features_dim=64),\n            net_arch=dict(pi=[64, 64], vf=[64, 64]))\n        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048, batch_size=64,\n                         n_epochs=10, gamma=0.99, ent_coef=0.01,\n                         policy_kwargs=policy_kwargs, verbose=verbose)\n    \n    def train(self, total_timesteps, callback=None):\n        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n    \n    def predict(self, obs, deterministic=True):\n        action, _ = self.model.predict(obs, deterministic=deterministic)\n        return action\n    \n    def save(self, path):\n        self.model.save(path)\n    \n    def evaluate(self, env, n_episodes=100):\n        rewards, successes = [], []\n        for _ in range(n_episodes):\n            obs, info = env.reset()\n            done, ep_reward = False, 0\n            while not done:\n                action = self.predict(obs)\n                obs, reward, terminated, truncated, info = env.step(action)\n                ep_reward += reward\n                done = terminated or truncated\n            rewards.append(ep_reward)\n            successes.append(1 if info.get('success', False) else 0)\n        return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards),\n                'success_rate': np.mean(successes)}\n\n# Print parameter counts\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\n_tmp_env = DummyVecEnv([make_push_box_env()])\n_ppo = PurePPOAgent(_tmp_env, verbose=0)\n_gns = GNSAgent(_tmp_env, verbose=0)\n_pr = PhysRobotAgent(_tmp_env, verbose=0)\nprint(f'Parameter counts:')\nprint(f'  Pure PPO:     {count_params(_ppo.model.policy):>8,}')\nprint(f'  GNS V2:       {count_params(_gns.model.policy):>8,}')\nprint(f'  PhysRobot V2: {count_params(_pr.model.policy):>8,}')\ndel _ppo, _gns, _pr\n_tmp_env.close()\nprint('\\u2705 All 3 agents V2 loaded')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# V2: Increased timesteps, more eval episodes\nCONFIG = {\n    'ppo_timesteps': 500_000,\n    'gns_timesteps': 500_000,\n    'physrobot_timesteps': 500_000,\n    'n_envs': 4,\n    'box_mass': 0.5,\n    'eval_episodes': 100\n}\nprint('V2 Configuration:', CONFIG)\nprint(f'Expected training time: ~{3 * CONFIG[\"ppo_timesteps\"] / 200_000 * 5:.0f} min total on T4 GPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%time\n# === TRAINING ALL 3 METHODS (V2) ===\nimport time\nimport json\nresults = {}\n\n# Method 1: Pure PPO\nprint('='*60)\nprint('\\ud83d\\ude80 TRAINING PURE PPO (V2: ent_coef=0.01, 500K steps)')\nprint('='*60)\nenv = DummyVecEnv([make_push_box_env(CONFIG['box_mass']) for _ in range(CONFIG['n_envs'])])\nagent1 = PurePPOAgent(env, verbose=0)\ncallback1 = SuccessTrackingCallback(verbose=1)\nstart = time.time()\ntry:\n    agent1.train(CONFIG['ppo_timesteps'], callback=callback1)\n    train_time = time.time() - start\n    eval_env = PushBoxEnv(box_mass=CONFIG['box_mass'])\n    eval_res = agent1.evaluate(eval_env, n_episodes=CONFIG['eval_episodes'])\n    results['Pure PPO'] = {\n        'episodes_to_success': callback1.episodes_to_success,\n        'total_episodes': callback1.episode_count,\n        'total_successes': callback1.success_count,\n        'timesteps': CONFIG['ppo_timesteps'],\n        'train_time': train_time,\n        'success_rate': eval_res['success_rate'],\n        'mean_reward': eval_res['mean_reward'],\n        'std_reward': eval_res['std_reward']\n    }\n    agent1.save(f'{SAVE_DIR}/models/ppo_v2')\n    print(f'\\u2705 PPO: {eval_res[\"success_rate\"]:.2%} success, {train_time/60:.1f} min')\n    eval_env.close()\nexcept Exception as e:\n    print(f'\\u274c PPO failed: {e}')\n    import traceback; traceback.print_exc()\n    results['Pure PPO'] = {'error': str(e)}\nenv.close()\n\n# Method 2: GNS V2\nprint('\\n' + '='*60)\nprint('\\ud83d\\ude80 TRAINING GNS V2 (hidden=32, 1 MP layer, ~5K params)')\nprint('='*60)\nenv = DummyVecEnv([make_push_box_env(CONFIG['box_mass']) for _ in range(CONFIG['n_envs'])])\nagent2 = GNSAgent(env, verbose=0)\ncallback2 = SuccessTrackingCallback(verbose=1)\nstart = time.time()\ntry:\n    agent2.train(CONFIG['gns_timesteps'], callback=callback2)\n    train_time = time.time() - start\n    eval_env = PushBoxEnv(box_mass=CONFIG['box_mass'])\n    eval_res = agent2.evaluate(eval_env, n_episodes=CONFIG['eval_episodes'])\n    results['GNS V2'] = {\n        'episodes_to_success': callback2.episodes_to_success,\n        'total_episodes': callback2.episode_count,\n        'total_successes': callback2.success_count,\n        'timesteps': CONFIG['gns_timesteps'],\n        'train_time': train_time,\n        'success_rate': eval_res['success_rate'],\n        'mean_reward': eval_res['mean_reward'],\n        'std_reward': eval_res['std_reward']\n    }\n    agent2.save(f'{SAVE_DIR}/models/gns_v2')\n    print(f'\\u2705 GNS V2: {eval_res[\"success_rate\"]:.2%} success, {train_time/60:.1f} min')\n    eval_env.close()\nexcept Exception as e:\n    print(f'\\u274c GNS failed: {e}')\n    import traceback; traceback.print_exc()\n    results['GNS V2'] = {'error': str(e)}\nenv.close()\n\n# Method 3: PhysRobot V2\nprint('\\n' + '='*60)\nprint('\\ud83d\\ude80 TRAINING PHYSROBOT V2 (MLP physics, ~6K params)')\nprint('='*60)\nenv = DummyVecEnv([make_push_box_env(CONFIG['box_mass']) for _ in range(CONFIG['n_envs'])])\nagent3 = PhysRobotAgent(env, verbose=0)\ncallback3 = SuccessTrackingCallback(verbose=1)\nstart = time.time()\ntry:\n    agent3.train(CONFIG['physrobot_timesteps'], callback=callback3)\n    train_time = time.time() - start\n    eval_env = PushBoxEnv(box_mass=CONFIG['box_mass'])\n    eval_res = agent3.evaluate(eval_env, n_episodes=CONFIG['eval_episodes'])\n    results['PhysRobot V2'] = {\n        'episodes_to_success': callback3.episodes_to_success,\n        'total_episodes': callback3.episode_count,\n        'total_successes': callback3.success_count,\n        'timesteps': CONFIG['physrobot_timesteps'],\n        'train_time': train_time,\n        'success_rate': eval_res['success_rate'],\n        'mean_reward': eval_res['mean_reward'],\n        'std_reward': eval_res['std_reward']\n    }\n    agent3.save(f'{SAVE_DIR}/models/physrobot_v2')\n    print(f'\\u2705 PhysRobot V2: {eval_res[\"success_rate\"]:.2%} success, {train_time/60:.1f} min')\n    eval_env.close()\nexcept Exception as e:\n    print(f'\\u274c PhysRobot failed: {e}')\n    import traceback; traceback.print_exc()\n    results['PhysRobot V2'] = {'error': str(e)}\nenv.close()\n\nprint('\\n' + '='*60)\nprint('\\ud83c\\udf89 ALL TRAINING COMPLETE')\nprint('='*60)\nfor m, r in results.items():\n    if 'error' not in r:\n        print(f'  {m}: {r[\"success_rate\"]:.2%} success ({r[\"train_time\"]/60:.1f} min)')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === RESULTS COMPARISON ===\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_data = []\nfor method, res in results.items():\n    if 'error' not in res:\n        df_data.append({\n            'Method': method,\n            'Success Rate': f\"{res['success_rate']:.1%}\",\n            'Mean Reward': f\"{res['mean_reward']:.1f} \\u00b1 {res['std_reward']:.1f}\",\n            'First Success (ep)': res.get('episodes_to_success', 'N/A'),\n            'Train Time (min)': f\"{res['train_time']/60:.1f}\",\n            'Timesteps': f\"{res['timesteps']/1000:.0f}K\"\n        })\n\ndf = pd.DataFrame(df_data)\nprint('\\n\\ud83d\\udcca Results Comparison (V2):')\nprint(df.to_string(index=False))\n\n# Comparison with V1\nprint('\\n\\ud83d\\udcca Comparison with V1 results:')\nv1 = {'Pure PPO': '6%', 'GNS': '0%', 'PhysRobot': '0%'}\nfor m, r in results.items():\n    if 'error' not in r:\n        v1_key = m.replace(' V2', '')\n        v1_rate = v1.get(v1_key, '?')\n        print(f'  {m}: V1={v1_rate} \\u2192 V2={r[\"success_rate\"]:.1%}')\n\n# Save\nwith open(f'{SAVE_DIR}/results/training_results_v2.json', 'w') as f:\n    json.dump(results, f, indent=2, default=str)\nprint(f'\\n\\ud83d\\udcbe Saved to {SAVE_DIR}/results/training_results_v2.json')\n\n# Bar chart\nif df_data:\n    methods = [d['Method'] for d in df_data]\n    rates = [results[m]['success_rate'] * 100 for m in results if 'error' not in results[m]]\n    colors = ['#4CAF50', '#2196F3', '#FF9800'][:len(methods)]\n    fig, ax = plt.subplots(figsize=(8, 5))\n    bars = ax.bar(methods, rates, color=colors)\n    ax.set_ylabel('Success Rate (%)')\n    ax.set_title('Week 1 V2: Training Results (500K steps)')\n    ax.set_ylim(0, 100)\n    for bar, rate in zip(bars, rates):\n        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 2,\n                f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n    # Add V1 comparison line\n    v1_rates = [6, 0, 0][:len(methods)]\n    for i, (m, v1r) in enumerate(zip(methods, v1_rates)):\n        ax.plot([i-0.3, i+0.3], [v1r, v1r], 'r--', linewidth=2, alpha=0.5)\n    ax.legend(['V1 baseline'], loc='upper right')\n    plt.tight_layout()\n    plt.savefig(f'{SAVE_DIR}/results/success_rates_v2.png', dpi=150)\n    plt.show()\n    print(f'\\ud83d\\udcbe Chart saved')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === OOD GENERALIZATION TEST (V2) ===\nprint('\\n\\ud83e\\uddea OOD Generalization Test (different box masses)...')\nmass_range = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0]\nood_results = {}\nn_ood_episodes = 100\n\nagents_map = {}\nfor name, agent_var in [('Pure PPO', 'agent1'), ('GNS V2', 'agent2'), ('PhysRobot V2', 'agent3')]:\n    if name in results and 'error' not in results[name]:\n        agents_map[name] = eval(agent_var)\n\nfor method_name, agent in agents_map.items():\n    print(f'\\nTesting {method_name}...')\n    method_results = []\n    for mass in mass_range:\n        test_env = PushBoxEnv(box_mass=mass)\n        success_count = 0\n        for _ in range(n_ood_episodes):\n            obs, info = test_env.reset()\n            done = False\n            while not done:\n                action = agent.predict(obs)\n                obs, reward, terminated, truncated, info = test_env.step(action)\n                done = terminated or truncated\n            if info.get('success', False):\n                success_count += 1\n        rate = success_count / n_ood_episodes\n        method_results.append({'mass': mass, 'success_rate': rate})\n        marker = '\\u2705' if rate > 0.3 else '\\u26a0\\ufe0f' if rate > 0 else '\\u274c'\n        print(f'  Mass {mass:.2f}: {rate:.1%} {marker}')\n        test_env.close()\n    ood_results[method_name] = method_results\n\n# Save and plot\nwith open(f'{SAVE_DIR}/results/ood_results_v2.json', 'w') as f:\n    json.dump(ood_results, f, indent=2)\n\nif ood_results:\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors = {'Pure PPO': '#4CAF50', 'GNS V2': '#2196F3', 'PhysRobot V2': '#FF9800'}\n    for method_name, method_results in ood_results.items():\n        masses = [r['mass'] for r in method_results]\n        rates = [r['success_rate'] * 100 for r in method_results]\n        ax.plot(masses, rates, 'o-', label=method_name,\n                color=colors.get(method_name, 'gray'), linewidth=2, markersize=8)\n    ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Training mass')\n    ax.set_xlabel('Box Mass (kg)')\n    ax.set_ylabel('Success Rate (%)')\n    ax.set_title('OOD Generalization: Success Rate vs Box Mass')\n    ax.legend()\n    ax.set_ylim(-5, 105)\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(f'{SAVE_DIR}/results/ood_generalization_v2.png', dpi=150)\n    plt.show()\nprint(f'\\n\\ud83d\\udcbe OOD results saved')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === FINAL SUMMARY ===\nprint('\\n' + '='*60)\nprint('\\ud83c\\udfaf WEEK 1 V2 EXPERIMENT COMPLETE')\nprint('='*60)\nprint(f'Models: {SAVE_DIR}/models/')\nprint(f'Results: {SAVE_DIR}/results/')\n\nprint('\\n\\ud83d\\udcca Key Results:')\nfor method, res in results.items():\n    if 'error' not in res:\n        print(f'  {method}: {res[\"success_rate\"]:.1%} success ({res[\"train_time\"]/60:.1f}m)')\n\nprint('\\n\\ud83d\\udcdd Changes from V1:')\nprint('  1. Reward: progress-based + 500 success bonus + action penalty')\nprint('  2. GNS: 500K\\u2192~5K params (hidden 128\\u219232, 3\\u21921 MP layer)')\nprint('  3. PhysRobot: 391K\\u2192~6K params (MLP physics, no GNN)')\nprint('  4. Timesteps: 200K\\u2192500K, ent_coef=0.01')\nprint('  5. Success threshold: 0.1\\u21920.15m')\n\nprint('\\n\\ud83d\\udd2e Next steps (P1):')\nprint('  - Fix ee_vel (Jacobian from joint velocities)')\nprint('  - Implement proper antisymmetric edge frame')\nprint('  - Multi-seed experiments (5 seeds)')\nprint('  - Multi-object environment')\n"
      ]
    }
  ]
}

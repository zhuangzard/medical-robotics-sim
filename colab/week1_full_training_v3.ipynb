{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 V3: PhysRobot with SV-Pipeline (Momentum-Conserving)\n",
    "\n",
    "## What's new in V3\n",
    "- **Unified PushBoxEnv**: single 16-dim obs environment (push_box_env.py retired)\n",
    "- **PhysRobot uses SV-pipeline**: SVPhysicsCore guarantees momentum conservation by construction\n",
    "- **Fixed training steps**: all 3 methods train for 500K steps for fair comparison\n",
    "- **Clean code**: no emoji surrogate pairs, all code inline\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "PhysRobot V3 = Policy Stream (MLP) + Physics Stream (SV-GNN) + Fusion (concat+Linear)\n",
    "\n",
    "SV-Pipeline: Scalarize -> MLP -> Vectorize\n",
    "  For each undirected edge {i,j}:\n",
    "    1. Build frame {e1, e2, e3} (antisymmetric)\n",
    "    2. Project velocities onto frame -> rotation-invariant scalars\n",
    "    3. MLP produces force coefficients alpha1, alpha2, alpha3\n",
    "    4. Reconstruct: F_ij = alpha1*e1 + alpha2*e2 + alpha3*e3\n",
    "    5. Assign +F to j, -F to i (Newton's 3rd law hard-coded)\n",
    "  Result: sum_i F_i = 0 for ANY network parameters.\n",
    "```\n",
    "\n",
    "## Methods\n",
    "1. **Pure PPO** -- standard MLP, no physics\n",
    "2. **GNS + PPO** -- graph network, learns physics but no conservation guarantee\n",
    "3. **PhysRobot (ours)** -- SV-pipeline dual-stream, momentum conservation by construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install mujoco gymnasium stable-baselines3[extra] torch torch-geometric matplotlib pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "SAVE_DIR = '/content/drive/MyDrive/physrobot_v3'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENVIRONMENT: PushBoxEnv (Canonical 16-dim, unified)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import mujoco\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "XML = '''<mujoco model=\"push_box\">\n",
    "  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n",
    "  <option timestep=\"0.002\" integrator=\"Euler\" gravity=\"0 0 -9.81\">\n",
    "    <flag warmstart=\"enable\"/>\n",
    "  </option>\n",
    "  <visual>\n",
    "    <global offwidth=\"1280\" offheight=\"720\"/>\n",
    "    <quality shadowsize=\"4096\"/>\n",
    "    <map force=\"0.1\" zfar=\"30\"/>\n",
    "  </visual>\n",
    "  <asset>\n",
    "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"0.3 0.5 0.7\" rgb2=\"0.1 0.2 0.3\" type=\"skybox\" width=\"100\"/>\n",
    "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
    "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0.2 0.2 0.2\" rgb2=\"0.3 0.3 0.3\" type=\"2d\" width=\"100\"/>\n",
    "    <material name=\"MatPlane\" reflectance=\"0.3\" shininess=\"0.5\" specular=\"0.5\" texrepeat=\"3 3\" texture=\"texplane\"/>\n",
    "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
    "  </asset>\n",
    "  <default>\n",
    "    <joint armature=\"0.01\" damping=\"0.1\" limited=\"true\"/>\n",
    "    <geom conaffinity=\"1\" condim=\"3\" contype=\"1\" friction=\"0.5 0.005 0.0001\" margin=\"0.001\" material=\"geom\" rgba=\"0.8 0.6 0.4 1\"/>\n",
    "  </default>\n",
    "  <worldbody>\n",
    "    <light directional=\"true\" diffuse=\"0.8 0.8 0.8\" pos=\"0 0 3\" dir=\"0 0 -1\"/>\n",
    "    <light directional=\"true\" diffuse=\"0.4 0.4 0.4\" pos=\"0 0 3\" dir=\"1 1 -1\"/>\n",
    "    <geom name=\"floor\" type=\"plane\" size=\"3 3 0.1\" rgba=\"0.8 0.8 0.8 1\" material=\"MatPlane\"/>\n",
    "    <body name=\"arm_base\" pos=\"0 0 0.02\">\n",
    "      <geom name=\"base_geom\" type=\"cylinder\" size=\"0.05 0.02\" rgba=\"0.3 0.3 0.3 1\"/>\n",
    "      <body name=\"upper_arm\" pos=\"0 0 0.02\">\n",
    "        <joint name=\"shoulder\" type=\"hinge\" axis=\"0 0 1\" range=\"-180 180\" damping=\"0.5\"/>\n",
    "        <geom name=\"upper_arm_geom\" type=\"capsule\" fromto=\"0 0 0 0.4 0 0\" size=\"0.025\" rgba=\"0.5 0.5 0.8 1\"/>\n",
    "        <body name=\"forearm\" pos=\"0.4 0 0\">\n",
    "          <joint name=\"elbow\" type=\"hinge\" axis=\"0 0 1\" range=\"-180 180\" damping=\"0.5\"/>\n",
    "          <geom name=\"forearm_geom\" type=\"capsule\" fromto=\"0 0 0 0.3 0 0\" size=\"0.025\" rgba=\"0.5 0.5 0.8 1\"/>\n",
    "          <site name=\"endeffector\" pos=\"0.3 0 0\" size=\"0.03\" rgba=\"1 0.5 0 0.8\"/>\n",
    "        </body>\n",
    "      </body>\n",
    "    </body>\n",
    "    <body name=\"box\" pos=\"0.35 0 0.05\">\n",
    "      <freejoint name=\"box_freejoint\"/>\n",
    "      <geom name=\"box_geom\" type=\"box\" size=\"0.05 0.05 0.05\" mass=\"0.5\" rgba=\"0.2 0.8 0.2 1\" friction=\"0.5 0.005 0.0001\"/>\n",
    "      <site name=\"box_center\" pos=\"0 0 0\" size=\"0.01\" rgba=\"0 1 0 1\"/>\n",
    "    </body>\n",
    "    <site name=\"goal\" pos=\"0.5 0.3 0.02\" size=\"0.06\" rgba=\"1 0 0 0.4\" type=\"sphere\"/>\n",
    "  </worldbody>\n",
    "  <actuator>\n",
    "    <motor name=\"shoulder_motor\" joint=\"shoulder\" gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-10 10\"/>\n",
    "    <motor name=\"elbow_motor\" joint=\"elbow\" gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-10 10\"/>\n",
    "  </actuator>\n",
    "</mujoco>\n",
    "'''\n",
    "\n",
    "class PushBoxEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    PushBox V3 (Canonical): 2-DOF arm pushes box to goal.\n",
    "    Obs: 16-dim [joint_pos(2), joint_vel(2), ee_pos(3), box_pos(3), box_vel(3), goal(3)]\n",
    "    Action: 2-dim joint torques in [-10, 10] Nm\n",
    "    \"\"\"\n",
    "    def __init__(self, render_mode=None, box_mass=0.5):\n",
    "        super().__init__()\n",
    "        self.model = mujoco.MjModel.from_xml_string(XML)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        self.box_mass = box_mass\n",
    "        self._set_box_mass(box_mass)\n",
    "        self._ee_site_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, 'endeffector')\n",
    "        self.action_space = spaces.Box(low=-10.0, high=10.0, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(16,), dtype=np.float32)\n",
    "        self.goal_pos = np.array([0.5, 0.3, 0.02])\n",
    "        self.max_episode_steps = 500\n",
    "        self.current_step = 0\n",
    "        self.success_threshold = 0.15\n",
    "        self._prev_dist_box_goal = None\n",
    "        self._prev_dist_ee_box = None\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def _set_box_mass(self, mass):\n",
    "        box_body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'box')\n",
    "        self.model.body_mass[box_body_id] = mass\n",
    "\n",
    "    def set_box_mass(self, mass):\n",
    "        self.box_mass = mass\n",
    "        self._set_box_mass(mass)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.data.qpos[0] = np.random.uniform(-0.5, 0.5)\n",
    "        self.data.qpos[1] = np.random.uniform(-0.5, 0.5)\n",
    "        self.data.qpos[2] = np.random.uniform(0.25, 0.45)\n",
    "        self.data.qpos[3] = np.random.uniform(-0.15, 0.15)\n",
    "        self.data.qpos[4] = 0.05\n",
    "        self.data.qpos[5:9] = [1, 0, 0, 0]\n",
    "        self.data.qvel[:] = 0.0\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "        self.current_step = 0\n",
    "        ee_pos = self.data.site_xpos[self._ee_site_id]\n",
    "        box_pos = self.data.qpos[2:5]\n",
    "        self._prev_dist_box_goal = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n",
    "        self._prev_dist_ee_box = np.linalg.norm(ee_pos[:2] - box_pos[:2])\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        joint_pos = self.data.qpos[:2].copy()\n",
    "        joint_vel = self.data.qvel[:2].copy()\n",
    "        ee_pos = self.data.site_xpos[self._ee_site_id].copy()\n",
    "        box_pos = self.data.qpos[2:5].copy()\n",
    "        box_vel = self.data.qvel[2:5].copy()\n",
    "        return np.concatenate([joint_pos, joint_vel, ee_pos, box_pos, box_vel, self.goal_pos]).astype(np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        box_pos = self.data.qpos[2:5]\n",
    "        d = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n",
    "        return {'distance_to_goal': d, 'success': d < self.success_threshold,\n",
    "                'box_mass': self.box_mass, 'timestep': self.current_step}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        for _ in range(5):\n",
    "            mujoco.mj_step(self.model, self.data)\n",
    "        ee_pos = self.data.site_xpos[self._ee_site_id].copy()\n",
    "        box_pos = self.data.qpos[2:5].copy()\n",
    "        dist_ee_box = np.linalg.norm(ee_pos[:2] - box_pos[:2])\n",
    "        dist_box_goal = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n",
    "        reach_progress = self._prev_dist_ee_box - dist_ee_box\n",
    "        push_progress = self._prev_dist_box_goal - dist_box_goal\n",
    "        self._prev_dist_ee_box = dist_ee_box\n",
    "        self._prev_dist_box_goal = dist_box_goal\n",
    "        reward = (\n",
    "            0.5 * (-dist_ee_box) +\n",
    "            1.0 * (-dist_box_goal) +\n",
    "            10.0 * reach_progress +\n",
    "            20.0 * push_progress +\n",
    "            -0.01 * np.sum(action ** 2)\n",
    "        )\n",
    "        success = dist_box_goal < self.success_threshold\n",
    "        if success:\n",
    "            remaining = self.max_episode_steps - self.current_step\n",
    "            reward += 500.0 + remaining * 1.0\n",
    "        self.current_step += 1\n",
    "        terminated = success\n",
    "        truncated = self.current_step >= self.max_episode_steps\n",
    "        return self._get_obs(), reward, terminated, truncated, self._get_info()\n",
    "\n",
    "    def render(self): pass\n",
    "    def close(self): pass\n",
    "\n",
    "def make_push_box_env(box_mass=0.5):\n",
    "    def _init():\n",
    "        return PushBoxEnv(box_mass=box_mass)\n",
    "    return _init\n",
    "\n",
    "# Sanity check\n",
    "env = PushBoxEnv()\n",
    "obs, info = env.reset()\n",
    "print(f'Obs shape: {obs.shape}, Action shape: {env.action_space.shape}')\n",
    "print(f'Success threshold: {env.success_threshold}m')\n",
    "env.close()\n",
    "print('[OK] PushBoxEnv V3 loaded (unified 16-dim, progress reward)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SV MESSAGE PASSING (Physics Core -- pure PyTorch, no PyG)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "EPS = 1e-7\n",
    "DEG_EPS = 1e-4\n",
    "\n",
    "def _make_mlp(in_dim, hidden_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, out_dim))\n",
    "\n",
    "def build_edge_frames(pos, vel, src, dst):\n",
    "    \"\"\"Build antisymmetric edge frames. Returns e1, e2, e3, r_ij, d_ij.\"\"\"\n",
    "    r_ij = pos[dst] - pos[src]\n",
    "    d_ij = torch.norm(r_ij, dim=-1, keepdim=True)\n",
    "    e1 = r_ij / (d_ij + EPS)\n",
    "    v_rel = vel[dst] - vel[src]\n",
    "    v_par = (v_rel * e1).sum(dim=-1, keepdim=True) * e1\n",
    "    v_perp = v_rel - v_par\n",
    "    v_perp_norm = torch.norm(v_perp, dim=-1, keepdim=True)\n",
    "    non_deg = (v_perp_norm > DEG_EPS).float()\n",
    "    e2_vel = v_perp / (v_perp_norm + EPS)\n",
    "    z_hat = torch.tensor([0., 0., 1.], device=pos.device).expand_as(e1)\n",
    "    e2_f_raw = torch.cross(e1, z_hat, dim=-1)\n",
    "    e2_f_norm = torch.norm(e2_f_raw, dim=-1, keepdim=True)\n",
    "    use_y = (e2_f_norm < DEG_EPS).float()\n",
    "    y_hat = torch.tensor([0., 1., 0.], device=pos.device).expand_as(e1)\n",
    "    e2_f_raw2 = torch.cross(e1, y_hat, dim=-1)\n",
    "    e2_f_raw = (1 - use_y) * e2_f_raw + use_y * e2_f_raw2\n",
    "    e2_f_norm = torch.norm(e2_f_raw, dim=-1, keepdim=True)\n",
    "    e2_fall = e2_f_raw / (e2_f_norm + EPS)\n",
    "    e2 = non_deg * e2_vel + (1 - non_deg) * e2_fall\n",
    "    e3 = torch.cross(e1, e2, dim=-1)\n",
    "    return e1, e2, e3, r_ij, d_ij\n",
    "\n",
    "class SVMessagePassing(nn.Module):\n",
    "    \"\"\"One round of SV message passing. Guarantees F_ij = -F_ji.\"\"\"\n",
    "    def __init__(self, node_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        n_scalar = 5  # ||r||, v_r, v_t, v_b, ||v_rel||\n",
    "        self.force_mlp = _make_mlp(n_scalar + 2 * node_dim, hidden_dim, 3)\n",
    "        self.node_update = _make_mlp(node_dim + 3, hidden_dim, node_dim)\n",
    "\n",
    "    def _extract_pairs(self, edge_index):\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        mask = src < dst\n",
    "        return torch.stack([src[mask], dst[mask]], dim=0)\n",
    "\n",
    "    def forward_with_forces(self, h, edge_index, pos, vel):\n",
    "        N = h.size(0)\n",
    "        pairs = self._extract_pairs(edge_index)\n",
    "        pi, pj = pairs[0], pairs[1]\n",
    "        e1, e2, e3, r_ij, d_ij = build_edge_frames(pos, vel, pi, pj)\n",
    "        v_rel = vel[pj] - vel[pi]\n",
    "        v_r = (v_rel * e1).sum(-1, keepdim=True)\n",
    "        v_t = (v_rel * e2).sum(-1, keepdim=True)\n",
    "        v_b = (v_rel * e3).sum(-1, keepdim=True)\n",
    "        v_norm = torch.norm(v_rel, dim=-1, keepdim=True)\n",
    "        scalars = torch.cat([d_ij, v_r, v_t, v_b, v_norm,\n",
    "                             h[pi] + h[pj], (h[pi] - h[pj]).abs()], dim=-1)\n",
    "        alphas = self.force_mlp(scalars)\n",
    "        force_ij = alphas[:, 0:1] * e1 + alphas[:, 1:2] * e2 + alphas[:, 2:3] * e3\n",
    "        F_agg = torch.zeros(N, 3, device=h.device, dtype=h.dtype)\n",
    "        F_agg.scatter_add_(0, pj.unsqueeze(-1).expand_as(force_ij), force_ij)\n",
    "        F_agg.scatter_add_(0, pi.unsqueeze(-1).expand_as(force_ij), -force_ij)\n",
    "        h_new = h + self.node_update(torch.cat([h, F_agg], dim=-1))\n",
    "        return h_new, F_agg\n",
    "\n",
    "class SVPhysicsCore(nn.Module):\n",
    "    \"\"\"Physics stream: node encoder + L x SV layers -> conserved forces.\"\"\"\n",
    "    def __init__(self, node_input_dim=6, hidden_dim=32, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder = _make_mlp(node_input_dim, hidden_dim, hidden_dim)\n",
    "        self.sv_layers = nn.ModuleList([\n",
    "            SVMessagePassing(node_dim=hidden_dim, hidden_dim=hidden_dim)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, positions, velocities, edge_index):\n",
    "        h = self.encoder(torch.cat([positions, velocities], dim=-1))\n",
    "        forces = None\n",
    "        for layer in self.sv_layers:\n",
    "            h, F_agg = layer.forward_with_forces(h, edge_index, positions, velocities)\n",
    "            forces = F_agg\n",
    "        return forces\n",
    "\n",
    "# Quick conservation check\n",
    "torch.manual_seed(42)\n",
    "_m = SVPhysicsCore()\n",
    "_p = torch.randn(4, 3); _v = torch.randn(4, 3)\n",
    "_ei = torch.tensor([[0,0,0,1,1,1,2,2,2,3,3,3],\n",
    "                     [1,2,3,0,2,3,0,1,3,0,1,2]], dtype=torch.long)\n",
    "_F = _m(_p, _v, _ei)\n",
    "print(f'Conservation check: ||sum F|| = {_F.sum(0).norm().item():.2e}')\n",
    "assert _F.sum(0).norm().item() < 1e-4, 'CONSERVATION VIOLATED'\n",
    "print('[OK] SVPhysicsCore loaded -- momentum conservation verified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AGENTS: Pure PPO, GNS, PhysRobot (SV-pipeline)\n",
    "# ============================================================\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "try:\n",
    "    from torch_geometric.nn import MessagePassing\n",
    "    from torch_geometric.data import Data as PyGData, Batch as PyGBatch\n",
    "    HAS_PYG = True\n",
    "except ImportError:\n",
    "    HAS_PYG = False\n",
    "    print('torch_geometric not available -- GNS will use fallback MLP')\n",
    "\n",
    "# ---------- Callback ----------\n",
    "class SuccessTrackingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_count = 0\n",
    "        self.success_count = 0\n",
    "        self.success_achieved = False\n",
    "        self.episodes_to_success = None\n",
    "    def _on_step(self):\n",
    "        infos = self.locals.get('infos', [{}])\n",
    "        dones = self.locals.get('dones', [False])\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_count += 1\n",
    "                info = infos[i] if i < len(infos) else {}\n",
    "                if info.get('success', False):\n",
    "                    self.success_count += 1\n",
    "                    if not self.success_achieved:\n",
    "                        self.success_achieved = True\n",
    "                        self.episodes_to_success = self.episode_count\n",
    "                        print(f'\\n** First success at episode {self.episode_count}! **')\n",
    "                if self.episode_count % 200 == 0:\n",
    "                    rate = self.success_count / self.episode_count * 100\n",
    "                    print(f'  [Ep {self.episode_count}] Success rate: {rate:.1f}%')\n",
    "        return True\n",
    "\n",
    "# ---------- Agent 1: Pure PPO ----------\n",
    "class PurePPOAgent:\n",
    "    def __init__(self, env, verbose=1):\n",
    "        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048,\n",
    "                         batch_size=64, n_epochs=10, gamma=0.99, ent_coef=0.01,\n",
    "                         verbose=verbose)\n",
    "    def train(self, total_timesteps, callback=None):\n",
    "        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        action, _ = self.model.predict(obs, deterministic=deterministic)\n",
    "        return action\n",
    "    def save(self, path): self.model.save(path)\n",
    "    def evaluate(self, env, n_episodes=100):\n",
    "        rewards, successes = [], []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, info = env.reset()\n",
    "            done, ep_reward = False, 0\n",
    "            while not done:\n",
    "                action = self.predict(obs)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                ep_reward += reward\n",
    "                done = terminated or truncated\n",
    "            rewards.append(ep_reward)\n",
    "            successes.append(1 if info.get('success', False) else 0)\n",
    "        return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards),\n",
    "                'success_rate': np.mean(successes)}\n",
    "\n",
    "# ---------- Agent 2: GNS (graph network, no conservation guarantee) ----------\n",
    "if HAS_PYG:\n",
    "    def obs_to_graph_batch(observations):\n",
    "        B = observations.shape[0]\n",
    "        dev = observations.device\n",
    "        graphs = []\n",
    "        for i in range(B):\n",
    "            o = observations[i]\n",
    "            ee_pos, box_pos = o[4:7], o[7:10]\n",
    "            ee_vel = torch.zeros(3, device=dev)\n",
    "            box_vel = o[10:13]\n",
    "            node_feats = torch.stack([torch.cat([ee_pos, ee_vel]),\n",
    "                                      torch.cat([box_pos, box_vel])])\n",
    "            ei = torch.tensor([[0,1],[1,0]], dtype=torch.long, device=dev).t().contiguous()\n",
    "            rel01 = box_pos - ee_pos\n",
    "            rel10 = ee_pos - box_pos\n",
    "            d01 = torch.norm(rel01).unsqueeze(0)\n",
    "            d10 = torch.norm(rel10).unsqueeze(0)\n",
    "            ea = torch.stack([torch.cat([rel01, d01]), torch.cat([rel10, d10])])\n",
    "            graphs.append(PyGData(x=node_feats, edge_index=ei, edge_attr=ea))\n",
    "        return PyGBatch.from_data_list(graphs)\n",
    "\n",
    "    class GNSGraphLayerV2(MessagePassing):\n",
    "        def __init__(self, node_dim, edge_dim, hidden_dim=32):\n",
    "            super().__init__(aggr='add')\n",
    "            self.edge_mlp = nn.Sequential(\n",
    "                nn.Linear(2*node_dim + edge_dim, hidden_dim), nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, edge_dim))\n",
    "            self.node_mlp = nn.Sequential(\n",
    "                nn.Linear(node_dim + edge_dim, hidden_dim), nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, node_dim))\n",
    "        def forward(self, x, edge_index, edge_attr):\n",
    "            return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        def message(self, x_i, x_j, edge_attr):\n",
    "            return self.edge_mlp(torch.cat([x_i, x_j, edge_attr], dim=-1))\n",
    "        def update(self, aggr_out, x):\n",
    "            return self.node_mlp(torch.cat([x, aggr_out], dim=-1))\n",
    "\n",
    "    class GNSFeaturesExtractorV2(BaseFeaturesExtractor):\n",
    "        def __init__(self, observation_space, features_dim=64):\n",
    "            super().__init__(observation_space, features_dim)\n",
    "            hid, edge_dim = 32, 4\n",
    "            self.node_encoder = nn.Sequential(nn.Linear(6, hid), nn.ReLU())\n",
    "            self.edge_encoder = nn.Sequential(nn.Linear(edge_dim, hid), nn.ReLU())\n",
    "            self.gn_layer = GNSGraphLayerV2(hid, hid, hid)\n",
    "            self.decoder = nn.Linear(hid, 3)\n",
    "            self.feature_proj = nn.Sequential(nn.Linear(3 + 16, features_dim), nn.ReLU())\n",
    "        def forward(self, observations):\n",
    "            graph = obs_to_graph_batch(observations)\n",
    "            x = self.node_encoder(graph.x)\n",
    "            ea = self.edge_encoder(graph.edge_attr)\n",
    "            x = x + self.gn_layer(x, graph.edge_index, ea)\n",
    "            acc = self.decoder(x)\n",
    "            box_acc = acc[1::2]\n",
    "            return self.feature_proj(torch.cat([box_acc, observations], dim=-1))\n",
    "\n",
    "class GNSAgent:\n",
    "    def __init__(self, env, verbose=1):\n",
    "        if HAS_PYG:\n",
    "            policy_kwargs = dict(\n",
    "                features_extractor_class=GNSFeaturesExtractorV2,\n",
    "                features_extractor_kwargs=dict(features_dim=64),\n",
    "                net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "        else:\n",
    "            policy_kwargs = dict(net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048,\n",
    "                         batch_size=64, n_epochs=10, gamma=0.99, ent_coef=0.01,\n",
    "                         policy_kwargs=policy_kwargs, verbose=verbose)\n",
    "    def train(self, total_timesteps, callback=None):\n",
    "        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        action, _ = self.model.predict(obs, deterministic=deterministic)\n",
    "        return action\n",
    "    def save(self, path): self.model.save(path)\n",
    "    def evaluate(self, env, n_episodes=100):\n",
    "        rewards, successes = [], []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, info = env.reset()\n",
    "            done, ep_reward = False, 0\n",
    "            while not done:\n",
    "                action = self.predict(obs)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                ep_reward += reward\n",
    "                done = terminated or truncated\n",
    "            rewards.append(ep_reward)\n",
    "            successes.append(1 if info.get('success', False) else 0)\n",
    "        return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards),\n",
    "                'success_rate': np.mean(successes)}\n",
    "\n",
    "# ---------- Agent 3: PhysRobot V3 (SV-pipeline, conservation by construction) ----------\n",
    "class PhysRobotFeaturesExtractorV3(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Dual-stream extractor:\n",
    "      Policy  = MLP(obs)         -> z_policy  [features_dim]\n",
    "      Physics = SV-GNN(graph)    -> z_physics  [3]  (stop-gradient)\n",
    "      Fusion  = concat + Linear  -> features   [features_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        self.physics_core = SVPhysicsCore(node_input_dim=6, hidden_dim=32, n_layers=1)\n",
    "        self.policy_stream = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64), nn.ReLU(), nn.Linear(64, features_dim), nn.ReLU())\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(features_dim + 3, features_dim), nn.ReLU())\n",
    "        self._edge_index_2 = torch.tensor([[0, 1], [1, 0]], dtype=torch.long).t()\n",
    "\n",
    "    def _obs_to_graph(self, obs):\n",
    "        ee_pos = obs[:, 4:7]\n",
    "        box_pos = obs[:, 7:10]\n",
    "        box_vel = obs[:, 10:13]\n",
    "        ee_vel = torch.zeros_like(ee_pos)\n",
    "        return ee_pos, ee_vel, box_pos, box_vel\n",
    "\n",
    "    def forward(self, observations):\n",
    "        B = observations.shape[0]\n",
    "        device = observations.device\n",
    "        z_policy = self.policy_stream(observations)\n",
    "        ee_pos, ee_vel, box_pos, box_vel = self._obs_to_graph(observations)\n",
    "        edge_index = self._edge_index_2.to(device)\n",
    "        box_acc_list = []\n",
    "        for i in range(B):\n",
    "            pos_i = torch.stack([ee_pos[i], box_pos[i]], dim=0)\n",
    "            vel_i = torch.stack([ee_vel[i], box_vel[i]], dim=0)\n",
    "            acc_i = self.physics_core(pos_i, vel_i, edge_index)\n",
    "            box_acc_list.append(acc_i[1])  # box node\n",
    "        z_physics = torch.stack(box_acc_list, dim=0).detach()  # stop-gradient\n",
    "        return self.fusion(torch.cat([z_policy, z_physics], dim=-1))\n",
    "\n",
    "class PhysRobotAgent:\n",
    "    def __init__(self, env, verbose=1):\n",
    "        policy_kwargs = dict(\n",
    "            features_extractor_class=PhysRobotFeaturesExtractorV3,\n",
    "            features_extractor_kwargs=dict(features_dim=64),\n",
    "            net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048,\n",
    "                         batch_size=64, n_epochs=10, gamma=0.99, ent_coef=0.01,\n",
    "                         policy_kwargs=policy_kwargs, verbose=verbose)\n",
    "    def train(self, total_timesteps, callback=None):\n",
    "        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        action, _ = self.model.predict(obs, deterministic=deterministic)\n",
    "        return action\n",
    "    def save(self, path): self.model.save(path)\n",
    "    def evaluate(self, env, n_episodes=100):\n",
    "        rewards, successes = [], []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, info = env.reset()\n",
    "            done, ep_reward = False, 0\n",
    "            while not done:\n",
    "                action = self.predict(obs)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                ep_reward += reward\n",
    "                done = terminated or truncated\n",
    "            rewards.append(ep_reward)\n",
    "            successes.append(1 if info.get('success', False) else 0)\n",
    "        return {'mean_reward': np.mean(rewards), 'std_reward': np.std(rewards),\n",
    "                'success_rate': np.mean(successes)}\n",
    "\n",
    "# Print parameter counts\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "_tmp = DummyVecEnv([make_push_box_env()])\n",
    "_a1 = PurePPOAgent(_tmp, verbose=0)\n",
    "_a2 = GNSAgent(_tmp, verbose=0)\n",
    "_a3 = PhysRobotAgent(_tmp, verbose=0)\n",
    "print(f'Parameter counts:')\n",
    "print(f'  Pure PPO:       {count_params(_a1.model.policy):>8,}')\n",
    "print(f'  GNS V2:         {count_params(_a2.model.policy):>8,}')\n",
    "print(f'  PhysRobot V3:   {count_params(_a3.model.policy):>8,}')\n",
    "del _a1, _a2, _a3; _tmp.close()\n",
    "print('[OK] All 3 agents loaded (PhysRobot V3 = SV-pipeline)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "CONFIG = {\n",
    "    'ppo_timesteps': 500_000,\n",
    "    'gns_timesteps': 500_000,\n",
    "    'physrobot_timesteps': 500_000,\n",
    "    'n_envs': 4,\n",
    "    'box_mass': 0.5,\n",
    "    'eval_episodes': 100\n",
    "}\n",
    "print('V3 Configuration:', CONFIG)\n",
    "est_min = 3 * CONFIG['ppo_timesteps'] / 200_000 * 5\n",
    "print(f'Estimated training time: ~{est_min:.0f} min total on T4 GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ============================================================\n",
    "# TRAINING ALL 3 METHODS\n",
    "# ============================================================\n",
    "import time\n",
    "import json\n",
    "results = {}\n",
    "\n",
    "def train_and_eval(name, AgentClass, timesteps_key):\n",
    "    print('=' * 60)\n",
    "    print(f'TRAINING {name} ({CONFIG[timesteps_key]:,} steps)')\n",
    "    print('=' * 60)\n",
    "    env = DummyVecEnv([make_push_box_env(CONFIG['box_mass']) for _ in range(CONFIG['n_envs'])])\n",
    "    agent = AgentClass(env, verbose=0)\n",
    "    callback = SuccessTrackingCallback(verbose=1)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        agent.train(CONFIG[timesteps_key], callback=callback)\n",
    "        train_time = time.time() - start\n",
    "        eval_env = PushBoxEnv(box_mass=CONFIG['box_mass'])\n",
    "        eval_result = agent.evaluate(eval_env, n_episodes=CONFIG['eval_episodes'])\n",
    "        eval_env.close()\n",
    "        res = {\n",
    "            'success_rate': eval_result['success_rate'],\n",
    "            'mean_reward': eval_result['mean_reward'],\n",
    "            'std_reward': eval_result['std_reward'],\n",
    "            'episodes_to_success': callback.episodes_to_success,\n",
    "            'total_episodes': callback.episode_count,\n",
    "            'train_time_sec': train_time\n",
    "        }\n",
    "        agent.save(f'{SAVE_DIR}/{name.lower().replace(\" \", \"_\")}')\n",
    "        print(f'\\n{name} results:')\n",
    "        print(f'  Success rate: {res[\"success_rate\"]:.1%}')\n",
    "        print(f'  Mean reward:  {res[\"mean_reward\"]:.1f}')\n",
    "        print(f'  First success: ep {res[\"episodes_to_success\"]}')\n",
    "        print(f'  Training time: {train_time:.0f}s')\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        res = {'error': str(e)}\n",
    "    finally:\n",
    "        env.close()\n",
    "    return res\n",
    "\n",
    "results['Pure PPO'] = train_and_eval('Pure PPO', PurePPOAgent, 'ppo_timesteps')\n",
    "results['GNS'] = train_and_eval('GNS', GNSAgent, 'gns_timesteps')\n",
    "results['PhysRobot V3'] = train_and_eval('PhysRobot V3', PhysRobotAgent, 'physrobot_timesteps')\n",
    "\n",
    "# Save results\n",
    "with open(f'{SAVE_DIR}/results_v3.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print('\\nAll results saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS COMPARISON\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rows = []\n",
    "for method, res in results.items():\n",
    "    if 'error' not in res:\n",
    "        rows.append({\n",
    "            'Method': method,\n",
    "            'Success Rate': f\"{res['success_rate']:.1%}\",\n",
    "            'Mean Reward': f\"{res['mean_reward']:.1f} +/- {res['std_reward']:.1f}\",\n",
    "            'Ep to 1st Success': res['episodes_to_success'] or 'N/A',\n",
    "            'Total Episodes': res['total_episodes'],\n",
    "            'Train Time (s)': f\"{res['train_time_sec']:.0f}\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print('\\n=== Table 1: Sample Efficiency Comparison ===')\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "methods = [r['Method'] for r in rows]\n",
    "success_rates = [results[m]['success_rate'] for m in results if 'error' not in results[m]]\n",
    "ep_to_success = [results[m].get('episodes_to_success') or 0 for m in results if 'error' not in results[m]]\n",
    "\n",
    "colors = ['#4C72B0', '#55A868', '#C44E52']\n",
    "\n",
    "axes[0].bar(methods, success_rates, color=colors)\n",
    "axes[0].set_ylabel('Success Rate')\n",
    "axes[0].set_title('Final Success Rate')\n",
    "axes[0].set_ylim(0, 1.05)\n",
    "\n",
    "axes[1].bar(methods, ep_to_success, color=colors)\n",
    "axes[1].set_ylabel('Episodes')\n",
    "axes[1].set_title('Episodes to First Success')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/results_v3.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'Figure saved to {SAVE_DIR}/results_v3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OOD GENERALIZATION TEST (Table 2)\n",
    "# ============================================================\n",
    "print('=== OOD Generalization Test ===')\n",
    "print('Testing with box masses: 0.5, 1.0, 2.0, 5.0 kg')\n",
    "\n",
    "ood_masses = [0.5, 1.0, 2.0, 5.0]\n",
    "ood_results = {}\n",
    "\n",
    "# Reload best agents\n",
    "agents = {\n",
    "    'Pure PPO': PurePPOAgent,\n",
    "    'GNS': GNSAgent,\n",
    "    'PhysRobot V3': PhysRobotAgent\n",
    "}\n",
    "\n",
    "for method_name, AgentClass in agents.items():\n",
    "    ood_results[method_name] = {}\n",
    "    # Create a dummy env to load the agent\n",
    "    dummy_env = DummyVecEnv([make_push_box_env()])\n",
    "    agent = AgentClass(dummy_env, verbose=0)\n",
    "    model_path = f'{SAVE_DIR}/{method_name.lower().replace(\" \", \"_\")}'\n",
    "    try:\n",
    "        agent.model = PPO.load(model_path)\n",
    "    except Exception:\n",
    "        print(f'  Could not load {model_path}, skipping...')\n",
    "        dummy_env.close()\n",
    "        continue\n",
    "    dummy_env.close()\n",
    "\n",
    "    for mass in ood_masses:\n",
    "        test_env = PushBoxEnv(box_mass=mass)\n",
    "        res = agent.evaluate(test_env, n_episodes=50)\n",
    "        ood_results[method_name][mass] = res['success_rate']\n",
    "        test_env.close()\n",
    "        print(f'  {method_name} @ mass={mass}: success={res[\"success_rate\"]:.1%}')\n",
    "\n",
    "# Plot OOD results\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for i, (method, mass_results) in enumerate(ood_results.items()):\n",
    "    if mass_results:\n",
    "        masses = sorted(mass_results.keys())\n",
    "        rates = [mass_results[m] for m in masses]\n",
    "        ax.plot(masses, rates, 'o-', label=method, color=colors[i], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Box Mass (kg)')\n",
    "ax.set_ylabel('Success Rate')\n",
    "ax.set_title('OOD Generalization (train mass=0.5, test on different masses)')\n",
    "ax.legend()\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/ood_v3.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'OOD figure saved to {SAVE_DIR}/ood_v3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONSERVATION VERIFICATION (PhysRobot only)\n",
    "# ============================================================\n",
    "print('=== Momentum Conservation Verification ===')\n",
    "print('Running 100 random trials across different graph sizes...\\n')\n",
    "\n",
    "# Test with the standalone SVPhysicsCore\n",
    "test_model = SVPhysicsCore(node_input_dim=6, hidden_dim=32, n_layers=1)\n",
    "all_errors = []\n",
    "\n",
    "for N in [2, 3, 4, 5, 8]:\n",
    "    errors = []\n",
    "    for _ in range(100):\n",
    "        pos = torch.randn(N, 3)\n",
    "        vel = torch.randn(N, 3)\n",
    "        src, dst = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    src.append(i); dst.append(j)\n",
    "        ei = torch.tensor([src, dst], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            F = test_model(pos, vel, ei)\n",
    "        err = F.sum(0).norm().item()\n",
    "        errors.append(err)\n",
    "    max_err = max(errors)\n",
    "    all_errors.extend(errors)\n",
    "    status = 'PASS' if max_err < 1e-3 else 'FAIL'\n",
    "    print(f'  N={N}: max ||sum F|| = {max_err:.2e}  [{status}]')\n",
    "\n",
    "print(f'\\nOverall: max error across all trials = {max(all_errors):.2e}')\n",
    "print(f'Conservation guaranteed by architecture (not learned).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print('=' * 60)\n",
    "print('V3 EXPERIMENT COMPLETE')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('Key results:')\n",
    "for method, res in results.items():\n",
    "    if 'error' not in res:\n",
    "        print(f'  {method}:')\n",
    "        print(f'    Success rate:       {res[\"success_rate\"]:.1%}')\n",
    "        print(f'    Ep to 1st success:  {res[\"episodes_to_success\"]}')\n",
    "        print(f'    Mean reward:        {res[\"mean_reward\"]:.1f}')\n",
    "print()\n",
    "print('What V3 proves:')\n",
    "print('  1. SV-pipeline PhysRobot maintains momentum conservation by construction')\n",
    "print('  2. All methods trained for same number of steps (fair comparison)')\n",
    "print('  3. Physics stream predicts forces, not just embeddings')\n",
    "print()\n",
    "print(f'All outputs saved to: {SAVE_DIR}')"
   ]
  }
 ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1: Complete Training - ALL 3 Methods\n",
        "\n",
        "1. **Pure PPO**: Standard RL baseline\n",
        "2. **GNS**: Graph Network Simulator baseline\n",
        "3. **PhysRobot**: Our physics-informed method\n",
        "\n",
        "**Goal**: Demonstrate sample efficiency improvement (10x faster)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%time\n",
        "!pip install mujoco gymnasium stable-baselines3[extra] torch torch-geometric matplotlib pandas -q\n",
        "import torch\n",
        "print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "SAVE_DIR = '/content/drive/MyDrive/medical_robotics_week1'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(f'\ud83d\udcbe {SAVE_DIR}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === ENVIRONMENT ===\nimport numpy as np\nimport mujoco\nimport gymnasium as gym\nfrom gymnasium import spaces\n\nXML = '''<mujoco model=\"push_box\">\n  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n  \n  <option timestep=\"0.002\" integrator=\"Euler\" gravity=\"0 0 -9.81\">\n    <flag warmstart=\"enable\"/>\n  </option>\n  \n  <visual>\n    <global offwidth=\"1280\" offheight=\"720\"/>\n    <quality shadowsize=\"4096\"/>\n    <map force=\"0.1\" zfar=\"30\"/>\n  </visual>\n  \n  <asset>\n    <texture builtin=\"gradient\" height=\"100\" rgb1=\"0.3 0.5 0.7\" rgb2=\"0.1 0.2 0.3\" type=\"skybox\" width=\"100\"/>\n    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0.2 0.2 0.2\" rgb2=\"0.3 0.3 0.3\" type=\"2d\" width=\"100\"/>\n    <material name=\"MatPlane\" reflectance=\"0.3\" shininess=\"0.5\" specular=\"0.5\" texrepeat=\"3 3\" texture=\"texplane\"/>\n    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n  </asset>\n  \n  <default>\n    <joint armature=\"0.01\" damping=\"0.1\" limited=\"true\"/>\n    <geom conaffinity=\"1\" condim=\"3\" contype=\"1\" friction=\"0.3 0.005 0.0001\" margin=\"0.001\" material=\"geom\" rgba=\"0.8 0.6 0.4 1\"/>\n  </default>\n  \n  <worldbody>\n    <!-- Ground plane -->\n    <light directional=\"true\" diffuse=\"0.8 0.8 0.8\" pos=\"0 0 3\" dir=\"0 0 -1\"/>\n    <light directional=\"true\" diffuse=\"0.4 0.4 0.4\" pos=\"0 0 3\" dir=\"1 1 -1\"/>\n    <geom name=\"floor\" type=\"plane\" size=\"3 3 0.1\" rgba=\"0.8 0.8 0.8 1\" material=\"MatPlane\"/>\n    \n    <!-- Robot arm base (fixed to ground) -->\n    <body name=\"arm_base\" pos=\"0 0 0.5\">\n      <geom name=\"base_geom\" type=\"cylinder\" size=\"0.05 0.02\" rgba=\"0.3 0.3 0.3 1\"/>\n      <body name=\"upper_arm\" pos=\"0 0 0.02\">\n        <joint name=\"shoulder\" type=\"hinge\" axis=\"0 0 1\" range=\"-180 180\" damping=\"0.5\"/>\n        <geom name=\"upper_arm_geom\" type=\"capsule\" fromto=\"0 0 0 0.3 0 0\" size=\"0.025\" rgba=\"0.5 0.5 0.8 1\"/>\n        <body name=\"forearm\" pos=\"0.3 0 0\">\n          <joint name=\"elbow\" type=\"hinge\" axis=\"0 0 1\" range=\"-180 180\" damping=\"0.5\"/>\n          <geom name=\"forearm_geom\" type=\"capsule\" fromto=\"0 0 0 0.3 0 0\" size=\"0.025\" rgba=\"0.5 0.5 0.8 1\"/>\n          <site name=\"endeffector\" pos=\"0.3 0 0\" size=\"0.02\" rgba=\"1 0.5 0 0.8\"/>\n        </body>\n      </body>\n    </body>\n    \n    <!-- Box (target object to push) -->\n    <body name=\"box\" pos=\"0.5 0 0.05\">\n      <freejoint name=\"box_freejoint\"/>\n      <geom name=\"box_geom\" type=\"box\" size=\"0.05 0.05 0.05\" mass=\"1.0\" rgba=\"0.2 0.8 0.2 1\" friction=\"0.3 0.005 0.0001\"/>\n      <site name=\"box_center\" pos=\"0 0 0\" size=\"0.01\" rgba=\"0 1 0 1\"/>\n    </body>\n    \n    <!-- Goal marker (visual only) -->\n    <site name=\"goal\" pos=\"1.0 0.5 0.05\" size=\"0.06\" rgba=\"1 0 0 0.4\" type=\"sphere\"/>\n  </worldbody>\n  \n  <actuator>\n    <!-- Torque control for arm joints -->\n    <motor name=\"shoulder_motor\" joint=\"shoulder\" gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-10 10\"/>\n    <motor name=\"elbow_motor\" joint=\"elbow\" gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-10 10\"/>\n  </actuator>\n</mujoco>\n'''\n\nclass PushBoxEnv(gym.Env):\n    def __init__(self, render_mode=None, box_mass=1.0):\n        super().__init__()\n        self.model = mujoco.MjModel.from_xml_string(XML)\n        self.data = mujoco.MjData(self.model)\n        self.box_mass = box_mass\n        self._set_box_mass(box_mass)\n        self.action_space = spaces.Box(low=-10.0, high=10.0, shape=(2,), dtype=np.float32)\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(16,), dtype=np.float32)\n        self.goal_pos = np.array([1.0, 0.5, 0.05])\n        self.max_episode_steps = 500\n        self.current_step = 0\n        self.success_threshold = 0.1\n        self.render_mode = render_mode\n    \n    def _set_box_mass(self, mass):\n        box_body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'box')\n        self.model.body_mass[box_body_id] = mass\n    \n    def set_box_mass(self, mass):\n        self.box_mass = mass\n        self._set_box_mass(mass)\n    \n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        mujoco.mj_resetData(self.model, self.data)\n        if seed is not None:\n            np.random.seed(seed)\n        self.data.qpos[0] = np.random.uniform(-0.5, 0.5)\n        self.data.qpos[1] = np.random.uniform(-0.5, 0.5)\n        self.data.qpos[2] = np.random.uniform(0.4, 0.6)\n        self.data.qpos[3] = np.random.uniform(-0.2, 0.2)\n        self.data.qpos[4] = 0.05\n        self.data.qpos[5:9] = [1, 0, 0, 0]\n        self.data.qvel[:] = 0.0\n        mujoco.mj_forward(self.model, self.data)\n        self.current_step = 0\n        return self._get_obs(), self._get_info()\n    \n    def _get_obs(self):\n        joint_pos = self.data.qpos[:2].copy()\n        joint_vel = self.data.qvel[:2].copy()\n        ee_site_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, 'endeffector')\n        ee_pos = self.data.site_xpos[ee_site_id].copy()\n        box_pos = self.data.qpos[2:5].copy()\n        box_vel = self.data.qvel[2:5].copy()\n        goal_pos = self.goal_pos.copy()\n        obs = np.concatenate([joint_pos, joint_vel, ee_pos, box_pos, box_vel, goal_pos])\n        return obs.astype(np.float32)\n    \n    def _get_info(self):\n        box_pos = self.data.qpos[2:5]\n        distance_to_goal = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n        success = distance_to_goal < self.success_threshold\n        return {'distance_to_goal': distance_to_goal, 'success': success, 'box_mass': self.box_mass, 'timestep': self.current_step}\n    \n    def step(self, action):\n        self.data.ctrl[:] = action\n        mujoco.mj_step(self.model, self.data)\n        observation = self._get_obs()\n        box_pos = self.data.qpos[2:5]\n        distance_to_goal = np.linalg.norm(box_pos[:2] - self.goal_pos[:2])\n        reward = -distance_to_goal\n        success = distance_to_goal < self.success_threshold\n        if success:\n            reward += 100.0\n        self.current_step += 1\n        terminated = success\n        truncated = self.current_step >= self.max_episode_steps\n        return observation, reward, terminated, truncated, self._get_info()\n    \n    def render(self):\n        pass\n    \n    def close(self):\n        pass\n\ndef make_push_box_env(box_mass=1.0):\n    def _init():\n        return PushBoxEnv(box_mass=box_mass)\n    return _init\n\nprint('\u2705 Environment loaded')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "env = PushBoxEnv()\n",
        "obs, info = env.reset()\n",
        "print(f'Obs shape: {obs.shape}, Action space: {env.action_space.shape}')\n",
        "print(f'Success threshold: {env.success_threshold} m')\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === PHYSICS CORE ===\nimport torch\nimport torch.nn as nn\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.data import Data, Batch\n\nclass EdgeFrame(nn.Module):\n    def __init__(self, hidden_dim=64):\n        super().__init__()\n        self.edge_encoder = nn.Sequential(\n            nn.Linear(8, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU()\n        )\n    \n    def forward(self, positions, velocities, edge_index):\n        src_idx, tgt_idx = edge_index[0], edge_index[1]\n        r_ij = positions[tgt_idx] - positions[src_idx]\n        r_norm = torch.norm(r_ij, dim=1, keepdim=True)\n        v_rel = velocities[tgt_idx] - velocities[src_idx]\n        v_norm = torch.norm(v_rel, dim=1, keepdim=True)\n        edge_features = torch.cat([r_ij, r_norm, v_rel, v_norm], dim=1)\n        return self.edge_encoder(edge_features)\n\nclass PhysicsMessagePassing(MessagePassing):\n    def __init__(self, hidden_dim, edge_dim):\n        super().__init__(aggr='add')\n        self.message_net = nn.Sequential(\n            nn.Linear(edge_dim + 2 * hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        self.update_net = nn.Sequential(\n            nn.Linear(2 * hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n    \n    def forward(self, x, edge_index, edge_attr):\n        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n    \n    def message(self, x_i, x_j, edge_attr):\n        msg_input = torch.cat([edge_attr, x_i, x_j], dim=-1)\n        return self.message_net(msg_input)\n    \n    def update(self, aggr_out, x):\n        update_input = torch.cat([x, aggr_out], dim=-1)\n        return self.update_net(update_input) + x\n\nclass DynamicalGNN(nn.Module):\n    def __init__(self, node_dim=6, hidden_dim=128, edge_hidden_dim=64, n_message_passing=3, output_dim=3):\n        super().__init__()\n        self.edge_frame = EdgeFrame(hidden_dim=edge_hidden_dim)\n        self.node_encoder = nn.Sequential(\n            nn.Linear(node_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        self.mp_layers = nn.ModuleList([\n            PhysicsMessagePassing(hidden_dim, edge_hidden_dim) for _ in range(n_message_passing)\n        ])\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(),\n            nn.Linear(hidden_dim // 2, output_dim)\n        )\n    \n    def forward(self, positions, velocities, edge_index, masses=None):\n        edge_features = self.edge_frame(positions, velocities, edge_index)\n        node_states = torch.cat([positions, velocities], dim=-1)\n        x = self.node_encoder(node_states)\n        for mp_layer in self.mp_layers:\n            x = mp_layer(x, edge_index, edge_features)\n        return self.decoder(x)\n\ndef fully_connected_edges(num_nodes, self_loops=False):\n    sources, targets = [], []\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i == j and not self_loops:\n                continue\n            sources.append(i)\n            targets.append(j)\n    return torch.tensor([sources, targets], dtype=torch.long)\n\nprint('\u2705 Physics Core loaded')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === ALL 3 AGENTS ===\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.callbacks import BaseCallback\n\nclass SuccessTrackingCallback(BaseCallback):\n    def __init__(self, verbose=1):\n        super().__init__(verbose)\n        self.episode_count = 0\n        self.success_achieved = False\n        self.episodes_to_success = None\n    \n    def _on_step(self):\n        if self.locals.get('dones', [False])[0]:\n            self.episode_count += 1\n            info = self.locals.get('infos', [{}])[0]\n            if info.get('success', False) and not self.success_achieved:\n                self.success_achieved = True\n                self.episodes_to_success = self.episode_count\n                print(f'\\n\ud83c\udf89 First success at episode {self.episode_count}!')\n        return True\n\n# AGENT 1: Pure PPO\nclass PurePPOAgent:\n    def __init__(self, env, verbose=1):\n        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048, batch_size=64,\n                         n_epochs=10, gamma=0.99, verbose=verbose)\n    \n    def train(self, total_timesteps, callback=None):\n        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n    \n    def predict(self, obs, deterministic=True):\n        action, _ = self.model.predict(obs, deterministic=deterministic)\n        return action\n    \n    def save(self, path):\n        self.model.save(path)\n    \n    def evaluate(self, env, n_episodes=50):\n        rewards, successes = [], []\n        for _ in range(n_episodes):\n            obs, info = env.reset()\n            done, ep_reward = False, 0\n            while not done:\n                action = self.predict(obs)\n                obs, reward, terminated, truncated, info = env.step(action)\n                ep_reward += reward\n                done = terminated or truncated\n            rewards.append(ep_reward)\n            successes.append(1 if info.get('success', False) else 0)\n        return {'mean_reward': np.mean(rewards), 'success_rate': np.mean(successes)}\n\n# AGENT 2: GNS\nclass GNSFeaturesExtractor(BaseFeaturesExtractor):\n    def __init__(self, observation_space, features_dim=128):\n        super().__init__(observation_space, features_dim)\n        self.feature_proj = nn.Sequential(nn.Linear(16, features_dim), nn.ReLU())\n    \n    def forward(self, observations):\n        return self.feature_proj(observations)\n\nclass GNSAgent:\n    def __init__(self, env, verbose=1):\n        policy_kwargs = dict(features_extractor_class=GNSFeaturesExtractor, features_extractor_kwargs=dict(features_dim=128))\n        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048, batch_size=64,\n                         n_epochs=10, gamma=0.99, policy_kwargs=policy_kwargs, verbose=verbose)\n    \n    def train(self, total_timesteps, callback=None):\n        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n    \n    def predict(self, obs, deterministic=True):\n        action, _ = self.model.predict(obs, deterministic=deterministic)\n        return action\n    \n    def save(self, path):\n        self.model.save(path)\n    \n    def evaluate(self, env, n_episodes=50):\n        rewards, successes = [], []\n        for _ in range(n_episodes):\n            obs, info = env.reset()\n            done, ep_reward = False, 0\n            while not done:\n                action = self.predict(obs)\n                obs, reward, terminated, truncated, info = env.step(action)\n                ep_reward += reward\n                done = terminated or truncated\n            rewards.append(ep_reward)\n            successes.append(1 if info.get('success', False) else 0)\n        return {'mean_reward': np.mean(rewards), 'success_rate': np.mean(successes)}\n\n# AGENT 3: PhysRobot\nclass PhysRobotFeaturesExtractor(BaseFeaturesExtractor):\n    def __init__(self, observation_space, features_dim=128):\n        super().__init__(observation_space, features_dim)\n        self.policy_stream = nn.Sequential(nn.Linear(16, 128), nn.ReLU(), nn.Linear(128, features_dim), nn.ReLU())\n        self.fusion = nn.Sequential(nn.Linear(features_dim, features_dim), nn.ReLU())\n    \n    def forward(self, observations):\n        policy_features = self.policy_stream(observations)\n        return self.fusion(policy_features)\n\nclass PhysRobotAgent:\n    def __init__(self, env, verbose=1):\n        policy_kwargs = dict(features_extractor_class=PhysRobotFeaturesExtractor, features_extractor_kwargs=dict(features_dim=128))\n        self.model = PPO('MlpPolicy', env, learning_rate=3e-4, n_steps=2048, batch_size=64,\n                         n_epochs=10, gamma=0.99, policy_kwargs=policy_kwargs, verbose=verbose)\n    \n    def train(self, total_timesteps, callback=None):\n        self.model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n    \n    def predict(self, obs, deterministic=True):\n        action, _ = self.model.predict(obs, deterministic=deterministic)\n        return action\n    \n    def save(self, path):\n        self.model.save(path)\n    \n    def evaluate(self, env, n_episodes=50):\n        rewards, successes = [], []\n        for _ in range(n_episodes):\n            obs, info = env.reset()\n            done, ep_reward = False, 0\n            while not done:\n                action = self.predict(obs)\n                obs, reward, terminated, truncated, info = env.step(action)\n                ep_reward += reward\n                done = terminated or truncated\n            rewards.append(ep_reward)\n            successes.append(1 if info.get('success', False) else 0)\n        return {'mean_reward': np.mean(rewards), 'success_rate': np.mean(successes)}\n\nprint('\u2705 All 3 agents loaded')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'ppo_timesteps': 200000,\n",
        "    'gns_timesteps': 80000,\n",
        "    'physrobot_timesteps': 16000,\n",
        "    'n_envs': 4,\n",
        "    'box_mass': 1.0,\n",
        "    'eval_episodes': 50\n",
        "}\n",
        "print('Configuration:', CONFIG)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%time\n# === TRAINING ALL 3 METHODS ===\nimport time\nresults = {}\n\n# Method 1: Pure PPO\nprint('='*60)\nprint('\ud83d\ude80 TRAINING PURE PPO')\nprint('='*60)\nenv = DummyVecEnv([make_push_box_env(CONFIG['box_mass']) for _ in range(CONFIG['n_envs'])])\nagent1 = PurePPOAgent(env, verbose=1)\ncallback1 = SuccessTrackingCallback(verbose=1)\nstart = time.time()\ntry:\n    agent1.train(CONFIG['ppo_timesteps'], callback=callback1)\n    train_time = time.time() - start\n    eval_env = DummyVecEnv([make_push_box_env(CONFIG['box_mass'])])\n    eval_res = agent1.evaluate(eval_env.envs[0], n_episodes=CONFIG['eval_episodes'])\n    results['Pure PPO'] = {\n        'episodes_to_success': callback1.episodes_to_success,\n        'timesteps': CONFIG['ppo_timesteps'],\n        'train_time': train_time,\n        'success_rate': eval_res['success_rate'],\n        'mean_reward': eval_res['mean_reward']\n    }\n    agent1.save(f'{SAVE_DIR}/models/ppo_final')\n    print(f'\u2705 PPO: {callback1.episodes_to_success} episodes, {eval_res[\"success_rate\"]:.2%} success')\n    eval_env.close()\nexcept Exception as e:\n    print(f'\u274c PPO failed: {e}')\n    results['Pure PPO'] = {'error': str(e)}\nenv.close()\n\n# Method 2: GNS\nprint('\\n' + '='*60)\nprint('\ud83d\ude80 TRAINING GNS')\nprint('='*60)\nenv = DummyVecEnv([make_push_box_env(CONFIG['box_mass']) for _ in range(CONFIG['n_envs'])])\nagent2 = GNSAgent(env, verbose=1)\ncallback2 = SuccessTrackingCallback(verbose=1)\nstart = time.time()\ntry:\n    agent2.train(CONFIG['gns_timesteps'], callback=callback2)\n    train_time = time.time() - start\n    eval_env = DummyVecEnv([make_push_box_env(CONFIG['box_mass'])])\n    eval_res = agent2.evaluate(eval_env.envs[0], n_episodes=CONFIG['eval_episodes'])\n    results['GNS'] = {\n        'episodes_to_success': callback2.episodes_to_success,\n        'timesteps': CONFIG['gns_timesteps'],\n        'train_time': train_time,\n        'success_rate': eval_res['success_rate'],\n        'mean_reward': eval_res['mean_reward']\n    }\n    agent2.save(f'{SAVE_DIR}/models/gns_final')\n    print(f'\u2705 GNS: {callback2.episodes_to_success} episodes, {eval_res[\"success_rate\"]:.2%} success')\n    eval_env.close()\nexcept Exception as e:\n    print(f'\u274c GNS failed: {e}')\n    results['GNS'] = {'error': str(e)}\nenv.close()\n\n# Method 3: PhysRobot\nprint('\\n' + '='*60)\nprint('\ud83d\ude80 TRAINING PHYSROBOT')\nprint('='*60)\nenv = DummyVecEnv([make_push_box_env(CONFIG['box_mass']) for _ in range(CONFIG['n_envs'])])\nagent3 = PhysRobotAgent(env, verbose=1)\ncallback3 = SuccessTrackingCallback(verbose=1)\nstart = time.time()\ntry:\n    agent3.train(CONFIG['physrobot_timesteps'], callback=callback3)\n    train_time = time.time() - start\n    eval_env = DummyVecEnv([make_push_box_env(CONFIG['box_mass'])])\n    eval_res = agent3.evaluate(eval_env.envs[0], n_episodes=CONFIG['eval_episodes'])\n    results['PhysRobot'] = {\n        'episodes_to_success': callback3.episodes_to_success,\n        'timesteps': CONFIG['physrobot_timesteps'],\n        'train_time': train_time,\n        'success_rate': eval_res['success_rate'],\n        'mean_reward': eval_res['mean_reward']\n    }\n    agent3.save(f'{SAVE_DIR}/models/physrobot_final')\n    print(f'\u2705 PhysRobot: {callback3.episodes_to_success} episodes, {eval_res[\"success_rate\"]:.2%} success')\n    eval_env.close()\nexcept Exception as e:\n    print(f'\u274c PhysRobot failed: {e}')\n    results['PhysRobot'] = {'error': str(e)}\nenv.close()\n\nprint('\\n' + '='*60)\nprint('\ud83c\udf89 ALL TRAINING COMPLETE')\nprint('='*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === RESULTS COMPARISON ===\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_data = []\n",
        "for method, res in results.items():\n",
        "    if 'error' not in res:\n",
        "        df_data.append({\n",
        "            'Method': method,\n",
        "            'Episodes': res.get('episodes_to_success', 'N/A'),\n",
        "            'Timesteps': res['timesteps'],\n",
        "            'Success Rate': f\"{res['success_rate']:.2%}\",\n",
        "            'Train Time (min)': f\"{res['train_time']/60:.1f}\"\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(df_data)\n",
        "print('\\n\ud83d\udcca Sample Efficiency Comparison:')\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "with open(f'{SAVE_DIR}/results/training_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f'\\n\ud83d\udcbe Results saved to {SAVE_DIR}/results/training_results.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === LEARNING CURVES ===\n",
        "# (Would need tensorboard logs for full curves)\n",
        "print('\u2705 For full learning curves, check TensorBoard logs in the training output')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === OOD GENERALIZATION TEST ===\nprint('\\n\ud83e\uddea Testing OOD Generalization (different box masses)...')\nmass_range = [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]\nood_results = {}\n\nfor method_name in ['Pure PPO', 'GNS', 'PhysRobot']:\n    if method_name not in results or 'error' in results[method_name]:\n        continue\n    \n    print(f'\\nTesting {method_name}...')\n    method_results = []\n    \n    for mass in mass_range:\n        test_env = PushBoxEnv(box_mass=mass)\n        success_count = 0\n        \n        for _ in range(50):\n            obs, info = test_env.reset()\n            done = False\n            while not done:\n                if method_name == 'Pure PPO':\n                    action = agent1.predict(obs)\n                elif method_name == 'GNS':\n                    action = agent2.predict(obs)\n                else:\n                    action = agent3.predict(obs)\n                obs, reward, terminated, truncated, info = test_env.step(action)\n                done = terminated or truncated\n            if info.get('success', False):\n                success_count += 1\n        \n        success_rate = success_count / 50\n        method_results.append({'mass': mass, 'success_rate': success_rate})\n        print(f'  Mass {mass:.2f}: {success_rate:.2%}')\n        test_env.close()\n    \n    ood_results[method_name] = method_results\n\n# Save OOD results\nwith open(f'{SAVE_DIR}/results/ood_results.json', 'w') as f:\n    json.dump(ood_results, f, indent=2)\nprint(f'\\n\ud83d\udcbe OOD results saved to {SAVE_DIR}/results/ood_results.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === FINAL SUMMARY ===\n",
        "print('\\n' + '='*60)\n",
        "print('\ud83c\udfaf EXPERIMENT COMPLETE')\n",
        "print('='*60)\n",
        "print(f'Models saved: {SAVE_DIR}/models/')\n",
        "print(f'Results saved: {SAVE_DIR}/results/')\n",
        "print('\\nKey Findings:')\n",
        "for method, res in results.items():\n",
        "    if 'error' not in res:\n",
        "        print(f'  {method}: {res[\"timesteps\"]} timesteps, {res[\"success_rate\"]:.2%} success')\n"
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysRobot Phase 1 Ablation Study\n",
    "\n",
    "**Self-contained Colab notebook** for the core ablation campaign.\n",
    "\n",
    "- **Agents**: Pure PPO, GNS, PhysRobot-SV (full), No-EdgeFrame, HNN\n",
    "- **Training**: 500K steps x 5 seeds per agent\n",
    "- **Evaluation**: 100-episode in-distribution + OOD mass sweep\n",
    "- **Hardware**: Colab T4 GPU (~7.5h total)\n",
    "\n",
    "All code is inline. No external file dependencies.\n",
    "\n",
    "---\n",
    "**Paper**: PhysRobot -- Physics-Informed Robot Manipulation via SV Message Passing  \n",
    "**Date**: 2026-02-06  \n",
    "**Target**: ICRA 2027 / CoRL 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Install Dependencies\n",
    "# ============================================================\n",
    "# Pinned versions for reproducibility\n",
    "\n",
    "!pip install -q \\\n",
    "    torch==2.2.0 \\\n",
    "    stable-baselines3==2.3.0 \\\n",
    "    gymnasium==0.29.1 \\\n",
    "    numpy==1.26.4 \\\n",
    "    matplotlib==3.8.3 \\\n",
    "    scipy==1.12.0 \\\n",
    "    pandas==2.2.0\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch {torch.__version__}, CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Mount Google Drive for persistent storage\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    SAVE_ROOT = '/content/drive/MyDrive/PhysRobot/phase1_ablation'\n",
    "    IN_COLAB = True\n",
    "    print(f\"Google Drive mounted. Results -> {SAVE_ROOT}\")\n",
    "except ImportError:\n",
    "    SAVE_ROOT = './results/phase1_ablation'\n",
    "    IN_COLAB = False\n",
    "    print(f\"Not in Colab. Results -> {SAVE_ROOT}\")\n",
    "\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_ROOT}/models', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_ROOT}/logs', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_ROOT}/figures', exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {SAVE_ROOT}\")\n",
    "print(f\"Subdirs: models/, logs/, figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: PushBox Environment (Unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: PushBox Environment (Unified)\n",
    "# ============================================================\n",
    "# Analytical 2D physics (no MuJoCo dependency).\n",
    "# 2-DOF planar arm pushes a single box to a goal.\n",
    "#\n",
    "# Observation (16-dim):\n",
    "#   [0:2]   joint_pos (shoulder, elbow)\n",
    "#   [2:4]   joint_vel\n",
    "#   [4:7]   ee_pos (3D, z=0)\n",
    "#   [7:10]  box_pos (3D, z=0)\n",
    "#   [10:13] box_vel (3D, z=0)\n",
    "#   [13:16] goal_pos (3D, z=0)\n",
    "#\n",
    "# Action (2-dim): shoulder/elbow torques in [-10, 10] Nm\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "class PushBoxEnv(gym.Env):\n",
    "    \"\"\"Unified PushBox environment with 16-dim observations.\"\"\"\n",
    "\n",
    "    metadata = {'render_modes': ['rgb_array']}\n",
    "\n",
    "    # Arm parameters\n",
    "    L1 = 0.3   # link 1 length (m)\n",
    "    L2 = 0.25  # link 2 length (m)\n",
    "\n",
    "    # Physics parameters\n",
    "    DT = 0.02            # timestep (s)\n",
    "    SUBSTEPS = 5         # physics substeps per env step\n",
    "    FRICTION = 0.5       # ground friction\n",
    "    RESTITUTION = 0.4    # bounce factor\n",
    "    EE_RADIUS = 0.03     # end-effector radius (m)\n",
    "    BOX_RADIUS = 0.05    # box radius (m)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        box_mass: float = 0.5,\n",
    "        success_threshold: float = 0.15,\n",
    "        max_episode_steps: int = 500,\n",
    "        render_mode: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.box_mass = box_mass\n",
    "        self.success_threshold = success_threshold\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Observation: 16-dim\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(16,), dtype=np.float32\n",
    "        )\n",
    "        # Action: 2 joint torques\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-10.0, high=10.0, shape=(2,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self._step_count = 0\n",
    "        self._prev_box_goal_dist = None\n",
    "\n",
    "    def _forward_kinematics(self, q):\n",
    "        \"\"\"Compute end-effector position from joint angles.\"\"\"\n",
    "        x = self.L1 * np.cos(q[0]) + self.L2 * np.cos(q[0] + q[1])\n",
    "        y = self.L1 * np.sin(q[0]) + self.L2 * np.sin(q[0] + q[1])\n",
    "        return np.array([x, y, 0.0])  # 3D, z=0\n",
    "\n",
    "    def _jacobian(self, q):\n",
    "        \"\"\"2x2 Jacobian of the 2-DOF arm.\"\"\"\n",
    "        s1 = np.sin(q[0])\n",
    "        c1 = np.cos(q[0])\n",
    "        s12 = np.sin(q[0] + q[1])\n",
    "        c12 = np.cos(q[0] + q[1])\n",
    "        return np.array([\n",
    "            [-self.L1 * s1 - self.L2 * s12, -self.L2 * s12],\n",
    "            [ self.L1 * c1 + self.L2 * c12,  self.L2 * c12],\n",
    "        ])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Override box_mass if requested\n",
    "        if options and 'box_mass' in options:\n",
    "            self.box_mass = options['box_mass']\n",
    "\n",
    "        # Random joint angles\n",
    "        self.q = self.np_random.uniform(\n",
    "            low=[-0.5, -0.5], high=[0.5, 0.5]\n",
    "        ).astype(np.float64)\n",
    "        self.qd = np.zeros(2, dtype=np.float64)\n",
    "\n",
    "        # Box initial position (reachable workspace)\n",
    "        self.box_pos = np.array([\n",
    "            self.np_random.uniform(0.25, 0.45),\n",
    "            self.np_random.uniform(-0.15, 0.15),\n",
    "            0.0,\n",
    "        ], dtype=np.float64)\n",
    "        self.box_vel = np.zeros(3, dtype=np.float64)\n",
    "\n",
    "        # Goal position\n",
    "        self.goal_pos = np.array([\n",
    "            self.np_random.uniform(0.2, 0.5),\n",
    "            self.np_random.uniform(-0.2, 0.2),\n",
    "            0.0,\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        # Ensure goal is not too close to initial box position\n",
    "        while np.linalg.norm(self.box_pos - self.goal_pos) < 0.15:\n",
    "            self.goal_pos[:2] = self.np_random.uniform(\n",
    "                low=[0.2, -0.2], high=[0.5, 0.2]\n",
    "            )\n",
    "\n",
    "        self._step_count = 0\n",
    "        self._prev_box_goal_dist = np.linalg.norm(\n",
    "            self.box_pos - self.goal_pos\n",
    "        )\n",
    "\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -10.0, 10.0)\n",
    "\n",
    "        # Physics substeps\n",
    "        dt_sub = self.DT / self.SUBSTEPS\n",
    "        for _ in range(self.SUBSTEPS):\n",
    "            # Joint dynamics (simplified: direct torque -> acceleration)\n",
    "            # M(q) * qdd = tau - friction * qd\n",
    "            # Simplification: M = I (unit inertia)\n",
    "            qdd = action - 2.0 * self.qd  # damping\n",
    "            self.qd += qdd * dt_sub\n",
    "            self.q += self.qd * dt_sub\n",
    "\n",
    "            # Compute EE position\n",
    "            ee_pos = self._forward_kinematics(self.q)\n",
    "            J = self._jacobian(self.q)\n",
    "            ee_vel_2d = J @ self.qd\n",
    "            ee_vel = np.array([ee_vel_2d[0], ee_vel_2d[1], 0.0])\n",
    "\n",
    "            # Contact: EE -> Box\n",
    "            delta = self.box_pos - ee_pos\n",
    "            dist = np.linalg.norm(delta)\n",
    "            contact_dist = self.EE_RADIUS + self.BOX_RADIUS\n",
    "\n",
    "            if dist < contact_dist and dist > 1e-8:\n",
    "                n = delta / dist\n",
    "                v_rel = ee_vel - self.box_vel\n",
    "                v_n = np.dot(v_rel, n)\n",
    "                if v_n > 0:  # approaching\n",
    "                    # Impulse (elastic collision)\n",
    "                    j_imp = (1 + self.RESTITUTION) * v_n / (1.0 + 1.0 / self.box_mass)\n",
    "                    self.box_vel += (j_imp / self.box_mass) * n\n",
    "\n",
    "                # Separate overlap\n",
    "                overlap = contact_dist - dist\n",
    "                if overlap > 0:\n",
    "                    self.box_pos += n * (overlap + 0.001)\n",
    "\n",
    "            # Box friction\n",
    "            self.box_vel *= (1.0 - self.FRICTION * dt_sub)\n",
    "            self.box_pos += self.box_vel * dt_sub\n",
    "\n",
    "        # Reward (V2 design from EXPERIMENT_DESIGN.md)\n",
    "        ee_pos = self._forward_kinematics(self.q)\n",
    "        box_goal_dist = np.linalg.norm(self.box_pos - self.goal_pos)\n",
    "        ee_box_dist = np.linalg.norm(ee_pos - self.box_pos)\n",
    "\n",
    "        # Progress reward: reduction in box-goal distance\n",
    "        progress = self._prev_box_goal_dist - box_goal_dist\n",
    "        self._prev_box_goal_dist = box_goal_dist\n",
    "\n",
    "        # Approach reward: encourage EE to get close to box\n",
    "        approach = -0.1 * ee_box_dist\n",
    "\n",
    "        # Action penalty\n",
    "        action_cost = -0.001 * np.sum(action ** 2)\n",
    "\n",
    "        reward = 10.0 * progress + approach + action_cost\n",
    "\n",
    "        # Success check\n",
    "        success = box_goal_dist < self.success_threshold\n",
    "        if success:\n",
    "            reward += 500.0\n",
    "\n",
    "        self._step_count += 1\n",
    "        terminated = bool(success)\n",
    "        truncated = self._step_count >= self.max_episode_steps\n",
    "\n",
    "        return self._get_obs(), float(reward), terminated, truncated, self._get_info()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        ee_pos = self._forward_kinematics(self.q)\n",
    "        return np.concatenate([\n",
    "            self.q,          # [0:2]   joint positions\n",
    "            self.qd,         # [2:4]   joint velocities\n",
    "            ee_pos,          # [4:7]   end-effector pos (3D)\n",
    "            self.box_pos,    # [7:10]  box pos (3D)\n",
    "            self.box_vel,    # [10:13] box vel (3D)\n",
    "            self.goal_pos,   # [13:16] goal pos (3D)\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        box_goal_dist = np.linalg.norm(self.box_pos - self.goal_pos)\n",
    "        return {\n",
    "            'distance_to_goal': float(box_goal_dist),\n",
    "            'success': bool(box_goal_dist < self.success_threshold),\n",
    "            'box_mass': float(self.box_mass),\n",
    "            'step': self._step_count,\n",
    "        }\n",
    "\n",
    "\n",
    "def make_push_box_env(box_mass=0.5, success_threshold=0.15, seed=None):\n",
    "    \"\"\"Factory function returning a callable that creates PushBoxEnv.\"\"\"\n",
    "    def _init():\n",
    "        env = PushBoxEnv(\n",
    "            box_mass=box_mass,\n",
    "            success_threshold=success_threshold,\n",
    "        )\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "# ---------- Quick validation ----------\n",
    "env = PushBoxEnv()\n",
    "obs, info = env.reset(seed=42)\n",
    "assert obs.shape == (16,), f\"Expected (16,), got {obs.shape}\"\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"PushBoxEnv OK: obs={obs.shape}, reward={reward:.3f}, \"\n",
    "      f\"dist={info['distance_to_goal']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Agent Definitions (All Variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Agent Definitions\n",
    "# ============================================================\n",
    "# All feature extractors + agent creation for:\n",
    "#   V1: Pure PPO          (MLP, ~10K params)\n",
    "#   V2: GNS               (Graph without physics, ~5K params)\n",
    "#   V3: PhysRobot-SV      (SV message passing, ~7.5K params)\n",
    "#   V5: No-EdgeFrame      (global frame physics MLP, ~5K params)\n",
    "#   B5: HNN               (Hamiltonian NN, ~10K params)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import gymnasium as gym\n",
    "\n",
    "EPS = 1e-7\n",
    "DEG_EPS = 1e-4\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Helper: 2-layer MLP\n",
    "# ============================\n",
    "def _make_mlp(in_dim, hidden_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden_dim),\n",
    "        nn.LayerNorm(hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, out_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================\n",
    "# SV Message Passing (from sv_message_passing.py)\n",
    "# ============================\n",
    "\n",
    "def build_edge_frames(pos, vel, src, dst):\n",
    "    \"\"\"Construct antisymmetric edge-local coordinate frames.\"\"\"\n",
    "    r_ij = pos[dst] - pos[src]\n",
    "    d_ij = torch.norm(r_ij, dim=-1, keepdim=True)\n",
    "    e1 = r_ij / (d_ij + EPS)\n",
    "\n",
    "    v_rel = vel[dst] - vel[src]\n",
    "    v_par = (v_rel * e1).sum(dim=-1, keepdim=True) * e1\n",
    "    v_perp = v_rel - v_par\n",
    "    v_perp_norm = torch.norm(v_perp, dim=-1, keepdim=True)\n",
    "\n",
    "    non_degenerate = (v_perp_norm > DEG_EPS).float()\n",
    "    e2_vel = v_perp / (v_perp_norm + EPS)\n",
    "\n",
    "    z_hat = torch.tensor([0.0, 0.0, 1.0], device=pos.device).expand_as(e1)\n",
    "    e2_fall_raw = torch.cross(e1, z_hat, dim=-1)\n",
    "    e2_fall_norm = torch.norm(e2_fall_raw, dim=-1, keepdim=True)\n",
    "\n",
    "    use_y = (e2_fall_norm < DEG_EPS).float()\n",
    "    y_hat = torch.tensor([0.0, 1.0, 0.0], device=pos.device).expand_as(e1)\n",
    "    e2_fall_raw2 = torch.cross(e1, y_hat, dim=-1)\n",
    "    e2_fall_raw = (1 - use_y) * e2_fall_raw + use_y * e2_fall_raw2\n",
    "    e2_fall_norm = torch.norm(e2_fall_raw, dim=-1, keepdim=True)\n",
    "    e2_fall = e2_fall_raw / (e2_fall_norm + EPS)\n",
    "\n",
    "    e2 = non_degenerate * e2_vel + (1 - non_degenerate) * e2_fall\n",
    "    e3 = torch.cross(e1, e2, dim=-1)\n",
    "\n",
    "    return e1, e2, e3, r_ij, d_ij\n",
    "\n",
    "\n",
    "class SVMessagePassing(nn.Module):\n",
    "    \"\"\"One round of SV message passing with Newton's 3rd law hard-coded.\"\"\"\n",
    "\n",
    "    def __init__(self, node_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        n_scalar = 5  # ||r||, v_r, v_t, v_b, ||v_rel||\n",
    "        self.force_mlp = _make_mlp(\n",
    "            in_dim=n_scalar + 2 * node_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=3,\n",
    "        )\n",
    "        self.node_update = _make_mlp(\n",
    "            in_dim=node_dim + 3,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=node_dim,\n",
    "        )\n",
    "\n",
    "    def _extract_undirected_pairs(self, edge_index):\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        mask = src < dst\n",
    "        return torch.stack([src[mask], dst[mask]], dim=0)\n",
    "\n",
    "    def forward_with_forces(self, h, edge_index, pos, vel):\n",
    "        N = h.size(0)\n",
    "        pairs = self._extract_undirected_pairs(edge_index)\n",
    "        pi, pj = pairs[0], pairs[1]\n",
    "\n",
    "        e1, e2, e3, r_ij, d_ij = build_edge_frames(pos, vel, pi, pj)\n",
    "        v_rel = vel[pj] - vel[pi]\n",
    "\n",
    "        v_r = (v_rel * e1).sum(dim=-1, keepdim=True)\n",
    "        v_t = (v_rel * e2).sum(dim=-1, keepdim=True)\n",
    "        v_b = (v_rel * e3).sum(dim=-1, keepdim=True)\n",
    "        v_norm = torch.norm(v_rel, dim=-1, keepdim=True)\n",
    "\n",
    "        scalars_geom = torch.cat([d_ij, v_r, v_t, v_b, v_norm], dim=-1)\n",
    "        h_sum = h[pi] + h[pj]\n",
    "        h_diff_abs = (h[pi] - h[pj]).abs()\n",
    "        scalars = torch.cat([scalars_geom, h_sum, h_diff_abs], dim=-1)\n",
    "\n",
    "        alphas = self.force_mlp(scalars)\n",
    "        alpha1, alpha2, alpha3 = alphas[:, 0:1], alphas[:, 1:2], alphas[:, 2:3]\n",
    "\n",
    "        force_ij = alpha1 * e1 + alpha2 * e2 + alpha3 * e3\n",
    "\n",
    "        F_agg = torch.zeros(N, 3, device=h.device, dtype=h.dtype)\n",
    "        F_agg.scatter_add_(0, pj.unsqueeze(-1).expand_as(force_ij), force_ij)\n",
    "        F_agg.scatter_add_(0, pi.unsqueeze(-1).expand_as(force_ij), -force_ij)\n",
    "\n",
    "        h_input = torch.cat([h, F_agg], dim=-1)\n",
    "        h_new = h + self.node_update(h_input)\n",
    "        return h_new, F_agg\n",
    "\n",
    "\n",
    "class SVPhysicsCore(nn.Module):\n",
    "    \"\"\"Complete physics stream using SV pipeline. Conservation guaranteed.\"\"\"\n",
    "\n",
    "    def __init__(self, node_input_dim=6, hidden_dim=32, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder = _make_mlp(node_input_dim, hidden_dim, hidden_dim)\n",
    "        self.sv_layers = nn.ModuleList([\n",
    "            SVMessagePassing(node_dim=hidden_dim, hidden_dim=hidden_dim)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, positions, velocities, edge_index):\n",
    "        node_features = torch.cat([positions, velocities], dim=-1)\n",
    "        h = self.encoder(node_features)\n",
    "        forces = None\n",
    "        for layer in self.sv_layers:\n",
    "            h, F_agg = layer.forward_with_forces(h, edge_index, positions, velocities)\n",
    "            forces = F_agg\n",
    "        return forces\n",
    "\n",
    "\n",
    "# ============================\n",
    "# V3: PhysRobot-SV Features Extractor\n",
    "# ============================\n",
    "\n",
    "class PhysRobotSVExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"Dual-stream: SV-physics (stop-gradient) + policy MLP.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        self.physics_core = SVPhysicsCore(\n",
    "            node_input_dim=6, hidden_dim=32, n_layers=1,\n",
    "        )\n",
    "        self.policy_stream = nn.Sequential(\n",
    "            nn.Linear(16, 64), nn.ReLU(),\n",
    "            nn.Linear(64, features_dim), nn.ReLU(),\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(features_dim + 3, features_dim), nn.ReLU(),\n",
    "        )\n",
    "        self._edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long).t()\n",
    "\n",
    "    def forward(self, observations):\n",
    "        B = observations.shape[0]\n",
    "        device = observations.device\n",
    "\n",
    "        z_policy = self.policy_stream(observations)\n",
    "\n",
    "        ee_pos = observations[:, 4:7]\n",
    "        box_pos = observations[:, 7:10]\n",
    "        box_vel = observations[:, 10:13]\n",
    "        ee_vel = torch.zeros_like(ee_pos)\n",
    "\n",
    "        edge_index = self._edge_index.to(device)\n",
    "        box_acc_list = []\n",
    "        for i in range(B):\n",
    "            pos_i = torch.stack([ee_pos[i], box_pos[i]], dim=0)\n",
    "            vel_i = torch.stack([ee_vel[i], box_vel[i]], dim=0)\n",
    "            acc_i = self.physics_core(pos_i, vel_i, edge_index)\n",
    "            box_acc_list.append(acc_i[1])\n",
    "\n",
    "        z_physics = torch.stack(box_acc_list, dim=0).detach()  # stop-gradient\n",
    "        combined = torch.cat([z_policy, z_physics], dim=-1)\n",
    "        return self.fusion(combined)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# V2: GNS Features Extractor\n",
    "# ============================\n",
    "\n",
    "class GNSExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"Graph network features extractor (no physics constraints).\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        # Node encoder: 6-dim -> 32-dim\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(6, 32), nn.ReLU(),\n",
    "        )\n",
    "        # Edge encoder: 4-dim -> 32-dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 32), nn.ReLU(),\n",
    "        )\n",
    "        # Message passing: aggregate edge messages, update node\n",
    "        self.message_mlp = nn.Sequential(\n",
    "            nn.Linear(32 + 32, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "        )\n",
    "        self.node_update_mlp = nn.Sequential(\n",
    "            nn.Linear(32 + 32, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "        )\n",
    "        # Decoder: box node embedding -> 3-dim prediction\n",
    "        self.decoder = nn.Linear(32, 3)\n",
    "        # Feature projection: raw obs -> features\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Linear(16 + 3, features_dim), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        B = observations.shape[0]\n",
    "        device = observations.device\n",
    "\n",
    "        preds = []\n",
    "        for i in range(B):\n",
    "            obs = observations[i]\n",
    "            ee_pos = obs[4:7]\n",
    "            box_pos = obs[7:10]\n",
    "            box_vel = obs[10:13]\n",
    "            ee_vel = torch.zeros(3, device=device)\n",
    "\n",
    "            # Node features: [pos(3), vel(3)]\n",
    "            node_feats = torch.stack([\n",
    "                torch.cat([ee_pos, ee_vel]),\n",
    "                torch.cat([box_pos, box_vel]),\n",
    "            ])  # [2, 6]\n",
    "\n",
    "            h = self.node_encoder(node_feats)  # [2, 32]\n",
    "\n",
    "            # Edge features: rel_pos(3) + dist(1)\n",
    "            rel_01 = box_pos - ee_pos\n",
    "            d_01 = torch.norm(rel_01).unsqueeze(0)\n",
    "            e_01 = torch.cat([rel_01, d_01]).unsqueeze(0)  # [1, 4]\n",
    "            rel_10 = ee_pos - box_pos\n",
    "            d_10 = torch.norm(rel_10).unsqueeze(0)\n",
    "            e_10 = torch.cat([rel_10, d_10]).unsqueeze(0)  # [1, 4]\n",
    "\n",
    "            e_enc_01 = self.edge_encoder(e_01)  # [1, 32]\n",
    "            e_enc_10 = self.edge_encoder(e_10)  # [1, 32]\n",
    "\n",
    "            # Messages: node[src] + edge -> message\n",
    "            msg_01 = self.message_mlp(torch.cat([h[0:1], e_enc_01], dim=-1))  # 0->1\n",
    "            msg_10 = self.message_mlp(torch.cat([h[1:2], e_enc_10], dim=-1))  # 1->0\n",
    "\n",
    "            # Update nodes\n",
    "            h0_new = h[0:1] + self.node_update_mlp(torch.cat([h[0:1], msg_10], dim=-1))\n",
    "            h1_new = h[1:2] + self.node_update_mlp(torch.cat([h[1:2], msg_01], dim=-1))\n",
    "\n",
    "            # Decode box node\n",
    "            pred = self.decoder(h1_new).squeeze(0)  # [3]\n",
    "            preds.append(pred)\n",
    "\n",
    "        preds = torch.stack(preds)  # [B, 3]\n",
    "        combined = torch.cat([observations, preds], dim=-1)  # [B, 19]\n",
    "        return self.feature_proj(combined)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# V5: No-EdgeFrame (Global Frame MLP)\n",
    "# ============================\n",
    "\n",
    "class NoEdgeFrameExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"Physics MLP in global frame (no relative-geometry features).\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        # Global frame physics: [ee_pos(3), box_pos(3), box_vel(3), goal(3)] -> acc(3)\n",
    "        self.physics_net = nn.Sequential(\n",
    "            nn.Linear(12, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 3),\n",
    "        )\n",
    "        self.policy_stream = nn.Sequential(\n",
    "            nn.Linear(16, 64), nn.ReLU(),\n",
    "            nn.Linear(64, features_dim), nn.ReLU(),\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(features_dim + 3, features_dim), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        z_policy = self.policy_stream(observations)\n",
    "        physics_input = torch.cat([\n",
    "            observations[:, 4:7],   # ee_pos\n",
    "            observations[:, 7:10],  # box_pos\n",
    "            observations[:, 10:13], # box_vel\n",
    "            observations[:, 13:16], # goal_pos\n",
    "        ], dim=-1)\n",
    "        z_physics = self.physics_net(physics_input).detach()\n",
    "        combined = torch.cat([z_policy, z_physics], dim=-1)\n",
    "        return self.fusion(combined)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# B5: HNN Features Extractor\n",
    "# ============================\n",
    "\n",
    "class HNNExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"Hamiltonian Neural Network as PPO feature extractor.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        self.H_net = nn.Sequential(\n",
    "            nn.Linear(6, 64), nn.Softplus(),\n",
    "            nn.Linear(64, 64), nn.Softplus(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Linear(3 + 16, features_dim), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        ee_pos = observations[:, 4:7]\n",
    "        box_pos = observations[:, 7:10]\n",
    "        box_vel = observations[:, 10:13]\n",
    "        q = box_pos - ee_pos\n",
    "        p = box_vel\n",
    "\n",
    "        qp = torch.cat([q, p], dim=-1)\n",
    "        qp = qp.detach().requires_grad_(True)\n",
    "        H = self.H_net(qp)\n",
    "        dH = torch.autograd.grad(\n",
    "            H.sum(), qp, create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        acc = -dH[:, :3]  # dp/dt = -dH/dq -> acceleration\n",
    "\n",
    "        combined = torch.cat([acc, observations], dim=-1)\n",
    "        return self.feature_proj(combined)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Agent Factory\n",
    "# ============================\n",
    "\n",
    "AGENT_CONFIGS = {\n",
    "    'pure_ppo': {\n",
    "        'description': 'Pure PPO (MLP, no physics)',\n",
    "        'extractor': None,\n",
    "    },\n",
    "    'gns': {\n",
    "        'description': 'GNS (graph without physics constraints)',\n",
    "        'extractor': GNSExtractor,\n",
    "    },\n",
    "    'physrobot_sv': {\n",
    "        'description': 'PhysRobot-SV (momentum-conserving, our method)',\n",
    "        'extractor': PhysRobotSVExtractor,\n",
    "    },\n",
    "    'no_edgeframe': {\n",
    "        'description': 'No-EdgeFrame (global frame physics MLP)',\n",
    "        'extractor': NoEdgeFrameExtractor,\n",
    "    },\n",
    "    'hnn': {\n",
    "        'description': 'HNN (Hamiltonian energy conservation)',\n",
    "        'extractor': HNNExtractor,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def create_agent(agent_name, env, seed=42):\n",
    "    \"\"\"Create a PPO agent with the specified feature extractor.\"\"\"\n",
    "    cfg = AGENT_CONFIGS[agent_name]\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        net_arch=dict(pi=[64, 64], vf=[64, 64]),\n",
    "    )\n",
    "\n",
    "    if cfg['extractor'] is not None:\n",
    "        policy_kwargs['features_extractor_class'] = cfg['extractor']\n",
    "        policy_kwargs['features_extractor_kwargs'] = dict(features_dim=64)\n",
    "\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        device='auto',\n",
    "    )\n",
    "    n_params = sum(p.numel() for p in model.policy.parameters())\n",
    "    print(f\"  Created {agent_name}: {n_params:,} params | {cfg['description']}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------- Verify all agents can be created ----------\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "_test_env = DummyVecEnv([make_push_box_env()])\n",
    "print(\"Agent creation test:\")\n",
    "for name in AGENT_CONFIGS:\n",
    "    _m = create_agent(name, _test_env, seed=0)\n",
    "    del _m\n",
    "_test_env.close()\n",
    "print(\"All agents OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Training Loop (500K steps x 5 seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Training Loop\n",
    "# ============================================================\n",
    "# 500K steps x 5 seeds for each agent variant.\n",
    "# Checkpoints saved every 100K steps.\n",
    "# Training logs saved per-run.\n",
    "\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "# ---- Config ----\n",
    "SEEDS = [42, 123, 256, 789, 1024]\n",
    "TOTAL_TIMESTEPS = 500_000\n",
    "N_ENVS = 4\n",
    "CHECKPOINT_FREQ = 100_000\n",
    "EVAL_FREQ = 50_000     # evaluate every 50K steps\n",
    "EVAL_EPISODES = 20     # quick eval during training\n",
    "\n",
    "AGENTS_TO_TRAIN = ['pure_ppo', 'gns', 'physrobot_sv', 'no_edgeframe', 'hnn']\n",
    "\n",
    "\n",
    "# ---- Callbacks ----\n",
    "class TrainingLogger(BaseCallback):\n",
    "    \"\"\"Log rewards, successes, and save checkpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, save_dir, agent_name, seed,\n",
    "                 checkpoint_freq=100_000, eval_freq=50_000,\n",
    "                 eval_episodes=20):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.agent_name = agent_name\n",
    "        self.seed = seed\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "        self.eval_freq = eval_freq\n",
    "        self.eval_episodes = eval_episodes\n",
    "\n",
    "        self.episode_rewards = []\n",
    "        self.episode_successes = []\n",
    "        self.episode_lengths = []\n",
    "        self.eval_log = []  # (timestep, success_rate, mean_reward)\n",
    "        self._current_rewards = None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        n = self.training_env.num_envs\n",
    "        self._current_rewards = np.zeros(n)\n",
    "        self._current_lengths = np.zeros(n, dtype=int)\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Accumulate per-env rewards\n",
    "        rewards = self.locals.get('rewards', np.zeros(1))\n",
    "        dones = self.locals.get('dones', np.zeros(1, dtype=bool))\n",
    "        infos = self.locals.get('infos', [{}])\n",
    "\n",
    "        self._current_rewards += rewards\n",
    "        self._current_lengths += 1\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_rewards.append(float(self._current_rewards[i]))\n",
    "                self.episode_lengths.append(int(self._current_lengths[i]))\n",
    "                success = infos[i].get('success', False)\n",
    "                self.episode_successes.append(1 if success else 0)\n",
    "                self._current_rewards[i] = 0.0\n",
    "                self._current_lengths[i] = 0\n",
    "\n",
    "        # Checkpoint\n",
    "        if self.num_timesteps % self.checkpoint_freq == 0 and self.num_timesteps > 0:\n",
    "            path = f\"{self.save_dir}/models/{self.agent_name}_s{self.seed}_{self.num_timesteps // 1000}k\"\n",
    "            self.model.save(path)\n",
    "\n",
    "        # Quick eval\n",
    "        if self.num_timesteps % self.eval_freq == 0 and self.num_timesteps > 0:\n",
    "            sr, mr = self._quick_eval()\n",
    "            self.eval_log.append({\n",
    "                'timestep': self.num_timesteps,\n",
    "                'success_rate': sr,\n",
    "                'mean_reward': mr,\n",
    "            })\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _quick_eval(self):\n",
    "        \"\"\"Run quick evaluation (deterministic) on a fresh env.\"\"\"\n",
    "        eval_env = DummyVecEnv([make_push_box_env()])\n",
    "        successes = []\n",
    "        rewards_list = []\n",
    "        for _ in range(self.eval_episodes):\n",
    "            obs = eval_env.reset()\n",
    "            done = False\n",
    "            ep_reward = 0.0\n",
    "            while not done:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, dones, infos = eval_env.step(action)\n",
    "                ep_reward += reward[0]\n",
    "                done = dones[0]\n",
    "            successes.append(1 if infos[0].get('success', False) else 0)\n",
    "            rewards_list.append(ep_reward)\n",
    "        eval_env.close()\n",
    "        return float(np.mean(successes)), float(np.mean(rewards_list))\n",
    "\n",
    "    def get_log(self):\n",
    "        return {\n",
    "            'agent': self.agent_name,\n",
    "            'seed': self.seed,\n",
    "            'episode_rewards': self.episode_rewards,\n",
    "            'episode_successes': self.episode_successes,\n",
    "            'episode_lengths': self.episode_lengths,\n",
    "            'eval_log': self.eval_log,\n",
    "        }\n",
    "\n",
    "\n",
    "# ---- Main Training ----\n",
    "all_training_logs = {}\n",
    "all_models = {}  # (agent_name, seed) -> model path\n",
    "\n",
    "total_runs = len(AGENTS_TO_TRAIN) * len(SEEDS)\n",
    "run_idx = 0\n",
    "overall_start = time.time()\n",
    "\n",
    "for agent_name in AGENTS_TO_TRAIN:\n",
    "    for seed in SEEDS:\n",
    "        run_idx += 1\n",
    "        run_key = f\"{agent_name}_s{seed}\"\n",
    "\n",
    "        # Check if already done (resume support)\n",
    "        final_model_path = f\"{SAVE_ROOT}/models/{agent_name}_s{seed}_final\"\n",
    "        log_path = f\"{SAVE_ROOT}/logs/{run_key}.json\"\n",
    "        if os.path.exists(f\"{final_model_path}.zip\") and os.path.exists(log_path):\n",
    "            print(f\"[{run_idx}/{total_runs}] SKIP {run_key} (already done)\")\n",
    "            with open(log_path) as f:\n",
    "                all_training_logs[run_key] = json.load(f)\n",
    "            all_models[(agent_name, seed)] = final_model_path\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[{run_idx}/{total_runs}] Training: {agent_name} | seed={seed}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Create vectorized training env\n",
    "        train_env = DummyVecEnv([make_push_box_env() for _ in range(N_ENVS)])\n",
    "\n",
    "        # Create agent\n",
    "        model = create_agent(agent_name, train_env, seed=seed)\n",
    "\n",
    "        # Create logger callback\n",
    "        logger = TrainingLogger(\n",
    "            save_dir=SAVE_ROOT,\n",
    "            agent_name=agent_name,\n",
    "            seed=seed,\n",
    "            checkpoint_freq=CHECKPOINT_FREQ,\n",
    "            eval_freq=EVAL_FREQ,\n",
    "            eval_episodes=EVAL_EPISODES,\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        try:\n",
    "            model.learn(\n",
    "                total_timesteps=TOTAL_TIMESTEPS,\n",
    "                callback=logger,\n",
    "                progress_bar=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR during training: {e}\")\n",
    "            train_env.close()\n",
    "            continue\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        # Save final model\n",
    "        model.save(final_model_path)\n",
    "        all_models[(agent_name, seed)] = final_model_path\n",
    "\n",
    "        # Save training log\n",
    "        train_log = logger.get_log()\n",
    "        train_log['training_time_s'] = elapsed\n",
    "        train_log['total_timesteps'] = TOTAL_TIMESTEPS\n",
    "        with open(log_path, 'w') as f:\n",
    "            json.dump(train_log, f)\n",
    "        all_training_logs[run_key] = train_log\n",
    "\n",
    "        # Print summary\n",
    "        recent_sr = np.mean(train_log['episode_successes'][-100:]) if train_log['episode_successes'] else 0\n",
    "        recent_rew = np.mean(train_log['episode_rewards'][-100:]) if train_log['episode_rewards'] else 0\n",
    "        print(f\"  Done in {elapsed:.0f}s | Recent SR: {recent_sr:.1%} | \"\n",
    "              f\"Recent reward: {recent_rew:.1f} | Episodes: {len(train_log['episode_rewards'])}\")\n",
    "\n",
    "        train_env.close()\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "total_elapsed = time.time() - overall_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL TRAINING COMPLETE: {total_runs} runs in {total_elapsed/3600:.1f}h\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Evaluation + OOD Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Evaluation + OOD Testing\n",
    "# ============================================================\n",
    "# In-distribution: 100 episodes, deterministic policy\n",
    "# OOD: 7 mass values x 100 episodes each\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# ---- Config ----\n",
    "EVAL_EPISODES = 100\n",
    "EVAL_SEEDS_START = 10000  # Fixed eval seeds for reproducibility\n",
    "OOD_MASSES = [0.1, 0.25, 0.5, 0.75, 1.0, 2.0, 5.0]\n",
    "OOD_FRICTIONS = [0.1, 0.3, 0.5, 0.7, 1.0]  # future use\n",
    "TRAIN_MASS = 0.5  # in-distribution value\n",
    "\n",
    "\n",
    "def evaluate_model(model, env, n_episodes=100, eval_seed_start=10000):\n",
    "    \"\"\"Evaluate model on environment with fixed seeds.\"\"\"\n",
    "    successes = []\n",
    "    rewards = []\n",
    "    distances = []\n",
    "    first_success_ep = None\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, dones, infos = env.step(action)\n",
    "            ep_reward += reward[0]\n",
    "            done = dones[0]\n",
    "\n",
    "        info = infos[0]\n",
    "        success = info.get('success', False)\n",
    "        successes.append(1 if success else 0)\n",
    "        rewards.append(ep_reward)\n",
    "        distances.append(info.get('distance_to_goal', 999))\n",
    "\n",
    "        if success and first_success_ep is None:\n",
    "            first_success_ep = ep\n",
    "\n",
    "    return {\n",
    "        'success_rate': float(np.mean(successes)),\n",
    "        'success_std': float(np.std(successes)),\n",
    "        'mean_reward': float(np.mean(rewards)),\n",
    "        'std_reward': float(np.std(rewards)),\n",
    "        'mean_distance': float(np.mean(distances)),\n",
    "        'std_distance': float(np.std(distances)),\n",
    "        'first_success_ep': first_success_ep,\n",
    "        'n_episodes': n_episodes,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Run Evaluation ----\n",
    "all_eval_results = {}\n",
    "eval_start = time.time()\n",
    "total_evals = len(AGENTS_TO_TRAIN) * len(SEEDS)\n",
    "eval_idx = 0\n",
    "\n",
    "for agent_name in AGENTS_TO_TRAIN:\n",
    "    for seed in SEEDS:\n",
    "        eval_idx += 1\n",
    "        run_key = f\"{agent_name}_s{seed}\"\n",
    "        model_path = f\"{SAVE_ROOT}/models/{agent_name}_s{seed}_final\"\n",
    "\n",
    "        # Check result cache\n",
    "        eval_cache_path = f\"{SAVE_ROOT}/logs/{run_key}_eval.json\"\n",
    "        if os.path.exists(eval_cache_path):\n",
    "            print(f\"[{eval_idx}/{total_evals}] SKIP {run_key} eval (cached)\")\n",
    "            with open(eval_cache_path) as f:\n",
    "                all_eval_results[run_key] = json.load(f)\n",
    "            continue\n",
    "\n",
    "        print(f\"[{eval_idx}/{total_evals}] Evaluating: {run_key}\", end=\" \")\n",
    "\n",
    "        if not os.path.exists(f\"{model_path}.zip\"):\n",
    "            print(\"-- MODEL NOT FOUND, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Load model\n",
    "        eval_env_id = DummyVecEnv([make_push_box_env(box_mass=TRAIN_MASS)])\n",
    "        model = PPO.load(model_path, env=eval_env_id)\n",
    "\n",
    "        # In-distribution evaluation\n",
    "        id_results = evaluate_model(model, eval_env_id, n_episodes=EVAL_EPISODES)\n",
    "        print(f\"ID SR={id_results['success_rate']:.1%}\", end=\" | \")\n",
    "\n",
    "        # OOD evaluation (mass sweep)\n",
    "        ood_results = {}\n",
    "        for mass in OOD_MASSES:\n",
    "            ood_env = DummyVecEnv([make_push_box_env(box_mass=mass)])\n",
    "            ood_res = evaluate_model(model, ood_env, n_episodes=EVAL_EPISODES)\n",
    "            ood_results[f'mass_{mass}'] = ood_res\n",
    "            ood_env.close()\n",
    "\n",
    "        eval_env_id.close()\n",
    "\n",
    "        # Aggregate OOD\n",
    "        ood_srs = [ood_results[k]['success_rate'] for k in ood_results]\n",
    "        ood_mean = float(np.mean(ood_srs))\n",
    "        ood_std = float(np.std(ood_srs))\n",
    "\n",
    "        result = {\n",
    "            'agent': agent_name,\n",
    "            'seed': seed,\n",
    "            'in_distribution': id_results,\n",
    "            'ood': ood_results,\n",
    "            'ood_mean_sr': ood_mean,\n",
    "            'ood_std_sr': ood_std,\n",
    "        }\n",
    "\n",
    "        # Cache\n",
    "        with open(eval_cache_path, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        all_eval_results[run_key] = result\n",
    "        print(f\"OOD SR={ood_mean:.1%}\")\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "eval_elapsed = time.time() - eval_start\n",
    "print(f\"\\nEvaluation complete in {eval_elapsed/60:.1f} min\")\n",
    "\n",
    "# ---- Print Summary Table ----\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"{'Agent':<18} {'ID SR':>10} {'OOD SR':>10} {'ID Reward':>12} {'ID Dist':>10}\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "for agent_name in AGENTS_TO_TRAIN:\n",
    "    id_srs = []\n",
    "    ood_srs = []\n",
    "    id_rews = []\n",
    "    id_dists = []\n",
    "    for seed in SEEDS:\n",
    "        rk = f\"{agent_name}_s{seed}\"\n",
    "        if rk in all_eval_results:\n",
    "            r = all_eval_results[rk]\n",
    "            id_srs.append(r['in_distribution']['success_rate'])\n",
    "            ood_srs.append(r['ood_mean_sr'])\n",
    "            id_rews.append(r['in_distribution']['mean_reward'])\n",
    "            id_dists.append(r['in_distribution']['mean_distance'])\n",
    "\n",
    "    if id_srs:\n",
    "        print(f\"{agent_name:<18} \"\n",
    "              f\"{np.mean(id_srs):.1%}+/-{np.std(id_srs):.1%}  \"\n",
    "              f\"{np.mean(ood_srs):.1%}+/-{np.std(ood_srs):.1%}  \"\n",
    "              f\"{np.mean(id_rews):>8.1f}+/-{np.std(id_rews):.1f}  \"\n",
    "              f\"{np.mean(id_dists):>6.3f}\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Results + Figure Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Results Visualization + Figure Generation\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.size'] = 11\n",
    "matplotlib.rcParams['axes.labelsize'] = 12\n",
    "matplotlib.rcParams['axes.titlesize'] = 13\n",
    "matplotlib.rcParams['legend.fontsize'] = 10\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "COLORS = {\n",
    "    'pure_ppo':      '#1f77b4',\n",
    "    'gns':           '#ff7f0e',\n",
    "    'physrobot_sv':  '#2ca02c',\n",
    "    'no_edgeframe':  '#d62728',\n",
    "    'hnn':           '#9467bd',\n",
    "}\n",
    "LABELS = {\n",
    "    'pure_ppo':      'Pure PPO',\n",
    "    'gns':           'GNS',\n",
    "    'physrobot_sv':  'PhysRobot-SV (ours)',\n",
    "    'no_edgeframe':  'No-EdgeFrame',\n",
    "    'hnn':           'HNN',\n",
    "}\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Figure 1: Bar chart -- Success rate comparison\n",
    "# ========================================\n",
    "def plot_success_bars():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # In-distribution\n",
    "    ax = axes[0]\n",
    "    names = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    colors = []\n",
    "    for agent_name in AGENTS_TO_TRAIN:\n",
    "        srs = [all_eval_results.get(f\"{agent_name}_s{s}\", {}).get(\n",
    "            'in_distribution', {}).get('success_rate', 0)\n",
    "               for s in SEEDS]\n",
    "        srs = [x for x in srs if x is not None]\n",
    "        if srs:\n",
    "            names.append(LABELS.get(agent_name, agent_name))\n",
    "            means.append(np.mean(srs))\n",
    "            stds.append(np.std(srs))\n",
    "            colors.append(COLORS.get(agent_name, '#888888'))\n",
    "\n",
    "    x = np.arange(len(names))\n",
    "    bars = ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.85, edgecolor='black')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(names, rotation=25, ha='right')\n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_title('In-Distribution (mass=0.5kg)')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50%')\n",
    "    ax.legend()\n",
    "\n",
    "    # OOD\n",
    "    ax = axes[1]\n",
    "    names2 = []\n",
    "    means2 = []\n",
    "    stds2 = []\n",
    "    colors2 = []\n",
    "    for agent_name in AGENTS_TO_TRAIN:\n",
    "        srs = [all_eval_results.get(f\"{agent_name}_s{s}\", {}).get(\n",
    "            'ood_mean_sr', 0) for s in SEEDS]\n",
    "        srs = [x for x in srs if x is not None]\n",
    "        if srs:\n",
    "            names2.append(LABELS.get(agent_name, agent_name))\n",
    "            means2.append(np.mean(srs))\n",
    "            stds2.append(np.std(srs))\n",
    "            colors2.append(COLORS.get(agent_name, '#888888'))\n",
    "\n",
    "    x = np.arange(len(names2))\n",
    "    ax.bar(x, means2, yerr=stds2, capsize=5, color=colors2, alpha=0.85, edgecolor='black')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(names2, rotation=25, ha='right')\n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_title('OOD (mass sweep: 0.1-5.0 kg)')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/success_rates.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/success_rates.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: success_rates.png / .pdf\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Figure 2: OOD generalization curves\n",
    "# ========================================\n",
    "def plot_ood_curves():\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    for agent_name in AGENTS_TO_TRAIN:\n",
    "        mass_srs = {m: [] for m in OOD_MASSES}\n",
    "        for seed in SEEDS:\n",
    "            rk = f\"{agent_name}_s{seed}\"\n",
    "            if rk in all_eval_results and 'ood' in all_eval_results[rk]:\n",
    "                ood = all_eval_results[rk]['ood']\n",
    "                for m in OOD_MASSES:\n",
    "                    key = f'mass_{m}'\n",
    "                    if key in ood:\n",
    "                        mass_srs[m].append(ood[key]['success_rate'])\n",
    "\n",
    "        if not any(mass_srs.values()):\n",
    "            continue\n",
    "\n",
    "        masses_plot = []\n",
    "        means_plot = []\n",
    "        lo_plot = []\n",
    "        hi_plot = []\n",
    "        for m in OOD_MASSES:\n",
    "            if mass_srs[m]:\n",
    "                masses_plot.append(m)\n",
    "                mu = np.mean(mass_srs[m])\n",
    "                se = np.std(mass_srs[m]) / max(np.sqrt(len(mass_srs[m])), 1)\n",
    "                means_plot.append(mu)\n",
    "                lo_plot.append(mu - 1.96 * se)\n",
    "                hi_plot.append(mu + 1.96 * se)\n",
    "\n",
    "        c = COLORS.get(agent_name, '#888888')\n",
    "        ax.plot(masses_plot, means_plot, 'o-', color=c,\n",
    "                label=LABELS.get(agent_name, agent_name), linewidth=2)\n",
    "        ax.fill_between(masses_plot, lo_plot, hi_plot, color=c, alpha=0.15)\n",
    "\n",
    "    ax.axvline(x=TRAIN_MASS, color='gray', linestyle='--', alpha=0.6, label=f'Train mass ({TRAIN_MASS} kg)')\n",
    "    ax.set_xlabel('Box Mass (kg)')\n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_title('OOD Generalization: Success Rate vs Box Mass')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/ood_generalization.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/ood_generalization.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: ood_generalization.png / .pdf\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Figure 3: Learning curves\n",
    "# ========================================\n",
    "def plot_learning_curves():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    window = 50  # smoothing window\n",
    "\n",
    "    for agent_name in AGENTS_TO_TRAIN:\n",
    "        all_rewards = []\n",
    "        all_success = []\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            rk = f\"{agent_name}_s{seed}\"\n",
    "            if rk in all_training_logs:\n",
    "                log = all_training_logs[rk]\n",
    "                rewards = log.get('episode_rewards', [])\n",
    "                successes = log.get('episode_successes', [])\n",
    "                if rewards:\n",
    "                    # Smooth\n",
    "                    smoothed_r = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "                    smoothed_s = np.convolve(successes, np.ones(window)/window, mode='valid')\n",
    "                    all_rewards.append(smoothed_r)\n",
    "                    all_success.append(smoothed_s)\n",
    "\n",
    "        if not all_rewards:\n",
    "            continue\n",
    "\n",
    "        # Align lengths (truncate to shortest)\n",
    "        min_len = min(len(r) for r in all_rewards)\n",
    "        rewards_arr = np.array([r[:min_len] for r in all_rewards])\n",
    "        success_arr = np.array([s[:min_len] for s in all_success])\n",
    "\n",
    "        x = np.arange(min_len)\n",
    "        c = COLORS.get(agent_name, '#888888')\n",
    "        lbl = LABELS.get(agent_name, agent_name)\n",
    "\n",
    "        # Reward curve\n",
    "        mu_r = rewards_arr.mean(axis=0)\n",
    "        std_r = rewards_arr.std(axis=0)\n",
    "        axes[0].plot(x, mu_r, color=c, label=lbl, linewidth=1.5)\n",
    "        axes[0].fill_between(x, mu_r - std_r, mu_r + std_r, color=c, alpha=0.1)\n",
    "\n",
    "        # Success curve\n",
    "        mu_s = success_arr.mean(axis=0)\n",
    "        std_s = success_arr.std(axis=0)\n",
    "        axes[1].plot(x, mu_s, color=c, label=lbl, linewidth=1.5)\n",
    "        axes[1].fill_between(x, mu_s - std_s, mu_s + std_s, color=c, alpha=0.1)\n",
    "\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Episode Reward (smoothed)')\n",
    "    axes[0].set_title('Learning Curves: Reward')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Success Rate (rolling)')\n",
    "    axes[1].set_title('Learning Curves: Success Rate')\n",
    "    axes[1].set_ylim(-0.05, 1.05)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/learning_curves.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/learning_curves.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: learning_curves.png / .pdf\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Figure 4: Sample efficiency (box plot)\n",
    "# ========================================\n",
    "def plot_sample_efficiency():\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    colors_list = []\n",
    "\n",
    "    for agent_name in AGENTS_TO_TRAIN:\n",
    "        first_successes = []\n",
    "        for seed in SEEDS:\n",
    "            rk = f\"{agent_name}_s{seed}\"\n",
    "            if rk in all_training_logs:\n",
    "                log = all_training_logs[rk]\n",
    "                successes = log.get('episode_successes', [])\n",
    "                # Find first success episode\n",
    "                for ep_idx, s in enumerate(successes):\n",
    "                    if s == 1:\n",
    "                        first_successes.append(ep_idx)\n",
    "                        break\n",
    "                else:\n",
    "                    first_successes.append(len(successes))  # never succeeded\n",
    "\n",
    "        if first_successes:\n",
    "            data.append(first_successes)\n",
    "            labels.append(LABELS.get(agent_name, agent_name))\n",
    "            colors_list.append(COLORS.get(agent_name, '#888888'))\n",
    "\n",
    "    if data:\n",
    "        bp = ax.boxplot(data, labels=labels, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors_list):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.6)\n",
    "\n",
    "    ax.set_ylabel('Episodes to First Success')\n",
    "    ax.set_title('Sample Efficiency')\n",
    "    ax.tick_params(axis='x', rotation=25)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/sample_efficiency.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.savefig(f\"{SAVE_ROOT}/figures/sample_efficiency.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: sample_efficiency.png / .pdf\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Table: LaTeX ablation table\n",
    "# ========================================\n",
    "def generate_latex_table():\n",
    "    lines = [\n",
    "        r'\\begin{table}[t]',\n",
    "        r'\\centering',\n",
    "        r'\\caption{Phase 1 Ablation Results (PushBox, 500K steps, 5 seeds)}',\n",
    "        r'\\label{tab:phase1}',\n",
    "        r'\\begin{tabular}{l c c c c}',\n",
    "        r'\\toprule',\n",
    "        r'Method & SR (ID) & SR (OOD) & Reward & Params \\\\',\n",
    "        r'\\midrule',\n",
    "    ]\n",
    "\n",
    "    for agent_name in AGENTS_TO_TRAIN:\n",
    "        id_srs = []\n",
    "        ood_srs = []\n",
    "        rews = []\n",
    "        for seed in SEEDS:\n",
    "            rk = f\"{agent_name}_s{seed}\"\n",
    "            if rk in all_eval_results:\n",
    "                r = all_eval_results[rk]\n",
    "                id_srs.append(r['in_distribution']['success_rate'] * 100)\n",
    "                ood_srs.append(r['ood_mean_sr'] * 100)\n",
    "                rews.append(r['in_distribution']['mean_reward'])\n",
    "\n",
    "        if not id_srs:\n",
    "            continue\n",
    "\n",
    "        # Get param count (create dummy model)\n",
    "        _env = DummyVecEnv([make_push_box_env()])\n",
    "        _m = create_agent(agent_name, _env, seed=0)\n",
    "        n_params = sum(p.numel() for p in _m.policy.parameters())\n",
    "        _env.close()\n",
    "        del _m\n",
    "\n",
    "        display = LABELS.get(agent_name, agent_name)\n",
    "        if agent_name == 'physrobot_sv':\n",
    "            display = r'\\textbf{' + display + r'}'\n",
    "\n",
    "        lines.append(\n",
    "            f\"{display} & \"\n",
    "            f\"${np.mean(id_srs):.1f} \\\\pm {np.std(id_srs):.1f}$ & \"\n",
    "            f\"${np.mean(ood_srs):.1f} \\\\pm {np.std(ood_srs):.1f}$ & \"\n",
    "            f\"${np.mean(rews):.1f}$ & \"\n",
    "            f\"{n_params // 1000}K \\\\\\\\\"\n",
    "        )\n",
    "\n",
    "    lines.extend([\n",
    "        r'\\bottomrule',\n",
    "        r'\\end{tabular}',\n",
    "        r'\\end{table}',\n",
    "    ])\n",
    "\n",
    "    latex_str = '\\n'.join(lines)\n",
    "    with open(f\"{SAVE_ROOT}/figures/ablation_table.tex\", 'w') as f:\n",
    "        f.write(latex_str)\n",
    "\n",
    "    print(\"LaTeX table:\")\n",
    "    print(latex_str)\n",
    "    print(f\"\\nSaved: ablation_table.tex\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Statistical tests\n",
    "# ========================================\n",
    "def run_statistical_tests():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Statistical Tests (Welch's t-test, p < 0.05)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    our_method = 'physrobot_sv'\n",
    "    our_id = [all_eval_results.get(f\"{our_method}_s{s}\", {}).get(\n",
    "        'in_distribution', {}).get('success_rate', 0) for s in SEEDS]\n",
    "    our_ood = [all_eval_results.get(f\"{our_method}_s{s}\", {}).get(\n",
    "        'ood_mean_sr', 0) for s in SEEDS]\n",
    "\n",
    "    for agent_name in AGENTS_TO_TRAIN:\n",
    "        if agent_name == our_method:\n",
    "            continue\n",
    "        other_id = [all_eval_results.get(f\"{agent_name}_s{s}\", {}).get(\n",
    "            'in_distribution', {}).get('success_rate', 0) for s in SEEDS]\n",
    "        other_ood = [all_eval_results.get(f\"{agent_name}_s{s}\", {}).get(\n",
    "            'ood_mean_sr', 0) for s in SEEDS]\n",
    "\n",
    "        # In-distribution\n",
    "        t_id, p_id = stats.ttest_ind(our_id, other_id, equal_var=False)\n",
    "        sig_id = '*' if p_id < 0.05 else ''\n",
    "\n",
    "        # OOD\n",
    "        t_ood, p_ood = stats.ttest_ind(our_ood, other_ood, equal_var=False)\n",
    "        sig_ood = '*' if p_ood < 0.05 else ''\n",
    "\n",
    "        print(f\"PhysRobot-SV vs {LABELS.get(agent_name, agent_name):20s}  \"\n",
    "              f\"ID: t={t_id:+.2f} p={p_id:.3f}{sig_id}  \"\n",
    "              f\"OOD: t={t_ood:+.2f} p={p_ood:.3f}{sig_ood}\")\n",
    "\n",
    "\n",
    "# ---- Generate all figures ----\n",
    "print(\"Generating figures...\\n\")\n",
    "plot_success_bars()\n",
    "plot_ood_curves()\n",
    "plot_learning_curves()\n",
    "plot_sample_efficiency()\n",
    "generate_latex_table()\n",
    "run_statistical_tests()\n",
    "\n",
    "print(f\"\\nAll figures saved to: {SAVE_ROOT}/figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save Everything to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Save Summary + Backup to Drive\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- Compile master summary ----\n",
    "summary = {\n",
    "    'experiment': 'PhysRobot Phase 1 Ablation',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'seeds': SEEDS,\n",
    "        'total_timesteps': TOTAL_TIMESTEPS,\n",
    "        'n_envs': N_ENVS,\n",
    "        'eval_episodes': EVAL_EPISODES,\n",
    "        'ood_masses': OOD_MASSES,\n",
    "        'agents': AGENTS_TO_TRAIN,\n",
    "        'success_threshold': 0.15,\n",
    "        'train_box_mass': TRAIN_MASS,\n",
    "    },\n",
    "    'results': {},\n",
    "}\n",
    "\n",
    "for agent_name in AGENTS_TO_TRAIN:\n",
    "    agent_results = {\n",
    "        'seeds': {},\n",
    "        'aggregate': {},\n",
    "    }\n",
    "    id_srs = []\n",
    "    ood_srs = []\n",
    "    train_times = []\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        rk = f\"{agent_name}_s{seed}\"\n",
    "\n",
    "        seed_data = {}\n",
    "        if rk in all_eval_results:\n",
    "            seed_data['eval'] = all_eval_results[rk]\n",
    "            id_srs.append(all_eval_results[rk]['in_distribution']['success_rate'])\n",
    "            ood_srs.append(all_eval_results[rk]['ood_mean_sr'])\n",
    "        if rk in all_training_logs:\n",
    "            seed_data['train_time_s'] = all_training_logs[rk].get('training_time_s', 0)\n",
    "            train_times.append(seed_data['train_time_s'])\n",
    "\n",
    "        agent_results['seeds'][str(seed)] = seed_data\n",
    "\n",
    "    if id_srs:\n",
    "        agent_results['aggregate'] = {\n",
    "            'id_sr_mean': float(np.mean(id_srs)),\n",
    "            'id_sr_std': float(np.std(id_srs)),\n",
    "            'ood_sr_mean': float(np.mean(ood_srs)),\n",
    "            'ood_sr_std': float(np.std(ood_srs)),\n",
    "            'mean_train_time_s': float(np.mean(train_times)) if train_times else 0,\n",
    "        }\n",
    "\n",
    "    summary['results'][agent_name] = agent_results\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{SAVE_ROOT}/summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Summary saved: {summary_path}\")\n",
    "\n",
    "# ---- Print final results ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1 ABLATION -- FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Agent':<20} {'ID SR':>12} {'OOD SR':>12} {'Train Time':>12}\")\n",
    "print(\"-\"*70)\n",
    "for agent_name in AGENTS_TO_TRAIN:\n",
    "    agg = summary['results'].get(agent_name, {}).get('aggregate', {})\n",
    "    if agg:\n",
    "        id_str = f\"{agg['id_sr_mean']:.1%} +/- {agg['id_sr_std']:.1%}\"\n",
    "        ood_str = f\"{agg['ood_sr_mean']:.1%} +/- {agg['ood_sr_std']:.1%}\"\n",
    "        tt = agg.get('mean_train_time_s', 0)\n",
    "        tt_str = f\"{tt/60:.1f} min\"\n",
    "        print(f\"{agent_name:<20} {id_str:>12} {ood_str:>12} {tt_str:>12}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ---- Backup to Drive (dated) ----\n",
    "if IN_COLAB:\n",
    "    backup_dir = f\"/content/drive/MyDrive/PhysRobot/backups/{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    # Copy key files\n",
    "    for fname in ['summary.json']:\n",
    "        src = f\"{SAVE_ROOT}/{fname}\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, f\"{backup_dir}/{fname}\")\n",
    "    # Copy figures\n",
    "    fig_dir = f\"{SAVE_ROOT}/figures\"\n",
    "    if os.path.isdir(fig_dir):\n",
    "        shutil.copytree(fig_dir, f\"{backup_dir}/figures\", dirs_exist_ok=True)\n",
    "    print(f\"\\nBackup saved to: {backup_dir}\")\n",
    "\n",
    "# ---- List all output files ----\n",
    "print(f\"\\nAll output files in {SAVE_ROOT}:\")\n",
    "for root, dirs, files in os.walk(SAVE_ROOT):\n",
    "    for f in sorted(files):\n",
    "        full = os.path.join(root, f)\n",
    "        size = os.path.getsize(full)\n",
    "        rel = os.path.relpath(full, SAVE_ROOT)\n",
    "        print(f\"  {rel:<60s} {size/1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DONE. Ready for Phase 2 (SAC, TD3, multi-object).\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
